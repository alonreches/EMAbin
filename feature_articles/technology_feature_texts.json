{"technology forecasting": "Technology forecasting attempts to predict the future characteristics of useful technological machines, procedures or techniques.\n\n\n== Important aspects ==\n\nPrimarily, a technological forecast deals with the characteristics of technology, such as levels of technical performance, like speed of a military aircraft, the power in watts of a particular future engine, the accuracy or precision of a measuring instrument, the number of transistors in a chip in the year 2015, etc. The forecast does not have to state how these characteristics will be achieved.\nSecondly, technological forecasting usually deals with only useful machines, procedures or techniques. This is to exclude from the domain of technological forecasting those commodities, services or techniques intended for luxury or amusement.\n\n\n== Rational and explicit methods ==\nThe whole purpose of the recitation of alternatives is to show that there really is no alternative to forecasting. If a decision maker has several alternatives open to him or her, s/he will choose among them on the basis of which provides him/her with the most desirable outcome. Thus his/her decision is inevitably based on a forecast. His/her only choice is whether the forecast is obtained by rational and explicit methods, or by intuitive means.\nThe virtues of the use of rational methods are as follows:\n\nThey can be taught and learned,\nThey can be described and explained,\nThey provide a procedure followable by anyone who has absorbed the necessary training, and in some cases,\nThese methods are even guaranteed to produce the same forecast regardless of who uses them.The virtue of the use of explicit methods is that they can be reviewed by others, and can be checked for consistency. Furthermore, the forecast can be reviewed at any subsequent time.\nTechnology forecasting is not imagination.\n\n\n== Methods of technology forecasting ==\nCommonly adopted methods of technology forecasting include the Delphi method, forecast by analogy, growth curves and extrapolation. Normative methods of technology forecasting \u2014 like the relevance trees, morphological models, and mission flow diagrams \u2014 are also commonly used.\n\n\n== Combining forecasts ==\nStudies of past forecasts have shown that one of the most frequent reasons why a forecast goes wrong is that the forecaster ignores related fields.\nA given technical approach may fail to achieve the level of capability forecast for it, because it is superseded by another technical approach which the forecaster ignored.\nAnother problem is that of inconsistency between forecasts. Because of these problems, it is often necessary to combine forecasts of different technologies. Therefore rather than to try to select the one method which is most appropriate, it may be better to try to combine the forecasts obtained by different methods.\nIf this is done, the strengths of one method may help compensate for the weaknesses of another.\n\n\n=== Reasons for combining forecasts ===\nThe primary reason for combining forecasts of the same technology is to attempt to offset the weaknesses of one forecasting method with the strengths of another.\nIn addition, the use of more than one forecasting method often gives the forecaster more insight into the processes at work which are responsible for the growth of the technology being forecast.\n\n\n=== Trend curve and growth curves ===\nA frequently used combination is that of growth curves and a trend curve for some technology.\nHere we see a succession of growth curves, each describing the level of functional capability achieved by a specific technical approach.\nAn overall trend curve is also shown, fitted to those items of historical data which represent the currently superior approach.\nThe use of growth curves and a trend curve in combination allows the forecaster to draw some conclusions about the future growth of a technology which might not be possible, were either method used alone.\nWith growth curves alone, the forecaster could not say anything about the time at which a given technical approach is likely to be supplanted by a successor approach.\nWith the trend curve alone, the forecaster could not say anything about the ability of a specific technical approach to meet the projected trend, or about the need to look for a successor approach.\nThus the need for combining forecasts.\n\n\n=== Identification of consistent deviations ===\nAnother frequently used combination of forecasts is that of the trend curve and one or more analogies.\nWe customarily consider the scatter of data points about a trend curve to be due to random influences which we can neither control nor even measure. However, consistent deviations may represent something other than just random influences.\nWhere such consistent deviations are identified, we may have an opportunity to apply an analogy. Typical events which bring about deviations from a trend are wars and depressions. Thus the purpose of combining analogies with a trend forecast is to predict deviations from the trend deviations which are associated with or caused by external events or influences.\nAs with other uses of analogy, it is important to determine the extent to which the analogy between the event used as the basis for the forecast, and the historical model event, satisfies the criteria for a valid analogy.\n\n\n=== Forecasts of different technologies ===\nCombining forecasts of different technologies may be even more important than combining the forecasts of the same technology.\nOne reason for this is the fact that technologies may interact or be interrelated in some fashion.\nAnother reason for this is that of consistency in an overall picture or scenario. One of the simplest examples of interacting trends is the projection to absurdity, i.e. simply projecting the given data indefinitely without getting any specific result. For instance, if one simply projects recent rates of growth of world population, one arrives at some fantastic conclusions about the density of population in a particular place by various dates in the next millennium.\nSome other trends which can confidently be expected to not continue indefinitely are:\n\nAnnual production of scientific papers.\nNumber of automobiles per capita.\nKilowatt hours of electricity generated annually.Another instance of interacting trends was in the case of the number of scientists in the U.S. growing faster than the overall population. Since the 1940s through the 1960s, science as an activity in the United States grew exponentially. The number of dollars spent on R&D was growing faster than the GNP (in the 1960s).\nIf projected indefinitely, these two curves would give the result that eventually every person in the U.S. would be working as a scientist and the entire GNP would be devoted to R&D alone, which are however absurd conclusions. Thus it is clear that the scientific discipline of technology forecasting is not mere trend extrapolation but also involves combining forecasts.\n\n\n== Uses in manufacturing ==\nAlmost all modern manufacturing firms utilize the services of a technological forecaster. Nevertheless, there are a number of alternatives to the rational and explicit forecasting of technology, such as 'no forecast', 'anything can happen' (i.e. relying on pure chance), 'window-blind forecasting', 'genius forecasting' and boasting of a 'glorious past' (i.e. adopting the same old techniques).\nThus technological forecasting is not mere astrology or palmistry, but a scientific and well defined procedure adopted by a technological forecaster or a consultancy for the forecasting of a particular technology. Even though technological forecasting is a scientific discipline, some experts are of the view that \"the only certainty of a particular forecast is that it is wrong to some degree.\"\n\n\n== Forecasting institutes ==\nTechCast Project\nSingularity Institute for Artificial Intelligence\nFuture of Humanity Institute\nThe Millennium Project\nInstitute for the Future\n\n\n== See also ==\nAccelerating change\nDelphi method\nForecasting\nFuturology\nList of emerging technologies\nOptimism bias\nReference class forecasting\nStrategic foresight\nTechnology scouting\nTechnology roadmap\n\n\n== Scientific Journals ==\nTechnological Forecasting and Social Change\nFutures\nFutures & Foresight Science\nForesight\nJournal of Futures Studies\n\n\n== References ==\n\nKlopfenstein, Bruce K. \"Forecasting consumer adoption of information technology and services  -  Lessons from home video forecasting\". Journal of the American Society for Information Science 1989 Jan;40(1):17-26.\nMartino, Joseph (January 1983). Technological Forecasting for Decision Making (2nd ed.). North-Holland. ISBN 0-444-00722-9. \nMakridakis, Spyros; Steven C. Wheelwright; Rob J. Hyndman (December 1998). Forecasting: Methods and Applications (3rd ed.). John Wiley. ISBN 0-471-53233-9. \nTwiss, Brian C. (July 1, 1992). Forecasting for Technologists and Engineers: A Practical Guide for Better Decisions. Institution of Electrical Engineers. ISBN 0-86341-265-3. \n\n\n== External links ==\nTechCast Article Series, William Halal Next Next Things\nTSTC Forecasting The emerging technology & forecasting office at Texas State Technical College\n[1] Unido Technology Foresight Manual.", "industry": "Industry is the production of goods or related services within an economy. The major source of revenue of a group or company is the indicator of its relevant industry.  When a large group has multiple sources of revenue generation, it is considered to be working in different industries. Manufacturing industry became a key sector of production and labour in European and North American countries during the Industrial Revolution, upsetting previous mercantile and feudal economies. This came through many successive rapid advances in technology, such as the production of steel and coal.\nFollowing the Industrial Revolution, possibly a third of the world's economic output are derived that is from manufacturing industries. Many developed countries and many developing/semi-developed countries (China, India etc.) depend significantly on manufacturing industry. Industries, the countries they reside in, and the economies of those countries are interlinked in a complex web of interdependence.\n\n\n== History ==\nIndustry is older than modern humans. Over time, the methods used by industrialists have varied considerably. \n\n\n=== Slavery ===\n\nSlavery, the practice of utilizing forced labor to produce goods and services, has occurred since antiquity throughout the world as a means of low-cost production. It typically produces goods for which profit depends on economies of scale, especially those for which labor was simple and easy to supervise.  International law has declared slavery illegal.\n\n\n=== Guilds ===\n\nGuilds, associations of artisans and merchants, oversee the production and distribution of a particular good. Guilds have their roots in the Roman Empire as collegia (singular: collegium) Membership in these early guilds was voluntary. The Roman collegia did not survive the fall of Rome. In the  early middle ages, guilds once again began to emerge in Europe, reaching a degree of maturity by the beginning of the 14th century. While few guilds remaining today, some modern labor structures resemble those of traditional guilds. Other guilds, such as the SAG-AFTRA act as trade unions rather than as classical guilds. Professor Sheilagh Ogilvie claims that guilds negatively affected quality, skills, and innovation in areas that they were present.\n\n\n=== Industrial Revolution ===\n\nThe industrial revolution (from the mid-18th century to the mid-19th century) saw the development and popularization of mechanized means of production as a replacement for hand production. The industrial revolution played a role in the abolition of slavery in Europe and in North America.\n\n\n=== Since the Industrial Revolution ===\n\n\n== Industrial development ==\n\nThe Industrial Revolution led to the development of factories for large-scale production with consequent changes in society. Originally the factories were steam-powered, but later transitioned to electricity once an electrical grid was developed. The mechanized assembly line was introduced to assemble parts in a repeatable fashion, with individual workers performing specific steps during the process. This led to significant increases in efficiency, lowering the cost of the end process. Later automation was increasingly used to replace human operators. This process has accelerated with the development of the computer and the robot.\n\n\n== Deindustrialisation ==\n\nHistorically certain manufacturing industries have gone into a decline due to various economic factors, including the development of replacement technology or the loss of competitive advantage. An example of the former is the decline in carriage manufacturing when the automobile was mass-produced.\nA recent trend has been the migration of prosperous, industrialized nations towards a post-industrial society. This is manifested by an increase in the service sector at the expense of manufacturing, and the development of an information-based economy, the so-called informational revolution. In a post-industrial society, manufacturers relocate to more profitable locations through a process of off-shoring.\nMeasurements of manufacturing industries outputs and economic effect are not historically stable. Traditionally, success has been measured in the number of jobs created. The reduced number of employees in the manufacturing sector has been assumed to result from a decline in the competitiveness of the sector, or the introduction of the lean manufacturing process.\nRelated to this change is the upgrading of the quality of the product being manufactured. While it is possible to produce a low-technology product with low-skill labour, the ability to manufacture high-technology products well is dependent on a highly skilled staff.\n\n\n== Society ==\n\nAn industrial society is a society driven by the use of technology to enable mass production, supporting a large population with a high capacity for division of labour. Today, industry is an important part of most societies and nations. A government must have some kind of industrial policy, regulating industrial placement, industrial pollution, financing and industrial labour.\n\n\n== Industrial labour ==\n\nIn an industrial society, industry employs a major part of the population. This occurs typically in the manufacturing sector. A labour union is an organization of workers who have banded together to achieve common goals in key areas such as wages, hours, and other working conditions. The trade union, through its leadership, bargains with the employer on behalf of union members (rank and file members) and negotiates labour contracts with employers. This movement first rose among industrial workers.\n\n\n== War ==\n\nThe Industrial Revolution changed warfare, with mass-produced weaponry and supplies, machine-powered transportation, mobilization, the total war concept and weapons of mass destruction. Early instances of industrial warfare were the Crimean War and the American Civil War, but its full potential showed during the world wars. See also military-industrial complex, arms industries, military industry and modern warfare.\n\n\n== List of countries by industrial output ==\n\n\n== See also ==\nIndustry information\nNorth American Industry Classification System\nNorth American Product Classification System\nOutline of industry\nStandard Industrial Classification\n\n\n== References ==\n\n\n== Bibliography ==\nKrahn, Harvey J., and Graham S. Lowe. Work, Industry, and Canadian Society. Second ed. Scarborough, Ont.: Nelson Canada, 1993. xii, 430 p. ISBN 0-17-603540-0\n\n\n== External links ==\n Media related to Industries at Wikimedia Commons\n Quotations related to industry at Wikiquote", "technological evolution": "Technological evolution is a theory that describes the radical transformation of society through technological development. This theory originated with Czech philosopher Radovan Richta.\n\n\n== Theory of technological evolution ==\nMankind In Transition; A View of the Distant Past, the Present and the Far Future, Masefield Books, 1993. Technology (which Richta defines as \"a material entity created by the application of mental and physical effort to nature in order to achieve some value\") evolves in three stages: tools, machine, automation. This evolution, he says, follows two trends:\n\n\n=== Development ===\nThe pretechnological period, in which all other animal species remain today aside from some avian and primate species was a non-rational period of the early prehistoric man.\nThe emergence of technology, made possible by the development of the rational faculty, paved the way for the first stage: the tool. A tool provides a mechanical advantage in accomplishing a physical task, such as an arrow, plow, or hammer that augments physical labor to more efficiently achieve his objective. Later animal-powered tools such as the plow and the horse, increased the productivity of food production about tenfold over the technology of the hunter-gatherers. Tools allow one to do things impossible to accomplish with one's body alone, such as seeing minute visual detail with a microscope, manipulating heavy objects with a pulley and cart, or carrying volumes of water in a bucket.\nThe second technological stage was the creation of the machine. A machine (a powered machine to be more precise) is a tool that substitutes the element of human physical effort, and requires  only to control its function. Machines became widespread with the industrial revolution, though windmills, a type of machine, are much older.\nExamples of this include cars, trains, computers, and lights. Machines allow humans to\nTremendously exceed the limitations of their bodies. Putting a machine on the farm, a tractor, increased food productivity at least tenfold over the technology of the plow and the horse.\nThe third, and final stage of technological evolution is the automation. The automation is a machine that removes the element of human control with an automatic algorithm. Examples of machines that exhibit this characteristic are digital watches, automatic telephone switches, pacemakers, and computer programs.\nIt's crucial to understand that the three stages outline the introduction of the fundamental types of technology, and so all three continue to be widely used today. A spear, a plow, a pen, a knife, a glove, a chicken and an optical microscope are all examples of tools.\n\n\n=== Theoretical implications ===\nThe process of technological evolution culminates with the ability to achieve all the material values technologically possible and desirable by mental effort.\nAn economic implication of the above idea is that intellectual labour will become increasingly more important relative to physical labour. Contracts and agreements around information will become increasingly more common at the marketplace. Expansion and creation of new kinds of institutes that works with information such as    universities, book stores, patent-trading companies, etc. is considered an indication that a civilization is in technological evolution.\nThis highlights the importance underlining the debate over intellectual property in conjunction with decentralized distribution systems such as today's internet. Where the price of information distribution is going towards zero with ever more efficient tools to distribute information is being invented. Growing amounts of information being distributed to an increasingly larger customer base as times goes by. With growing disintermediation in said markets and growing concerns over the protection of intellectual property rights it is not clear what form markets for information will take with the evolution of the information age.\n\n\n== See also ==\nAccelerating change\nDifferential technological development\nHistory of technology\nKardashev scale\nMankind in Transition\nSocial progress\nSociocultural evolution\nTechnological revolution\nTechnological singularity\nThe Automated Society (book)\nTheories of technology\n\n\n== References ==", "management": "Management (or managing) is the administration of an organization, whether it is a business, a not-for-profit organization, or government body. Management includes the activities of setting the strategy of an organization and coordinating the efforts of its employees (or of volunteers) to accomplish its objectives through the application of available resources, such as financial, natural, technological, and human resources. The term \"management\" may also refer to those people who manage an organization.\n Social scientists study management as an academic discipline, investigating areas such as social organization and organizational leadership. Some people study management at colleges or universities; major degrees in management include the Bachelor of Commerce (B.Com.) and Master of Business Administration (MBA.) and, for the public sector, the Master of Public Administration (MPA) degree. Individuals who aim to become management specialists or experts, management researchers, or professors may complete the Doctor of Management (DM), the Doctor of Business Administration (DBA), or the PhD in Business Administration or Management.\nLarger organizations generally have three levels of managers, which are typically organized in a  hierarchical, pyramid structure:\n\n Senior managers, such as members of a Board of Directors and a Chief Executive Officer (CEO) or a President of an organization. They set the strategic goals of the organization and make decisions on how the overall organization will operate. Senior managers are generally executive-level professionals, and provide direction to middle management who directly or indirectly report to them.\n Middle managers, examples of which would include branch managers, regional managers, department managers and section managers, provide direction to front-line managers. Middle managers communicate the strategic goals of senior management to the front-line managers.\n Lower managers, such as supervisors and front-line team leaders, oversee the work of regular employees (or volunteers, in some voluntary organizations) and provide direction on their work.In smaller organizations, an individual manager may have a much wider scope. A single manager may perform several roles or even all of the roles commonly observed in a large organization.\n\n\n== Definitions ==\nViews on the definition and scope of management include:\n\nAccording to Henri Fayol, \"to manage is to forecast and to plan, to organise, to command, to co-ordinate and to control.\"\nFredmund Malik defines it as \"the transformation of resources into utility.\"\nManagement included as one of the factors of production \u2013 along with machines, materials and money.\nGhislain Deslandes defines it as \u201ca vulnerable force, under pressure to achieve results and endowed with the triple power of constraint, imitation and imagination, operating on subjective, interpersonal, institutional and environmental levels\u201d.\nPeter Drucker (1909\u20132005) saw the basic task of management as twofold: marketing and innovation. Nevertheless, innovation is also linked to marketing (product innovation is a central strategic marketing issue). Peter Drucker identifies marketing as a key essence for business success, but management and marketing are generally understood as two different branches of business administration knowledge.\n\n\n=== Theoretical scope ===\nManagement involves identifying the mission, objective, procedures, rules and manipulation\nof the human capital of an enterprise to contribute to the success of the enterprise. This implies effective communication: an enterprise environment (as opposed to a physical or mechanical mechanism) implies human motivation and implies some sort of successful progress or system outcome. As such, management is not the manipulation of a mechanism (machine or automated program), not the herding of animals, and can occur either in a legal or in an illegal enterprise or environment. From an individual's perspective, management does not need to be seen solely from an enterprise point of view, because management is an essential function to improve one's life and relationships.  Management is therefore everywhere and it has a wider range of application. Based on this, management must have humans. Communication and a positive endeavor are two main aspects of it either through enterprise or independent pursuit. Plans, measurements, motivational psychological tools, goals, and economic measures (profit, etc.) may or may not be necessary components for there to be management. At first, one views management functionally, such as measuring quantity, adjusting plans, meeting goals. This applies even in situations where planning does not take place. From this perspective, Henri Fayol (1841\u20131925)\nconsiders management to consist of six functions:\n\nforecasting\nplanning\norganizing\ncommanding\ncoordinating\ncontrollingIn another way of thinking, Mary Parker Follett (1868\u20131933), allegedly defined management as \"the art of getting things done through people\".\nShe described management as philosophy.Critics, however, find this definition useful but far too narrow. The phrase \"management is what managers do\" occurs widely,\nsuggesting the difficulty of defining management without circularity, the shifting nature of definitions and the connection of managerial practices with the existence of a managerial cadre or of a class.\nOne habit of thought regards management as equivalent to \"business administration\" and thus excludes management in places outside commerce, as for example in charities and in the public sector. More broadly, every organization must \"manage\" its work, people, processes, technology, etc. to maximize effectiveness. Nonetheless, many people refer to university departments that teach management as \"business schools\". Some such institutions (such as the Harvard Business School) use that name, while others (such as the Yale School of Management) employ the broader term \"management\".\nEnglish-speakers may also use the term \"management\" or \"the management\" as a collective word describing the managers of an organization, for example of a corporation.\nHistorically this use of the term often contrasted with the term \"labor\" \u2013 referring to those being managed.But in the present era the concept of management is identified in the wide areas and its frontiers have been pushed to a broader range. Apart from profitable organizations even non-profitable organizations (NGOs) apply management concepts. The concept and its uses are not constrained. Management on the whole is the process of planning, organizing, coordinating, leading and controlling.\n\n\n== Nature of work ==\nIn profitable organizations, management's primary function is the satisfaction of a range of stakeholders. This typically involves making a profit (for the shareholders), creating valued products at a reasonable cost (for customers), and providing great employment opportunities for employees. In nonprofit management, add the importance of keeping the faith of donors. In most models of management and governance, shareholders vote for the board of directors, and the board then hires senior management. Some organizations have experimented with other methods (such as employee-voting models) of selecting or reviewing managers, but this is rare.\n\n\n== History ==\nSome see management (by definition) as a late-modern (in the sense of late modernity) conceptualization. On those terms it cannot have a pre-modern history - only harbingers (such as stewards). Others, however, detect management-like thought among ancient Sumerian traders and the builders of the pyramids of ancient Egypt. Slave-owners through the centuries faced the problems of exploiting/motivating a dependent but sometimes unenthusiastic or recalcitrant workforce, but many pre-industrial enterprises, given their small scale, did not feel compelled to face the issues of management systematically. However, innovations such as the spread of  Hindu numerals (5th to 15th centuries) and the codification of  double-entry book-keeping (1494) provided  tools for management assessment, planning and control.\nMachiavelli  wrote about how to make organisations efficient and effective. The principles that Machiavelli set forth in Discourses (1531) can apply in adapted form to the management of organisations today:\nAn organisation is more stable if members have the right to express their differences and solve their conflicts within  it.\nWhile one person can begin an organisation, \"it is lasting when it is left in the care of many and when many desire to maintain it\".\nA weak manager can follow a strong one, but not another weak one, and maintain authority.\nA manager seeking to change an established organization \"should retain at least a shadow of the ancient customs\".With the changing workplaces of industrial revolutions in the 18th and 19th centuries, military theory and practice contributed approaches to managing the newly-popular factories.Given the scale of most commercial operations and the lack of mechanized record-keeping and recording before the industrial revolution, it made sense for most owners of enterprises in those times to carry out management functions by and for themselves. But with growing size and complexity of organizations, a distinction between owners (individuals, industrial dynasties or groups of shareholders) and day-to-day managers (independent specialists in planning and control) gradually became more common.\n\n\n=== Etymology ===\nThe English verb \"manage\" comes from the Italian maneggiare (to handle, especially tools or a horse), which derives from the two Latin words manus (hand) and agere (to act). The French word for housekeeping, m\u00e9nagerie, derived from m\u00e9nager (\"to keep house\"; compare m\u00e9nage for \"household\"), also encompasses taking care of domestic animals. M\u00e9nagerie is the French translation of Xenophon's famous book Oeconomicus (Greek: \u039f\u1f30\u03ba\u03bf\u03bd\u03bf\u03bc\u03b9\u03ba\u03cc\u03c2) on household matters and husbandry. The French word mesnagement (or m\u00e9nagement) influenced the semantic development of the English word management in the 17th and 18th centuries.\n\n\n=== Early writing ===\nManagement (according to some definitions) has existed for millennia, and several writers have produced background works that have contributed to modern management theories. Some theorists have cited ancient military texts as providing lessons for civilian managers.  For example, Chinese general Sun Tzu in his 6th-century BC work The Art of War recommends (when re-phrased in modern terminology) being aware of and acting on strengths and weaknesses of both a manager's organization and a foe's. The writings of influential Chinese Legalist philosopher Shen Buhai may be considered to embody a rare premodern example of abstract theory of administration.Various ancient and medieval civilizations produced \"mirrors for princes\" books, which aimed to advise new monarchs on how to govern. Plato described job specialization in 350 BC, and Alfarabi listed several leadership traits in AD 900. Other examples include the Indian Arthashastra by Chanakya (written around 300 BC), and The Prince by Italian author\nNiccol\u00f2 Machiavelli (c. 1515).\n\nWritten in 1776 by Adam Smith, a Scottish moral philosopher, The Wealth of Nations discussed efficient organization of work through division of labour.\nSmith described how changes in processes could boost productivity in the manufacture of pins. While individuals could produce 200 pins per day, Smith analyzed the steps involved in manufacture and, with 10 specialists, enabled production of 48,000 pins per day.\n\n\n=== 19th century ===\nClassical economists such as Adam Smith (1723\u20131790) and John Stuart Mill (1806\u20131873) provided a theoretical background to resource-allocation, production, and pricing issues. About the same time, innovators like Eli Whitney (1765\u20131825), James Watt (1736\u20131819), and Matthew Boulton (1728\u20131809) developed elements of technical production such as standardization,  quality-control procedures,  cost-accounting, interchangeability of parts, and  work-planning. Many of these aspects of management existed in the pre-1861 slave-based sector of the US economy. That environment saw 4 million people, as the contemporary usages had it, \"managed\" in profitable quasi-mass production.\nSalaried managers as an identifiable group first became prominent in the late 19th century.\n\n\n=== 20th century ===\nBy about 1900 one finds managers trying to place their theories on what they regarded as a thoroughly scientific basis (see scientism for perceived limitations of this belief). Examples include Henry R. Towne's Science of management in the 1890s, Frederick Winslow Taylor's The Principles of Scientific Management (1911), Lillian Gilbreth's Psychology of Management (1914), Frank and  Lillian Gilbreth's Applied motion study (1917), and Henry L. Gantt's charts (1910s). J. Duncan wrote the first college management-textbook in 1911. In 1912 Yoichi Ueno introduced Taylorism to Japan and became the first management consultant of the \"Japanese-management style\". His son Ichiro Ueno pioneered Japanese quality assurance.\nThe first comprehensive theories of management appeared around 1920. The Harvard Business School offered the first Master of Business Administration degree (MBA) in 1921. People like Henri Fayol (1841\u20131925) and Alexander Church (1866-1936) described the various branches of management and their inter-relationships. In the early-20th century, people like Ordway Tead (1891\u20131973),  Walter Scott (1869-1955) and J. Mooney applied the principles of psychology to management. Other writers, such as Elton Mayo (1880\u20131949), Mary Parker Follett (1868\u20131933), Chester Barnard (1886\u20131961), Max Weber (1864\u20131920), who saw what he called the \"administrator\" as bureaucrat, Rensis Likert (1903\u20131981), and Chris Argyris (born 1923) approached the phenomenon of management from a sociological perspective.\nPeter Drucker (1909\u20132005) wrote one of the earliest books on applied management: Concept of the Corporation (published in 1946). It resulted from Alfred Sloan (chairman of General Motors until 1956) commissioning a study of the organisation. Drucker went on to write 39 books, many in the same vein.\nH. Dodge, Ronald Fisher (1890\u20131962), and Thornton C. Fry introduced statistical techniques into management-studies. In the 1940s, Patrick Blackett worked in the development of the applied-mathematics science of operations research, initially for military operations. Operations research, sometimes known as \"management science\" (but distinct from Taylor's scientific management), attempts to take a scientific approach to solving decision-problems, and can apply directly to multiple management problems, particularly in the areas of logistics and operations.\nSome of the more recent developments include the Theory of Constraints, management by objectives,  reengineering, Six Sigma, the Viable system model, and various information-technology-driven theories such as agile software development, as well as group-management theories such as Cog's Ladder.\nAs the general recognition of managers as a class solidified during the 20th century and gave perceived practitioners of the art/science of management a certain amount of prestige, so the way opened for  popularised systems of management ideas to peddle their wares. In this context many management fads may have had more to do with pop psychology than with scientific theories of management.\nTowards the end of the 20th century, business management came to consist of six separate branches, namely:\n\nfinancial management\nhuman resource management\ninformation technology management (responsible for management information systems)\nmarketing management\noperations management and production management\nstrategic management\n\n\n=== 21st century ===\nIn the 21st century observers find it increasingly difficult to subdivide management into functional categories in this way. More and more processes simultaneously involve several categories. Instead, one tends to think in terms of the various processes, tasks, and objects subject to management.Branches of management theory also exist relating to nonprofits and to government: such as public administration, public management, and educational management. Further, management programs related to  civil-society organizations have also spawned programs in nonprofit management and social entrepreneurship.\nNote that many of the assumptions made by management have come under attack from  business-ethics viewpoints, critical management studies, and anti-corporate activism.\nAs one consequence, workplace democracy (sometimes referred to as Workers' self-management) has become both more common and more advocated, in some places distributing all management functions among workers, each of whom takes on a portion of the work. However, these models predate any current political issue, and may occur more naturally than does a command hierarchy. All management embraces to some degree a democratic principle\u2014in that in the long term, the majority of workers must support management. Otherwise, they leave to find other work or go on strike. Despite the move toward workplace democracy, command-and-control organization structures remain commonplace as de facto organization structures. Indeed, the entrenched nature of command-and-control is evident in the way that recent layoffs have been conducted with management ranks affected far less than employees at the lower levels. In some cases, management has even rewarded itself with bonuses after laying off lower-level workers.According to leadership-academic Manfred F.R. Kets de Vries, a contemporary senior-management team will almost inevitably have some personality disorders.\n\n\n== Topics ==\n\n\n=== Basics ===\nAccording to Fayol, management operates through five basic functions: planning, organizing, coordinating, commanding, and controlling.\n\nPlanning: Deciding what needs to happen in the future and generating plans for action (deciding in advance).\nOrganizing (or staffing): Making sure the human and nonhuman resources are put into place.\nCoordinating: Creating a structure through which an organization's goals can be accomplished.\nCommanding (or leading): Determining what must be done in a situation and getting people to do it.\nControlling: Checking progress against plans.\n\n\n=== Basic roles ===\nInterpersonal: roles that involve coordination and interaction with employeesFigurehead, leader\n\nInformational: roles that involve handling, sharing, and analyzing informationNerve centre, disseminator\n\nDecision: roles that require decision-makingEntrepreneur, negotiator, allocator\n\n\n=== Skills ===\nManagement skills include:\n\npolitical: used to build a power base and to establish connections\nconceptual: used to analyze complex situations\ninterpersonal: used to communicate, motivate, mentor and delegate\ndiagnostic: ability to visualize appropriate responses to a situation\nleadership: ability to lead and to provide guidance to a specific group\ntechnical: expertise in one's particular functional area.\nBehavioral:Perception towards others.\n\n\n=== Implementation of policies and strategies ===\nAll policies and strategies must be discussed with all managerial personnel and staff.\nManagers must understand where and how they can implement their policies and strategies.\nA plan of action must be devised for each department.\nPolicies and strategies must be reviewed regularly.\nContingency plans must be devised in case the environment changes.\nTop-level managers should carry out regular progress assessments.\nThe business requires team spirit and a good environment.\nThe missions, objectives, strengths and weaknesses of each department must be analyzed to determine their roles in achieving the business's mission.\nThe forecasting method develops a reliable picture of the business' future environment.\nA planning unit must be created to ensure that all plans are consistent and that policies and strategies are aimed at achieving the same mission and objectives.\n\n\n== Policies and strategies in the planning process ==\nThey give mid and lower-level managers a good idea of the future plans for each department in an organization.\nA framework is created whereby plans and decisions are made.\nMid and lower-level management may add their own plans to the business's strategies.\n\n\n== Levels ==\nMost organizations have three management levels: first-level, middle-level, and top-level managers. First-line managers are the lowest level of management and manage the work of nonmanagerial individuals who are directly involved with the production or creation of the organization's products. First-line managers are often called supervisors, but may also be called line managers, office managers, or even foremen. Middle managers include all levels of management between the first-line level and the top level of the organization. These managers manage the work of first-line managers and may have titles such as department head, project leader, plant manager, or division manager. Top managers are responsible for making organization-wide decisions and establishing the plans and goals that affect the entire organization. These individuals typically have titles such as executive vice president, president, managing director, chief operating officer, chief executive officer, or chairman of the board.\nThese managers are classified in a hierarchy of authority, and perform different tasks. In many organizations, the number of managers in every level resembles a pyramid.  Each level is explained below in specifications of their different responsibilities and likely job titles.\n\n\n=== Top ===\nThe top or senior layer of management consists of the board of directors (including non-executive directors and executive directors), president, vice-president, CEOs and other members of the C-level executives. Different organizations have various members in their C-suite, which may include a Chief Financial Officer, Chief Technology Officer, and so on. They are responsible for controlling and overseeing the operations of the entire organization. They set a \"tone at the top\" and develop strategic plans, company policies, and make decisions on the overall direction of the organization. In addition, top-level managers play a significant role in the mobilization of outside resources. Senior managers are accountable to the shareholders, the general public and to public bodies that oversee corporations and similar organizations. Some members of the senior management may serve as the public face of the organization, and they may make speeches to introduce new strategies or appear in marketing.\nThe board of directors is typically primarily composed of non-executives who owe a fiduciary duty to shareholders and are not closely involved in the day-to-day activities of the organization, although this varies depending on the type (e.g., public versus private), size and culture of the organization. These directors are theoretically liable for breaches of that duty and typically insured under directors and officers liability insurance. Fortune 500 directors are estimated to spend 4.4 hours per week on board duties, and median compensation was $212,512 in 2010. The board sets corporate strategy, makes major decisions such as major acquisitions, and hires, evaluates, and fires the top-level manager (Chief Executive Officer or CEO). The CEO typically hires other positions. However, board involvement in the hiring of other positions such as the Chief Financial Officer (CFO) has increased. In 2013, a survey of over 160 CEOs and directors of public and private companies found that the top weaknesses of CEOs were \"mentoring skills\" and \"board engagement\", and 10% of companies never evaluated the CEO. The board may also have certain employees (e.g., internal auditors) report to them or directly hire independent contractors; for example, the board (through the audit committee) typically selects the auditor.\nHelpful skills of top management vary by the type of organization but typically include a broad understanding of competition, world economies, and politics. In addition, the CEO is responsible for implementing and determining (within the board's framework) the broad policies of the organization. Executive management accomplishes the day-to-day details, including: instructions for preparation of department budgets, procedures, schedules; appointment of middle level executives such as department managers; coordination of departments; media and governmental relations; and shareholder communication.\n\n\n=== Middle ===\nConsist of general managers, branch managers and department managers. They are accountable to the top management for their  department's function. They devote more time to organizational and directional functions. Their roles can be emphasized as executing organizational plans in conformance with the company's policies and the objectives of the top management, they define and discuss information and policies from top management to lower management, and most importantly they inspire and provide guidance to lower level managers towards better performance.\nMiddle management is the midway management of a categorized organization, being secondary to the senior management but above the deepest levels of operational members.  An operational manager may be well-thought-out by middle management, or may be categorized as non-management operate, liable to the policy of the specific organization. Efficiency of the middle level is vital in any organization, since they bridge the gap between top level and bottom level staffs.\nTheir functions include:\n\nDesign and implement effective group and inter-group work and information systems.\nDefine and monitor group-level performance indicators.\nDiagnose and resolve problems within and among work groups.\nDesign and implement reward systems that support cooperative behavior. They also make decision and share ideas with top managers.\n\n\n=== Lower ===\nLower managers include supervisors, section leaders, forepersons and team leaders. They focus on controlling and directing regular employees. They are usually responsible for assigning employees' tasks, guiding and supervising employees on day-to-day activities, ensuring the quality and quantity of production and/or service, making recommendations and suggestions to employees on their work, and channeling employee concerns that they cannot resolve to mid-level managers or other administrators. First-level or \"front line\" managers also act as role models for their employees. In some types of work, front line managers may also do some of the same tasks that employees do, at least some of the time. For example, in some restaurants, the front line managers will also serve customers during a very busy period of the day.\nFront-line managers typically provide:\n\nTraining for new employees\nBasic supervision\nMotivation\nPerformance feedback and guidanceSome front-line managers may also provide career planning for employees who aim to rise within the organization.\n\n\n== Training ==\nColleges and universities around the world offer bachelor's degrees, graduate degrees, diplomas and certificates in management, generally within their colleges of business, business schools or faculty of management but also in other related departments. In the 2010s, there has been an increase in online management education and training in the form of electronic educational technology ( also called e-learning). Online education has increased the accessibility of management training to people who do not live near a college or university, or who cannot afford to travel to a city where such training is available.\nWhile some professions require academic credentials in order to work in the profession (e.g., law, medicine, engineering, which require, respectively the Bachelor of Law, Doctor of Medicine and Bachelor of Engineering degrees), management and administration positions do not necessarily require the completion of academic degrees. Some well-known senior executives in the US who did not complete a degree include Steve Jobs, Bill Gates and Mark Zuckerberg. However, many managers and executives have completed some type of business or management training, such as a Bachelor of Commerce or a Master of Business Administration degree. Some major organizations, including companies, not-for-profit organizations and governments, require applicants to managerial or executive positions to hold at minimum Bachelor's degree in a field related to administration or management, or in the case of business jobs, a Bachelor of Commerce or a similar degree.\n\n\n=== United States of America ===\n\n\n==== Undergraduate ====\nAt the undergraduate level, the most common business program is the Bachelor of Commerce (B.Com.). A B.Com. is typically a four-year program that includes courses that give students an overview of the role of managers in planning and directing within an organization. Course topics include accounting, financial management, statistics, marketing, strategy, and other related areas. There are many other undergraduate degrees that include the study of management, such as Bachelor of Arts degrees with a major in business administration or management and Bachelor of Public Administration (B.P.A), a degree designed for individuals aiming to work as bureaucrats in the government jobs. Many colleges and universities also offer certificates and diplomas in business administration or management, which typically require one to two years of full-time study.\n\n\n==== Graduate ====\nAt the graduate level students aiming at careers as managers or executives may choose to specialize in major subareas of management or business administration such as entrepreneurship, human resources, international business, organizational behavior, organizational theory, strategic management, accounting, corporate finance, entertainment, global management, healthcare management, investment management, sustainability and real estate. A Master of Business Administration (MBA) is the most popular professional degree at the master's level and can be obtained from many universities in the United States. MBA programs provide further education in management and leadership for graduate students. Other master's degrees in business and management include Master of Management (MM) and the Master of Science (M.Sc.) in business administration or management, which is typically taken by students aiming to become researchers or professors. There are also specialized master's degrees in administration for individuals aiming at careers outside of business, such as the Master of Public Administration (MPA) degree (also offered as a Master of Arts in Public Administration in some universities), for students aiming to become managers or executives in the public service and the Master of Health Administration, for students aiming to become managers or executives in the health care and hospital sector.\nManagement doctorates are the most advanced terminal degrees in the field of business and management. Most individuals obtaining management doctorates take the programs to obtain the training in research methods, statistical analysis and writing academic papers that they will need to seek careers as researchers, senior consultants and/or professors in business administration or management. There are three main types of management doctorates: the Doctor of Management (D.M.), the Doctor of Business Administration (D.B.A.), and the Ph.D. in Business Administration or Management. In the 2010s, doctorates in business administration and management are available with many specializations.\n\n\n=== Good practices ===\nWhile management trends can change so fast, the long term trend in management has been defined by a market embracing diversity and a rising service industry. Managers are currently being trained to encourage greater equality for minorities and women in the workplace, by offering increased flexibility in working hours, better retraining, and innovative (and usually industry-specific) performance markers. Managers destined for the service sector are being trained to use unique measurement techniques, better worker support and more charismatic leadership styles. Human resources finds itself increasingly working with management in a training capacity to help collect management data on the success (or failure) of management actions with employees.\n\n\n== See also ==\nOutline of business management\n\n\n== References ==\n\n\n== External links ==\nLibrary resources in your library and in other libraries about Management\n Media related to Management at Wikimedia Commons\n Quotations related to Management at Wikiquote", "food science": "Food science is the applied science devoted to the study of food. The Institute of Food Technologists defines food science as \"the discipline in which the engineering, biological, and physical sciences are used to study the nature of foods, the causes of deterioration, the principles underlying food processing, and the improvement of foods for the consuming public\". The textbook Food Science defines food science in simpler terms as \"the application of basic sciences and engineering to study the physical, chemical, and biochemical nature of foods and the principles of food processing\".Activities of food scientists include the development of new food products, design of processes to produce these foods, choice of packaging materials, shelf-life studies, sensory evaluation of products using survey panels or potential consumers, as well as microbiological and chemical testing. Food scientists may study more fundamental phenomena that are directly linked to the production of food products and its properties.\nFood science brings together multiple scientific disciplines. It incorporates concepts from fields such as microbiology, chemical engineering, and biochemistry.\n\n\n== Disciplines ==\nSome of the subdisciplines of food science are described below.\n\n\n=== Food chemistry ===\n\nFood chemistry is the study of chemical processes and interactions of all biological and non-biological components of foods. The biological substances include such items as meat, poultry, lettuce, beer, and milk as examples.\nIt is similar to biochemistry in its main components such as carbohydrates, lipids, and protein, but it also includes areas such as water, vitamins, minerals, enzymes, food additives, flavors, and colors. This discipline also encompasses how products change under certain food processing techniques and ways either to enhance or to prevent them from happening.\n\n\n==== Food physical chemistry ====\n\nFood physical chemistry is the study of both physical and chemical interactions in foods in terms of physical and chemical principles applied to food systems, as well as the application of physicochemical techniques and instrumentation for the study and analysis of foods.\n\n\n=== Food engineering ===\n\nFood engineering is the industrial processes used to manufacture food.\n\n\n=== Food microbiology ===\n\nFood microbiology is the study of the microorganisms that inhabit, create, or contaminate food, including the study of microorganisms causing food spoilage.  \"Good\" bacteria, however, such as probiotics, are becoming increasingly important in food science. In addition, microorganisms are essential for the production of foods such as cheese, yogurt, bread, beer, wine and, other fermented foods.\n\n\n=== Food packaging ===\n\n\n=== Food preservation ===\n\nFood preservation involves the causes and prevention of quality spoilage\n\n\n=== Food substitution ===\nFood substitution refers to the replacement of fat, sugar, or calories from a product while maintaining similar shape, texture, color, or taste.\n\n\n=== Food technology ===\n\nFood technology is the technological aspects.\nEarly scientific research into food technology concentrated on food preservation. Nicolas Appert\u2019s development in 1810 of the canning process was a decisive event.  The process wasn\u2019t called canning then and Appert did not really know the principle on which his process worked, but canning has had a major impact on food preservation techniques.\n\n\n=== Molecular gastronomy ===\n\nMolecular gastronomy is the scientific investigation of processes in cooking, social and artistic gastronomical phenomena.\nMolecular gastronomy is a subdiscipline of food science that seeks to investigate the physical and chemical transformations of ingredients that occur in cooking. Its program includes three axis, as cooking was recognized to have three components, which are social, artistic and technical.\n\n\n=== New product development ===\n\nNew product development includes the invention of new food products.\n\n\n=== Quality control ===\n\nQuality control involves the causes, prevention and communication dealing with food-borne illness. \nQuality control also ensures that product meets specs to ensure the customer receives what they expect from the packaging to the physical properties of the product itself.\n\n\n=== Sensory analysis ===\n\nSensory analysis is the study of how consumers' senses perceive food.\n\n\n== By country ==\n\n\n=== Australia ===\n\nThe Commonwealth Scientific and Industrial Research Organisation (CSIRO) is the federal government agency for scientific research in Australia. CSIRO maintains more than 50 sites across Australia and biological control research stations in France and Mexico. It has nearly 6,500 employees.\n\n\n=== South Korea ===\nThe Korean Society of Food Science and Technology, or KoSFoST, claims to be the first society in South Korea for food science.\n\n\n=== United States ===\nIn the United States, food science is typically studied at land-grant universities. Many of the country's pioneering food scientists were women who had attended chemistry programs at land-grant universities (which were state-run and largely under state mandates to allow for sex-blind admission), but then graduated and had difficulty finding jobs due to widespread sexism in the chemistry industry in the late 19th and early 20th centuries. Finding conventional career paths blocked, they found alternative employment as instructors in home economics departments and used that as a base to launch the foundation of many modern food science programs.\nThe main US organization regarding food science and food technology is the Institute of Food Technologists (IFT), headquartered in Chicago, Illinois, which is the US member organisation of the International Union of Food Science and Technology (IUFoST).\n\n\n== See also ==\n\n\n== Publications ==\n\n\n=== Books ===\nPopular books on some aspects of food science or kitchen science have been written by Harold McGee and Howard Hillman, among others.\n\n\n=== Journals ===\n\n\n== Notes and references ==\n\n\n== Further reading ==\nWanucha, Genevieve (February 24, 2009). \"Two Happy Clams: The Friendship that Forged Food Science\". MIT Technology Review.\n\n\n== External links ==\n\n Media related to Food science at Wikimedia Commons\nFood science at Curlie (based on DMOZ)\nLearn about Food Science", "space exploration": "Space exploration is the discovery and exploration of celestial structures in outer space by means of evolving and growing space technology. While the study of space is carried out mainly by astronomers with telescopes, the physical exploration of space is conducted both by unmanned robotic space probes and human spaceflight.\nWhile the observation of objects in space, known as astronomy, predates reliable recorded history, it was the development of large and relatively efficient rockets during the mid-twentieth century that allowed physical space exploration to become a reality. Common rationales for exploring space include advancing scientific research, national prestige, uniting different nations, ensuring the future survival of humanity, and developing military and strategic advantages against other countries.Space exploration has often been used as a proxy competition for geopolitical rivalries such as the Cold War. The early era of space exploration was driven by a \"Space Race\" between the Soviet Union and the United States. The launch of the first human-made object to orbit Earth, the Soviet Union's Sputnik 1, on 4 October 1957, and the first Moon landing by the American Apollo 11 mission on 20 July 1969 are often taken as landmarks for this initial period. The Soviet Space Program achieved many of the first milestones, including the first living being in orbit in 1957, the first human spaceflight (Yuri Gagarin aboard Vostok 1) in 1961, the first spacewalk (by Aleksei Leonov) on 18 March 1965, the first automatic landing on another celestial body in 1966, and the launch of the first space station (Salyut 1) in 1971.\nAfter the first 20 years of exploration, focus shifted from one-off flights to renewable hardware, such as the Space Shuttle program, and from competition to cooperation as with the International Space Station (ISS).\nWith the substantial completion of the ISS following STS-133 in March 2011, plans for space exploration by the U.S. remain in flux. Constellation, a Bush Administration program for a return to the Moon by 2020 was judged inadequately funded and unrealistic by an expert review panel reporting in 2009.  \nThe Obama Administration proposed a revision of Constellation in 2010 to focus on the development of the capability for crewed missions beyond low Earth orbit (LEO), envisioning extending the operation of the ISS beyond 2020, transferring the development of launch vehicles for human crews from NASA to the private sector, and developing technology to enable missions to beyond LEO, such as Earth\u2013Moon L1, the Moon, Earth\u2013Sun L2, near-Earth asteroids, and Phobos or Mars orbit.In the 2000s, the People's Republic of China initiated a successful manned spaceflight program, while the European Union, Japan, and India have also planned future crewed space missions. China, Russia, Japan, and India have advocated crewed missions to the Moon during the 21st century, while the European Union has advocated manned missions to both the Moon and Mars during the 20th and 21st century.\nFrom the 1990s onwards, private interests began promoting space tourism and then public space exploration of the Moon (see Google Lunar X Prize).\n\n\n== History of exploration in the 20th century ==\n\nThe highest known projectiles prior to the rockets of the 1940s were the shells of the Paris Gun, a type of German long-range siege gun, which reached at least 40 kilometers altitude during World War One. Steps towards putting a human-made object into space were taken by German scientists during World War II while testing the V-2 rocket, which became the first human-made object in space on 3 October 1942 with the launching of the A-4. After the war, the U.S. used German scientists and their captured rockets in programs for both military and civilian research. The first scientific exploration from space was the cosmic radiation experiment launched by the U.S. on a V-2 rocket on 10 May 1946. The first images of Earth taken from space followed the same year while the first animal experiment saw fruit flies lifted into space in 1947, both also on modified V-2s launched by Americans. Starting in 1947, the Soviets, also with the help of German teams, launched sub-orbital V-2 rockets and their own variant, the R-1, including radiation and animal experiments on some flights. These suborbital experiments only allowed a very short time in space which limited their usefulness.\n\n\n=== First orbital flights ===\n\nThe first successful orbital launch was of the Soviet uncrewed Sputnik 1 (\"Satellite 1\") mission on 4 October 1957. The satellite weighed about 83 kg (183 lb), and is believed to have orbited Earth at a height of about 250 km (160 mi). It had two radio transmitters (20 and 40 MHz), which emitted \"beeps\" that could be heard by radios around the globe. Analysis of the radio signals was used to gather information about the electron density of the ionosphere, while temperature and pressure data was encoded in the duration of radio beeps. The results indicated that the satellite was not punctured by a meteoroid. Sputnik 1 was launched by an R-7 rocket. It burned up upon re-entry on 3 January 1958.\nThe second one was Sputnik 2. Launched by the USSR on November 3, 1957, it carried the dog Laika, who became the first animal in orbit.\nThis success led to an escalation of the American space program, which unsuccessfully attempted to launch a Vanguard satellite into orbit two months later. On 31 January 1958, the U.S. successfully orbited Explorer 1 on a Juno rocket.\n\n\n=== First human flights ===\nThe first successful human spaceflight was Vostok 1 (\"East 1\"), carrying 27-year-old Russian cosmonaut Yuri Gagarin on 12 April 1961. The spacecraft completed one orbit around the globe, lasting about 1 hour and 48 minutes. Gagarin's flight resonated around the world; it was a demonstration of the advanced Soviet space program and it opened an entirely new era in space exploration: human spaceflight.\nThe U.S. first launched a person into space within a month of Vostok 1 with Alan Shepard's suborbital flight on Freedom 7. Orbital flight was achieved by the United States when John Glenn's Friendship 7 orbited Earth on 20 February 1962.\nValentina Tereshkova, the first woman in space, orbited Earth 48 times aboard Vostok 6 on 16 June 1963.\nChina first launched a person into space 42 years after the launch of Vostok 1, on 15 October 2003, with the flight of Yang Liwei aboard the Shenzhou 5 (Divine Vessel 5) spacecraft.\n\n\n=== First planetary explorations ===\nThe first artificial object to reach another celestial body was Luna 2 in 1959. The first automatic landing on another celestial body was performed by Luna 9 in 1966. Luna 10 became the first artificial satellite of the Moon.The first crewed landing on another celestial body was performed by Apollo 11 on 20 July 1969.\nThe first successful interplanetary flyby was the 1962 Mariner 2 flyby of Venus (closest approach 34,773 kilometers). The other planets were first flown by in 1965 for Mars by Mariner 4, 1973 for Jupiter by Pioneer 10, 1974 for Mercury by Mariner 10, 1979 for Saturn by Pioneer 11, 1986 for Uranus by Voyager 2, 1989 for Neptune by Voyager 2. In 2015, the dwarf planets Ceres and Pluto were orbited by Dawn and passed by New Horizons, respectively.\nThe first interplanetary surface mission to return at least limited surface data from another planet was the 1970 landing of Venera 7 on Venus which returned data to Earth for 23 minutes. In 1975 the Venera 9 was the first to return images from the surface of another planet. In 1971 the Mars 3 mission achieved the first soft landing on Mars returning data for almost 20 seconds. Later much longer duration surface missions were achieved, including over six years of Mars surface operation by Viking 1 from 1975 to 1982 and over two hours of transmission from the surface of Venus by Venera 13 in 1982, the longest ever Soviet planetary surface mission.\n\n\n=== Key people in early space exploration ===\nThe dream of stepping into the outer reaches of Earth's atmosphere was driven by the fiction of Peter Francis Geraci and H. G. Wells, and rocket technology was developed to try to realize this vision. The German V-2 was the first rocket to travel into space, overcoming the problems of thrust and material failure. During the final days of World War II this technology was obtained by both the Americans and Soviets as were its designers. The initial driving force for further development of the technology was a weapons race for intercontinental ballistic missiles (ICBMs) to be used as long-range carriers for fast nuclear weapon delivery, but in 1961 when the Soviet Union launched the first man into space, the United States declared itself to be in a \"Space Race\" with the Soviets.\nKonstantin Tsiolkovsky, Robert Goddard, Hermann Oberth, and Reinhold Tiling laid the groundwork of rocketry in the early years of the 20th century.\nWernher von Braun was the lead rocket engineer for Nazi Germany's World War II V-2 rocket project. In the last days of the war he led a caravan of workers in the German rocket program to the American lines, where they surrendered and were brought to the United States to work on their rocket development (\"Operation Paperclip\"). He acquired American citizenship and led the team that developed and launched Explorer 1, the first American satellite. Von Braun later led the team at NASA's Marshall Space Flight Center which developed the Saturn V moon rocket.\nInitially the race for space was often led by Sergei Korolyov, whose legacy includes both the R7 and Soyuz\u2014which remain in service to this day. Korolev was the mastermind behind the first satellite, first man (and first woman) in orbit and first spacewalk. Until his death his identity was a closely guarded state secret; not even his mother knew that he was responsible for creating the Soviet space program.\nKerim Kerimov was one of the founders of the Soviet space program and was one of the lead architects behind the first human spaceflight (Vostok 1) alongside Sergey Korolyov. After Korolyov's death in 1966, Kerimov became the lead scientist of the Soviet space program and was responsible for the launch of the first space stations from 1971 to 1991, including the Salyut and Mir series, and their precursors in 1967, the Cosmos 186 and Cosmos 188.\n\n\n==== Other key people ====\nValentin Glushko was Chief Engine Designer for the Soviet Union. Glushko designed many of the engines used on the early Soviet rockets, but was constantly at odds with Korolyov.\nVasily Mishin was Chief Designer working under Sergey Korolyov and one of the first Soviets to inspect the captured German V-2 design. Following the death of Sergei Korolev, Mishin was held responsible for the Soviet failure to be first country to place a man on the Moon.\nRobert Gilruth was the NASA head of the Space Task Force and director of 25 crewed space flights. Gilruth was the person who suggested to John F. Kennedy that the Americans take the bold step of reaching the Moon in an attempt to reclaim space superiority from the Soviets.\nChristopher C. Kraft, Jr. was NASA's first flight director, who oversaw development of Mission Control and associated technologies and procedures.\nMaxime Faget was the designer of the Mercury capsule; he played a key role in designing the Gemini and Apollo spacecraft, and contributed to the design of the Space Shuttle.\nSusan Finley who designed the Deep Space Network, allowing communication from space to Earth.\n\n\n== Targets of exploration ==\n\n\n=== The Sun ===\nAlthough the Sun will probably not be physically explored at all, the study of the Sun has nevertheless been a major focus of space exploration. Being above the atmosphere in particular and Earth's magnetic field gives access to the solar wind and infrared and ultraviolet radiations that cannot reach Earth's surface. The Sun generates most space weather, which can affect power generation and transmission systems on Earth and interfere with, and even damage, satellites and space probes. Numerous spacecraft dedicated to observing the Sun, beginning with the Apollo Telescope Mount, have been launched and still others have had solar observation as a secondary objective. Parker Solar Probe, planned for a 2018 launch, will approach the Sun to within 1/8th the orbit of Mercury.\n\n\n== Mercury ==\nMercury remains the least explored of the Terrestrial planets. As of May 2013, the Mariner 10 and MESSENGER missions have been the only missions that have made close observations of Mercury. MESSENGER entered orbit around Mercury in March 2011, to further investigate the observations made by Mariner 10 in 1975 (Munsell, 2006b).\nA third mission to Mercury, scheduled to arrive in 2025, BepiColombo is to include two probes. BepiColombo is a joint mission between Japan and the European Space Agency. MESSENGER and BepiColombo are intended to gather complementary data to help scientists understand many of the mysteries discovered by Mariner 10's flybys.\nFlights to other planets within the Solar System are accomplished at a cost in energy, which is described by the net change in velocity of the spacecraft, or delta-v. Due to the relatively high delta-v to reach Mercury and its proximity to the Sun, it is difficult to explore and orbits around it are rather unstable.\n\n\n== Venus ==\n\nVenus was the first target of interplanetary flyby and lander missions and, despite one of the most hostile surface environments in the Solar System, has had more landers sent to it (nearly all from the Soviet Union) than any other planet in the Solar System. The first successful Venus flyby was the American Mariner 2 spacecraft, which flew past Venus in 1962. Mariner 2 has been followed by several other flybys by multiple space agencies often as part of missions using a Venus flyby to provide a gravitational assist en route to other celestial bodies. In 1967 Venera 4 became the first probe to enter and directly examine the atmosphere of Venus. In 1970, Venera 7 became the first successful lander to reach the surface of Venus and by 1985 it had been followed by eight additional successful Soviet Venus landers which provided images and other direct surface data. Starting in 1975 with the Soviet orbiter Venera 9 some ten successful orbiter missions have been sent to Venus, including later missions which were able to map the surface of Venus using radar to pierce the obscuring atmosphere.\n\n\n== Earth ==\n\nSpace exploration has been used as a tool to understand Earth as a celestial object in its own right. Orbital missions can provide data for Earth that can be difficult or impossible to obtain from a purely ground-based point of reference.\nFor example, the existence of the Van Allen radiation belts was unknown until their discovery by the United States' first artificial satellite, Explorer 1. These belts contain radiation trapped by Earth's magnetic fields, which currently renders construction of habitable space stations above 1000 km impractical.\nFollowing this early unexpected discovery, a large number of Earth observation satellites have been deployed specifically to explore Earth from a space based perspective. These satellites have significantly contributed to the understanding of a variety of Earth-based phenomena. For instance, the hole in the ozone layer was found by an artificial satellite that was exploring Earth's atmosphere, and satellites have allowed for the discovery of archeological sites or geological formations that were difficult or impossible to otherwise identify.\n\n\n=== The Moon ===\n\nThe Moon was the first celestial body to be the object of space exploration. It holds the distinctions of being the first remote celestial object to be flown by, orbited, and landed upon by spacecraft, and the only remote celestial object ever to be visited by humans.\nIn 1959 the Soviets obtained the first images of the far side of the Moon, never previously visible to humans. The U.S. exploration of the Moon began with the Ranger 4 impactor in 1962. Starting in 1966 the Soviets successfully deployed a number of landers to the Moon which were able to obtain data directly from the Moon's surface; just four months later, Surveyor 1 marked the debut of a successful series of U.S. landers. The Soviet uncrewed missions culminated in the Lunokhod program in the early 1970s, which included the first uncrewed rovers and also successfully brought lunar soil samples to Earth for study. This marked the first (and to date the only) automated return of extraterrestrial soil samples to Earth. Uncrewed exploration of the Moon continues with various nations periodically deploying lunar orbiters, and in 2008 the Indian Moon Impact Probe.\nCrewed exploration of the Moon began in 1968 with the Apollo 8 mission that successfully orbited the Moon, the first time any extraterrestrial object was orbited by humans. In 1969, the Apollo 11 mission marked the first time humans set foot upon another world. Crewed exploration of the Moon did not continue for long, however. The Apollo 17 mission in 1972 marked the most recent human visit there, and the next, Exploration Mission 2, is due to orbit the Moon in 2021. Robotic missions are still pursued vigorously.\n\n\n== Mars ==\n\nThe exploration of Mars has been an important part of the space exploration programs of the Soviet Union (later Russia), the United States, Europe, Japan and India. Dozens of robotic spacecraft, including orbiters, landers, and rovers, have been launched toward Mars since the 1960s. These missions were aimed at gathering data about current conditions and answering questions about the history of Mars. The questions raised by the scientific community are expected to not only give a better appreciation of the red planet but also yield further insight into the past, and possible future, of Earth.\nThe exploration of Mars has come at a considerable financial cost with roughly two-thirds of all spacecraft destined for Mars failing before completing their missions, with some failing before they even began. Such a high failure rate can be attributed to the complexity and large number of variables involved in an interplanetary journey, and has led researchers to jokingly speak of The Great Galactic Ghoul which subsists on a diet of Mars probes. This phenomenon is also informally known as the \"Mars Curse\".\nIn contrast to overall high failure rates in the exploration of Mars, India has become the first country to achieve success of its maiden attempt.  India's Mars Orbiter Mission (MOM) is one of the least expensive interplanetary missions ever undertaken with an approximate total cost of \u20b9450 Crore (US$73 million). The first mission to Mars by any Arab country has been taken up by the United Arab Emirates. Called the Emirates Mars Mission, it is scheduled for launch in 2020. The uncrewed exploratory probe has been named \"Hope Probe\" and will be sent to Mars to study its atmosphere in detail.\n\n\n=== Phobos ===\n\nThe Russian space mission Fobos-Grunt, which launched on 9 November 2011 experienced a failure leaving it stranded in low Earth orbit. It was to begin exploration of the Phobos and Martian circumterrestrial orbit, and study whether the moons of Mars, or at least Phobos, could be a \"trans-shipment point\" for spaceships traveling to Mars.\n\n\n== Jupiter ==\nThe exploration of Jupiter has consisted solely of a number of automated NASA spacecraft visiting the planet since 1973. A large majority of the missions have been \"flybys\", in which detailed observations are taken without the probe landing or entering orbit; such as in Pioneer and Voyager programs. The Galileo and Juno spacecraft are the only spacecraft to have entered the planet's orbit. As Jupiter is believed to have only a relatively small rocky core and no real solid surface, a landing mission is precluded.\nReaching Jupiter from Earth requires a delta-v of 9.2 km/s, which is comparable to the 9.7 km/s delta-v needed to reach low Earth orbit. Fortunately, gravity assists through planetary flybys can be used to reduce the energy required at launch to reach Jupiter, albeit at the cost of a significantly longer flight duration.Jupiter has 69 known moons, many of which have relatively little known information about them.\n\n\n== Saturn ==\n\nSaturn has been explored only through uncrewed spacecraft launched by NASA, including one mission (Cassini\u2013Huygens) planned and executed in cooperation with other space agencies. These missions consist of flybys in 1979 by Pioneer 11, in 1980 by Voyager 1, in 1982 by Voyager 2 and an orbital mission by the Cassini spacecraft, which lasted from 2004 until 2017.\nSaturn has at least 62 known moons, although the exact number is debatable since Saturn's rings are made up of vast numbers of independently orbiting objects of varying sizes. The largest of the moons is Titan, which holds the distinction of being the only moon in the Solar System with an atmosphere denser and thicker than that of Earth.  Titan holds the distinction of being the only object in the Outer Solar System that has been explored with a lander, the Huygens probe deployed by the Cassini spacecraft.\n\n\n== Uranus ==\n\nThe exploration of Uranus has been entirely through the Voyager 2 spacecraft, with no other visits currently planned. Given its axial tilt of 97.77\u00b0, with its polar regions exposed to sunlight or darkness for long periods, scientists were not sure what to expect at Uranus. The closest approach to Uranus occurred on 24 January 1986. Voyager 2 studied the planet's unique atmosphere and magnetosphere. Voyager 2 also examined its ring system and the moons of Uranus including all five of the previously known moons, while discovering an additional ten previously unknown moons.\nImages of Uranus proved to have a very uniform appearance, with no evidence of the dramatic storms or atmospheric banding evident on Jupiter and Saturn. Great effort was required to even identify a few clouds in the images of the planet. The magnetosphere of Uranus, however, proved to be unique, being profoundly affected by the planet's unusual axial tilt. In contrast to the bland appearance of Uranus itself, striking images were obtained of the Moons of Uranus, including evidence that Miranda had been unusually geologically active.\n\n\n== Neptune ==\n\nThe exploration of Neptune began with the 25 August 1989 Voyager 2 flyby, the sole visit to the system as of 2014. The possibility of a Neptune Orbiter has been discussed, but no other missions have been given serious thought.\nAlthough the extremely uniform appearance of Uranus during Voyager 2's visit in 1986 had led to expectations that Neptune would also have few visible atmospheric phenomena, the spacecraft found that Neptune had obvious banding, visible clouds, auroras, and even a conspicuous anticyclone storm system rivaled in size only by Jupiter's small Spot. Neptune also proved to have the fastest winds of any planet in the Solar System, measured as high as 2,100 km/h. Voyager 2 also examined Neptune's ring and moon system. It discovered 900 complete rings and additional partial ring \"arcs\" around Neptune. In addition to examining Neptune's three previously known moons, Voyager 2 also discovered five previously unknown moons, one of which, Proteus, proved to be the last largest moon in the system. Data from Voyager 2 supported the view that Neptune's largest moon, Triton, is a captured Kuiper belt object.\n\n\n=== Other objects in the Solar System ===\n\nThe dwarf planet Pluto  presents significant challenges for spacecraft because of its great distance from Earth (requiring high velocity for reasonable trip times) and small mass (making capture into orbit very difficult at present). Voyager 1 could have visited Pluto, but controllers opted instead for a close flyby of Saturn's moon Titan, resulting in a trajectory incompatible with a Pluto flyby. Voyager 2 never had a plausible trajectory for reaching Pluto.Pluto continues to be of great interest, despite its reclassification as the lead and nearest member of a new and growing class of distant icy bodies of intermediate size (and also the first member of the important subclass, defined by orbit and known as \"plutinos\"). After an intense political battle, a mission to Pluto dubbed New Horizons was granted funding from the United States government in 2003. New Horizons was launched successfully on 19 January 2006. In early 2007 the craft made use of a gravity assist from Jupiter. Its closest approach to Pluto was on 14 July 2015; scientific observations of Pluto began five months prior to closest approach and continued for 16 days after the encounter.\n\n\n==== Asteroids and comets ====\n\nUntil the advent of space travel, objects in the asteroid belt were merely pinpricks of light in even the largest telescopes, their shapes and terrain remaining a mystery.\nSeveral asteroids have now been visited by probes, the first of which was Galileo, which flew past two: 951 Gaspra in 1991, followed by 243 Ida in 1993. Both of these lay near enough to Galileo's planned trajectory to Jupiter that they could be visited at acceptable cost. The first landing on an asteroid was performed by the NEAR Shoemaker probe in 2000, following an orbital survey of the object. The dwarf planet Ceres and the asteroid 4 Vesta, two of the three largest asteroids, were visited by NASA's Dawn spacecraft, launched in 2007.\nAlthough many comets have been studied from Earth sometimes with centuries-worth of observations, only a few comets have been closely visited. In 1985, the International Cometary Explorer conducted the first comet fly-by (21P/Giacobini-Zinner) before joining the Halley Armada studying the famous comet. The Deep Impact probe smashed into 9P/Tempel to learn more about its structure and composition and the Stardust mission returned samples of another comet's tail. The Philae lander successfully landed on Comet Churyumov\u2013Gerasimenko in 2014 as part of the broader Rosetta mission.\nHayabusa was an unmanned spacecraft developed by the Japan Aerospace Exploration Agency to return a sample of material from the small near-Earth asteroid 25143 Itokawa to Earth for further analysis. Hayabusa was launched on 9 May 2003 and rendezvoused with Itokawa in mid-September 2005. After arriving at Itokawa, Hayabusa studied the asteroid's shape, spin, topography, color, composition, density, and history. In November 2005, it landed on the asteroid to collect samples. The spacecraft returned to Earth on 13 June 2010.\n\n\n=== Deep space exploration ===\n\nDeep space exploration is the branch of astronomy, astronautics and space technology that is involved with the exploration of distant regions of outer space. Physical exploration of space is conducted both by human spaceflights (deep-space astronautics) and by robotic spacecraft.\nSome of the best candidates for future deep space engine technologies include anti-matter, nuclear power and beamed propulsion. The latter, beamed propulsion, appears to be the best candidate for deep space exploration presently available, since it uses known physics and known technology that is being developed for other purposes.\n\n\n== Future of space exploration ==\n\nIn the 2000s, several plans for space exploration were announced; both government entities and the private sector have space exploration objectives. China has announced plans to have a 60-ton multi-module space station in orbit by 2020.\nThe NASA Authorization Act of 2010 provided a re-prioritized list of objectives for the American space program, as well as funding for the first priorities. NASA proposes to move forward with the development of the Space Launch System (SLS), which will be designed to carry the Orion Multi-Purpose Crew Vehicle, as well as important cargo, equipment, and science experiments to Earth's orbit and destinations beyond. Additionally, the SLS will serve as a back up for commercial and international partner transportation services to the International Space Station. The SLS rocket will incorporate technological investments from the Space Shuttle program and the Constellation program in order to take advantage of proven hardware and reduce development and operations costs. The first developmental flight is targeted for the end of 2017.\n\n\n=== AI in space exploration ===\nThe idea of using high level automated systems for space missions has become a desirable goal to space agencies all around the world. Such systems are believed to yield benefits such as lower cost, less human oversight, and ability to explore deeper in space which is usually restricted by long communications with human controllers.\n\n\n==== Autonomous system ====\nAutonomy is defined by three requirements:\nBeing able to sense the world and their state, make decisions, and carry them out on their own\nCan interpret the given goal as a list of actions to take\nFail flexibly\n\n\n==== Benefits ====\nAutonomous technologies would be able to perform beyond predetermined actions. They would analyze all possible states and events happening around them and come up with a safe response. In addition, such technologies can reduce launch cost and ground involvement. Performance would increase as well. Autonomy would be able to quickly respond upon encountering an unforeseen event, especially in deep space exploration where communication back to Earth would take too long.\n\n\n==== NASA's Autonomous Science Experiment ====\nNASA began its autonomous science experiment (ASE) on Earth Observing 1 (EO-1) which is NASA's first satellite in the new millennium program Earth-observing series launched on 21 November 2000. The autonomy of ASE is capable of on-board science analysis, replanning, robust execution, and later the addition of model-based diagnostic. Images obtained by the EO-1 are analyzed on-board and downlinked when a change or an interesting event occur. The ASE software has successfully provided over 10,000 science images.\n\n\n===== Asteroids in space exploration =====\nAn article in science magazine Nature suggested the use of asteroids as a gateway for space exploration, with the ultimate destination being Mars. In order to make such an approach viable, three requirements need to be fulfilled: first, \"a thorough asteroid survey to find thousands of nearby bodies suitable for astronauts to visit\"; second,  \"extending flight duration and distance capability to ever-increasing ranges out to Mars\"; and finally,  \"developing better robotic vehicles and tools to enable astronauts to explore an asteroid regardless of its size, shape or spin.\" Furthermore, using asteroids would provide astronauts with protection from galactic cosmic rays, with mission crews being able to land on them in times of greater risk to radiation exposure.\n\n\n== Rationales ==\n\nThe research that is conducted by national space exploration agencies, such as NASA and Roscosmos, is one of the reasons supporters cite to justify government expenses. Economic analyses of the NASA programs often showed ongoing economic benefits (such as NASA spin-offs), generating many times the revenue of the cost of the program. It is also argued that space exploration would lead to the extraction of resources on other planets and especially asteroids, which contain billions of dollars worth of minerals and metals. Such expeditions could generate a lot of revenue. As well, it has been argued that space exploration programs help inspire youth to study in science and engineering.Another claim is that space exploration is a necessity to mankind and that staying on Earth will lead to extinction. Some of the reasons are lack of natural resources, comets, nuclear war, and worldwide epidemic. Stephen Hawking, renowned British theoretical physicist, said that \"I don't think the human race will survive the next thousand years, unless we spread into space. There are too many accidents that can befall life on a single planet. But I'm an optimist. We will reach out to the stars.\"NASA has produced a series of public service announcement videos supporting the concept of space exploration.Overall, the public remains largely supportive of both crewed and uncrewed space exploration. According to an Associated Press Poll conducted in July 2003, 71% of U.S. citizens agreed with the statement that the space program is \"a good investment\", compared to 21% who did not.Arthur C. Clarke (1950) presented a summary of motivations for the human exploration of space in his non-fiction semi-technical monograph Interplanetary Flight. He argued that humanity's choice is essentially between expansion off Earth into space, versus cultural (and eventually biological) stagnation and death.\n\n\n== Topics ==\n\n\n=== Spaceflight ===\n\nSpaceflight is the use of space technology to achieve the flight of spacecraft into and through outer space.\nSpaceflight is used in space exploration, and also in commercial activities like space tourism and satellite telecommunications. Additional non-commercial uses of spaceflight include space observatories, reconnaissance satellites and other Earth observation satellites.\nA spaceflight typically begins with a rocket launch, which provides the initial thrust to overcome the force of gravity and propels the spacecraft from the surface of Earth. Once in space, the motion of a spacecraft\u2014both when unpropelled and when under propulsion\u2014is covered by the area of study called astrodynamics. Some spacecraft remain in space indefinitely, some disintegrate during atmospheric reentry, and others reach a planetary or lunar surface for landing or impact.\n\n\n=== Satellites ===\n\nSatellites are used for a large number of purposes. Common types include military (spy) and civilian Earth observation satellites, communication satellites, navigation satellites, weather satellites, and research satellites. Space stations and human spacecraft in orbit are also satellites.\n\n\n=== Commercialization of space ===\n\nCurrent examples of the commercial use of space include satellite navigation systems, satellite television and satellite radio. Space tourism is the recent phenomenon of space travel by individuals for the purpose of personal pleasure.\nPrivate spaceflight companies such as SpaceX and Blue Origin, and commercial space stations such as the Axiom Space and the Bigelow Commercial Space Station have dramatically changed the landscape of space exploration, and will continue to do so in the near future.\n\n\n=== Alien life ===\n\nAstrobiology is the interdisciplinary study of life in the universe, combining aspects of astronomy, biology and geology. It is focused primarily on the study of the origin, distribution and evolution of life. It is also known as exobiology (from Greek: \u03ad\u03be\u03c9, exo, \"outside\").  The term \"Xenobiology\" has been used as well, but this is technically incorrect because its terminology means \"biology of the foreigners\".  Astrobiologists must also consider the possibility of life that is chemically entirely distinct from any life found on Earth. In the Solar System some of the prime locations for current or past astrobiology are on Enceladus, Europa, Mars, and Titan.\n\n\n=== Living in space ===\n\nSpace colonization, also called space settlement and space humanization, would be the permanent autonomous (self-sufficient) human habitation of locations outside Earth, especially of natural satellites or planets such as the Moon or Mars, using significant amounts of in-situ resource utilization.\nTo date, the longest human occupation of space is the International Space Station which has been in continuous use for 17 years, 272 days. Valeri Polyakov's record single spaceflight of almost 438 days aboard the Mir space station has not been surpassed. Long-term stays in space reveal issues with bone and muscle loss in low gravity, immune system suppression, and radiation exposure.\nMany past and current concepts for the continued exploration and colonization of space focus on a return to the Moon as a \"stepping stone\" to the other planets, especially Mars. At the end of 2006 NASA announced they were planning to build a permanent Moon base with continual presence by 2024.Beyond the technical factors that could make living in space more widespread, it has been suggested that the lack of private property, the inability or difficulty in establishing property rights in space, has been an impediment to the development of space for human habitation. Since the advent of space technology in the latter half of the twentieth century, the ownership of property in space has been murky, with strong arguments both for and against. In particular, the making of national territorial claims in outer space and on celestial bodies has been specifically proscribed by the Outer Space Treaty, which had been, as of  2012, ratified by all spacefaring nations.\n\n\n== See also ==\n\nDiscovery and exploration of the Solar System\nIn-space propulsion technologies\nList of missions to Mars\nList of missions to the outer planets\n\n\n=== Robotic space exploration programs ===\n\n\n=== Living in space ===\nInterplanetary contamination\n\n\n==== Animals in space ====\n\n\n==== Humans in space ====\n\n\n=== Recent and future developments ===\n\n\n=== Other ===\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\nSeth Shostak on Space Exploration\nChronology of space exploration, astrobiology, exoplanets and news\nSpace related news\nSpace Exploration Network\nNasa's website on human space travel\nNasa's website on space exploration technology\n\"America's Space Program: Exploring a New Frontier\", a National Park Service Teaching with Historic Places (TwHP) lesson plan\nThe Soviet-Russian Spaceflight's History Photoarchive\nThe 21 Greatest Space Photos Ever \u2013 slideshow by Life Magazine\n\"From Stargazers to Starships\", extensive educational web site and course covering spaceflight, astronomy and related physics\nWe Are The Explorers, NASA Promotional Video (Press Release)", "manufacturing": "Manufacturing is the  production of merchandise for use or sale using labour and machines, tools, chemical and biological processing, or formulation. The term may refer to a range of human activity, from handicraft to high tech, but is most commonly applied to industrial production, in which raw materials are transformed into finished goods on a large scale. Such finished goods may be sold to other manufacturers for the production of other, more complex products, such as aircraft, household appliances, furniture, sports equipment or automobiles, or sold to wholesalers, who in turn sell them to retailers, who then sell them to end users and consumers.\nManufacturing engineering or manufacturing process are the steps through which raw materials are transformed into a final product. The manufacturing process begins with the product design, and materials specification from which the product is made. These materials are then modified through manufacturing processes to become the required part.\nModern manufacturing includes all intermediate processes required in the production and integration of a product's components. Some industries, such as semiconductor and steel manufacturers use the term fabrication instead.\nThe manufacturing sector is closely connected with engineering and industrial design. Examples of major manufacturers in North America include General Motors Corporation, General Electric, Procter & Gamble, General Dynamics, Boeing, Pfizer, and Precision Castparts. Examples in Europe include Volkswagen Group, Siemens, FCA  and Michelin. Examples in Asia include Toyota, Yamaha, Panasonic, Mitsubishi, LG, Samsung and Tata Motors.\n\n\n== History and development ==\n\nIn its earliest form, manufacturing was usually carried out by a single skilled artisan with assistants. Training was by apprenticeship. In much of the pre-industrial world, the guild system protected the privileges and trade secrets of urban artisans.\nBefore the Industrial Revolution, most manufacturing occurred in rural areas, where household-based manufacturing served as a supplemental subsistence strategy to agriculture (and continues to do so in places). Entrepreneurs organized a number of manufacturing households into a single enterprise through the putting-out system.\nToll manufacturing is an arrangement whereby a first firm with specialized equipment processes raw materials or semi-finished goods for a second firm.\n\n\n=== Manufacturing systems: changes in methods of manufacturing ===\nManufacturing Engineering\nAgile manufacturing\nAmerican system of manufacturing\nBritish factory system of manufacturing\nCraft or guild system\nFabrication\nFlexible manufacturing\nJust-in-time manufacturing\nLean manufacturing\nMass customization (2000s) \u2013 3D printing, design-your-own web sites for sneakers, fast fashion\nMass production\nOwnership\nPackaging and labeling\nPrefabrication\nPutting-out system\nRapid manufacturing\nReconfigurable manufacturing system\nSoviet collectivism in manufacturing\nHistory of numerical control\n\n\n== Industrial policy ==\n\n\n=== Economics of manufacturing ===\nEmerging technologies have provided some new growth in advanced manufacturing employment opportunities in the Manufacturing Belt in the United States. Manufacturing provides important material support for national infrastructure and for national defense.\nOn the other hand, most manufacturing may involve significant social and environmental costs. The clean-up costs of hazardous waste, for example, may outweigh the benefits of a product that creates it. Hazardous materials may expose workers to health risks. These costs are now well known and there is effort to address them by improving efficiency, reducing waste, using industrial symbiosis, and eliminating harmful chemicals.\nThe negative costs of manufacturing can also be addressed legally. Developed countries regulate manufacturing activity with labor laws and environmental laws. Across the globe, manufacturers can be subject to regulations and pollution taxes to offset the environmental costs of manufacturing activities. Labor unions and craft guilds have played a historic role in the negotiation of worker rights and wages. Environment laws and labor protections that are available in developed nations may not be available in the third world. Tort law and product liability impose additional costs on manufacturing.  These are significant dynamics in the ongoing process, occurring over the last few decades, of manufacture-based industries relocating operations to \"developing-world\" economies where the costs of production are significantly lower than in \"developed-world\" economies.\n\n\n=== Manufacturing and investment ===\n\nSurveys and analyses of trends and issues in manufacturing and investment around the world focus on such things as:\n\nThe nature and sources of the considerable variations that occur cross-nationally in levels of manufacturing and wider industrial-economic growth;\nCompetitiveness; and\nAttractiveness to foreign direct investors.In addition to general overviews, researchers have examined the features and factors affecting particular key aspects of manufacturing development. They have compared production and investment in a range of Western and non-Western countries and presented case studies of growth and performance in important individual industries and market-economic sectors.On June 26, 2009, Jeff Immelt, the CEO of General Electric, called for the United States to increase its manufacturing base employment to 20% of the workforce, commenting that the U.S. has outsourced too much in some areas and can no longer rely on the financial sector and consumer spending to drive demand. Further, while U.S. manufacturing performs well compared to the rest of the U.S. economy, research shows that it performs poorly compared to manufacturing in other high-wage countries. A total of 3.2 million \u2013 one in six U.S. manufacturing jobs \u2013 have disappeared between 2000 and 2007. In the UK, EEF the manufacturers organisation has led calls for the UK economy to be rebalanced to rely less on financial services and has actively promoted the manufacturing agenda.\n\n\n== Countries by manufacturing output using the most recent known data ==\nList of top 20 manufacturing countries by total value of manufacturing in US dollars for its noted year according to Worldbank.\n\n\n== Manufacturing processes ==\nList of manufacturing processes\nManufacturing Process Management\n\n\n== Control ==\nManagement\nList of management topics\nTotal quality management\nQuality control\nSix Sigma\n\n\n== See also ==\n\nList of largest manufacturing companies by revenue\nIndustrial robot\nManufacturing engineering\nManufacturing in the United States\nIndustrial engineering\nAdvanced manufacturing\nMetal fabrication\nMicrofabrication\nOptics fabrication\nSemiconductor device fabrication\nBiomanufacturing\nMesoscale Manufacturing\nCyber manufacturing\nTaylorism/Scientific management\nFordism\nManufacturing Program of the US National Institute for Occupational Safety and Health\n\n\n== References ==\n\n\n== Sources ==\nKalpakjian, Serope; Steven Schmid (August 2005). Manufacturing, Engineering & Technology. Prentice Hall. pp. 22\u201336, 951\u201388. ISBN 0-13-148965-8. \n\n\n== External links ==\nHow Everyday Things Are Made: video presentations\nGrant Thornton IBR 2008 Manufacturing industry focus\nEEF, the manufacturers' organisation \u2013 industry group representing uk manufacturers \nIndustry Today \u2013 Industrial and Manufacturing Methodologies\nEnabling the Digital Thread for Smart Manufacturing\n \"Manufactures\". New International Encyclopedia. 1905.", "internet": "The Internet is the global system of interconnected computer networks that use the Internet protocol suite (TCP/IP) to link devices worldwide. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries a vast range of information resources and services, such as the inter-linked hypertext documents and applications of the World Wide Web (WWW), electronic mail, telephony, and file sharing.\nThe origins of the Internet date back to research commissioned by the federal government of the United States in the 1960s to build robust, fault-tolerant communication with computer networks. The primary precursor network, the ARPANET, initially served as a backbone for interconnection of regional academic and military networks in the 1980s. The funding of the National Science Foundation Network as a new backbone in the 1980s, as well as private funding for other commercial extensions, led to worldwide participation in the development of new networking technologies, and the merger of many networks. The linking of commercial networks and enterprises by the early 1990s marks the beginning of the transition to the modern Internet, and generated a sustained exponential growth as generations of institutional, personal, and mobile computers were connected to the network. Although the Internet was widely used by academia since the 1980s, the commercialization incorporated its services and technologies into virtually every aspect of modern life.\nMost traditional communications media, including telephony, radio, television, paper mail and newspapers are reshaped, redefined, or even bypassed by the Internet, giving birth to new services such as email, Internet telephony, Internet television, online music, digital newspapers, and video streaming websites. Newspaper, book, and other print publishing are adapting to website technology, or are reshaped into blogging, web feeds and online news aggregators. The Internet has enabled and accelerated new forms of personal interactions through instant messaging, Internet forums, and social networking. Online shopping has grown exponentially both for major retailers and small businesses and entrepreneurs, as it enables firms to extend their \"brick and mortar\" presence to serve a larger market or even sell goods and services entirely online.  Business-to-business and financial services on the Internet affect supply chains across entire industries.\nThe Internet has no centralized governance in either technological implementation or policies for access and usage; each constituent network sets its own policies. Only the overreaching definitions of the two principal name spaces in the Internet, the Internet Protocol address (IP address) space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise.\n\n\n== TerminologyEdit ==\n\nWhen the term Internet is used to refer to the specific global system of interconnected Internet Protocol (IP) networks, the word is a proper noun that should be written with an initial capital letter. In common use and the media, it is often erroneously not capitalized, viz. the internet. Some guides specify that the word should be capitalized when used as a noun, but not capitalized when used as an adjective. The Internet is also often referred to as the Net, as a short form of network. Historically, as early as 1849, the word internetted was used uncapitalized as an adjective, meaning interconnected or interwoven. The designers of early computer networks used internet both as a noun and as a verb in shorthand form of internetwork or internetworking, meaning interconnecting computer networks.The terms Internet and World Wide Web are often used interchangeably in everyday speech; it is common to speak of \"going on the Internet\" when using a web browser to view web pages. However, the World Wide Web or the Web is only one of a large number of Internet services. The Web is a collection of interconnected documents (web pages) and other web resources, linked by hyperlinks and URLs. As another point of comparison, Hypertext Transfer Protocol, or HTTP, is the language used on the Web for information transfer, yet it is just one of many languages or protocols that can be used for communication on the Internet. The term Interweb is a portmanteau of Internet and World Wide Web typically used sarcastically to parody a technically unsavvy user.\n\n\n== HistoryEdit ==\n\nResearch into packet switching, one of the fundamental Internet technologies started in the early 1960s in the work of Paul Baran, and packet switched networks such as the NPL network by Donald Davies, ARPANET, Tymnet, the Merit Network, Telenet, and CYCLADES, were developed in the late 1960s and 1970s using a variety of protocols. The ARPANET project led to the development of protocols for internetworking, by which multiple separate networks could be joined into a network of networks. ARPANET development began with two network nodes which were interconnected between the Network Measurement Center at the University of California, Los Angeles (UCLA) Henry Samueli School of Engineering and Applied Science directed by Leonard Kleinrock, and the NLS system at SRI International (SRI) by Douglas Engelbart in Menlo Park, California, on 29 October 1969. The third site was the Culler-Fried Interactive Mathematics Center at the University of California, Santa Barbara, followed by the University of Utah Graphics Department. In an early sign of future growth, fifteen sites were connected to the young ARPANET by the end of 1971. These early years were documented in the 1972 film Computer Networks: The Heralds of Resource Sharing.\nEarly international collaborations on the ARPANET were rare. European developers were concerned with developing the X.25 networks. Notable exceptions were the Norwegian Seismic Array (NORSAR) in June 1973, followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter T. Kirstein's research group in the United Kingdom, initially at the Institute of Computer Science, University of London and later at University College London. In December 1974, RFC 675 (Specification of Internet Transmission Control Program), by Vinton Cerf, Yogen Dalal, and Carl Sunshine, used the term internet as a shorthand for internetworking and later RFCs repeated this use.  Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET).  In 1982, the Internet Protocol Suite (TCP/IP) was standardized, which permitted worldwide proliferation of interconnected networks.\n\nTCP/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNet) provided access to supercomputer sites in the United States for researchers, first at speeds of 56 kbit/s and later at 1.5 Mbit/s and 45 Mbit/s. Commercial Internet service providers (ISPs) emerged in the late 1980s and early 1990s. The ARPANET was decommissioned in 1990. By 1995, the Internet was fully commercialized in the U.S. when the NSFNet was decommissioned, removing the last restrictions on use of the Internet to carry commercial traffic. The Internet rapidly expanded in Europe and Australia in the mid to late 1980s and to Asia in the late 1980s and early 1990s. The beginning of dedicated transatlantic communication between the NSFNET and networks in Europe was established with a low-speed satellite relay between Princeton University and Stockholm, Sweden in December 1988. Although other network protocols such as UUCP had global reach well before this time, this marked the beginning of the Internet as an intercontinental network.\nPublic commercial use of the Internet began in mid-1989 with the connection of MCI Mail and Compuserve's email capabilities to the 500,000 users of the Internet. Just months later on 1 January 1990, PSInet launched an alternate Internet backbone for commercial use; one of the networks that would grow into the commercial Internet we know today. In March 1990, the first high-speed T1 (1.5 Mbit/s) link between the NSFNET and Europe was installed between Cornell University and CERN, allowing much more robust communications than were capable with satellites. Six months later Tim Berners-Lee would begin writing WorldWideWeb, the first web browser after two years of lobbying CERN management. By Christmas 1990, Berners-Lee had built all the tools necessary for a working Web: the HyperText Transfer Protocol (HTTP) 0.9, the HyperText Markup Language (HTML), the first Web browser (which was also a HTML editor and could access Usenet newsgroups and FTP files), the first HTTP server software (later known as CERN httpd), the first web server, and the first Web pages that described the project itself. In 1991 the Commercial Internet eXchange was founded, allowing PSInet to communicate with the other commercial networks CERFnet and Alternet. Since 1995 the Internet has tremendously impacted culture and commerce, including the rise of near instant communication by email, instant messaging, telephony (Voice over Internet Protocol or VoIP), two-way interactive video calls, and the World Wide Web with its discussion forums, blogs, social networking, and online shopping sites. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1-Gbit/s, 10-Gbit/s, or more.\n\nThe Internet continues to grow, driven by ever greater amounts of online information and knowledge, commerce, entertainment and social networking. During the late 1990s, it was estimated that traffic on the public Internet grew by 100 percent per year, while the mean annual growth in the number of Internet users was thought to be between 20% and 50%. This growth is often attributed to the lack of central administration, which allows organic growth of the network, as well as the non-proprietary nature of the Internet protocols, which encourages vendor interoperability and prevents any one company from exerting too much control over the network. As of 31 March 2011, the estimated total number of Internet users was 2.095 billion (30.2% of world population). It is estimated that in 1993 the Internet carried only 1% of the information flowing through two-way telecommunication, by 2000 this figure had grown to 51%, and by 2007 more than 97% of all telecommunicated information was carried over the Internet.\n\n\n== GovernanceEdit ==\n\nThe Internet is a global network that comprises many voluntarily interconnected autonomous networks. It operates without a central governing body. The technical underpinning and standardization of the core protocols (IPv4 and IPv6) is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise. To maintain interoperability, the principal name spaces of the Internet are administered by the Internet Corporation for Assigned Names and Numbers (ICANN). ICANN is governed by an international board of directors drawn from across the Internet technical, business, academic, and other non-commercial communities. ICANN coordinates the assignment of unique identifiers for use on the Internet, including domain names, Internet Protocol (IP) addresses, application port numbers in the transport protocols, and many other parameters. Globally unified name spaces are essential for maintaining the global reach of the Internet.  This role of ICANN distinguishes it as perhaps the only central coordinating body for the global Internet.Regional Internet Registries (RIRs) allocate IP addresses:\n\nAfrican Network Information Center (AfriNIC) for Africa\nAmerican Registry for Internet Numbers (ARIN) for North America\nAsia-Pacific Network Information Centre (APNIC) for Asia and the Pacific region\nLatin American and Caribbean Internet Addresses Registry (LACNIC) for Latin America and the Caribbean region\nR\u00e9seaux IP Europ\u00e9ens \u2013 Network Coordination Centre (RIPE NCC) for Europe, the Middle East, and Central AsiaThe National Telecommunications and Information Administration, an agency of the United States Department of Commerce, had final approval over changes to the DNS root zone until the IANA stewardship transition on 1 October 2016. The Internet Society (ISOC) was founded in 1992 with a mission to \"assure the open development, evolution and use of the Internet for the benefit of all people throughout the world\". Its members include individuals (anyone may join) as well as corporations, organizations, governments, and universities. Among other activities ISOC provides an administrative home for a number of less formally organized groups that are involved in developing and managing the Internet, including: the Internet Engineering Task Force (IETF), Internet Architecture Board (IAB), Internet Engineering Steering Group (IESG), Internet Research Task Force (IRTF), and Internet Research Steering Group (IRSG). On 16 November 2005, the United Nations-sponsored World Summit on the Information Society in Tunis established the Internet Governance Forum (IGF) to discuss Internet-related issues.\n\n\n== InfrastructureEdit ==\n\nThe communications infrastructure of the Internet consists of its hardware components and a system of software layers that control various aspects of the architecture.\n\n\n=== Routing and service tiersEdit ===\n\nInternet service providers establish the worldwide connectivity between individual networks at various levels of scope. End-users who only access the Internet when needed to perform a function or obtain information, represent the bottom of the routing hierarchy. At the top of the routing hierarchy are the tier 1 networks, large telecommunication companies that exchange traffic directly with each other via peering agreements. Tier 2 and lower level networks buy Internet transit from other providers to reach at least some parties on the global Internet, though they may also engage in peering. An ISP may use a single upstream provider for connectivity, or implement multihoming to achieve redundancy and load balancing. Internet exchange points are major traffic exchanges with physical connections to multiple ISPs. Large organizations, such as academic institutions, large enterprises, and governments, may perform the same function as ISPs, engaging in peering and purchasing transit on behalf of their internal networks. Research networks tend to interconnect with large subnetworks such as GEANT, GLORIAD, Internet2, and the UK's national research and education network, JANET. Both the Internet IP routing structure and hypertext links of the World Wide Web are examples of scale-free networks. Computers and routers use routing tables in their operating system to direct IP packets to the next-hop router or destination. Routing tables are maintained by manual configuration or automatically by routing protocols. End-nodes typically use a default route that points toward an ISP providing transit, while ISP routers use the Border Gateway Protocol to establish the most efficient routing across the complex connections of the global Internet.\nAn estimated 70 percent of the world's Internet traffic passes through Ashburn, Virginia.\n\n\n=== AccessEdit ===\nCommon methods of Internet access by users include dial-up with a computer modem via telephone circuits, broadband over coaxial cable, fiber optics or copper wires, Wi-Fi, satellite, and cellular telephone technology (e.g. 3G, 4G). The Internet may often be accessed from computers in libraries and Internet cafes. Internet access points exist in many public places such as airport halls and coffee shops. Various terms are used, such as public Internet kiosk, public access terminal, and Web payphone. Many hotels also have public terminals that are usually fee-based. These terminals are widely accessed for various usages, such as ticket booking, bank deposit, or online payment. Wi-Fi provides wireless access to the Internet via local computer networks. Hotspots providing such access include Wi-Fi cafes, where users need to bring their own wireless devices such as a laptop or PDA. These services may be free to all, free to customers only, or fee-based.\nGrassroots efforts have led to wireless community networks. Commercial Wi-Fi services covering large city areas are in many cities, such as New York, London, Vienna, Toronto, San Francisco, Philadelphia, Chicago and Pittsburgh. The Internet can then be accessed from places, such as a park bench. Apart from Wi-Fi, there have been experiments with proprietary mobile wireless networks like Ricochet, various high-speed data services over cellular phone networks, and fixed wireless services. High-end mobile phones such as smartphones in general come with Internet access through the phone network. Web browsers such as Opera are available on these advanced handsets, which can also run a wide variety of other Internet software. More mobile phones have Internet access than PCs, although this is not as widely used. An Internet access provider and protocol matrix differentiates the methods used to get online.\n\n\n==== Internet and mobileEdit ====\n\nAccording to the International Telecommunication Union (ITU), by the end of 2017, an estimated 48 per cent of individuals regularly connect to the internet, up from 34 per cent in 2012. Mobile internet connectivity has played an important role in expanding access in recent years especially in Asia and the Pacific and in Africa. The number of unique mobile cellular subscriptions increased from 3.89 billion in 2012 to 4.83 billion in 2016, two-thirds of the world\u2019s population, with more than half of subscriptions located in Asia and the Pacific. The number of subscriptions is predicted to rise to 5.69 billion users in 2020. As of 2016, almost 60 per cent of the world\u2019s population had access to a 4G broadband cellular network, up from almost 50 per cent in 2015 and 11 per cent in 2012. The limits that users face on accessing information via mobile applications coincide with a broader process of fragmentation of the internet. Fragmentation restricts access to media content and tends to affect poorest users the most.Zero-rating, the practice of internet providers allowing users free connectivity to access specific content or applications for free, has offered some opportunities for individuals to surmount economic hurdles, but has also been accused by its critics as creating a \u2018two-tiered\u2019 internet. To address the issues with zero-rating, an alternative model has emerged in the concept of \u2018equal rating\u2019 and is being tested in experiments by Mozilla and Orange in Africa. Equal rating prevents prioritization of one type of content and zero-rates all content up to a specified data cap. A study published by Chatham House, 15 out of 19 countries researched in Latin America had some kind of hybrid or zero-rated product offered. Some countries in the region had a handful of plans to choose from (across all mobile network operators) while others, such as Colombia, offered as many as 30 pre-paid and 34 post-paid plans.A study of eight countries in the Global South found that zero-rated data plans exist in every country, although there is a great range in the frequency with which they are offered and actually used in each. Across the 181 plans examined, 13 per cent were offering zero-rated services. Another study, covering Ghana, Kenya, Nigeria and South Africa, found Facebook\u2019s Free Basics and Wikipedia Zero to be the most commonly zero-rated content.\n\n\n== ProtocolsEdit ==\nWhile the hardware components in the Internet infrastructure can often be used to support other software systems, it is the design and the standardization process of the software that characterizes the Internet and provides the foundation for its scalability and success. The responsibility for the architectural design of the Internet software systems has been assumed by the Internet Engineering Task Force (IETF). The IETF conducts standard-setting work groups, open to any individual, about the various aspects of Internet architecture. Resulting contributions and standards are published as Request for Comments (RFC) documents on the IETF web site. The principal methods of networking that enable the Internet are contained in specially designated RFCs that constitute the Internet Standards. Other less rigorous documents are simply informative, experimental, or historical, or document the best current practices (BCP) when implementing Internet technologies.\nThe Internet standards describe a framework known as the Internet protocol suite. This is a model architecture that divides methods into a layered system of protocols, originally documented in RFC 1122 and RFC 1123. The layers correspond to the environment or scope in which their services operate. At the top is the application layer, space for the application-specific networking methods used in software applications. For example, a web browser program uses the client-server application model and a specific protocol of interaction between servers and clients, while many file-sharing systems use a peer-to-peer paradigm. Below this top layer, the transport layer connects applications on different hosts with a logical channel through the network with appropriate data exchange methods.\nUnderlying these layers are the networking technologies that interconnect networks at their borders and exchange traffic across them. The Internet layer enables computers to identify and locate each other via Internet Protocol (IP) addresses, and routes their traffic via intermediate (transit) networks. Last, at the bottom of the architecture is the link layer, which provides logical connectivity between hosts on the same network link, such as a local area network (LAN) or a dial-up connection. The model, also known as TCP/IP, is designed to be independent of the underlying hardware used for the physical connections, which the model does not concern itself with in any detail. Other models have been developed, such as the OSI model, that attempt to be comprehensive in every aspect of communications. While many similarities exist between the models, they are not compatible in the details of description or implementation. Yet, TCP/IP protocols are usually included in the discussion of OSI networking.\n\nThe most prominent component of the Internet model is the Internet Protocol (IP), which provides addressing systems, including IP addresses, for computers on the network. IP enables internetworking and, in essence, establishes the Internet itself. Internet Protocol Version 4 (IPv4) is the initial version used on the first generation of the Internet and is still in dominant use. It was designed to address up to ~4.3 billion (109) hosts. However, the explosive growth of the Internet has led to IPv4 address exhaustion, which entered its final stage in 2011, when the global address allocation pool was exhausted.  A new protocol version, IPv6, was developed in the mid-1990s, which provides vastly larger addressing capabilities and more efficient routing of Internet traffic. IPv6 is currently in growing deployment around the world, since Internet address registries (RIRs) began to urge all resource managers to plan rapid adoption and conversion.IPv6 is not directly interoperable by design with IPv4. In essence, it establishes a parallel version of the Internet not directly accessible with IPv4 software. Thus, translation facilities must exist for internetworking or nodes must have duplicate networking software for both networks. Essentially all modern computer operating systems support both versions of the Internet Protocol. Network infrastructure, however, has been lagging in this development. Aside from the complex array of physical connections that make up its infrastructure, the Internet is facilitated by bi- or multi-lateral commercial contracts, e.g., peering agreements, and by technical specifications or protocols that describe the exchange of data over the network. Indeed, the Internet is defined by its interconnections and routing policies.\n\n\n== ServicesEdit ==\nThe Internet carries many network services, most prominently mobile apps such as social media apps, the World Wide Web, electronic mail, multiplayer online games, Internet telephony, and file sharing services.\n\n\n=== World Wide WebEdit ===\n\nMany people use, erroneously, the terms Internet and World Wide Web, or just the Web, interchangeably, but the two terms are not synonymous. The World Wide Web is the primary application program that billions of people use on the Internet, and it has changed their lives immeasurably. However, the Internet provides many other services. The Web is a global set of documents, images and other resources, logically interrelated by hyperlinks and referenced with Uniform Resource Identifiers (URIs). URIs symbolically identify services, servers, and other databases, and the documents and resources that they can provide. Hypertext Transfer Protocol (HTTP) is the main access protocol of the World Wide Web. Web services also use HTTP to allow software systems to communicate in order to share and exchange business logic and data.\nWorld Wide Web browser software, such as Microsoft's Internet Explorer/Edge, Mozilla Firefox, Opera, Apple's Safari, and Google Chrome, lets users navigate from one web page to another via hyperlinks embedded in the documents. These documents may also contain any combination of computer data, including graphics, sounds, text, video, multimedia and interactive content that runs while the user is interacting with the page. Client-side software can include animations, games, office applications and scientific demonstrations. Through keyword-driven Internet research using search engines like Yahoo!, Bing and Google, users worldwide have easy, instant access to a vast and diverse amount of online information. Compared to printed media, books, encyclopedias and traditional libraries, the World Wide Web has enabled the decentralization of information on a large scale.\nThe Web has also enabled individuals and organizations to publish ideas and information to a potentially large audience online at greatly reduced expense and time delay. Publishing a web page, a blog, or building a website involves little initial cost and many cost-free services are available. However, publishing and maintaining large, professional web sites with attractive, diverse and up-to-date information is still a difficult and expensive proposition. Many individuals and some companies and groups use web logs or blogs, which are largely used as easily updatable online diaries. Some commercial organizations encourage staff to communicate advice in their areas of specialization in the hope that visitors will be impressed by the expert knowledge and free information, and be attracted to the corporation as a result.\nAdvertising on popular web pages can be lucrative, and e-commerce, which is the sale of products and services directly via the Web, continues to grow. Online advertising is a form of marketing and advertising which uses the Internet to deliver promotional marketing messages to consumers.  It includes email marketing, search engine marketing (SEM), social media marketing, many types of display advertising (including web banner advertising), and mobile advertising. In 2011, Internet advertising revenues in the United States surpassed those of cable television and nearly exceeded those of broadcast television. Many common online advertising practices are controversial and increasingly subject to regulation.\nWhen the Web developed in the 1990s, a typical web page was stored in completed form on a web server, formatted in HTML, complete for transmission to a web browser in response to a request. Over time, the process of creating and serving web pages has become dynamic, creating a flexible design, layout, and content. Websites are often created using content management software with, initially, very little content. Contributors to these systems, who may be paid staff, members of an organization or the public, fill underlying databases with content using editing pages designed for that purpose while casual visitors view and read this content in HTML form. There may or may not be editorial, approval and security systems built into the process of taking newly entered content and making it available to the target visitors.\n\n\n=== CommunicationEdit ===\nEmail is an important communications service available on the Internet. The concept of sending electronic text messages between parties in a way analogous to mailing letters or memos predates the creation of the Internet. Pictures, documents, and other files are sent as email attachments. Emails can be cc-ed to multiple email addresses.\nInternet telephony is another common communications service made possible by the creation of the Internet. VoIP stands for Voice-over-Internet Protocol, referring to the protocol that underlies all Internet communication. The idea began in the early 1990s with walkie-talkie-like voice applications for personal computers. In recent years many VoIP systems have become as easy to use and as convenient as a normal telephone. The benefit is that, as the Internet carries the voice traffic, VoIP can be free or cost much less than a traditional telephone call, especially over long distances and especially for those with always-on Internet connections such as cable or ADSL. VoIP is maturing into a competitive alternative to traditional telephone service. Interoperability between different providers has improved and the ability to call or receive a call from a traditional telephone is available. Simple, inexpensive VoIP network adapters are available that eliminate the need for a personal computer.\nVoice quality can still vary from call to call, but is often equal to and can even exceed that of traditional calls. Remaining problems for VoIP include emergency telephone number dialing and reliability. Currently, a few VoIP providers provide an emergency service, but it is not universally available. Older traditional phones with no \"extra features\" may be line-powered only and operate during a power failure; VoIP can never do so without a backup power source for the phone equipment and the Internet access devices. VoIP has also become increasingly popular for gaming applications, as a form of communication between players. Popular VoIP clients for gaming include Ventrilo and Teamspeak. Modern video game consoles also offer VoIP chat features.\n\n\n=== Data transferEdit ===\nFile sharing is an example of transferring large amounts of data across the Internet. A computer file can be emailed to customers, colleagues and friends as an attachment. It can be uploaded to a website or File Transfer Protocol (FTP) server for easy download by others. It can be put into a \"shared location\" or onto a file server for instant use by colleagues. The load of bulk downloads to many users can be eased by the use of \"mirror\" servers or peer-to-peer networks. In any of these cases, access to the file may be controlled by user authentication, the transit of the file over the Internet may be obscured by encryption, and money may change hands for access to the file. The price can be paid by the remote charging of funds from, for example, a credit card whose details are also passed \u2013 usually fully encrypted \u2013 across the Internet. The origin and authenticity of the file received may be checked by digital signatures or by MD5 or other message digests. These simple features of the Internet, over a worldwide basis, are changing the production, sale, and distribution of anything that can be reduced to a computer file for transmission. This includes all manner of print publications, software products, news, music, film, video, photography, graphics and the other arts. This in turn has caused seismic shifts in each of the existing industries that previously controlled the production and distribution of these products.\nStreaming media is the real-time delivery of digital media for the immediate consumption or enjoyment by end users. Many radio and television broadcasters provide Internet feeds of their live audio and video productions. They may also allow time-shift viewing or listening such as Preview, Classic Clips and Listen Again features. These providers have been joined by a range of pure Internet \"broadcasters\" who never had on-air licenses. This means that an Internet-connected device, such as a computer or something more specific, can be used to access on-line media in much the same way as was previously possible only with a television or radio receiver. The range of available types of content is much wider, from specialized technical webcasts to on-demand popular multimedia services. Podcasting is a variation on this theme, where \u2013 usually audio \u2013 material is downloaded and played back on a computer or shifted to a portable media player to be listened to on the move. These techniques using simple equipment allow anybody, with little censorship or licensing control, to broadcast audio-visual material worldwide.\nDigital media streaming increases the demand for network bandwidth.  For example, standard image quality needs 1 Mbit/s link speed for SD 480p, HD 720p quality requires 2.5 Mbit/s, and the top-of-the-line HDX quality needs 4.5 Mbit/s for 1080p.Webcams are a low-cost extension of this phenomenon. While some webcams can give full-frame-rate video, the picture either is usually small or updates slowly. Internet users can watch animals around an African waterhole, ships in the Panama Canal, traffic at a local roundabout or monitor their own premises, live and in real time. Video chat rooms and video conferencing are also popular with many uses being found for personal webcams, with and without two-way sound. YouTube was founded on 15 February 2005 and is now the leading website for free streaming video with a vast number of users. It uses a flash-based web player to stream and show video files. Registered users may upload an unlimited amount of video and build their own personal profile. YouTube claims that its users watch hundreds of millions, and upload hundreds of thousands of videos daily.\nCurrently, YouTube also uses an HTML5 player.\n\n\n== Social impactEdit ==\nThe Internet has enabled new forms of social interaction, activities, and social associations. This phenomenon has given rise to the scholarly study of the sociology of the Internet.\n\n\n=== UsersEdit ===\n\nInternet usage has seen tremendous growth. From 2000 to 2009, the number of Internet users globally rose from 394 million to 1.858 billion. By 2010, 22 percent of the world's population had access to computers with 1 billion Google searches every day, 300 million Internet users reading blogs, and 2 billion videos viewed daily on YouTube. In 2014 the world's Internet users surpassed 3 billion or 43.6 percent of world population, but two-thirds of the users came from richest countries, with 78.0 percent of Europe countries population using the Internet, followed by 57.4 percent of the Americas.The prevalent language for communication on the Internet has been English. This may be a result of the origin of the Internet, as well as the language's role as a lingua franca. Early computer systems were limited to the characters in the American Standard Code for Information Interchange (ASCII), a subset of the Latin alphabet.\nAfter English (27%), the most requested languages on the World Wide Web are Chinese (25%), Spanish (8%), Japanese (5%), Portuguese and German (4% each), Arabic, French and Russian (3% each), and Korean (2%). By region, 42% of the world's Internet users are based in Asia, 24% in Europe, 14% in North America, 10% in Latin America and the Caribbean taken together, 6% in Africa, 3% in the Middle East and 1% in Australia/Oceania.  The Internet's technologies have developed enough in recent years, especially in the use of Unicode, that good facilities are available for development and communication in the world's widely used languages. However, some glitches such as mojibake (incorrect display of some languages' characters) still remain.\nIn an American study in 2005, the percentage of men using the Internet was very slightly ahead of the percentage of women, although this difference reversed in those under 30. Men logged on more often, spent more time online, and were more likely to be broadband users, whereas women tended to make more use of opportunities to communicate (such as email). Men were more likely to use the Internet to pay bills, participate in auctions, and for recreation such as downloading music and videos. Men and women were equally likely to use the Internet for shopping and banking.\nMore recent studies indicate that in 2008, women significantly outnumbered men on most social networking sites, such as Facebook and Myspace, although the ratios varied with age. In addition, women watched more streaming content, whereas men downloaded more. In terms of blogs, men were more likely to blog in the first place; among those who blog, men were more likely to have a professional blog, whereas women were more likely to have a personal blog.Forecasts predict that 44% of the world's population will be users of the Internet by 2020. Splitting by country, in 2012 Iceland, Norway, Sweden, the Netherlands, and Denmark had the highest Internet penetration by the number of users, with 93% or more of the population with access.Several neologisms exist that refer to Internet users: Netizen (as in as in \"citizen of the net\") refers to those actively involved in improving online communities, the Internet in general or surrounding political affairs and rights such as free speech, Internaut refers to operators or technically highly capable users of the Internet, digital citizen refers to a person using the Internet in order to engage in society, politics, and government participation.\n\n\n=== UsageEdit ===\nThe Internet allows greater flexibility in working hours and location, especially with the spread of unmetered high-speed connections. The Internet can be accessed almost anywhere by numerous means, including through mobile Internet devices. Mobile phones, datacards, handheld game consoles and cellular routers allow users to connect to the Internet wirelessly. Within the limitations imposed by small screens and other limited facilities of such pocket-sized devices, the services of the Internet, including email and the web, may be available. Service providers may restrict the services offered and mobile data charges may be significantly higher than other access methods.\nEducational material at all levels from pre-school to post-doctoral is available from websites. Examples range from CBeebies, through school and high-school revision guides and virtual universities, to access to top-end scholarly literature through the likes of Google Scholar. For distance education, help with homework and other assignments, self-guided learning, whiling away spare time, or just looking up more detail on an interesting fact, it has never been easier for people to access educational information at any level from anywhere. The Internet in general and the World Wide Web in particular are important enablers of both formal and informal education. Further, the Internet allows universities, in particular, researchers from the social and behavioral sciences, to conduct research remotely via virtual laboratories, with profound changes in reach and generalizability of findings as well as in communication between scientists and in the publication of results.The low cost and nearly instantaneous sharing of ideas, knowledge, and skills have made collaborative work dramatically easier, with the help of collaborative software. Not only can a group cheaply communicate and share ideas but the wide reach of the Internet allows such groups more easily to form. An example of this is the free software movement, which has produced, among other things, Linux, Mozilla Firefox, and OpenOffice.org (later forked into LibreOffice). Internet chat, whether using an IRC chat room, an instant messaging system, or a social networking website, allows colleagues to stay in touch in a very convenient way while working at their computers during the day.  Messages can be exchanged even more quickly and conveniently than via email. These systems may allow files to be exchanged, drawings and images to be shared, or voice and video contact between team members.\nContent management systems allow collaborating teams to work on shared sets of documents simultaneously without accidentally destroying each other's work. Business and project teams can share calendars as well as documents and other information. Such collaboration occurs in a wide variety of areas including scientific research, software development, conference planning, political activism and creative writing. Social and political collaboration is also becoming more widespread as both Internet access and computer literacy spread.\nThe Internet allows computer users to remotely access other computers and information stores easily from any access point. Access may be with computer security, i.e. authentication and encryption technologies, depending on the requirements. This is encouraging new ways of working from home, collaboration and information sharing in many industries. An accountant sitting at home can audit the books of a company based in another country, on a server situated in a third country that is remotely maintained by IT specialists in a fourth. These accounts could have been created by home-working bookkeepers, in other remote locations, based on information emailed to them from offices all over the world. Some of these things were possible before the widespread use of the Internet, but the cost of private leased lines would have made many of them infeasible in practice. An office worker away from their desk, perhaps on the other side of the world on a business trip or a holiday, can access their emails, access their data using cloud computing, or open a remote desktop session into their office PC using a secure virtual private network (VPN) connection on the Internet. This can give the worker complete access to all of their normal files and data, including email and other applications, while away from the office. It has been referred to among system administrators as the Virtual Private Nightmare, because it extends the secure perimeter of a corporate network into remote locations and its employees' homes.\n\n\n=== Social networking and entertainmentEdit ===\n\nMany people use the World Wide Web to access news, weather and sports reports, to plan and book vacations and to pursue their personal interests. People use chat, messaging and email to make and stay in touch with friends worldwide, sometimes in the same way as some previously had pen pals. Social networking websites such as Facebook, Twitter, and Myspace have created new ways to socialize and interact. Users of these sites are able to add a wide variety of information to pages, to pursue common interests, and to connect with others. It is also possible to find existing acquaintances, to allow communication among existing groups of people. Sites like LinkedIn foster commercial and business connections. YouTube and Flickr specialize in users' videos and photographs. While social networking sites were initially for individuals only, today they are widely used by businesses and other organizations to promote their brands, to market to their customers and to encourage posts to \"go viral\". \"Black hat\" social media techniques are also employed by some organizations, such as spam accounts and astroturfing.\nA risk for both individuals and organizations writing posts (especially public posts) on social networking websites, is that especially foolish or controversial posts occasionally lead to an unexpected and possibly large-scale backlash on social media from other Internet users. This is also a risk in relation to controversial offline behavior, if it is widely made known. The nature of this backlash can range widely from counter-arguments and public mockery, through insults and hate speech, to, in extreme cases, rape and death threats. The online disinhibition effect describes the tendency of many individuals to behave more stridently or offensively online than they would in person. A significant number of feminist women have been the target of various forms of harassment in response to posts they have made on social media, and Twitter in particular has been criticised in the past for not doing enough to aid victims of online abuse.For organizations, such a backlash can cause overall brand damage, especially if reported by the media. However, this is not always the case, as any brand damage in the eyes of people with an opposing opinion to that presented by the organization could sometimes be outweighed by strengthening the brand in the eyes of others. Furthermore, if an organization or individual gives in to demands that others perceive as wrong-headed, that can then provoke a counter-backlash.\nSome websites, such as Reddit, have rules forbidding the posting of personal information of individuals (also known as doxxing), due to concerns about such postings leading to mobs of large numbers of Internet users directing harassment at the specific individuals thereby identified. In particular, the Reddit rule forbidding the posting of personal information is widely understood to imply that all identifying photos and names must be censored in Facebook screenshots posted to Reddit. However, the interpretation of this rule in relation to public Twitter posts is less clear, and in any case, like-minded people online have many other ways they can use to direct each other's attention to public social media posts they disagree with.\nChildren also face dangers online such as cyberbullying and approaches by sexual predators, who sometimes pose as children themselves. Children may also encounter material which they may find upsetting, or material which their parents consider to be not age-appropriate. Due to naivety, they may also post personal information about themselves online, which could put them or their families at risk unless warned not to do so. Many parents choose to enable Internet filtering, and/or supervise their children's online activities, in an attempt to protect their children from inappropriate material on the Internet. The most popular social networking websites, such as Facebook and Twitter, commonly forbid users under the age of 13. However, these policies are typically trivial to circumvent by registering an account with a false birth date, and a significant number of children aged under 13 join such sites anyway. Social networking sites for younger children, which claim to provide better levels of protection for children, also exist.The Internet has been a major outlet for leisure activity since its inception, with entertaining social experiments such as MUDs and MOOs being conducted on university servers, and humor-related Usenet groups receiving much traffic. Many Internet forums have sections devoted to games and funny videos. The Internet pornography and online gambling industries have taken advantage of the World Wide Web, and often provide a significant source of advertising revenue for other websites. Although many governments have attempted to restrict both industries' use of the Internet, in general, this has failed to stop their widespread popularity.Another area of leisure activity on the Internet is multiplayer gaming. This form of recreation creates communities, where people of all ages and origins enjoy the fast-paced world of multiplayer games. These range from MMORPG to first-person shooters, from role-playing video games to online gambling. While online gaming has been around since the 1970s, modern modes of online gaming began with subscription services such as GameSpy and MPlayer. Non-subscribers were limited to certain types of game play or certain games. Many people use the Internet to access and download music, movies and other works for their enjoyment and relaxation. Free and fee-based services exist for all of these activities, using centralized servers and distributed peer-to-peer technologies. Some of these sources exercise more care with respect to the original artists' copyrights than others.\nInternet usage has been correlated to users' loneliness.  Lonely people tend to use the Internet as an outlet for their feelings and to share their stories with others, such as in the \"I am lonely will anyone speak to me\" thread.\nCybersectarianism is a new organizational form which involves: \"highly dispersed small groups of practitioners that may remain largely anonymous within the larger social context and operate in relative secrecy, while still linked remotely to a larger network of believers who share a set of practices and texts, and often a common devotion to a particular leader.  Overseas supporters provide funding and support; domestic practitioners distribute tracts, participate in acts of resistance, and share information on the internal situation with outsiders.  Collectively, members and practitioners of such sects construct viable virtual communities of faith, exchanging personal testimonies and engaging in the collective study via email, on-line chat rooms, and web-based message boards.\" In particular, the British government has raised concerns about the prospect of young British Muslims being indoctrinated into Islamic extremism by material on the Internet, being persuaded to join terrorist groups such as the so-called \"Islamic State\", and then potentially committing acts of terrorism on returning to Britain after fighting in Syria or Iraq.\nCyberslacking can become a drain on corporate resources; the average UK employee spent 57 minutes a day surfing the Web while at work, according to a 2003 study by Peninsula Business Services. Internet addiction disorder is excessive computer use that interferes with daily life. Nicholas G. Carr believes that Internet use has other effects on individuals, for instance improving skills of scan-reading and interfering with the deep thinking that leads to true creativity.\n\n\n=== Electronic businessEdit ===\nElectronic business (e-business) encompasses business processes spanning the entire value chain: purchasing, supply chain management, marketing, sales, customer service, and business relationship. E-commerce seeks to add revenue streams using the Internet to build and enhance relationships with clients and partners. According to International Data Corporation, the size of worldwide e-commerce, when global business-to-business and -consumer transactions are combined, equate to $16 trillion for 2013. A report by Oxford Economics adds those two together to estimate the total size of the digital economy at $20.4 trillion, equivalent to roughly 13.8% of global sales.While much has been written of the economic advantages of Internet-enabled commerce, there is also evidence that some aspects of the Internet such as maps and location-aware services may serve to reinforce economic inequality and the digital divide. Electronic commerce may be responsible for consolidation and the decline of mom-and-pop, brick and mortar businesses resulting in increases in income inequality.Author Andrew Keen, a long-time critic of the social transformations caused by the Internet, has recently focused on the economic effects of consolidation from Internet businesses. Keen cites a 2013 Institute for Local Self-Reliance report saying brick-and-mortar retailers employ 47 people for every $10 million in sales while Amazon employs only 14. Similarly, the 700-employee room rental start-up Airbnb was valued at $10 billion in 2014, about half as much as Hilton Hotels, which employs 152,000 people. And car-sharing Internet startup Uber employs 1,000 full-time employees and is valued at $18.2 billion, about the same valuation as Avis and Hertz combined, which together employ almost 60,000 people.\n\n\n=== TelecommutingEdit ===\nTelecommuting is the performance within a traditional worker and employer relationship when it is facilitated by tools such as groupware, virtual private networks, conference calling, videoconferencing, and voice over IP (VOIP) so that work may be performed from any location, most conveniently the worker's home. It can be efficient and useful for companies as it allows workers to communicate over long distances, saving significant amounts of travel time and cost. As broadband Internet connections become commonplace, more workers have adequate bandwidth at home to use these tools to link their home to their corporate intranet and internal communication networks.\n\n\n=== Collaborative publishingEdit ===\nWikis have also been used in the academic community for sharing and dissemination of information across institutional and international boundaries. In those settings, they have been found useful for collaboration on grant writing, strategic planning, departmental documentation, and committee work. The United States Patent and Trademark Office uses a wiki to allow the public to collaborate on finding prior art relevant to examination of pending patent applications. Queens, New York has used a wiki to allow citizens to collaborate on the design and planning of a local park. The English Wikipedia has the largest user base among wikis on the World Wide Web and ranks in the top 10 among all Web sites in terms of traffic.\n\n\n=== Politics and political revolutionsEdit ===\n\nThe Internet has achieved new relevance as a political tool. The presidential campaign of Howard Dean in 2004 in the United States was notable for its success in soliciting donation via the Internet. Many political groups use the Internet to achieve a new method of organizing for carrying out their mission, having given rise to Internet activism, most notably practiced by rebels in the Arab Spring. The New York Times suggested that social media websites, such as Facebook and Twitter, helped people organize the political revolutions in Egypt, by helping activists organize protests, communicate grievances, and disseminate information.Many have understood the Internet as an extension of the Habermasian notion of the public sphere, observing how network communication technologies provide something like a global civic forum. However, incidents of politically motivated Internet censorship have now been recorded in many countries, including western democracies.\n\n\n=== PhilanthropyEdit ===\nThe spread of low-cost Internet access in developing countries has opened up new possibilities for peer-to-peer charities, which allow individuals to contribute small amounts to charitable projects for other individuals.  Websites, such as DonorsChoose and GlobalGiving, allow small-scale donors to direct funds to individual projects of their choice. A popular twist on Internet-based philanthropy is the use of peer-to-peer lending for charitable purposes. Kiva pioneered this concept in 2005, offering the first web-based service to publish individual loan profiles for funding. Kiva raises funds for local intermediary microfinance organizations which post stories and updates on behalf of the borrowers. Lenders can contribute as little as $25 to loans of their choice, and receive their money back as borrowers repay. Kiva falls short of being a pure peer-to-peer charity, in that loans are disbursed before being funded by lenders and borrowers do not communicate with lenders themselves.However, the recent spread of low-cost Internet access in developing countries has made genuine international person-to-person  philanthropy increasingly feasible. In 2009, the US-based nonprofit Zidisha tapped into this trend to offer the first person-to-person microfinance platform to link lenders and borrowers across international borders without intermediaries. Members can fund loans for as little as a dollar, which the borrowers then use to develop business activities that improve their families' incomes while repaying loans to the members with interest. Borrowers access the Internet via public cybercafes, donated laptops in village schools, and even smart phones, then create their own profile pages through which they share photos and information about themselves and their businesses. As they repay their loans, borrowers continue to share updates and dialogue with lenders via their profile pages. This direct web-based connection allows members themselves to take on many of the communication and recording tasks traditionally performed by local organizations, bypassing geographic barriers and dramatically reducing the cost of microfinance services to the entrepreneurs.\n\n\n== SecurityEdit ==\n\nInternet resources, hardware, and software components are the target of criminal or malicious attempts to gain unauthorized control to cause interruptions, commit fraud, engage in blackmail or access private information.\n\n\n=== MalwareEdit ===\n\nMalicious software used and spread on the Internet includes computer viruses which copy with the help of humans, computer worms which copy themselves automatically, software for denial of service attacks, ransomware, botnets, and spyware that reports on the activity and typing of users. Usually, these activities constitute cybercrime. Defense theorists have also speculated about the possibilities of cyber warfare using similar methods on a large scale.\n\n\n=== SurveillanceEdit ===\n\nThe vast majority of computer surveillance involves the monitoring of data and traffic on the Internet. In the United States for example, under the Communications Assistance For Law Enforcement Act, all phone calls and broadband Internet traffic (emails, web traffic, instant messaging, etc.) are required to be available for unimpeded real-time monitoring by Federal law enforcement agencies. Packet capture is the monitoring of data traffic on a computer network. Computers communicate over the Internet by breaking up messages (emails, images, videos, web pages, files, etc.) into small chunks called \"packets\", which are routed through a network of computers, until they reach their destination, where they are assembled back into a complete \"message\" again. Packet Capture Appliance intercepts these packets as they are traveling through the network, in order to examine their contents using other programs. A packet capture is an information gathering tool, but not an analysis tool. That is it gathers \"messages\" but it does not analyze them and figure out what they mean. Other programs are needed to perform traffic analysis and sift through intercepted data looking for important/useful information. Under the Communications Assistance For Law Enforcement Act all U.S. telecommunications providers are required to install packet sniffing technology to allow Federal law enforcement and intelligence agencies to intercept all of their customers' broadband Internet and voice over Internet protocol (VoIP) traffic.The large amount of data gathered from packet capturing requires surveillance software that filters and reports relevant information, such as the use of certain words or phrases, the access of certain types of web sites, or communicating via email or chat with certain parties. Agencies, such as the Information Awareness Office, NSA, GCHQ and the FBI, spend billions of dollars per year to develop, purchase, implement, and operate systems for interception and analysis of data. Similar systems are operated by Iranian secret police to identify and suppress dissidents. The required hardware and software was allegedly installed by German Siemens AG and Finnish Nokia.\n\n\n=== CensorshipEdit ===\n\nSome governments, such as those of Burma, Iran, North Korea, the Mainland China, Saudi Arabia and the United Arab Emirates restrict access to content on the Internet within their territories, especially to political and religious content, with domain name and keyword filters.In Norway, Denmark, Finland, and Sweden, major Internet service providers have voluntarily agreed to restrict access to sites listed by authorities. While this list of forbidden resources is supposed to contain only known child pornography sites, the content of the list is secret. Many countries, including the United States, have enacted laws against the possession or distribution of certain material, such as child pornography, via the Internet, but do not mandate filter software. Many free or commercially available software programs, called content-control software are available to users to block offensive websites on individual computers or networks, in order to limit access by children to pornographic material or depiction of violence.\n\n\n== PerformanceEdit ==\nAs the Internet is a heterogeneous network, the physical characteristics, including for example the data transfer rates of connections, vary widely. It exhibits emergent phenomena that depend on its large-scale organization.\n\n\n=== OutagesEdit ===\nAn Internet blackout or outage can be caused by local signalling interruptions. Disruptions of submarine communications cables may cause blackouts or slowdowns to large areas, such as in the 2008 submarine cable disruption.  Less-developed countries are more vulnerable due to a small number of high-capacity links.  Land cables are also vulnerable, as in 2011 when a woman digging for scrap metal severed most connectivity for the nation of Armenia. Internet blackouts affecting almost entire countries can be achieved by governments as a form of Internet censorship, as in the blockage of the Internet in Egypt, whereby approximately 93% of networks were without access in 2011 in an attempt to stop mobilization for anti-government protests.\n\n\n=== Energy useEdit ===\nIn 2011, researchers estimated the energy used by the Internet to be between 170 and 307 GW, less than two percent of the energy used by humanity. This estimate included the energy needed to build, operate, and periodically replace the estimated 750 million laptops, a billion smart phones and 100 million servers worldwide as well as the energy that routers, cell towers, optical switches, Wi-Fi transmitters and cloud storage devices use when transmitting Internet traffic.\n\n\n== See alsoEdit ==\n\n\n== SourcesEdit ==\n This article incorporates text from a free content work.  License statement: World Trends in Freedom of Expression and Media Development Global Report 2017/2018, 202,  UNESCO. To learn how to add open license text to Wikipedia articles, please see Wikipedia:Adding open license text to Wikipedia. For information on reusing text from Wikipedia, please see the terms of use.\n\n\n== ReferencesEdit ==\n\n\n== Further readingEdit ==\nFirst Monday, a peer-reviewed journal on the Internet established in 1996 as a Great Cities Initiative of the University Library of the University of Illinois at Chicago, ISSN 1396-0466\nRise of the Network Society, Manual Castells, Wiley-Blackwell, 1996 (1st ed) and 2009 (2nd ed), ISBN 978-1-4051-9686-4\n\"The Internet: Changing the Way We Communicate\" in America's Investment in the Future, National Science Foundation, Arlington, Va. USA, 2000\n\"Lessons from the History of the Internet\", Manuel Castells, in The Internet Galaxy, Ch. 1, pp 9\u201335, Oxford University Press, 2001, ISBN 978-0-19-925577-1\n\"Media Freedom Internet Cookbook\" by the OSCE Representative on Freedom of the Media Vienna, 2004\nThe Internet Explained, Vincent Zegna & Mike Pepper, Sonet Digital, November 2005, Pages 1 \u2013 7.\n\"How Much Does The Internet Weigh?\", by Stephen Cass, Discover, 2007\n\"The Internet spreads its tentacles\", Julie Rehmeyer, Science News, Vol. 171, No. 25, pp. 387\u2013388, 23 June 2007\nInternet, Lorenzo Cantoni & Stefano Tardini, Routledge, 2006, ISBN 978-0-203-69888-4\n\n\n== External linksEdit ==\n\nThe Internet Society\nBerkman Center for Internet and Society\nEuropean Commission Information Society\nLiving Internet, Internet history and related information, including information from many creators of the Internet", "biotechnology": "Biotechnology is the broad area of science involving living systems and organisms to develop or make products, or \"any technological application that uses biological systems, living organisms, or derivatives thereof, to make or modify products or processes for specific use\" (UN Convention on Biological Diversity, Art. 2).  Depending on the tools and applications, it often overlaps with the (related) fields of molecular biology, bio-engineering, biomedical engineering, biomanufacturing, molecular engineering, etc.\n\nFor thousands of years, humankind has used biotechnology in agriculture, food production, and medicine.  The term is largely believed to have been coined in 1919 by Hungarian engineer K\u00e1roly Ereky. In the late 20th and early 21st centuries, biotechnology has expanded to include new and diverse sciences such as genomics, recombinant gene techniques, applied immunology, and development of pharmaceutical therapies and diagnostic tests.\n\n\n== Definitions ==\nThe wide concept of \"biotech\" or \"biotechnology\" encompasses a wide range of procedures for modifying living organisms according to human purposes, going back to domestication of animals, cultivation of the plants, and \"improvements\" to these through breeding programs that employ artificial selection and hybridization. Modern usage also includes genetic engineering as well as cell and tissue culture technologies.  The American Chemical Society defines biotechnology as the application of biological organisms, systems, or processes by various industries to learning about the science of life and the improvement of the value of materials and organisms such as pharmaceuticals, crops, and livestock. As per European Federation of Biotechnology, biotechnology is the integration of natural science and organisms, cells, parts thereof, and molecular analogues for products and services. Biotechnology is based on the basic biological sciences (e.g. molecular biology, biochemistry, cell biology, embryology, genetics, microbiology) and conversely provides methods to support and perform basic research in biology.\nBiotechnology is the research and development in the laboratory using bioinformatics for exploration, extraction, exploitation and production from any living organisms and any source of biomass by means of biochemical engineering where high value-added products could be planned (reproduced by biosynthesis, for example), forecasted, formulated, developed, manufactured, and marketed for the purpose of sustainable operations (for the return from bottomless initial investment on R & D) and gaining durable patents rights (for exclusives rights for sales, and prior to this to receive national and international approval from the results on animal experiment and human experiment, especially on the pharmaceutical branch of biotechnology to prevent any undetected side-effects or safety concerns by using the products).By contrast, bioengineering is generally thought of as a related field that more heavily emphasizes higher systems approaches (not necessarily the altering or using of biological materials directly) for interfacing with and utilizing living things. Bioengineering is the application of the principles of engineering and natural sciences to tissues, cells and molecules. This can be considered as the use of knowledge from working with and manipulating biology to achieve a result that can improve functions in plants and animals.  Relatedly, biomedical engineering is an overlapping field that often draws upon and applies biotechnology (by various definitions), especially in certain sub-fields of biomedical or chemical engineering such as tissue engineering, biopharmaceutical engineering, and genetic engineering.\n\n\n== History ==\n\nAlthough not normally what first comes to mind, many forms of human-derived agriculture clearly fit the broad definition of \"'utilizing a biotechnological system to make products\".  Indeed, the cultivation of plants may be viewed as the earliest biotechnological enterprise.\nAgriculture has been theorized to have become the dominant way of producing food since the Neolithic Revolution.  Through early biotechnology, the earliest farmers selected and bred the best suited crops, having the highest yields, to produce enough food to support a growing population.  As crops and fields became increasingly large and difficult to maintain, it was discovered that specific organisms and their by-products could effectively fertilize, restore nitrogen, and control pests. Throughout the history of agriculture, farmers have inadvertently altered the genetics of their crops through introducing them to new environments and breeding them with other plants \u2014 one of the first forms of biotechnology.\nThese processes also were included in early fermentation of beer. These processes were introduced in early Mesopotamia, Egypt, China and India, and still use the same basic biological methods.  In brewing, malted grains (containing enzymes) convert starch from grains into sugar and then adding specific yeasts to produce beer. In this process, carbohydrates in the grains broke down into alcohols, such as ethanol. Later, other cultures produced the process of lactic acid fermentation, which produced other preserved foods, such as soy sauce. Fermentation was also used in this time period to produce leavened bread. Although the process of fermentation was not fully understood until Louis Pasteur's work in 1857, it is still the first use of biotechnology to convert a food source into another form.\nBefore the time of Charles Darwin's work and life, animal and plant scientists had already used selective breeding. Darwin added to that body of work with his scientific observations about the ability of science to change species. These accounts contributed to Darwin's theory of natural selection.For thousands of years, humans have used selective breeding to improve production of crops and livestock to use them for food. In selective breeding, organisms with desirable characteristics are mated to produce offspring with the same characteristics. For example, this technique was used with corn to produce the largest and sweetest crops.In the early twentieth century scientists gained a greater understanding of microbiology and explored ways of manufacturing specific products. In 1917, Chaim Weizmann first used a pure microbiological culture in an industrial process, that of manufacturing corn starch using Clostridium acetobutylicum, to produce acetone, which the United Kingdom desperately needed to manufacture explosives during World War I.Biotechnology has also led to the development of antibiotics. In 1928, Alexander Fleming discovered the mold Penicillium. His work led to the purification of the antibiotic compound formed by the mold by Howard Florey, Ernst Boris Chain and Norman Heatley \u2013 to form what we today know as penicillin. In 1940, penicillin became available for medicinal use to treat bacterial infections in humans.The field of modern biotechnology is generally thought of as having been born in 1971 when Paul Berg's (Stanford) experiments in gene splicing had early success.  Herbert W. Boyer (Univ. Calif. at San Francisco) and Stanley N. Cohen (Stanford) significantly advanced the new technology in 1972 by transferring genetic material into a bacterium, such that the imported material would be reproduced.  The commercial viability of a biotechnology industry was significantly expanded on June 16, 1980, when the United States Supreme Court ruled that a genetically modified microorganism could be patented in the case of Diamond v. Chakrabarty. Indian-born Ananda Chakrabarty, working for General Electric, had modified a bacterium (of the genus Pseudomonas) capable of breaking down crude oil, which he proposed to use in treating oil spills.  (Chakrabarty's work did not involve gene manipulation but rather the transfer of entire organelles between strains of the Pseudomonas bacterium.\nRevenue in the industry is expected to grow by 12.9% in 2008. Another factor influencing the biotechnology sector's success is improved intellectual property rights legislation\u2014and enforcement\u2014worldwide, as well as strengthened demand for medical and pharmaceutical products to cope with an ageing, and ailing, U.S. population.Rising demand for biofuels is expected to be good news for the biotechnology sector, with the Department of Energy estimating ethanol usage could reduce U.S. petroleum-derived fuel consumption by up to 30% by 2030. The biotechnology sector has allowed the U.S. farming industry to rapidly increase its supply of corn and soybeans\u2014the main inputs into biofuels\u2014by developing genetically modified seeds that resist pests and drought. By increasing farm productivity, biotechnology boosts biofuel production.\n\n\n== Examples ==\n\nBiotechnology has applications in four major industrial areas, including health care (medical), crop production and agriculture, non-food (industrial) uses of crops and other products (e.g. biodegradable plastics, vegetable oil, biofuels), and environmental uses.\nFor example, one application of biotechnology is the directed use of organisms for the manufacture of organic products (examples include beer and milk products). Another example is using naturally present bacteria by the mining industry in bioleaching. Biotechnology is also used to recycle, treat waste, clean up sites contaminated by industrial activities (bioremediation), and also to produce biological weapons.\nA series of derived terms have been coined to identify several branches of biotechnology, for example:\n\nBioinformatics is an interdisciplinary field that addresses biological problems using computational techniques, and makes the rapid organization as well as analysis of biological data possible. The field may also be referred to as computational biology, and can be defined as, \"conceptualizing biology in terms of molecules and then applying informatics techniques to understand and organize the information associated with these molecules, on a large scale.\" Bioinformatics plays a key role in various areas, such as functional genomics, structural genomics, and proteomics, and forms a key component in the biotechnology and pharmaceutical sector.\nBlue biotechnology is a term that has been used to describe the marine and aquatic applications of biotechnology, but its use is relatively rare.\nGreen biotechnology is biotechnology applied to agricultural processes. An example would be the selection and domestication of plants via micropropagation. Another example is the designing of transgenic plants to grow under specific environments in the presence (or absence) of chemicals. One hope is that green biotechnology might produce more environmentally friendly solutions than traditional industrial agriculture. An example of this is the engineering of a plant to express a pesticide, thereby ending the need of external application of pesticides. An example of this would be Bt corn. Whether or not green biotechnology products such as this are ultimately more environmentally friendly is a topic of considerable debate.\nRed biotechnology is applied to medical processes. Some examples are the designing of organisms to produce antibiotics, and the engineering of genetic cures through genetic manipulation.\nWhite biotechnology, also known as industrial biotechnology, is biotechnology applied to industrial processes. An example is the designing of an organism to produce a useful chemical. Another example is the using of enzymes as industrial catalysts to either produce valuable chemicals or destroy hazardous/polluting chemicals. White biotechnology tends to consume less in resources than traditional processes used to produce industrial goods.The investment and economic output of all of these types of applied biotechnologies is termed as \"bioeconomy\".\n\n\n=== Medicine ===\nIn medicine, modern biotechnology finds many applications in areas such as pharmaceutical drug discoveries and production, pharmacogenomics, and genetic testing (or genetic screening).\n\nPharmacogenomics (a combination of pharmacology and genomics) is the technology that analyses how genetic makeup affects an individual's response to drugs. It deals with the influence of genetic variation on drug responses in patients by correlating gene expression or single-nucleotide polymorphisms with a drug's efficacy or toxicity.  By doing so, pharmacogenomics aims to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficacy with minimal adverse effects.  Such approaches promise the advent of \"personalized medicine\"; in which drugs and drug combinations are optimized for each individual's unique genetic makeup.\n\nBiotechnology has contributed to the discovery and manufacturing of traditional small molecule pharmaceutical drugs as well as drugs that are the product of biotechnology \u2013 biopharmaceutics. Modern biotechnology can be used to manufacture existing medicines relatively easily and cheaply. The first genetically engineered products were medicines designed to treat human diseases. To cite one example, in 1978 Genentech developed synthetic humanized insulin by joining its gene with a plasmid vector inserted into the bacterium Escherichia coli. Insulin, widely used for the treatment of diabetes, was previously extracted from the pancreas of abattoir animals (cattle or pigs). The resulting genetically engineered bacterium enabled the production of vast quantities of synthetic human insulin at relatively low cost. Biotechnology has also enabled emerging therapeutics like gene therapy.  The application of biotechnology to basic science (for example through the Human Genome Project) has also dramatically improved our understanding of biology and as our scientific knowledge of normal and disease biology has increased, our ability to develop new medicines to treat previously untreatable diseases has increased as well.Genetic testing allows the genetic diagnosis of vulnerabilities to inherited diseases, and can also be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders.  Genetic testing identifies changes in chromosomes, genes, or proteins. Most of the time, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. As of 2011 several hundred genetic tests were in use.  Since genetic testing may open up ethical or psychological problems, genetic testing is often accompanied by genetic counseling.\n\n\n=== Agriculture ===\nGenetically modified crops (\"GM crops\", or \"biotech crops\") are plants used in agriculture, the DNA of which has been modified with genetic engineering techniques.  In most cases, the main aim is to introduce a new trait that does not occur naturally in the species.\nExamples in food crops include resistance to certain pests, diseases, stressful environmental conditions, resistance to chemical treatments (e.g. resistance to a herbicide), reduction of spoilage, or improving the nutrient profile of the crop.  Examples in non-food crops include production of pharmaceutical agents, biofuels, and other industrially useful goods, as well as for bioremediation.Farmers have widely adopted GM technology. Between 1996 and 2011, the total surface area of land cultivated with GM crops had increased by a factor of 94, from 17,000 square kilometers (4,200,000 acres) to 1,600,000 km2 (395 million acres). 10% of the world's crop lands were planted with GM crops in 2010. As of 2011, 11 different transgenic crops were grown commercially on 395 million acres (160 million hectares) in 29 countries such as the USA, Brazil, Argentina, India, Canada, China, Paraguay, Pakistan, South Africa, Uruguay, Bolivia, Australia, Philippines, Myanmar, Burkina Faso, Mexico and Spain.Genetically modified foods are foods produced from organisms that have had specific changes introduced into their DNA with the methods of genetic engineering. These techniques have allowed for the introduction of new crop traits as well as a far greater control over a food's genetic structure than previously afforded by methods such as selective breeding and mutation breeding.  Commercial sale of genetically modified foods began in 1994, when Calgene first marketed its Flavr Savr delayed ripening tomato. To date most genetic modification of foods have primarily focused on cash crops in high demand by farmers such as soybean, corn, canola, and cotton seed oil. These have been engineered for resistance to pathogens and herbicides and better nutrient profiles. GM livestock have also been experimentally developed; in November 2013 none were available on the market, but in 2015 the FDA approved the first GM salmon for commercial production and consumption.There is a scientific consensus that currently available food derived from GM crops poses no greater risk to human health than conventional food, but that each GM food must be tested on a case-by-case basis before introduction. Nonetheless, members of the public are much less likely than scientists to perceive GM foods as safe. The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.GM crops also provide a number of ecological benefits, if not used in excess.  However, opponents have objected to GM crops per se on several grounds, including environmental concerns, whether food produced from GM crops is safe, whether GM crops are needed to address the world's food needs, and economic concerns raised by the fact these organisms are subject to intellectual property law.\n\n\n=== Industrial ===\nIndustrial biotechnology (known mainly in Europe as white biotechnology) is the application of biotechnology for industrial purposes, including industrial fermentation. It includes the practice of using cells such as micro-organisms, or components of cells like enzymes, to generate industrially useful products in sectors such as chemicals, food and feed, detergents, paper and pulp, textiles and biofuels. In doing so, biotechnology uses renewable raw materials and may contribute to lowering greenhouse gas emissions and moving away from a petrochemical-based economy.\n\n\n=== Environmental ===\nThe environment can be affected by biotechnologies, both positively and adversely. Vallero and others have argued that the difference between beneficial biotechnology (e.g.bioremediation is to clean up an oil spill or hazard chemical leak) versus the adverse effects stemming from biotechnological enterprises (e.g. flow of genetic material from transgenic organisms into wild strains) can be seen as applications and implications, respectively. Cleaning up environmental wastes is an example of an application of environmental biotechnology; whereas loss of biodiversity or loss of containment of a harmful microbe are examples of environmental implications of biotechnology.\n\n\n=== Regulation ===\n\nThe regulation of genetic engineering concerns approaches taken by governments to assess and manage the risks associated with the use of genetic engineering technology, and the development and release of genetically modified organisms (GMO), including genetically modified crops and genetically modified fish. There are differences in the regulation of GMOs between countries, with some of the most marked differences occurring between the USA and Europe.  Regulation varies in a given country depending on the intended use of the products of the genetic engineering.  For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety. The European Union differentiates between approval for cultivation within the EU and approval for import and processing. While only a few GMOs have been approved for cultivation in the EU a number of GMOs have been approved for import and processing. The cultivation of GMOs has triggered a debate about coexistence of GM and non GM crops. Depending on the coexistence regulations incentives for cultivation of GM crops differ.\n\n\n== Learning ==\nIn 1988, after prompting from the United States Congress, the National Institute of General Medical Sciences (National Institutes of Health) (NIGMS) instituted a funding mechanism for biotechnology training. Universities nationwide compete for these funds to establish Biotechnology Training Programs (BTPs). Each successful application is generally funded for five years then must be competitively renewed. Graduate students in turn compete for acceptance into a BTP; if accepted, then stipend, tuition and health insurance support is provided for two or three years during the course of their Ph.D. thesis work. Nineteen institutions offer NIGMS supported BTPs. Biotechnology training is also offered at the undergraduate level and in community colleges.\n\n\n== See also ==\n\n\n== References and notes ==\n\n\n== Further reading ==\n\n\n== External links ==\n Media related to Biotechnology at Wikimedia Commons\nFoundation for Biotechnology Awareness and Education,\nA report on Agricultural Biotechnology focusing on the impacts of \"Green\" Biotechnology with a special emphasis on economic aspects. fao.org.\nUS Economic Benefits of Biotechnology to Business and Society NOAA Economics, economics.noaa.gov\nDatabase of the Safety and Benefits of Biotechnology \u2013 a database of peer-reviewed scientific papers and the safety and benefits of biotechnology.\nWhat is Biotechnology? \u2013 A curated collection of resources about the people, places and technologies that have enabled biotechnology to transform the world we live in today", "engineering": "Engineering is the creative application of science, mathematical methods, and empirical evidence to the innovation, design, construction, operation and maintenance of structures, machines, materials, devices, systems, processes, and organizations. The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied mathematics, applied science, and types of application. See glossary of engineering.\nThe term engineering is derived from the Latin ingenium, meaning \"cleverness\" and  ingeniare, meaning \"to contrive, devise\".\n\n\n== Definition ==\nThe American Engineers' Council for Professional Development (ECPD, the predecessor of ABET) has defined \"engineering\" as:\n\nThe creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation and safety to life and property.\n\n\n== History ==\n\nEngineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley.\nThe term engineering  is derived from the word engineer, which itself dates back to 1390 when an engine'er (literally, one who operates an engine) referred to \"a constructor of military engines.\" In this context, now obsolete, an \"engine\" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers.\nThe word \"engine\" itself is of even older origin, ultimately deriving from the Latin ingenium (c. 1250), meaning \"innate quality, especially mental power, hence a clever invention.\"Later, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.\n\n\n=== Ancient era ===\n\nThe pyramids in Egypt, the Acropolis and the Parthenon in Greece, the Roman aqueducts, Via Appia and the Colosseum, Teotihuac\u00e1n, the Great Wall of China, the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon, and the Pharos of Alexandria were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World.\nThe earliest civil engineer known by name is Imhotep. As one of the officials of the Pharaoh, Djos\u00e8r, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630\u20132611 BC.Ancient Greece developed machines in both civilian and military domains. The Antikythera mechanism, the first known mechanical computer, and the mechanical inventions of Archimedes are examples of early mechanical engineering. Some of Archimedes' inventions as well as the Antikythera mechanism required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are still widely used today in diverse fields such as robotics and automotive engineering.Ancient Chinese, Greek, Roman and Hungarian armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century B.C., the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed.\n\n\n=== Renaissance era ===\nThe first steam engine was built in 1698 by Thomas Savery. The development of this device gave rise to the Industrial Revolution in the coming decades, allowing for the beginnings of mass production.\nWith the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.\n\n\n=== Modern era ===\n\nThe inventions of Thomas Newcomen and James Watt gave rise to modern mechanical engineering. The development of specialized machines and machine tools during the industrial revolution led to the rapid growth of mechanical engineering both in its birthplace Britain and abroad.\n\nJohn Smeaton was the first self-proclaimed civil engineer and is often regarded as the \"father\" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbours, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Smeaton designed the third Eddystone Lighthouse (1755\u201359) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. His lighthouse remained in use until 1877 and was dismantled and partially rebuilt at Plymouth Hoe where it is known as Smeaton's Tower. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain \"hydraulicity\" in lime; work which led ultimately to the invention of Portland cement.\nThe United States census of 1850 listed the occupation of \"engineer\" for the first time with a count of 2,000.  There were fewer than 50 engineering graduates in the U.S. before 1865. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875.  In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical.There was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907.  Germany established technical universities earlier.The foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.Chemical engineering developed in the late nineteenth century. Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants. The role of the chemical engineer was the design of these chemical plants and processes.\n\nAeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.The first PhD in engineering (technically, applied science and engineering) awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.Only a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.\nIn 1990, with the rise of computer technology, the first search engine was built by computer engineer Alan Emtage.\n\n\n== Main branches of engineering ==\n\nEngineering is a broad discipline which is often broken down into several sub-disciplines. Although an engineer will usually be trained in a specific discipline, he or she may become multi-disciplined through experience. Engineering is often characterized as having four main branches: chemical engineering, civil engineering, electrical engineering, and mechanical engineering.\n\n\n=== Chemical engineering ===\n\nChemical engineering is the application of physics, chemistry, biology, and engineering principles in order to carry out chemical processes on a commercial scale, such as the manufacture of commodity chemicals, specialty chemicals, petroleum refining, microfabrication, fermentation, and biomolecule production.\n\n\n=== Civil engineering ===\n\nCivil engineering is the design and construction of public and private works, such as infrastructure (airports, roads, railways, water supply, and treatment etc.), bridges, tunnels, dams, and buildings. Civil engineering is traditionally broken into a number of sub-disciplines, including structural engineering, environmental engineering, and surveying. It is traditionally considered to be separate from military engineering.\n\n\n=== Electrical engineering ===\n\nElectrical engineering is the design, study, and manufacture of various electrical and electronic systems, such as Broadcast engineering, electrical circuits, generators, motors, electromagnetic/electromechanical devices, electronic devices, electronic circuits, optical fibers, optoelectronic devices, computer systems, telecommunications, instrumentation, controls, and electronics.\n\n\n=== Mechanical engineering ===\n\nMechanical engineering is the design and manufacture of physical or mechanical systems, such as power and energy systems, aerospace/aircraft products, weapon systems, transportation products, engines, compressors, powertrains, kinematic chains, vacuum technology, vibration isolation equipment, manufacturing, and mechatronics.\n\n\n=== Other branches ===\n\nBeyond these \"Big 4\", a number of other branches are recognized, though many can be thought of as sub-disciplines of the four major branches, or as cross-curricular disciplines among multiple.  Historically, naval engineering and mining engineering were major branches.  Other engineering fields are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, aerospace, automotive, computer, electronic, petroleum, environmental, systems, audio, software, architectural, agricultural, biosystems, biomedical, geological, textile, industrial, materials, and nuclear engineering.  These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council.\nNew specialties sometimes combine with the traditional fields and form new branches \u2013 for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering.\n\n\n== Practice ==\nOne who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer, or Designated Engineering Representative.\n\n\n== Methodology ==\n\nIn the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. More than ever, engineers are now required to have a proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their career.\nIf multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The crucial and unique task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements.\nConstraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.\nA general methodology and epistemology of engineering can be inferred from the historical case studies and comments provided by Walter Vincenti. Though Vincenti's case studies are from the domain of aeronautical engineering, his conclusions can be transferred into many other branches of engineering, too.\nAccording to Billy Vaughn Koen, the \"engineering method is the use of heuristics to cause the best change in a poorly understood situation within the available resources.\" Koen argues that the definition of what makes one an engineer should not be based on what he produces, but rather how he goes about it.\n\n\n=== Problem solving ===\n\nEngineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions.\nUsually, multiple reasonable solutions exist, so engineers must evaluate the different design choices on their merits and choose the solution that best meets their requirements. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of \"low-level\" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.\nEngineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected.\nEngineers take on the responsibility of producing designs that will perform as well as expected and will not cause unintended harm to the public at large. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure. However, the greater the safety factor, the less efficient the design may be.The study of failed products is known as forensic engineering and can help the product designer in evaluating his or her design in the light of real conditions. The discipline is of greatest value after disasters, such as bridge collapses, when careful analysis is needed to establish the cause or causes of the failure.\n\n\n=== Computer use ===\n\nAs with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.\nOne of the most widely used design tools in the profession is computer-aided design (CAD) software like CATIA, Autodesk Inventor, DSS SolidWorks or Pro Engineer which enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.\nThese allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.There are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and Architecture, engineering and construction (AEC) software for civil engineering.\nIn recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).\n\n\n== Social context ==\n\nThe engineering profession engages in a wide range of activities, from large collaboration at the societal level, and also smaller individual projects.  Almost all engineering projects are obligated to some sort of financing agency: a company, a set of investors, or a government.  The few types of engineering that are minimally constrained by such issues are pro bono engineering and open-design engineering.\nBy its very nature engineering has interconnections with society, culture and human behavior.  Every product or construction used by modern society is influenced by engineering. The results of engineering activity influence changes to the environment, society and economies, and its application brings with it a responsibility and public safety.\nEngineering projects can be subject to controversy. Examples from different engineering disciplines include the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some western engineering companies have enacted serious corporate and social responsibility policies.\nEngineering is a key driver of innovation and human development. Sub-Saharan Africa, in particular, has a very small engineering capacity which results in many African nations being unable to develop crucial infrastructure without outside aid. The attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.\n\nAll overseas development and relief NGOs make considerable use of engineers to apply solutions in disaster and development scenarios. A number of charitable organizations aim to use engineering directly for the good of mankind:\n\nEngineers Without Borders\nEngineers Against Poverty\nRegistered Engineers for Disaster Relief\nEngineers for a Sustainable World\nEngineering for Change\nEngineering Ministries InternationalEngineering companies in many established economies are facing significant challenges with regard to the number of professional engineers being trained, compared with the number retiring.  This problem is very prominent in the UK where engineering has a poor image and low status. There are many negative economic and political issues that this can cause, as well as ethical issues. It is widely agreed that the engineering profession faces an \"image crisis\", rather than it being fundamentally an unattractive career.  Much work is needed to avoid huge problems in the UK and other western economies.\n\n\n=== Code of ethics ===\n\nMany engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large.  The National Society of Professional Engineers code of ethics states:\n\n Engineering is an important and learned profession. As members of this profession, engineers are expected to exhibit the highest standards of honesty and integrity. Engineering has a direct and vital impact on the quality of life for all people. Accordingly, the services provided by engineers require honesty, impartiality, fairness, and equity, and must be dedicated to the protection of the public health, safety, and welfare. Engineers must perform under a standard of professional behavior that requires adherence to the highest principles of ethical conduct.\nIn Canada, many engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession.\n\n\n== Relationships with other disciplines ==\n\n\n=== Science ===\n\nScientists study the world as it is; engineers create the world that has never been.\n\nThere exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena.  Both use mathematics and classification criteria to analyze and communicate observations.Scientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely \"engineering scientists\".In the book What Engineers Know and How They Know It, Walter Vincenti asserts that engineering research has a character different from that of scientific research.  First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.\nThere is a \"real and important\" difference between engineering and physics as similar to any science field has to do with technology. Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology. For technology, physics is an auxiliary and in a way technology is considered as applied physics. Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer's job. A physicist would typically require additional and relevant training. Physicists and engineers engage in different lines of work. But PhD physicists who specialize in sectors of technology and applied science are titled as Technology officer, R&D Engineers and System Engineers.An example of this is the use of numerical approximations to the Navier\u2013Stokes equations to describe aerodynamic flow over an aircraft, or the use of Miner's rule to calculate fatigue damage. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.As stated by Fung et al. in the revision to the classic engineering text Foundations of Solid Mechanics:\n\nEngineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined.  In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born.\nAlthough engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.\n\n\n=== Medicine and biology ===\n\nThe study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.\n\nModern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers. The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.\nConversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics.  There are also substantial interdisciplinary interactions between engineering and medicine.Both fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.\nMedicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.The heart for example functions much like a pump, the skeleton is like a linked structure with levers, the brain produces electrical signals etc. These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.\nNewly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.\n\n\n=== Art ===\nThere are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering).The Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design. Robert Maillart's bridge design is perceived by some to have been deliberately artistic. At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.Among famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.\n\n\n=== Business ===\nBusiness Engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or \"Management engineering\" is a specialized field of management concerned with engineering practice or the engineering industry sector.  The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking.  Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods.  Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector.  This work often deals with large scale complex business transformation or Business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical & electronics, power distribution & generation, utilities and transportation systems.  This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.\n\n\n=== Other fields ===\nIn political science, the term engineering has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Financial engineering has similarly borrowed the term.\n\n\n== See also ==\n\nLists\nGlossaries\nRelated subjects\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\nNational Society of Professional Engineers position statement on Licensure and Qualifications for Practice\nNational Academy of Engineering (NAE)\nAmerican Society for Engineering Education (ASEE)\nThe US Library of Congress Engineering in History bibliography\nEngineering videos at a secondary school level.\nHistory of engineering bibliography at University of Minnesota", "mass production": "Mass production, also known as flow production or continuous production, is the production of large amounts of standardized products, including and especially on assembly lines. Together with job production and batch production, it is one of the three main production methods.The term mass production was popularized by a 1926 article in the Encyclop\u00e6dia Britannica supplement that was written based on correspondence with Ford Motor Company.  The New York Times used the term in the title of an article that appeared before publication of the Britannica article.The concepts of mass production are applied to various kinds of products, from fluids and particulates handled in bulk (such as food, fuel, chemicals, and mined minerals) to discrete solid parts (such as fasteners) to assemblies of such parts (such as household appliances and automobiles).\nMass production is a diverse field, but it can generally be contrasted with craft production or distributed manufacturing. Some mass production techniques, such as standardized sizes and production lines, predate the Industrial Revolution by many centuries; however, it was not until the introduction of machine tools and techniques to produce interchangeable parts were developed in the mid 19th century that modern mass production was possible.\n\n\n== Overview ==\nMass production involves making many copies of products, very quickly, using assembly line techniques to send partially complete products to workers who each work on an individual step, rather than having a worker work on a whole product from start to finish.\nMass production of fluid matter typically involves pipes with centrifugal pumps or screw conveyors (augers) to transfer raw materials or partially complete products between vessels.  Fluid flow processes such as oil refining and bulk materials such as wood chips and pulp are automated using a system of process control which uses various instruments to measure variables such as temperature, pressure, volumetric  and level, providing feedback\nBulk materials such as coal, ores, grains and wood chips are handled by belt, chain, slat, pneumatic or screw conveyors, bucket elevators and mobile equipment such as front-end loaders.  Materials on pallets are handled with forklifts.  Also used for handling heavy items like reels of paper, steel or machinery are electric overhead cranes, sometimes called bridge cranes because they span large factory bays.\nMass production is capital intensive and energy intensive, as it uses a high proportion of machinery and energy in relation to workers. It is also usually automated while total expenditure per unit of product is decreased. However, the machinery that is needed to set up a mass production line (such as robots and machine presses) is so expensive that there must be some assurance that the product is to be successful to attain profits.\nOne of the descriptions of mass production is that \"the skill is built into the tool\", which means that the worker using the tool may not need the skill. For example, in the 19th or early 20th century, this could be expressed as \"the craftsmanship is in the workbench itself\" (not the training of the worker). Rather than having a skilled worker measure every dimension of each part of the product against the plans or the other parts as it is being formed, there were jigs ready at hand to ensure that the part was made to fit this set-up. It had already been checked that the finished part would be to specifications to fit all the other finished parts\u2014and it would be made more quickly, with no time spent on finishing the parts to fit one another. Later, once computerized control came about (for example, CNC), jigs were obviated, but it remained true that the skill (or knowledge) was built into the tool (or process, or documentation) rather than residing in the worker's head. This is the specialized capital required for mass production; each workbench and set of tools (or each CNC cell, or each fractionating column) is different (fine-tuned to its task).\n\n\n== History ==\n\n\n=== Pre-industrial ===\nStandardized parts and sizes and factory production techniques were developed in pre-industrial times; however, before the invention of machine tools the manufacture of precision parts, especially metal ones, was very labor-intensive.\n\nCrossbows made with bronze parts were produced in China during the Warring States period. The Qin Emperor unified China at least in part by equipping large armies with these weapons, which were equipped with a sophisticated trigger mechanism made of interchangeable parts.  Ships of war were produced on a large scale at a moderate cost by the Carthaginians in their excellent harbors, allowing them to efficiently maintain their control of the Mediterranean. The Venetians themselves also produced ships using prefabricated parts and assembly lines many centuries later. The Venetian Arsenal apparently produced nearly one ship every day, in what was effectively the world's first factory which, at its height, employed 16,000 people. Mass production in the publishing industry has been commonplace since the Gutenberg Bible was published using a printing press in the mid-15th century.\n\n\n=== Industrial ===\nIn the Industrial Revolution simple mass production techniques were used at the Portsmouth Block Mills in England to make ships' pulley blocks for the Royal Navy in the Napoleonic Wars. It was achieved in 1803 by Marc Isambard Brunel in cooperation with Henry Maudslay under the management of Sir Samuel Bentham.\n\nThe Navy was in a state of expansion that required 100,000 pulley blocks to be manufactured a year. Bentham had already achieved remarkable efficiency at the docks by introducing power-driven machinery and reorganising the dockyard system. Brunel, a pioneering engineer, and Maudslay, a pioneer of machine tool technology who had developed the first industrially practical screw-cutting lathe in 1800 which standardized screw thread sizes for the first time which in turn allowed the application of interchangeable parts, collaborated on plans to manufacture block-making machinery. By 1805, the dockyard had been fully updated with the revolutionary, purpose-built machinery at a time when products were still built individually with different components. A total of 45 machines were required to perform 22 processes on the blocks, which could be made into one of three possible sizes. The machines were almost entirely made of metal thus improving their accuracy and durability. The machines would make markings and indentations on the blocks to ensure alignment throughout the process. One of the many advantages of this new method was the increase in labour productivity due to the less labour-intensive requirements of managing the machinery. Richard Beamish, assistant to Brunel's son and engineer, Isambard Kingdom Brunel, wrote:\n\nSo that ten men, by the aid of this machinery, can accomplish with uniformity, celerity and ease, what formerly required the uncertain labour of one hundred and ten.\nBy 1808, annual production from the 45 machines had reached 130,000 blocks and some of the equipment was still in operation as late as the mid-twentieth century. Mass production techniques were also used to rather limited extent to make clocks and watches, and to make small arms, though parts were usually non-interchangeable. Though produced on a very small scale, Crimean War gunboat engines designed and assembled by John Penn of Greenwich are recorded as the first instance of the application of mass production techniques (though not necessarily the assembly-line method) to marine engineering. In filling an Admiralty order for 90 sets to his high-pressure and high-revolution horizontal trunk engine design, Penn produced them all in 90 days. He also used Whitworth Standard threads throughout. Prerequisites for the wide use of mass production were interchangeable parts, machine tools and power, especially in the form of electricity.\nSome of the organizational management concepts needed to create 20th-century mass production, such as scientific management, had been pioneered by other engineers (most of whom are not famous, but Frederick Winslow Taylor is one of the well-known ones), whose work would later be synthesized into fields such as industrial engineering, manufacturing engineering, operations research, and management consultancy. Although after leaving the Henry Ford Company which was rebranded as Cadillac and later was awarded the Dewar Trophy in 1908 for creating interchangeable mass-produced precision engine parts, Henry Ford downplayed the role of Taylorism in the development of mass production at his company. However, Ford management performed time studies and experiments to mechanize their factory processes, focusing on minimizing worker movements. The difference is that while Taylor focused mostly on efficiency of the worker, Ford also substituted for labor by using machines, thoughtfully arranged, wherever possible.\nThe United States Department of War sponsored the development of interchangeable parts for guns produced at the arsenals at Springfield, Massachusetts and Harpers Ferry, Virginia (now West Virginia) in the early decades of the 19th century, finally achieving reliable interchangeability by about 1850. This period coincided with the development of machine tools, with the armories designing and building many of their own.  Some of the methods employed were a system of gauges for checking dimensions of the various parts and jigs and fixtures for guiding the machine tools and properly holding and aligning the work pieces.  This system came to be known as armory practice or the American system of manufacturing, which spread throughout New England aided by skilled mechanics from the armories who were instrumental in transferring the technology to the sewing machines manufacturers and other industries such as machine tools, harvesting machines and bicycles. Singer Manufacturing Co., at one time the largest sewing machine manufacturer, did not achieve interchangeable parts until the late 1880s, around the same time Cyrus McCormick adopted modern manufacturing practices in making harvesting machines.Mass production benefited from the development of materials such as inexpensive steel, high strength steel and plastics. Machining of metals was greatly enhanced with high speed steel and later very hard materials such as tungsten carbide for cutting edges.  Fabrication using steel components was aided by the development of electric welding and stamped steel parts, both which appeared in industry in about 1890. Plastics such as polyethylene, polystyrene and polyvinyl chloride (PVC) can be easily formed into shapes by extrusion, blow molding or injection molding, resulting in very low cost manufacture of consumer products, plastic piping, containers and parts.\nAn influential article that helped to frame and popularize the 20th century's definition of mass production appeared in a 1926 Encyclop\u00e6dia Britannica supplement. The article was written based on correspondence with Ford Motor Company and is sometimes credited as the first use of the term.\n\n\n=== Factory electrification ===\nElectrification of factories began very gradually in the 1890s after the introduction of a practical DC motor by Frank J. Sprague and accelerated after the AC motor was developed by Galileo Ferraris, Nikola Tesla and Westinghouse, Mikhail Dolivo-Dobrovolsky and others. Electrification of factories was fastest between 1900 and 1930, aided by the establishment of electric utilities with central stations and the lowering of electricity prices from 1914 to 1917.Electric motors were several times more efficient than small steam engines because central station generation were more efficient than small steam engines and because line shafts and belts had high friction losses.  Electric motors allowed also more flexibility in manufacturing and required less maintenance than line shafts and belts.  Many factories saw a 30% increase in output just from changing over to electric motors.\nElectrification enabled modern mass production, as with Thomas Edison\u2019s iron ore processing plant (about 1893) that could process 20,000 tons of ore per day with two shifts of five men each. At that time it was still common to handle bulk materials with shovels, wheelbarrows and small narrow gauge rail cars, and for comparison, a canal digger in previous decades typically handled 5 tons per 12-hour day.\nThe biggest impact of early mass production was in manufacturing everyday items, such as at the Ball Brothers Glass Manufacturing Company, which electrified its mason jar plant in Muncie, Indiana, U.S. around 1900. The new automated process used glass blowing machines to replace 210 craftsman glass blowers and helpers. A small electric truck was used to handle 150 dozen bottles at a time where previously a hand truck would carry 6 dozen. Electric mixers replaced men with shovels handling sand and other ingredients that were fed into the glass furnace. An electric overhead crane replaced 36 day laborers for moving heavy loads across the factory.According to Henry Ford:\nThe provision of a whole new system of electric generation emancipated industry from the leather belt and line shaft, for it eventually became possible to provide each tool with its own electric motor. This may seem only a detail of minor importance. In fact, modern industry could not be carried out with the belt and line shaft for a number of reasons. The motor enabled machinery to be arranged in the order of the work, and that alone has probably doubled the efficiency of industry, for it has cut out a tremendous amount of useless handling and hauling. The belt and line shaft were also tremendously wasteful \u2013 so wasteful indeed that no factory could be really large, for even the longest line shaft was small according to modern requirements. Also high speed tools were impossible under the old conditions \u2013 neither the pulleys nor the belts could stand modern speeds.  Without high speed tools and the finer steels which they brought about, there could be nothing of what we call modern industry.\n\nMass production was popularized in the late 1910s and 1920s by Henry Ford's Ford Motor Company, when introduced electric motors to the then-well-known technique of chain or sequential production. Ford also bought or designed and built special purpose machine tools and fixtures such as multiple spindle drill presses that could drill every hole on one side of an engine block in one operation and a multiple head milling machine that could simultaneously machine 15 engine blocks held on a single fixture. All of these machine tools were arranged systematically in the production flow and some had special carriages for rolling heavy items into machining position. Production of the Ford Model T used 32,000 machine tools.\n\n\n== The use of assembly lines ==\n\nMass production systems for items made of numerous parts are usually organized into assembly lines.  The assemblies pass by on a conveyor, or if they are heavy, hung from an overhead crane or monorail.\nIn a factory for a complex product, rather than one assembly line, there may be many auxiliary assembly lines feeding sub-assemblies (i.e. car engines or seats) to a backbone \"main\" assembly line.  A diagram of a typical mass-production factory looks more  like the skeleton of a fish than a single line.\n\n\n== Vertical integration ==\nVertical integration is a business practice that involves gaining complete control over a product's production, from raw materials to final assembly.\nIn the age of mass production, this caused shipping and trade problems in that shipping systems were unable to transport huge volumes of finished automobiles (in Henry Ford's case) without causing damage, and also government policies imposed trade barriers on finished units.Ford built the Ford River Rouge Complex with the idea of making the company's own iron and steel in the same large factory site as parts and car assembly took place.  River Rouge also generated its own electricity.\nUpstream vertical integration, such as to raw materials, is away from leading technology toward mature, low return industries. Most companies chose to focus on their core business rather than vertical integration.  This included buying parts from outside suppliers, who could often produce them as cheaply or cheaper.\nStandard Oil, the major oil company in the 19th century, was vertically integrated partly because there was no demand for unrefined crude oil, but kerosene and some other products were in great demand.  The other reason was that Standard Oil monopolized the oil industry.  The major oil companies were, and many still are, vertically integrated, from production to refining and with their own retail stations, although some sold off their retail operations.   Some oil companies also have chemical divisions.\nLumber and paper companies at one time owned most of their timber lands and sold some finished products such as corrugated boxes.  The tendency has been to divest of timber lands to raise cash and to avoid property taxes.\n\n\n== Advantages and disadvantages ==\nThe economies of mass production come from several sources.  The primary cause is a reduction of nonproductive effort of all types. In craft production, the craftsman must bustle about a shop, getting parts and assembling them. He must locate and use many tools many times for varying tasks. In mass production, each worker repeats one or a few related tasks that use the same tool to perform identical or near-identical operations on a stream of products. The exact tool and parts are always at hand, having been moved down the assembly line consecutively. The worker spends little or no time retrieving and/or preparing materials and tools, and so the time taken to manufacture a product using mass production is shorter than when using traditional methods.\nThe probability of human error and variation is also reduced, as tasks are predominantly carried out by machinery; error in operating such machinery, however, has more far-reaching consequences. A reduction in labour costs, as well as an increased rate of production, enables a company to produce a larger quantity of one product at a lower cost than using traditional, non-linear methods.\nHowever, mass production is inflexible because it is difficult to alter a design or production process after a production line is implemented. Also, all products produced on one production line will be identical or very similar, and introducing variety to satisfy individual tastes is not easy. However, some variety can be achieved by applying different finishes and decorations at the end of the production line if necessary. The starter cost for the machinery can be expensive so the producer must be sure it sells or the producers will lose a lot of money.\nThe Ford Model T produced tremendous affordable output but was not very good at responding to demand for variety, customization, or design changes. As a consequence Ford eventually lost market share to General Motors, who introduced annual model changes, more accessories and a choice of colors.With each passing decade, engineers have found ways to increase the flexibility of mass production systems, driving down the lead times on new product development and allowing greater customization and variety of products.\n\n\n== Socioeconomic impacts ==\nIn the 1830s, French political thinker and historian Alexis de Tocqueville identified one of the key characteristics of America that would later make it so amenable to the development of mass production: the homogeneous consumer base. De Tocqueville wrote in his Democracy in America (1835) that \"The absence in the United States of those vast accumulations of wealth which favor the expenditures of large sums on articles of mere luxury... impact to the productions of American industry a character distinct from that of other countries' industries. [Production is geared toward] articles suited to the wants of the whole people\".\nMass production improved productivity, which was a contributing factor to economic growth and the decline in work week hours, alongside other factors such as transportation infrastructures (canals, railroads and highways) and agricultural mechanization.  These factors caused the typical work week to decline from 70 hours in the early 19th century to 60 hours late in the century, then to 50 hours in the early 20th century and finally to 40 hours in the mid-1930s.\nMass production permitted great increases in total production.  Using a European crafts system into the late 19th century it was difficult to meet demand for products such as sewing machines and animal powered mechanical harvesters.  By the late 1920s many previously scarce goods were in good supply. One economist has argued that this constituted \"overproduction\" and contributed to high unemployment during the Great Depression. Say's law denies the possibility of general overproduction and for this reason classical economists deny that it had any role in the Great Depression.\nMass production allowed the evolution of consumerism by lowering the unit cost of many goods used.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nBeaudreau, Bernard C. (1996). Mass Production, the Stock Market Crash and the Great Depression. New York, Lincoln, Shanghi: Authors Choice Press. \nBorth, Christy. Masters of Mass Production, Bobbs-Merrill Company, Indianapolis, IN, 1945.\nHerman, Arthur. Freedom's Forge: How American Business Produced Victory in World War II, Random House, New York, NY, 2012. ISBN 978-1-4000-6964-4.\n\n\n== External links ==\n Quotations related to Mass production at Wikiquote\n Media related to Mass production at Wikimedia Commons", "mass communication": "Mass communication is the study of how people exchange information through mass media to large segments of the population at the same time. In other words, mass communication refers to the imparting and exchanging of information on a large scale to a wide range of people. It is usually understood to relate  newspaper, magazine, and book publishing, as well as radio, television and film, even via internet as these mediums are used for disseminating information, news and advertising.  Mass communication differs from the studies of other forms of communication, such as interpersonal communication or organizational communication, in that it focuses on a single source transmitting information to a large number of receivers.  The study of mass communication is chiefly concerned with how the content of mass communication persuades or otherwise affects the behavior, attitude, opinion, or emotion of the person or people receiving the information.\nDefinition of mass communication\u0589\nNormally, transmission of messages to many persons at a time is called Mass Communication. But in complete sense, mass communication can be defined as the process through which a message is circulated extensively among people nearby & also throughout far extending areas such as entire countries or the globe.\nWhat is Mass Communication?\n\nMass communication is the process of transmitting messages to a large number of scattered audiences.\nThrough mass communication, information can be transmitted quickly to a large number of people who generally stay far away from the sources of information. Mass communication is done through many mediums, such as radio, television, social networking, billboards, and newspapers.\n\n\n== Field of study ==\nMass communication is \"the process by which a person, group of people, or organization creates a message and transmits it through some type of medium to a large, anonymous, heterogeneous audience.\" This implies that the audience of mass communication are mostly made up of different cultures, behavior and belief systems. Mass communication is regularly associated with media influence or media effects, and media studies.  Mass communication is a branch of social science that falls under the larger umbrella of communication studies of communication.\nThe history of communication stretches from prehistoric forms of art and writing through modern communication methods such as the Internet.  Mass communication began when humans could transmit messages from a single source to multiple receivers.  Mass communication has moved from theories such as the hypodermic needle model (or magic bullet theory) through more modern theories such as computer-mediated communication.\nIn the United States, the study of mass communication is often associated with the practical applications of journalism (Print media), television and radio broadcasting, film, public relations, or advertising.  With the diversification of media options, the study of communication has extended to include social media and new media, which have stronger feedback models than traditional media sources.  While the field of mass communication is continually evolving, the following four fields are generally considered the major areas of study within mass communication.  They exist in different forms and configurations at different schools or universities, but are (in some form) practiced at most institutions that study mass communication. Advertising, in relation to mass communication, refers to marketing a product or service in a persuasive manner that encourages the audience to buy the product or use the service.  Because advertising generally takes place through some form of mass media, such as television, studying the effects and methods of advertising is relevant to the study of mass communication. Advertising is the paid, impersonal, one-way marketing of persuasive information from a sponsor. Through mass communication channels, the sponsor promotes the adoption of goods, services or ideas. Advertisers have full control of the message being sent to their audience.Characteristics or Features of Mass Communication\nMass Communication has all the features of general communication. In addition, it offers some unique characteristics because of its specialty in nature. \nLarge Number of Audience\nThe foremost feature of mass communication is that it has large number of audience. No other communication gets as many receivers as it gets.\nHeterogeneous Audience\nMass Communication is not only composed of a large number of audiences but also aims to heterogeneous audience. The heterogeneity here means that the audience may belong to different races, groups, section, cultures etc.\nScattered Audience\nThe audiences of Mass Communication are not organized in a certain area rather they are highly scattered in different geographical areas. The receivers of message of mass communication may stay any place of the country and even any place of the world.\nWide Area\nThe area of Mass Communication is wider than any other communication systems. In case of mass communication system, the message is structured, formal and standardized and that\u2019s why it has acceptance all over the world.\nUse of Channel\nMass Communication system uses various types of mass media channels such as-radio, television, newspapers, magazines etc.\nUse of Common Message\nAnother unique characteristic of mass communication is that it sends the same message simultaneously to a large number of audiences staying far away from each other. If the audiences have the proper access to the media used by the sender they can easily get message wherever they stay in the world.\nNo Direct Feedback\nMass Communication does not produce any direct feedback. The reaction of audience cannot be known quickly here.\nOutward Flow\nThe flow of message in mass communication is outward, not inward. The basic objective of mass communication is also to send message to the people outside the organization who say far away.\nUse of Technology\nMass Communication system uses modern technology in the process of production and dissemination of the message to be sent.\n\n\n=== Journalism ===\n\nJournalism, is the collection, verification, presentation, and editing of news for presentation through the media, in this sense, refers to the study of the product and production of news.  The study of journalism involves looking at how news is produced, and how it is disseminated to the public through mass mediaoutlets such as newspapers, news channel, radio station, television station, and more recently, e-readers and smartphones. The information provided pertains to current events, trends, issues, and people.\n\n\n=== Public relations ===\n\nPublic relations is the process of providing information to the public in order to present a specific view of a product or organization.  Public relations differs from advertising in that it is less obtrusive, and aimed at providing a more comprehensive opinion to a large audience in order to shape public opinion. Unlike advertising, public relations professionals only have control until the message is related to media gatekeepers who decide where to pass the information on to the audience.\n\n\n== Major theories ==\nCommunication researchers have identified several major theories associated with the study of mass communication. Communication theory addresses the processes and mechanisms that allow communication to take place.\n\nCultivation theory, developed by George Gerbner and Marshall McLuhan, discusses the long-term effects of watching television, and hypothesizes that the more television an individual consumes, the more likely that person is to believe the real world is similar to what they have seen on television.  Cultivation is closely related to the idea of the mean world syndrome.\nAgenda setting theory centers around the idea that media outlets tell the public \"not what to think, but what to think about.\"  Agenda setting hypothesizes that media have the power to influence the public discourse, and tell people what are important issues facing society.\nThe spiral of silence, developed by Elisabeth Noelle-Neumann, hypothesizes that people are more likely to reveal their opinion in public if they believe that they are of the majority opinion, for fear that revealing an unpopular opinion would subject them to being a social outcast.  This theory is relevant to mass communication because it hypothesizes that mass media have the power to shape people's opinions, as well as relay the opinion that is believed to be the majority opinion.\nMedia ecology hypothesizes that individuals are shaped by their interaction with media, and that communication and media profoundly affect how individuals view and interact with their environment.\nAccording to the Semiotic theory, communication characteristics such as words, images, gestures, and situations are always interpretive. All sign systems, entitled to be \u201cread\u201d or interpreted, regardless of form, may be referred to as \u201ctexts.\u201d In the study of Semiotics, there is no such thing as a literal reading.\n\n\n== Methods of study ==\nCommunication researchers study communication through various methods that have been verified through repetitive, cumulative processes.  Both quantitative and qualitative methods have been used in the study of mass communication.  The main focus of mass communication research is to learn how the content of mass communication affects the attitudes, opinions, emotions, and ultimately behaviors of the people who receive the message.  Several prominent methods of study are as follows:\nStudying cause and effect relationships in communication can only be done through an experiment. This quantitative method regularly involves exposing participants to various media content and recording their reactions.  To show causation, mass communication researchers must isolate the variable they are studying, show that it occurs before the observed effect, and that it is the only variable that could cause the observed effect.\nSurvey, another quantitative method, involves asking individuals to respond to a set of questions in order to generalize their responses to a larger population.\nContent analysis (sometimes known as textual analysis) refers to the process of identifying categorial properties of a piece of communication, such as a newspaper article, book, television program, film, or broadcast news script.  This process allows researchers to see what the content of communication looks like.\nA qualitative method known as ethnography allows a researcher to immerse themselves into a culture to observe and record the qualities of communication that exist there.\n\n\n== Professional organizations ==\nThe Association for Education in Journalism and Mass Communication is the major membership organization for academics in the field, offering regional and national conferences and refereed publications. The International Communication Association and National Communication Association (formerly the Speech Communication Association) are also prominent professional organizations.  Each of these organizations publishes a different refereed academic journal that reflects the research that is being performed in the field of mass communication.\n\n\n== See also ==\nCommunication\nCommunication studies\nCommunication Theory as a Field\nHistory of communication\nMedia influence\nMedia studies\nSocial science\n\n\n== Notes ==\n\n\n== References ==\nPearce, K.J. (2009). Media and Mass Communication Theories. In Encyclopedia of Communication Theory (p. 624-628). SAGE Publications.\nHartley, J.: \"Mass communication\", in O'Sullivan; Fiske (eds): Key Concepts in Communication and Cultural Studies (Routledge, 1997).\nMackay, H.; O'Sullivan T.: The Media Reader: Continuity and Transformation (Sage, 1999).\nMcQuail, D.: McQuail's Mass Communication Theory (fifth edition) (Sage, 2005). *Thompson, John B.: The Media and Modernity (Polity, 1995).\nGriffin, E. (2009). A first look at communication theory. McGraw Hill: New York, NY. ISBN 978-0-07-338502-0\nBabbie, E. (2007). The practice of social research. Thomas Higher Education: Belmont, California. ISBN 0-495-09325-4\nBraison agesa E'[2013] study of mass communication\nCyrll magical realist (2018)  key in communication skills", "programming": "Computer programming is the process of building and designing an executable computer program for accomplishing a specific computing task. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem. The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.\nRelated tasks include testing, debugging, maintaining a program's source code, implementation of build systems, and management of derived artifacts such as machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software development practices.\n\n\n== History ==\n\nProgrammable devices have existed at least as far back as 1206 AD, when the automata of Al-Jazari were programmable, via pegs and cams, to play various rhythms and drum patterns; and the 1801 Jacquard loom could produce entirely different weaves by changing the \"program\" - a series of pasteboard cards with holes punched in them.\nHowever, the first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.\n\nIn the 1880s Herman Hollerith invented the concept of storing data in machine-readable form. Later a control panel (plugboard) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way; as were the first electronic computers. However, with the concept of the stored-program computers introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.\nMachine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format, (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language, any two machines with different instruction sets also have different assembly languages.\n\nHigh-level languages allow the programmer to write programs in terms that are more abstract, and less bound to the underlying hardware. They harness the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula directly (e.g., Y = X*2 + 5*X + 9). FORTRAN, the first widely used high-level language  to have a functional implementation, came out in 1957 and many other languages were soon developed - in particular, COBOL aimed at commercial data processing, and Lisp for computer research.\nPrograms were mostly still entered using punched cards or paper tape. See computer programming in the punch card era. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were developed that allowed changes and corrections to be made much more easily than with punched cards.\n\n\n== Modern programming ==\n\n\n=== Quality requirements ===\nWhatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:\n\nReliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms, and minimization of programming mistakes, such as mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off-by-one errors).\nRobustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of needed resources such as memory, operating system services and network connections, user error, and unexpected power outages.\nUsability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical and sometimes hardware elements that improve the clarity, intuitiveness, cohesiveness and completeness of a program's user interface.\nPortability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware and operating system, and availability of platform specific compilers (and sometimes libraries) for the language of the source code.\nMaintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or customizations, fix bugs and security holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term.\nEfficiency/performance: Measure of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating memory leaks.\n\n\n=== Readability of source code ===\nIn computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.\nReadability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.\nFollowing a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:\n\nDifferent indent styles (whitespace)\nComments\nDecomposition\nNaming conventions for objects (such as variables, classes, procedures, etc.)The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.\nVarious visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.\n\n\n=== Algorithmic complexity ===\nThe academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problem. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.\n\n\n=== Methodologies ===\nThe first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.\nPopular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.\nA similar technique used for database design is Entity-Relationship Modeling (ER Modeling).\nImplementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.\n\n\n=== Measuring language usage ===\n\nIt is very difficult to determine what are the most popular of modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).\nSome languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).\n\n\n=== Debugging ===\n\nDebugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it crash when parsing some large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Such simplification can be done manually, using a divide-and-conquer approach. The programmer will try to remove some parts of original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.\nDebugging is often done with IDEs like Eclipse, Visual Studio, Xcode, Kdevelop, NetBeans and Code::Blocks. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.\n\n\n== Programming languages ==\n\nDifferent programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from \"low-level\" to \"high-level\"; \"low-level\" languages are typically more machine-oriented and faster to execute, whereas \"high-level\" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in \"high-level\" languages than in \"low-level\" ones.\nAllen Downey, in his book How To Think Like A Computer Scientist, writes:\n\nThe details look different in different languages, but a few basic instructions appear in just about every language:\nInput: Gather data from the keyboard, a file, or some other device.\nOutput: Display data on the screen or send data to a file or other device.\nArithmetic: Perform basic arithmetical operations like addition and multiplication.\nConditional Execution: Check for certain conditions and execute the appropriate sequence of statements.\nRepetition: Perform some action repeatedly, usually with some variation.Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.\n\n\n== Programmers ==\n\nComputer programmers are those who write computer software. Their jobs usually involve:\n\n\n== See also ==\n\nACCU\nAssociation for Computing Machinery\nComputer networking\nHello world program\nInstitution of Analysts and Programmers\nNational Coding Week\nSystem programming\nThe Art of Computer Programming\n\n\n== References ==\n\n\n== Further reading ==\nA.K. Hartmann, Practical Guide to Computer Simulations, Singapore: World Scientific (2009)\nA. Hunt, D. Thomas, and W. Cunningham, The Pragmatic Programmer. From Journeyman to Master, Amsterdam: Addison-Wesley Longman (1999)\nBrian W. Kernighan, The Practice of Programming, Pearson (1999)\nWeinberg, Gerald M., The Psychology of Computer Programming, New York: Van Nostrand Reinhold (1971)\nEdsger W. Dijkstra, A Discipline of Programming, Prentice-Hall (1976)\nO.-J. Dahl, E.W.Dijkstra, C.A.R. Hoare, Structured Pogramming, Academic Press (1972)\nDavid Gries, The Science of Programming, Springer-Verlag (1981)\n\n\n== External links ==\n Media related to Programming at Wikimedia Commons\n Quotations related to Programming at Wikiquote\nSoftware engineering at Curlie (based on DMOZ)", "innovation": "Innovation can be defined simply as a \"new idea, device or method\".  However, innovation is often also viewed as the application of better  solutions that meet new requirements, unarticulated needs, or existing  market needs. Such innovation takes place through the provision of more-effective products, processes, services, technologies, or business models that are made available to markets, governments and society. The term \"innovation\" can be defined as something original and more effective and, as a consequence, new, that \"breaks into\" the market or society. Innovation is related to, but not the same as, invention, as innovation is more apt to involve the practical implementation of an invention (i.e. new/improved ability) to make a meaningful impact in the market or society, and not all innovations require an invention. Innovation often manifests itself via the engineering process, when the problem being solved is of a technical or scientific nature. The opposite of innovation is exnovation.\nWhile a novel device is often described as an innovation, in economics, management science, and other fields of practice and analysis, innovation is generally considered to be the result of a process that brings together various novel ideas in such a way that they affect society. In industrial economics, innovations are created and found empirically from services to meet growing consumer demand.\n\n\n== Definition ==\nA 2014 survey of literature on innovation found over 40 definitions. In an industrial survey of how the software industry defined innovation, the following definition given by Crossan and Apaydin was considered to be the most complete, which  builds on the Organisation for Economic Co-operation and Development (OECD) manual's definition: Innovation is: production or adoption, assimilation, and exploitation of a value-added novelty in economic and social spheres; renewal and enlargement of products, services, and markets; development of new methods of production; and establishment of new management systems. It is both a process and an outcome.\n\nAccording to Kanter innovation includes original invention and creative use and defines innovation as a generation, admission and realization of new ideas, products, services and processes.\nTwo main dimensions of innovation were degree of novelty (patent) (i.e. whether an innovation is new to the firm, new to the market, new to the industry, or new to the world) and type of innovation (i.e. whether it is process or product-service system innovation). In recent organizational scholarship, researchers of workplaces have also distinguished innovation to be separate from creativity, by providing an updated definition of these two related but distinct constructs:Workplace creativity concerns the cognitive and behavioral processes applied when attempting to generate novel ideas. Workplace innovation concerns the processes applied when attempting to implement new ideas. Specifically, innovation involves some combination of problem/opportunity identification, the introduction, adoption or modification of new ideas germane to organizational needs, the promotion of these ideas, and the practical implementation of these ideas.\n\n\n== Inter-disciplinary views ==\n\n\n=== Business and economics ===\n\nIn business and in economics, innovation can become a catalyst for growth. With rapid advancements in transportation and communications over the past few decades, the old-world concepts of factor endowments and comparative advantage which focused on an area's unique inputs are outmoded for today's global economy. Economist Joseph Schumpeter (1883-1950), who contributed greatly to the study of innovation economics, argued that industries must incessantly revolutionize the economic structure from within, that is innovate with better or more effective processes and products, as well as market distribution, such as the connection from the craft shop to factory. He famously asserted that \"creative destruction is the essential fact about capitalism\". Entrepreneurs continuously look for better ways to satisfy their consumer base with improved quality, durability, service, and price which come to fruition in innovation with advanced technologies and organizational strategies.A prime example of innovation involved the explosive boom of Silicon Valley startups out of the Stanford Industrial Park. In 1957, dissatisfied employees of Shockley Semiconductor, the company of Nobel laureate and co-inventor of the transistor William Shockley, left to form an independent firm, Fairchild Semiconductor. After several years, Fairchild developed into a formidable presence in the sector. Eventually, these founders left to start their own companies based on their own, unique, latest ideas, and then leading employees started their own firms. Over the next 20 years, this snowball process launched the momentous startup-company explosion of information-technology firms. Essentially, Silicon Valley began as 65 new enterprises born out of Shockley's eight former employees. Since then, hubs of innovation have sprung up globally with similar metonyms, including Silicon Alley encompassing New York City.\nAnother example involves business incubators - a phenomenon nurtured by governments around the world, close to knowledge clusters (mostly research-based) like universities or other Government Excellence Centres - which aim primarily to channel generated knowledge to applied innovation outcomes in order to stimulate regional or national economic growth.\n\n\n=== Organizations ===\nIn the organizational context, innovation may be linked to positive changes in efficiency, productivity, quality, competitiveness, and market share. However, recent research findings highlight the complementary role of organizational culture in enabling organizations to translate innovative activity into tangible performance improvements. Organizations can also improve profits and performance by providing work groups opportunities and resources to innovate, in addition to employee's core job tasks. Peter Drucker wrote:\n\nInnovation is the specific function of entrepreneurship, whether in an existing business, a public service institution, or a new venture started by a lone individual in the family kitchen. It is the means by which the entrepreneur either creates new wealth-producing resources or endows existing resources with enhanced potential for creating wealth. \u2013Drucker\nAccording to Clayton Christensen, disruptive innovation is the key to future success in business. The organisation requires a proper structure in order to retain competitive advantage. It is necessary to create and nurture an environment of innovation. Executives and managers need to break away from traditional ways of thinking and use change to their advantage. It is a time of risk but even greater opportunity. The world of work is changing with the increase in the use of technology and both companies and businesses are becoming increasingly competitive. Companies will have to downsize and re-engineer their operations to remain competitive. This will affect employment as businesses will be forced to reduce the number of people employed while accomplishing the same amount of work if not more.While disruptive innovation will typically \"attack a traditional business model with a lower-cost solution and overtake incumbent firms quickly,\" foundational innovation is slower, and typically has the potential to create new foundations for global technology systems over the longer term. Foundational innovation tends to transform business operating models as entirely new business models emerge over many years, with gradual and steady adoption of the innovation leading to waves of technological and institutional change that gain momentum more slowly.  The advent of the packet-switched communication protocol TCP/IP\u2014originally introduced in 1972 to support a single use case for United States Department of Defense electronic communication (email), and which gained widespread adoption only in the mid-1990s with the advent of the World Wide Web\u2014is a foundational technology.All organizations can innovate, including for example hospitals, universities, and local governments. For instance, former Mayor Martin O\u2019Malley pushed the City of Baltimore to use CitiStat, a performance-measurement data and management system that allows city officials to maintain statistics on several areas from crime trends to the conditions of potholes. This system aids in better evaluation of policies and procedures with accountability and efficiency in terms of time and money. In its first year, CitiStat saved the city $13.2 million. Even mass transit systems have innovated with hybrid bus fleets to real-time tracking at bus stands. In addition, the growing use of mobile data terminals in vehicles, that serve as communication hubs between vehicles and a control center, automatically send data on location, passenger counts, engine performance, mileage and other information. This tool helps to deliver and manage transportation systems.Still other innovative strategies include hospitals digitizing medical information in electronic medical records. For example, the U.S. Department of Housing and Urban Development's HOPE VI initiatives turned severely distressed public housing in urban areas into revitalized, mixed-income environments; the Harlem Children\u2019s Zone used a community-based approach to educate local area children; and the Environmental Protection Agency's brownfield grants facilitates turning over brownfields for environmental protection, green spaces, community and commercial development.\n\n\n=== Sources ===\nThere are several sources of innovation. It can occur as a result of a focus effort by a range of different agents, by chance, or as a result of a major system failure.\nAccording to Peter F. Drucker, the general sources of innovations are different changes in industry structure, in market structure, in local and global demographics, in human perception, mood and meaning, in the amount of already available scientific knowledge, etc.\n\nIn the simplest linear model of innovation the traditionally recognized source is manufacturer innovation. This is where an agent (person or business) innovates in order to sell the innovation. Specifically, R&D measurement is the commonly used input for innovation, in particular in the business sector, named Business Expenditure on R&D (BERD) that grew over the years on the expenses of the declining R&D invested by the public sector.Another source of innovation, only now becoming widely recognized, is end-user innovation. This is where an agent (person or company) develops an innovation for their own (personal or in-house) use because existing products do not meet their needs. MIT economist Eric von Hippel has identified end-user innovation as, by far, the most important and critical in his classic book on the subject, The Sources of Innovation.The robotics engineer Joseph F. Engelberger asserts that innovations require only three things:\n\nA recognized need,\nCompetent people with relevant technology, and\nFinancial support.However, innovation processes usually involve: identifying customer needs, macro and meso trends, developing competences, and finding financial support.\nThe Kline chain-linked model of innovation places emphasis on potential market needs as drivers of the innovation process, and describes the complex and often iterative feedback loops between marketing, design, manufacturing, and R&D.\nInnovation by businesses is achieved in many ways, with much attention now given to formal research and development (R&D) for \"breakthrough innovations\". R&D help spur on patents and other scientific innovations that leads to productive growth in such areas as industry, medicine, engineering, and government. Yet, innovations can be developed by less formal on-the-job modifications of practice, through exchange and combination of professional experience and by many other routes. Investigation of relationship between the concepts of innovation and technology transfer revealed overlap. The more radical and revolutionary innovations tend to emerge from R&D, while more incremental innovations may emerge from practice \u2013 but there are many exceptions to each of these trends.\nInformation technology and changing business processes and management style can produce a work climate favorable to innovation.  For example, the software tool company Atlassian conducts quarterly \"ShipIt Days\" in which employees may work on anything related to the company's products.  Google employees work on self-directed projects for 20% of their time (known as Innovation Time Off). Both companies cite these bottom-up processes as major sources for new products and features.\nAn important innovation factor includes customers buying products or using services. As a result, firms may incorporate users in focus groups (user centred approach), work closely with so called lead users (lead user approach) or users might adapt their products themselves. The lead user method focuses on idea generation based on leading users to develop breakthrough innovations. U-STIR, a project to innovate Europe\u2019s surface transportation system, employs such workshops. Regarding this user innovation, a great deal of innovation is done by those actually implementing and using technologies and products as part of their normal activities. Sometimes user-innovators may become entrepreneurs, selling their product, they may choose to trade their innovation in exchange for other innovations, or they may be adopted by their suppliers. Nowadays, they may also choose to freely reveal their innovations, using methods like open source. In such networks of innovation the users or communities of users can further develop technologies and reinvent their social meaning.One technique for innovating a solution to an identified problem is to actually attempt an experiment with many possibile solutions. This technique was famously used by Thomas Edison's laboratory to find a version of the incandescent light bulb economically viable for home use, which involved searching through thousands of possible filament designs before settling on carbonized bamboo.\nThis technique is sometimes used in pharmaceutical drug discovery. Thousands of chemical compounds are subjected to high-throughput screening to see if they have any activity against a target molecule which has been identified as biologically significant to a disease. Promising compounds can then be studied; modified to improve efficacy, reduce side effects, and reduce cost of manufacture; and if successful turned into treatments.\nThe related technique of A/B testing is often used to help optimize the design of web sites and mobile apps.  This is used by major sites such as amazon.com, Facebook, Google, and Netflix. Procter & Gamble uses computer-simulated products and onlinen user panels to conduct larger numbers of experiments to guide the design, packaging, and shelf placement of consumer products. Capital One uses this technique to drive credit card marketing offers.\n\n\n=== Goals and failures ===\nPrograms of organizational innovation are typically tightly linked to organizational goals and objectives, to the business plan, and to market competitive positioning. One driver for innovation programs in corporations is to achieve growth objectives. As Davila et al. (2006) notes, \"Companies cannot grow through cost reduction and reengineering alone... Innovation is the key element in providing aggressive top-line growth, and for increasing bottom-line results\".One survey across a large number of manufacturing and services organizations found, ranked in decreasing order of popularity, that systematic programs of organizational innovation are most frequently driven by: improved quality, creation of new markets, extension of the product range, reduced labor costs, improved production processes, reduced materials, reduced environmental damage, replacement of products/services, reduced energy consumption, conformance to regulations.These goals vary between improvements to products, processes and services and dispel a popular myth that innovation deals mainly with new product development. Most of the goals could apply to any organisation be it a manufacturing facility, marketing firm, hospital or local government. Whether innovation goals are successfully achieved or otherwise depends greatly on the environment prevailing in the firm.Conversely, failure can develop in programs of innovations. The causes of failure have been widely researched and can vary considerably. Some causes will be external to the organization and outside its influence of control. Others will be internal and ultimately within the control of the organization. Internal causes of failure can be divided into causes associated with the cultural infrastructure and causes associated with the innovation process itself. Common causes of failure within the innovation process in most organizations can be distilled into five types: poor goal definition, poor alignment of actions to goals, poor participation in teams, poor monitoring of results, poor communication and access to information.\n\n\n== Diffusion ==\n\nDiffusion of innovation research was first started in 1903 by seminal researcher Gabriel Tarde, who first plotted the S-shaped diffusion curve. Tarde  defined the innovation-decision process as a series of steps that includes:\nFirst knowledge\nForming an attitude\nA decision to adopt or reject\nImplementation and use\nConfirmation of the decisionOnce innovation occurs, innovations may be spread from the innovator to other individuals and groups. This process has been proposed that the life cycle of innovations can be described using the 's-curve' or diffusion curve. The s-curve maps growth of revenue or productivity against time. In the early stage of a particular innovation, growth is relatively slow as the new product establishes itself. At some point customers begin to demand and the product growth increases more rapidly. New incremental innovations or changes to the product allow growth to continue. Towards the end of its lifecycle, growth slows and may even begin to decline. In the later stages, no amount of new investment in that product will yield a normal rate of return\nThe s-curve derives from an assumption that new products are likely to have \"product life\" \u2013 i.e., a start-up phase, a rapid increase in revenue and eventual decline. In fact the great majority of innovations never get off the bottom of the curve, and never produce normal returns.\nInnovative companies will typically be working on new innovations that will eventually replace older ones. Successive s-curves will come along to replace older ones and continue to drive growth upwards. In the figure above the first curve shows a current technology. The second shows an emerging technology that currently yields lower growth but will eventually overtake current technology and lead to even greater levels of growth. The length of life will depend on many factors.\n\n\n== Measures ==\nMeasuring innovation is inherently difficult as it implies commensurability so that comparisons can be made in quantitative terms. Innovation, however, is by definition novelty. Comparisons are thus often meaningless across products or service. Nevertheless, Edison et al. in their review of literature on innovation management found 232 innovation metrics. They categorized these measures along five dimensions i.e. inputs to the innovation process, output from the innovation process, effect of the innovation output, measures to access the activities in an innovation process and availability of factors that facilitate such a process.There are two different types of measures for innovation: the organizational level and the political level.\n\n\n=== Organizational level ===\nThe measure of innovation at the organizational level relates to individuals, team-level assessments, and private companies from the smallest to the largest company. Measure of innovation for organizations can be conducted by surveys, workshops, consultants, or internal benchmarking. There is today no established general way to measure organizational innovation. Corporate measurements are generally structured around balanced scorecards which cover several aspects of innovation such as business measures related to finances, innovation process efficiency, employees' contribution and motivation, as well benefits for customers. Measured values will vary widely between businesses, covering for example new product revenue, spending in R&D, time to market, customer and employee perception & satisfaction, number of patents, additional sales resulting from past innovations.\n\n\n=== Political level ===\nFor the political level, measures of innovation are more focused on a country or region competitive advantage through innovation. In this context, organizational capabilities can be evaluated through various evaluation frameworks, such as those of the European Foundation for Quality Management. The OECD Oslo Manual (1995) suggests standard guidelines on measuring technological product and process innovation. Some people consider the Oslo Manual complementary to the Frascati Manual from 1963. The new Oslo manual from 2005 takes a wider perspective to innovation, and includes marketing and organizational innovation. These standards are used for example in the European Community Innovation Surveys.Other ways of measuring innovation have traditionally been expenditure, for example, investment in R&D (Research and Development) as percentage of GNP (Gross National Product). Whether this is a good measurement of innovation has been widely discussed and the Oslo Manual has incorporated some of the critique against earlier methods of measuring. The traditional methods of measuring still inform many policy decisions. The EU Lisbon Strategy has set as a goal that their average expenditure on R&D should be 3% of GDP.\n\n\n=== Indicators ===\nMany scholars claim that there is a great bias towards the \"science and technology mode\" (S&T-mode or STI-mode), while the \"learning by doing, using and interacting mode\" (DUI-mode) is ignored and measurements and research about it rarely done. For example, an institution may be high tech with the latest equipment, but lacks crucial doing, using and interacting tasks important for innovation.A common industry view (unsupported by empirical evidence) is that comparative cost-effectiveness research is a form of price control which reduces returns to industry, and thus limits R&D expenditure, stifles future innovation and compromises new products access to markets. \nSome academics claim cost-effectiveness research is a valuable value-based measure of innovation which accords \"truly significant\" therapeutic advances (i.e. providing \"health gain\") higher prices than free market mechanisms. Such value-based pricing has been viewed as a means of indicating to industry the type of innovation that should be rewarded from the public purse.An Australian academic developed the case that national comparative cost-effectiveness analysis systems should be viewed as measuring \"health innovation\" as an evidence-based policy concept for valuing innovation distinct from valuing through competitive markets, a method which requires strong anti-trust laws to be effective, on the basis that both methods of assessing pharmaceutical innovations are mentioned in annex 2C.1 of the Australia-United States Free Trade Agreement.\n\n\n=== Indices ===\nSeveral indices attempt to measure innovation and rank entities based on these measures, such as:\n\nThe Bloomberg Innovation Index\nThe \"Bogota Manual\" similar to the Oslo Manual, is focused on Latin America and the Caribbean countries.\nThe \"Creative Class\" developed by Richard Florida\nThe EIU Innovation Ranking\nThe Global Competitiveness Report\nThe Global Innovation Index (GII), by INSEAD\nThe Information Technology and Innovation Foundation (ITIF) Index\nInnovation 360 - From the World Bank. Aggregates innovation indicators (and more) from a number of different public sources\nThe Innovation Capacity Index (ICI) published by a large number of international professors working in a collaborative fashion. The top scorers of ICI 2009\u20132010 were: 1. Sweden 82.2; 2. Finland 77.8; and 3. United States 77.5.\nThe Innovation Index, developed by the Indiana Business Research Center, to measure innovation capacity at the county or regional level in the United States.\nThe Innovation Union Scoreboard\nThe innovationsindikator for Germany, developed by the Federation of German Industries (Bundesverband der Deutschen Industrie) in 2005\nThe INSEAD Innovation Efficacy Index\nThe International Innovation Index, produced jointly by The Boston Consulting Group, the National Association of Manufacturers and its nonpartisan research affiliate The Manufacturing Institute, is a worldwide index measuring the level of innovation in a country. NAM describes it as the \"largest and most comprehensive global index of its kind\".\nThe Management Innovation Index - Model for Managing Intangibility of Organizational Creativity: Management Innovation Index\nThe NYCEDC Innovation Index, by the New York City Economic Development Corporation, tracks New York City\u2019s \"transformation into a center for high-tech innovation. It measures innovation in the City\u2019s growing science and technology industries and is designed to capture the effect of innovation on the City\u2019s economy.\"\nThe Oslo Manual is focused on North America, Europe, and other rich economies.\nThe State Technology and Science Index, developed by the Milken Institute, is a U.S.-wide benchmark to measure the science and technology capabilities that furnish high paying jobs based around key components.\nThe World Competitiveness Scoreboard\n\n\n=== Rankings ===\nMany research studies try to rank countries based on measures of innovation. Common areas of focus include: high-tech companies, manufacturing, patents, post secondary education, research and development,  and research personnel. The left ranking of the top 10 countries below is based on the 2016 Bloomberg Innovation Index. However, studies may vary widely; for example the Global Innovation Index 2016 ranks Switzerland as number one wherein countries like South Korea and Japan do not even make the top ten.", "electronics": "Electronics is the discipline dealing with the development and application of devices and systems involving the flow of electrons in a vacuum, in gaseous media, and in semiconductors. Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, integrated circuits, optoelectronics, and sensors, associated passive electrical components, and interconnection technologies. Commonly, electronic devices contain circuitry consisting primarily or exclusively of active semiconductors supplemented with passive elements; such a circuit is described as an electronic circuit.\nElectronics is considered to be a branch of physics and electrical engineering.The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible.  Electronics is widely used in information processing, telecommunication, and signal processing.  The ability of electronic devices to act as switches makes digital information processing possible.  Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.\nElectrical and electromechanical science and technology deals with the generation, distribution, switching, storage, and conversion of electrical energy to and from other energy forms (using wires, motors, generators, batteries, switches, relays, transformers, resistors, and other passive components). This distinction started around 1906 with the invention by Lee De Forest of the triode, which made electrical amplification of weak radio signals and audio signals possible with a non-mechanical device.  Until 1950 this field was called \"radio technology\" because its principal application was the design and theory of radio transmitters, receivers, and vacuum tubes.\nToday, most electronic devices use semiconductor components to perform electron control.  The study of semiconductor devices and related technology is considered a branch of solid-state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering.  This article focuses on engineering aspects of electronics.\n\n\n== Branches of electronics ==\nElectronics has branches as follows:\n\nDigital electronics\nAnalogue electronics\nMicroelectronics\nCircuit design\nIntegrated circuits\nPower electronics\nOptoelectronics\nSemiconductor devices\nEmbedded systems\n\n\n== Electronic devices and components ==\n\nAn electronic component is any physical entity in an electronic system used to affect the electrons or their associated fields in a manner consistent with the intended function of the electronic system. Components are generally intended to be connected together, usually by being soldered to a printed circuit board (PCB), to create an electronic circuit with a particular function (for example an amplifier, radio receiver, or oscillator). Components may be packaged singly, or in more complex groups as integrated circuits. Some common electronic components are capacitors, inductors, resistors, diodes, transistors, etc. Components are often categorized as active (e.g. transistors and thyristors) or passive (e.g. resistors, diodes, inductors and capacitors).\n\n\n== History of electronic components ==\n\nVacuum tubes (Thermionic valves) were among the earliest electronic components.  They were almost solely responsible for the electronics revolution of the first half of the twentieth century.  They took electronics from parlor tricks and gave us radio, television, phonographs, radar, long-distance telephony and much more.  They played a leading role in the field of microwave and high power transmission as well as television receivers until the middle of the 1980s.  Since that time, solid-state devices have all but completely taken over. Vacuum tubes are still used in some specialist applications such as high power RF amplifiers, cathode ray tubes, specialist audio equipment, guitar amplifiers and some microwave devices.\nIn April 1955, the IBM 608 was the first IBM product to use transistor circuits without any vacuum tubes and is believed to be the first all-transistorized calculator to be manufactured for the commercial market. The 608 contained more than 3,000 germanium transistors.  Thomas J. Watson Jr. ordered all future IBM products to use transistors in their design.  From that time on transistors were almost exclusively used for computer logic and peripherals.\n\n\n== Types of circuits ==\nCircuits and components can be divided into two groups: analog and digital. A particular device may consist of circuitry that has one or the other or a mix of the two types.\n\n\n=== Analog circuits ===\n\nMost analog electronic appliances, such as radio receivers, are constructed from combinations of a few types of basic circuits. Analog circuits use a continuous range of voltage or current as opposed to discrete levels as in digital circuits.\nThe number of different analog circuits so far devised is huge, especially because a 'circuit' can be defined as anything from a single component, to systems containing thousands of components.\nAnalog circuits are sometimes called linear circuits although many non-linear effects are used in analog circuits such as mixers, modulators, etc. Good examples of analog circuits include vacuum tube and transistor amplifiers, operational amplifiers and oscillators.\nOne rarely finds modern circuits that are entirely analog. These days analog circuitry may use digital or even microprocessor techniques to improve performance. This type of circuit is usually called \"mixed signal\" rather than analog or digital.\nSometimes it may be difficult to differentiate between analog and digital circuits as they have elements of both linear and non-linear operation. An example is the comparator which takes in a continuous range of voltage but only outputs one of two levels as in a digital circuit. Similarly, an overdriven transistor amplifier can take on the characteristics of a controlled switch having essentially two levels of output.  In fact, many digital circuits are actually implemented as variations of analog circuits similar to this example\u2014after all, all aspects of the real physical world are essentially analog, so digital effects are only realized by constraining analog behavior.\n\n\n=== Digital circuits ===\n\nDigital circuits are electric circuits based on a number of discrete voltage levels. Digital circuits are the most common physical representation of Boolean algebra, and are the basis of all digital computers. To most engineers, the terms \"digital circuit\", \"digital system\" and \"logic\" are interchangeable in the context of digital circuits.\nMost digital circuits use a binary system with two voltage levels labeled \"0\" and \"1\". Often logic \"0\" will be a lower voltage and referred to as \"Low\" while logic \"1\" is referred to as \"High\".  However, some systems use the reverse definition (\"0\" is \"High\") or are current based.  Quite often the logic designer may reverse these definitions from one circuit to the next as he sees fit to facilitate his design.  The definition of the levels as \"0\" or \"1\" is arbitrary.\nTernary (with three states) logic has been studied, and some prototype computers made.\nComputers, electronic clocks, and programmable logic controllers (used to control industrial processes) are constructed of digital circuits. Digital signal processors are another example.\nBuilding blocks:\n\nLogic gates\nAdders\nFlip-flops\nCounters\nRegisters\nMultiplexers\nSchmitt triggersHighly integrated devices:\n\nMicroprocessors\nMicrocontrollers\nApplication-specific integrated circuit (ASIC)\nDigital signal processor (DSP)\nField-programmable gate array (FPGA)\n\n\n== Heat dissipation and thermal management ==\n\nHeat generated by electronic circuitry must be dissipated to prevent immediate failure and improve long term reliability. Heat dissipation is mostly achieved by passive conduction/convection. Means to achieve greater dissipation include heat sinks and fans for air cooling, and other forms of computer cooling such as water cooling. These techniques use convection, conduction, and radiation of heat energy.\n\n\n== Noise ==\n\nElectronic noise is defined as unwanted disturbances superposed on a useful signal that tend to obscure its information content. Noise is not the same as signal distortion caused by a circuit. Noise is associated with all electronic circuits. Noise may be electromagnetically or thermally generated, which can be decreased by lowering the operating temperature of the circuit. Other types of noise, such as shot noise cannot be removed as they are due to limitations in physical properties.\n\n\n== Electronics theory ==\n\nMathematical methods are integral to the study of electronics. To become proficient in electronics it is also necessary to become proficient in the mathematics of circuit analysis.\nCircuit analysis is the study of methods of solving generally linear systems for unknown variables such as the voltage at a certain node or the current through a certain branch of a network. A common analytical tool for this is the SPICE circuit simulator.\nAlso important to electronics is the study and understanding of electromagnetic field theory.\n\n\n== Electronics lab ==\n\nDue to the complex nature of electronics theory, laboratory experimentation is an important part of the development of electronic devices.  These experiments are used to test or verify the engineer\u2019s design and detect errors.  Historically, electronics labs have consisted of electronics devices and equipment located in a physical space, although in more recent years the trend has been towards electronics lab simulation software, such as CircuitLogix, Multisim, and PSpice.\n\n\n== Computer aided design (CAD) ==\n\nToday's electronics engineers have the ability to design circuits using premanufactured building blocks such as power supplies, semiconductors (i.e. semiconductor devices, such as transistors), and integrated circuits. Electronic design automation software programs include schematic capture programs and printed circuit board design programs. Popular names in the EDA software world are NI Multisim, Cadence (ORCAD), EAGLE PCB and Schematic, Mentor (PADS PCB and LOGIC Schematic), Altium (Protel), LabCentre Electronics (Proteus), gEDA, KiCad and many others.\n\n\n== Construction methods ==\n\nMany different methods of connecting components have been used over the years. For instance, early electronics often used point to point wiring with components attached to wooden breadboards to construct circuits. Cordwood construction and wire wrap were other methods used. Most modern day electronics now use printed circuit boards made of materials such as FR4, or the cheaper (and less hard-wearing) Synthetic Resin Bonded Paper (SRBP, also known as Paxoline/Paxolin (trade marks) and FR2) - characterised by its brown colour. Health and environmental concerns associated with electronics assembly have gained increased attention in recent years, especially for products destined to the European Union, with its Restriction of Hazardous Substances Directive (RoHS) and Waste Electrical and Electronic Equipment Directive (WEEE), which went into force in July 2006.\n\n\n== See also ==\n\nOutline of electronics\n\n\n== References ==\n\n\n== Further reading ==\nThe Art of Electronics ISBN 978-0-521-37095-0\n\n\n== External links ==\nElectronics at Curlie (based on DMOZ)\nhttp://www.dictionary.com/browse/electronics\nNavy 1998 Navy Electricity and Electronics Training Series (NEETS)\nDOE 1998 Electrical Science, Fundamentals Handbook, 4 vols.\nVol. 1, Basic Electrical Theory, Basic DC Theory\nVol. 2, DC Circuits, Batteries, Generators, Motors\nVol. 3, Basic AC Theory, Basic AC Reactive Components, Basic AC Power, Basic AC Generators\nVol. 4, AC Motors, Transformers, Test Instruments & Measuring Devices, Electrical Distribution Systems", "weapons": "A weapon, arm or armament is any device used with intent to inflict damage or harm. Weapons are used to increase the efficacy and efficiency of activities such as hunting, crime, law enforcement, self-defense, and warfare. In broader context, weapons may be construed to include anything used to gain a strategic, material or mental advantage over an adversary or enemy target.\nWhile ordinary objects such as sticks, stones, cars, or pencils can be used as weapons, many are expressly designed for the purpose \u2013 ranging from simple implements such as clubs, swords and axes, to complicated modern intercontinental ballistic missiles, biological weapons and cyberweapons. Something that has been re-purposed, converted, or enhanced to become a weapon of war is termed weaponized, such as a weaponized virus or weaponized laser.\n\n\n== History ==\n\n\n=== Prehistoric ===\n\nThe use of objects as weapons has been observed among chimpanzees, leading to speculation that early hominids used weapons as early as five million years ago. However, this can not be confirmed using physical evidence because wooden clubs, spears, and unshaped stones would have left an ambiguous record. The earliest unambiguous weapons to be found are the Sch\u00f6ninger Spear, eight wooden throwing spears dating back more than 300,000 years. At the site of Nataruk in Turkana, Kenya, numerous human skeletons dating to 10,000 years ago may present evidence of traumatic injuries to the head, neck, ribs, knees and hands, including obsidian projectiles embedded in the bones that might have been caused from arrows and clubs during conflict between two hunter-gatherer groups. But the evidence interpretation of warfare at Nataruk has been challenged.\n\n\n=== Ancient history ===\n\nThe earliest Ancient weapons were evolutionary improvements of late neolithic implements, but significant improvements in materials and crafting techniques led to a series of revolutions in military technology.\nThe development of metal tools began with copper during the Copper Age (about 3,300 BC) and was followed shortly by the bronze Age, leading to the creation of the Bronze Age sword and similar weapons.\nDuring the Bronze Age, the first defensive structures and fortifications appeared as well, indicating an increased need for security. Weapons designed to breach fortifications followed soon after, such as the battering ram was in use by 2500 BC.The development of iron-working around 1300 BC in Greece had an important impact on the development of ancient weapons. It was not the introduction of  early Iron Age swords, however, as they were not superior to their bronze predecessors, \nbut rather the Domestication of the horse and widespread use of spoked wheels by ca. 2000 BC. This led to the creation of the light, horse-drawn chariot, whose improved mobility proved important during this era. Spoke-wheeled chariot usage peaked around 1300 BC and then declined, ceasing to be militarily relevant by the 4th century BC.Cavalry developed once horses were bred to support the weight of a human. The horse extended the range and increased the speed of attacks.\nIn addition to land based weaponry, warships, such as the trireme, were in use by the 7th century BC.\n\n\n=== Post-classical history ===\n\nEuropean warfare during the Post-classical history was dominated by elite groups of knights supported by massed infantry (both in combat and ranged roles). They were involved in mobile combat and sieges which involved various siege weapons and tactics. Knights on horseback developed tactics for charging with lances providing an impact on the enemy formations and then drawing more practical weapons (such as swords) once they entered into the melee. By contrast, infantry, in the age before structured formations, relied on cheap, sturdy weapons such as spears and billhooks in close combat and bows from a distance. As armies became more professional, their equipment was standardized and infantry transitioned to pikes. Pikes are normally seven to eight feet in length, and used in conjunction with smaller side-arms (short sword).\nIn Eastern and Middle Eastern warfare, similar tactics were developed independent of European influences.\nThe introduction of gunpowder from the Asia at the end of this period revolutionized warfare. Formations of musketeers, protected by pikemen came to dominate open battles, and the cannon replaced the trebuchet as the dominant siege weapon.\n\n\n=== Modern history ===\n\n\n==== Early modern ====\nThe European Renaissance marked the beginning of the implementation of firearms in western warfare. Guns and rockets were introduced to the battlefield.\nFirearms are qualitatively different from earlier weapons because they release energy from combustible propellants such as gunpowder, rather than from a counter-weight or spring. This energy is released very rapidly and can be replicated without much effort by the user. Therefore even early firearms such as the arquebus were much more powerful than human-powered weapons. Firearms became increasingly important and effective during the 16th century to 19th century, with progressive improvements in ignition mechanisms followed by revolutionary changes in ammunition handling and propellant. During the U.S. Civil War new applications of firearms including the machine gun and ironclad warship emerged that would still be recognizable and useful military weapons today, particularly in limited conflicts. In the 19th century warship propulsion changed from sail power to fossil fuel-powered steam engines.\n\nSince the mid-18th century North American French-Indian war through the beginning of the 20th century, human-powered weapons were reduced from the primary weaponry of the battlefield yielding to gunpowder-based weaponry. Sometimes referred to as the \"Age of Rifles\", this period was characterized by the development of firearms for infantry and cannons for support, as well as the beginnings of mechanized weapons such as the machine gun. \nOf particular note, Howitzers were able to destroy masonry fortresses and other fortifications, and this single invention caused a Revolution in Military Affairs (RMA), establishing tactics and doctrine that are still in use today. See Technology during World War I for a detailed discussion.\n\n\n==== Late modern ====\n\nAn important feature of industrial age warfare was technological escalation \u2013 innovations were rapidly matched through replication or countered by another innovation. The technological escalation during World War I (WW I) was profound, including the wide introduction of aircraft into warfare, and naval warfare with the introduction of aircraft carriers.\nWorld War I marked the entry of fully industrialized warfare as well as weapons of mass destruction (e.g., chemical and biological weapons), and new weapons were developed quickly to meet wartime needs. Above all, it promised to the military commanders the independence from the horse and the resurgence in maneuver warfare through extensive use of motor vehicles. The changes that these military technologies underwent before and during the Second World War were evolutionary, but defined the development for the rest of the century.\nThis period of innovation in weapon design continued in the inter-war period (between WW I and WW II) with continuous evolution of weapon systems by all major industrial powers. Many modern military weapons, particularly ground-based ones, are relatively minor improvements of weapon systems developed during World War II. See military technology during World War II for a detailed discussion.\nWorld War II however, perhaps marked the most frantic period of weapons development in the history of humanity. Massive numbers of new designs and concepts were fielded, and all existing technologies were improved between 1939 and 1945. The most powerful weapon invented during this period was the atomic bomb, however many other weapons influenced the world in ways overshadowed by the importance of nuclear weapons.\n\n\n===== Nuclear weapons =====\n\nSince the realization of mutually assured destruction (MAD), the nuclear option of all-out war is no longer considered a survivable scenario.  During the Cold War in the  years following World War II, both the United States and the Soviet Union engaged in a nuclear arms race. Each country and their allies continually attempted to out-develop each other in the field of nuclear armaments. Once the joint technological capabilities reached the point of being able to ensure the destruction of the Earth x100 fold, then a new tactic had to be developed.  With this realization, armaments development funding shifted back to primarily sponsoring the development of conventional arms technologies for support of limited wars rather than total war.During the late 2010s, tensions between the West and the East escalate as nuclear-based issues arise. Such event has been since dubbed as Cold War II.\n\n\n== Types ==\n\n\n=== By user ===\n- what person or unit uses the weaponPersonal weapons (or small arms) \u2013 designed to be used by a single person.\nLight weapons \u2013 'man-portable' weapons that may require a small team to operate.\nHeavy weapons \u2013 artillery and similar weapons larger than light weapons (see SALW).\nHunting weapon \u2013 used by hunters for sport or getting food.\nCrew served weapons \u2013 larger than personal weapons, requiring two or more people to operate correctly.\nFortification weapons \u2013 mounted in a permanent installation, or used primarily within a fortification.\nMountain weapons \u2013 for use by mountain forces or those operating in difficult terrain.\nVehicle weapons \u2013 to be mounted on any type of combat vehicle.\nRailway weapons \u2013 designed to be mounted on railway cars, including armored trains.\nAircraft weapons \u2013 carried on and used by some type of aircraft, helicopter, or other aerial vehicle.\nNaval weapons \u2013 mounted on ships and submarines.\nSpace weapons \u2013 are designed to be used in or launched from space.\nAutonomous weapons \u2013 are capable of accomplishing a mission with limited or no human intervention.\n\n\n=== By function ===\n- the construction of the weapon and principle of operationAntimatter weapons (theoretical) would combine matter and antimatter to cause a powerful explosion.\nArchery weapons operate by using a tensioned string and bent solid to launch a projectile.\nArtillery are firearms capable of launching heavy projectiles over long distances.\nBiological weapons spread biological agents, causing disease or infection.\nChemical weapons, poisoning and causing reactions.\nEnergy weapons rely on concentrating forms of energy to attack, such as lasers or sonic attack.\nExplosive weapons use a physical explosion to create blast concussion or spread shrapnel.\nFirearms use a chemical charge to launch projectiles.\nImprovised weapons are common objects, reused as weapons, such as crowbars and kitchen knives.\nIncendiary weapons cause damage by fire.\nNon-lethal weapons are designed to subdue without killing.\nMagnetic weapons use magnetic fields to propel projectiles, or to focus particle beams.\nMelee weapons operate as physical extensions of the user's body and directly impact their target.\nSome melee weapons can be penetrating weapons, which are known as blade weapons, which are designed to pierce through flesh and cause bleeding.\nOther melee weapons, known as blunt instruments, are non-penetrating weapons, designed more for creating a strain on the flesh through bruising.\nMissiles are rockets which are guided to their target after launch. (Also a general term for projectile weapons).\nLoitering Munitions are weapons that are designed to loiter over or in the battlefield searching for targets, striking once a target is located.\nNuclear weapons use radioactive materials to create nuclear fission and/or nuclear fusion detonations.\nPrimitive weapons make little or no use of technological or industrial elements.\nRanged weapons (unlike M\u00eal\u00e9e weapons), target a distant object or person.\nRockets use chemical propellant to accelerate a projectile\nSuicide weapons exploit the willingness of their operator not surviving the attack.\n\n\n=== By target ===\n- the type of target the weapon is designed to attackAnti-aircraft weapons target missiles and aerial vehicles in flight.\nAnti-fortification weapons are designed to target enemy installations.\nAnti-personnel weapons are designed to attack people, either individually or in numbers.\nAnti-radiation weapons target sources of electronic radiation, particularly radar emitters.\nAnti-satellite weapons target orbiting satellites.\nAnti-ship weapons target ships and vessels on water.\nAnti-submarine weapons target submarines and other underwater targets.\nAnti-tank weapons are designed to defeat armored targets.\nArea denial weapons target territory, making it unsafe or unsuitable for enemy use or travel.\nHunting weapons are weapons used to hunt game animals.\nInfantry support weapons are designed to attack various threats to infantry units.\n\n\n== Manufacture of weapons ==\n\nThe arms industry is a global industry that involves the sales and manufacture of weaponry. It consists of a commercial industry involved in the research and development, engineering, production, and servicing of military material, equipment, and facilities.\n\n\n== Legislation ==\nThe production, possession, trade and use of many weapons are controlled. This may be at a local or central government level, or international treaty. Examples of such controls include:\n\n\n=== Gun laws ===\n\nAll countries have laws and policies regulating aspects such as the manufacture, sale, transfer, possession, modification and use of small arms by civilians. \nCountries which regulate access to firearms will typically restrict access to certain categories of firearms and then restrict the categories of persons who may be granted a license for access to such firearms. There may be separate licenses for hunting, sport shooting (a.k.a. target shooting), self-defense, collecting, and concealed carry, with different sets of requirements, permissions, and responsibilities.\n\n\n=== Arms control laws ===\n\nInternational treaties and agreements place restrictions upon the development, production, stockpiling, proliferation and usage of weapons from small arms and heavy weapons to weapons of mass destruction. Arms control is typically exercised through the use of diplomacy which seeks to impose such limitations upon consenting participants, although it may also comprise efforts by a nation or group of nations to enforce limitations upon a non-consenting country.\n\n\n=== Arms trafficking laws ===\n\nArms trafficking is the trafficking of contraband weapons and ammunition. What constitutes legal trade in firearms varies widely, depending on local and national laws.\n\n\n== Lifecycle problems ==\nThere are a number of issue around the potential ongoing risks from deployed weapons, the safe storage of weapons, and their eventual disposal when no longer effective or safe.\n\nOcean dumping of unused weapons and bombs, including ordinary bombs, UXO, landmines and chemical weapons has been common practice by many nations, and often caused hazards.\nUnexploded ordnance (UXO) are bombs, land mines and naval mines and similar that did not explode when they were employed and still pose a risk for many years or decades.\nDemining or mine clearance from areas of past conflict is a difficult process, but every year, landmines kill 15,000 to 20,000 people and severely maim countless more.\nNuclear terrorism was a serious concern after the fall of the Soviet Union, with the prospect of \"loose nukes\" being available. While this risk may have receded, similar situation may arise in the future.\n\n\n== In science fiction ==\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n The dictionary definition of weapon at Wiktionary\n Quotations related to Weapon at Wikiquote\n Media related to Weapons at Wikimedia Commons", "robotics": "Robotics is an interdisciplinary branch of engineering and science that includes mechanical engineering, electronics engineering, computer science, and others. Robotics deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing.\nThese technologies are used to develop machines that can substitute for humans and replicate human actions. Robots can be used in any situation and for any purpose, but today many are used in dangerous environments (including bomb detection and deactivation), manufacturing processes, or where humans cannot survive. Robots can take on any form but some are made to resemble humans in appearance. This is said to help in the acceptance of a robot in certain replicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, and basically anything a human can do. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.\nThe concept of creating machines that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (science, technology, engineering, and mathematics) as a teaching aid.\nRobotics is a branch of engineering that involves the conception, design, manufacture, and operation of robots. This field overlaps with electronics, computer science, artificial intelligence, mechatronics, nanotechnology and bioengineering.\n\n\n== Etymology ==\nThe word robotics was derived from the word robot, which was introduced to the public by Czech writer Karel \u010capek in his play R.U.R. (Rossum's Universal Robots), which was published in 1920. The word robot comes from the Slavic word rabota, which means labour/work. The play begins in a factory that makes artificial people called robots, creatures who can be mistaken for humans \u2013 very similar to the modern ideas of androids. Karel \u010capek himself did not coin the word. He wrote a short letter in reference to an etymology in the Oxford English Dictionary in which he named his brother Josef \u010capek as its actual originator.According to the Oxford English Dictionary, the word robotics was first used in print by Isaac Asimov, in his science fiction short story \"Liar!\", published in May 1941 in Astounding Science Fiction. Asimov was unaware that he was coining the term; since the science and technology of electrical devices is electronics, he assumed robotics already referred to the science and technology of robots. In some of Asimov's other works, he states that the first use of the word robotics was in his short story Runaround (Astounding Science Fiction, March 1942), where he introduced his concept of The Three Laws of Robotics. However, the original publication of \"Liar!\" predates that of \"Runaround\" by ten months, so the former is generally cited as the word's origin.\n\n\n== History ==\n\nIn 1948, Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.\nFully autonomous only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. Commercial and industrial robots are widespread today and used to perform jobs more cheaply, more accurately and more reliably, than humans. They are also employed in some jobs which are too dirty, dangerous, or dull to be suitable for humans. Robots are widely used in manufacturing, assembly, packing and packaging, mining, transport, earth and space exploration, surgery, weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.\n\n\n== Robotic aspects ==\n\nThere are many types of robots; they are used in many different environments and for many different uses, although being very diverse in application and form they all share three basic similarities when it comes to their construction:\n\nRobots all have some kind of mechanical construction, a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud, might use caterpillar tracks. The mechanical aspect is mostly the creator's solution to completing the assigned task and dealing with the physics of the environment around it. Form follows function.\nRobots have electrical components which power and control the machinery. For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol powered machines that get their power mainly from petrol still require an electric current to start the combustion process which is why most petrol powered machines like cars, have batteries. The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status) and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations)\nAll robots contain some level of computer programming code. A program is how a robot decides when or how to do something. In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction and receive the correct amount of power from its battery, but would not go anywhere without a program telling it to move. Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly constructed its performance will be very poor (or it may not perform at all). There are three different types of robotic programs: remote control, artificial intelligence and hybrid. A robot with remote control programing has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with a remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. Hybrid is a form of programming that incorporates both AI and RC functions.\n\n\n== Applications ==\nAs more and more robots are designed for specific tasks this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed as \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot  i.e. the welding equipment along with other material handling facilities like turntables etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labelled as \"heavy duty robots\".\nCurrent and potential applications include:\n\nMilitary robots\nCaterpillar plans to develop remote controlled machines and expects to develop fully autonomous heavy robots by 2021.  Some cranes already are remote controlled.\nIt was demonstrated that a robot can perform a herding task.\nRobots are increasingly used in manufacturing (since the 1960s). In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that is 100% automated.\nRobots such as HOSPI are used as couriers in hospitals (hospital robot). Other hospital tasks performed by robots are receptionists, guides and porters helpers.\nRobots can serve as waiters and cooks, also at home.  Boris is a robot that can load a dishwasher. Rotimatic is a robotics kitchen appliance that cooks flatbreads automatically.\nRobot combat for sport \u2013 hobby or sport event where two or more robots fight in an arena to disable each other. This has developed from a hobby in the 1990s to several TV series worldwide.\nCleanup of contaminated areas, such as toxic waste or nuclear facilities.\nAgricultural robots  (AgRobots).\nDomestic robots, cleaning and caring for the elderly\nMedical robots performing low-invasive surgery\nHousehold robots with full use.\nNanorobots\nSwarm robotics\n\n\n== Components ==\n\n\n=== Power source ===\n\nAt present, mostly (lead\u2013acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead\u2013acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver\u2013cadmium batteries that are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need a fuel, require heat dissipation and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage. Potential power sources could be:\n\npneumatic (compressed gases)\nSolar power (using the sun's energy and converting it into electrical power)\nhydraulics (liquids)\nflywheel energy storage\norganic garbage (through anaerobic digestion)\nnuclear\n\n\n=== Actuation ===\n\nActuators are the \"muscles\" of a robot, the parts which convert stored energy into movement. By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\n\n\n==== Electric motors ====\n\nThe vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\n\n\n==== Linear actuators ====\n\nVarious types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator).\n\n\n==== Series elastic actuators ====\nA flexure is designed as part of the motor actuator, to improve safety and provide robust force control, energy efficiency, shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. The resultant lower reflected inertia can improve safety when a robot is interacting with humans or during collisions. It has been used in various robots, particularly advanced manufacturing robots and walking humanoid robots.\n\n\n==== Air muscles ====\n\nPneumatic artificial muscles, also known as air muscles, are special tubes that expand(typically up to 40%) when air is forced inside them. They are used in some robot applications.\n\n\n==== Muscle wire ====\n\nMuscle wire, also known as shape memory alloy, Nitinol\u00ae or Flexinol\u00ae wire, is a material which contracts (under 5%) when electricity is applied. They have been used for some small robot applications.\n\n\n==== Electroactive polymers ====\n\nEAPs or EPAMs are a new plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots, and to enable new robots to float, fly, swim or walk.\n\n\n==== Piezo motors ====\n\nRecent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line. Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size. These motors are already available commercially, and being used on some robots.\n\n\n==== Elastic nanotubes ====\n\nElastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.\n\n\n=== Sensing ===\n\nSensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information of the task it is performing.\n\n\n==== Touch ====\n\nCurrent robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips. The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting robotic grip on held objects.\nScientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one\u2014allowing patients to write with it, type on a keyboard, play piano and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feeling in its fingertips.\n\n\n==== Vision ====\n\nComputer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\nIn most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\nComputer vision systems rely on image sensors which detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\nThere is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have their background in biology.\n\n\n==== Other ====\nOther common forms of sensing in robotics use lidar, radar, and sonar.\n\n\n=== Manipulation ===\n\nRobots need to manipulate objects; pick up, modify, destroy, or otherwise have an effect. Thus the \"hands\" of a robot are often referred to as end effectors, while the \"arm\" is referred to as a manipulator. Most robot arms have replaceable effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator which cannot be replaced, while a few have one very general purpose manipulator, for example, a humanoid hand.\nLearning how to manipulate a robot often requires a close feedback between human to the robot, although there are several methods for remote manipulation of robots.\n\n\n==== Mechanical grippers ====\nOne of the most common effectors is the gripper. In its simplest manifestation, it consists of just two fingers which can open and close to pick up and let go of a range of small objects. Fingers can for example, be made of a chain with a metal wire run through it. Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand. Hands that are of a mid-level complexity include the Delft hand. Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\n\n\n==== Vacuum grippers ====\nVacuum grippers are very simple astrictive devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\nPick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum grippers.\n\n\n==== General purpose effectors ====\nSome advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS, and the Schunk hand. These are highly dexterous manipulators, with as many as 20 degrees of freedom and hundreds of tactile sensors.\n\n\n=== Locomotion ===\n\n\n==== Rolling robots ====\n\nFor simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\n\n\n===== Two-wheeled balancing robots =====\nBalancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum. Many different balancing robots have been designed. While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.\n\n\n===== One-wheeled balancing robots =====\n\nA one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" that is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\". Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.\n\n\n===== Spherical orb robots =====\n\nSeveral attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball, or by rotating the outer shells of the sphere. These have also been referred to as an orb bot or a ball bot.\n\n\n===== Six-wheeled robots =====\nUsing six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\n\n\n===== Tracked robots =====\n\nTank tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor and military robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".\n\n\n==== Walking applied to robots ====\nWalking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University. Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct. Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Hybrids too have been proposed in movies such as I, Robot, where they walk on two legs and switch to four (arms+legs) when going to a sprint. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\n\n\n===== ZMP technique =====\n\nThe zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over). However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory. ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\n\n\n===== Hopping =====\nSeveral robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself. Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults. A quadruped was also demonstrated which could trot, run, pace, and bound. For a full list of these robots, see the MIT Leg Lab Robots page.\n\n\n===== Dynamic balancing (controlled falling) =====\nA more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability. This technique was recently demonstrated by Anybots' Dexter Robot, which is so stable, it can even jump. Another example is the TU Delft Flame.\n\n\n===== Passive dynamics =====\n\nPerhaps the most promising approach utilizes passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.\n\n\n==== Other methods of locomotion ====\n\n\n===== Flying =====\n\nA modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing. Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, propelled by paddles, and guided by sonar.\n\n\n===== Snaking =====\nSeveral snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings. The Japanese ACM-R5 snake robot can even navigate both on land and in water.\n\n\n===== Skating =====\nA small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll. Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.\n\n\n===== Climbing =====\nSeveral different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin, built by Dr. Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot and Stickybot. China's Technology Daily reported on November 15, 2008, that Dr. Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Dr. Li, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole..\n\n\n===== Swimming (Piscine) =====\nIt is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%. Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion. Notable examples are the Essex University Computer Science Robotic Fish G9, and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion. The Aqua Penguin, designed and built by Festo of Germany, copies the streamlined shape and propulsion by front \"flippers\" of penguins. Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.\n\nIn 2014 iSplash-II was developed by PhD student Richard James Clapham and Prof. Huosheng Hu at Essex University. It was the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained. This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s). The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.\n\n\n===== Sailing =====\n\nSailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos built by IFREMER and ENSTA-Bretagne. Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\n\n\n=== Environmental interaction and navigation ===\n\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Mein\u00fc robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information. Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\n\n\n=== Human-robot interaction ===\n\nThe state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation.\n\n\n==== Speech recognition ====\n\nInterpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech. The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent. Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952. Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.\n\n\n==== Robotic voice ====\nOther hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium, making it necessary to develop the emotional component of robotic voice through various techniques.\n\n\n==== Gestures ====\n\nOne can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots. A great many systems have been developed to recognize human hand gestures.\n\n\n==== Facial expression ====\n\nFacial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos). The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.\n\n\n==== Artificial emotions ====\nArtificial emotions can also be generated, composed of a sequence of facial expressions and/or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots.\n\n\n==== Personality ====\nMany of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future. Nevertheless, researchers are trying to create robots which appear to have a personality: i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.\n\n\n==== Social Intelligence ====\nThe Socially Intelligent Machines Lab of the  Georgia Institute of Technology researches new concepts of guided teaching interaction with robots. The aim of the projects is a social robot that learns task and goals from human demonstrations without prior knowledge of high-level concepts. These new concepts are grounded from low-level continuous sensor data through unsupervised learning, and task goals are subsequently learned using a Bayesian approach. These concepts can be used to transfer knowledge to future tasks, resulting in faster learning of those tasks. The results are demonstrated by the robot Curi who can scoop some pasta from a pot onto a plate and serve the sauce on top.\n\n\n== Control ==\n\nThe mechanical structure of a robot must be controlled to perform tasks. The control of a robot involves three distinct phases \u2013 perception, processing, and action (robotic paradigms). Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors) which move the mechanical.\nThe processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands. Sensor fusion may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction) is inferred from these estimates. Techniques from control theory convert the task into commands that drive the actuators.\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how they interact. Pattern recognition and computer vision can be used to track objects. Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\n\n\n=== Autonomy levels ===\n\nControl systems may also have varying levels of autonomy.\n\nDirect interaction is used for haptic or teleoperated devices, and the human has nearly complete control over the robot's motion.\nOperator-assist modes have the operator commanding medium-to-high-level tasks, with the robot automatically figuring out how to achieve them.\nAn autonomous robot may go without human interaction for extended periods of time . Higher levels of autonomy do not necessarily require more complex cognitive capabilities. For example, robots in assembly plants are completely autonomous but operate in a fixed pattern.Another classification takes into account the interaction between human control and the machine motions.\n\nTeleoperation. A human controls each movement, each machine actuator change is specified by the operator.\nSupervisory. A human specifies general moves or position changes and the machine decides specific movements of its actuators.\nTask-level autonomy. The operator specifies only the task and the robot manages itself to complete it.\nFull autonomy. The machine will create and complete all its tasks without human interaction.\n\n\n== Research ==\n\nMuch of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.\nA first particular new innovation in robot design is the open sourcing of robot-projects. To describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have the intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.The second is evolutionary robots. This is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots, and to explore the nature of evolution. Because the process often requires many generations of robots to be simulated, this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough. Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.\n\n\n=== Dynamics and kinematics ===\n\nThe study of motion can be divided into kinematics and dynamics. Direct kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\nIn each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\n\n\n=== Bionics and biomimetics ===\nBionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots.  For example, the design of BionicKangaroo was based on the way kangaroos jump.\n\n\n== Education and training ==\n\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics. Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students. First-year computer science courses at some universities now include programming of a robot in addition to traditional software engineering-based coursework.\n\n\n=== Career training ===\nUniversities offer bachelors, masters, and doctoral degrees in the field of robotics. Vocational schools offer robotics training aimed at careers in robotics.\n\n\n=== Certification ===\nThe Robotics Certification Standards Alliance (RCSA) is an international robotics certification authority that confers various industry- and educational-related robotics certifications.\n\n\n=== Summer robotics camp ===\nSeveral national summer camp programs include robotics as part of their core curriculum. In addition, youth summer robotics programs are frequently offered by celebrated museums and institutions.\n\n\n=== Robotics competitions ===\n\nThere are lots of competitions all around the globe. One of the most important competitions is the FLL or FIRST Lego League. The idea of this specific competition is that kids start developing knowledge and getting into robotics while playing with Legos since they are 9 years old. This competition is associated with Ni or National Instruments.\n\n\n=== Robotics afterschool programs ===\nMany schools across the country are beginning to add robotics programs to their after school curriculum. Some major programs for afterschool robotics include FIRST Robotics Competition, Botball and B.E.S.T. Robotics. Robotics competitions often include aspects of business and marketing as well as engineering and design.\nThe Lego company began a program for children to learn and get excited about robotics at a young age.\n\n\n== Employment ==\n\nRobotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics\u2013related jobs grow and have been observed to be steadily rising. The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long term investment for benefactors. A paper by Michael Osborne and Carl Benedikt Frey found that 47 per cent of US jobs are at risk to automation \"over some unspecified number of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment.\n\n\n== Occupational safety and health implications ==\n\nA discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defence, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.Despite these advances, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programmes and trying to promote a safe and flexible co-operation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\nIn future, co-operation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards aiming to protect employees from the risk of working with collaborative robots will have to be revised.\n\n\n== See also ==\n\nAnderson Powerpole connector\nArtificial intelligence\nAutonomous robot\nCloud robotics\nCognitive robotics\nEvolutionary robotics\nGlossary of robotics\nIndex of robotics articles\nMechatronics\nMulti-agent system\nOutline of robotics\nRoboethics\nRobot rights\nRobotic governance\nSoft robotics\n\n\n== References ==\n\n\n== Further reading ==\nR. Andrew Russell (1990). Robot Tactile Sensing. New York: Prentice Hall. ISBN 0-13-781592-1. \nE McGaughey, 'Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy' (2018) SSRN, part 2(3)\nDH Autor, \u2018Why Are There Still So Many Jobs? The History and Future of Workplace Automation\u2019 (2015) 29(3) Journal of Economic Perspectives 3\n\n\n== External links ==\n\nRobotics at Curlie (based on DMOZ)\nIEEE Robotics and Automation Society\nFuture Robotics \u2013 The Human Algorithm\nInvestigation of social robots \u2013 Robots that mimic human behaviors and gestures.\nWired's guide to the '50 best robots ever', a mix of robots in fiction (Hal, R2D2, K9) to real robots (Roomba, Mobot, Aibo).\nNotable Chinese Firms Emerging in Medical Robots Sector(GCiS)", "nuclear technology": "Nuclear technology is technology that involves the nuclear reactions of atomic nuclei. Among the notable nuclear technologies are nuclear reactors, nuclear medicine and nuclear weapons. It is also used, among other things, in smoke detectors and gun sights.\n\n\n== History and scientific background ==\n\n\n=== Discovery ===\n\nThe vast majority of common, natural phenomena on Earth only involve gravity and electromagnetism, and not nuclear reactions. This is because atomic nuclei are generally kept apart because they contain positive electrical charges and therefore repel each other.\nIn 1896, Henri Becquerel was investigating phosphorescence in uranium salts when he discovered a new phenomenon which came to be called radioactivity. He, Pierre Curie and Marie Curie began investigating the phenomenon. In the process, they isolated the element radium, which is highly radioactive. They discovered that radioactive materials produce intense, penetrating rays of three distinct sorts, which they labeled alpha, beta, and gamma after the first three Greek letters. Some of these kinds of radiation could pass through ordinary matter, and all of them could be harmful in large amounts. All of the early researchers received various radiation burns, much like sunburn, and thought little of it.\nThe new phenomenon of radioactivity was seized upon by the manufacturers of quack medicine (as had the discoveries of electricity and magnetism, earlier), and a number of patent medicines and treatments involving radioactivity were put forward.\nGradually it was realized that the radiation produced by radioactive decay was ionizing radiation, and that even quantities too small to burn could pose a severe long-term hazard. Many of the scientists working on radioactivity died of cancer as a result of their exposure. Radioactive patent medicines mostly disappeared, but other applications of radioactive materials persisted, such as the use of radium salts to produce glowing dials on meters.\nAs the atom came to be better understood, the nature of radioactivity became clearer. Some larger atomic nuclei are unstable, and so decay (release matter or energy) after a random interval. The three forms of radiation that Becquerel and the Curies discovered are also more fully understood. Alpha decay is when a nucleus releases an alpha particle, which is two protons and two neutrons, equivalent to a helium nucleus. Beta decay is the release of a beta particle, a high-energy electron. Gamma decay releases gamma rays, which unlike alpha and beta radiation are not matter but electromagnetic radiation of very high frequency, and therefore energy. This type of radiation is the most dangerous and most difficult to block. All three types of radiation occur naturally in certain elements.\nIt has also become clear that the ultimate source of most terrestrial energy is nuclear, either through radiation from the Sun caused by stellar thermonuclear reactions or by radioactive decay of uranium within the Earth, the principal source of geothermal energy.\n\n\n=== Nuclear fission ===\n\nIn natural nuclear radiation, the byproducts are very small compared to the nuclei from which they originate. Nuclear fission is the process of splitting a nucleus into roughly equal parts, and releasing energy and neutrons in the process. If these neutrons are captured by another unstable nucleus, they can fission as well, leading to a chain reaction. The average number of neutrons released per nucleus that go on to fission another nucleus is referred to as k. Values of k larger than 1 mean that the fission reaction is releasing more neutrons than it absorbs, and therefore is referred to as a self-sustaining chain reaction. A mass of fissile material large enough (and in a suitable configuration) to induce a self-sustaining chain reaction is called a critical mass.\nWhen a neutron is captured by a suitable nucleus, fission may occur immediately, or the nucleus may persist in an unstable state for a short time. If there are enough immediate decays to carry on the chain reaction, the mass is said to be prompt critical, and the energy release will grow rapidly and uncontrollably, usually leading to an explosion.\nWhen discovered on the eve of World War II, this insight led multiple countries to begin programs investigating the possibility of constructing an atomic bomb \u2014 a weapon which utilized fission reactions to generate far more energy than could be created with chemical explosives. The Manhattan Project, run by the United States with the help of the United Kingdom and Canada, developed multiple fission weapons which were used against Japan in 1945 at Hiroshima and Nagasaki. During the project, the first fission reactors were developed as well, though they were primarily for weapons manufacture and did not generate electricity.\nIn 1951, the first nuclear fission power plant was the first to produce electricity at the Experimental Breeder Reactor No. 1 (EBR-1), in Arco, Idaho, ushering in the \"Atomic Age\" of more intensive human energy use.However, if the mass is critical only when the delayed neutrons are included, then the reaction can be controlled, for example by the introduction or removal of neutron absorbers. This is what allows nuclear reactors to be built. Fast neutrons are not easily captured by nuclei; they must be slowed (slow neutrons), generally by collision with the nuclei of a neutron moderator, before they can be easily captured. Today, this type of fission is commonly used to generate electricity.\n\n\n=== Nuclear fusion ===\n\nIf nuclei are forced to collide, they can undergo nuclear fusion. This process may release or absorb energy. When the resulting nucleus is lighter than that of iron, energy is normally released; when the nucleus is heavier than that of iron, energy is generally absorbed. This process of fusion occurs in stars, which derive their energy from hydrogen and helium. They form, through stellar nucleosynthesis, the light elements (lithium to calcium) as well as some of the heavy elements (beyond iron and nickel, via the S-process). The remaining abundance of heavy elements, from nickel to uranium and beyond, is due to supernova nucleosynthesis, the R-process.\nOf course, these natural processes of astrophysics are not examples of nuclear \"technology\". Because of the very strong repulsion of nuclei, fusion is difficult to achieve in a controlled fashion. Hydrogen bombs obtain their enormous destructive power from fusion, but their energy cannot be controlled. Controlled fusion is achieved in particle accelerators; this is how many synthetic elements are produced. A fusor can also produce controlled fusion and is a useful neutron source. However, both of these devices operate at a net energy loss. Controlled, viable fusion power has proven elusive, despite the occasional hoax. Technical and theoretical difficulties have hindered the development of working civilian fusion technology, though research continues to this day around the world.\nNuclear fusion was initially pursued only in theoretical stages during World War II, when scientists on the Manhattan Project (led by Edward Teller) investigated it as a method to build a bomb. The project abandoned fusion after concluding that it would require a fission reaction to detonate. It took until 1952 for the first full hydrogen bomb to be detonated, so-called because it used reactions between deuterium and tritium. Fusion reactions are much more energetic per unit mass of fuel than fission reactions, but starting the fusion chain reaction is much more difficult.\n\n\n== Nuclear weapons ==\n\nA nuclear weapon is an explosive device that derives its destructive force from nuclear reactions, either fission or a combination of fission and fusion. Both reactions release vast quantities of energy from relatively small amounts of matter. Even small nuclear devices can devastate a city by blast, fire and radiation. Nuclear weapons are considered weapons of mass destruction, and their use and control has been a major aspect of international policy since their debut.\nThe design of a nuclear weapon is more complicated than it might seem. Such a weapon must hold one or more subcritical fissile masses stable for deployment, then induce criticality (create a critical mass) for detonation. It also is quite difficult to ensure that such a chain reaction consumes a significant fraction of the fuel before the device flies apart. The procurement of a nuclear fuel is also more difficult than it might seem, since sufficiently unstable substances for this process do not currently occur naturally on Earth in suitable amounts.\nOne isotope of uranium, namely uranium-235, is naturally occurring and sufficiently unstable, but it is always found mixed with the more stable isotope uranium-238. The latter accounts for more than 99% of the weight of natural uranium. Therefore, some method of isotope separation based on the weight of three neutrons must be performed to enrich (isolate) uranium-235.\nAlternatively, the element plutonium possesses an isotope that is sufficiently unstable for this process to be usable. Terrestrial plutonium does not currently occur naturally in sufficient quantities for such use, so it must be manufactured in a nuclear reactor.\nUltimately, the Manhattan Project manufactured nuclear weapons based on each of these elements. They detonated the first nuclear weapon in a test code-named \"Trinity\", near Alamogordo, New Mexico, on July 16, 1945. The test was conducted to ensure that the implosion method of detonation would work, which it did. A uranium bomb, Little Boy, was dropped on the Japanese city Hiroshima on August 6, 1945, followed three days later by the plutonium-based Fat Man on Nagasaki. In the wake of unprecedented devastation and casualties from a single weapon, the Japanese government soon surrendered, ending World War II.\nSince these bombings, no nuclear weapons have been deployed offensively. Nevertheless, they prompted an arms race to develop increasingly destructive bombs to provide a nuclear deterrent. Just over four years later, on August 29, 1949, the Soviet Union detonated its first fission weapon. The United Kingdom followed on October 2, 1952; France, on February 13, 1960; and China component to a nuclear weapon. Approximately half of the deaths from Hiroshima and Nagasaki died two to five years afterward from radiation exposure. A radiological weapons is a type of nuclear weapon designed to distribute hazardous nuclear material in enemy areas. Such a weapon would not have the explosive capability of a fission or fusion bomb, but would kill many people and contaminate a large area. A radiological weapon has never been deployed. While considered useless by a conventional military, such a weapon raises concerns over nuclear terrorism.\nThere have been over 2,000 nuclear tests conducted since 1945. In 1963, all nuclear and many non-nuclear states signed the Limited Test Ban Treaty, pledging to refrain from testing nuclear weapons in the atmosphere, underwater, or in outer space. The treaty permitted underground nuclear testing. France continued atmospheric testing until 1974, while China continued up until 1980. The last underground test by the United States was in 1992, the Soviet Union in 1990, the United Kingdom in 1991, and both France and China continued testing until 1996. After signing the Comprehensive Test Ban Treaty in 1996 (which had as of 2011 not entered into force), all of these states have pledged to discontinue all nuclear testing. Non-signatories India and Pakistan last tested nuclear weapons in 1998.\nNuclear weapons are the most destructive weapons known - the archetypal weapons of mass destruction. Throughout the Cold War, the opposing powers had huge nuclear arsenals, sufficient to kill hundreds of millions of people. Generations of people grew up under the shadow of nuclear devastation, portrayed in films such as Dr. Strangelove and The Atomic Cafe.\nHowever, the tremendous energy release in the detonation of a nuclear weapon also suggested the possibility of a new energy source.\n\n\n== Civilian uses ==\n\n\n=== Nuclear power ===\n\nNuclear power is a type of nuclear technology involving the controlled use of nuclear fission to release energy for work including propulsion, heat, and the generation of electricity. Nuclear energy is produced by a controlled nuclear chain reaction which creates heat\u2014and which is used to boil water, produce steam, and drive a steam turbine. The turbine is used to generate electricity and/or to do mechanical work.\nCurrently nuclear power provides approximately 15.7% of the world's electricity (in 2004) and is used to propel aircraft carriers, icebreakers and submarines (so far economics and fears in some ports have prevented the use of nuclear power in transport ships). All nuclear power plants use fission. No man-made fusion reaction has resulted in a viable source of electricity.\n\n\n=== Medical applications ===\n\nThe medical applications of nuclear technology are divided into diagnostics and radiation treatment.\nImaging - The largest use of ionizing radiation in medicine is in medical radiography to make images of the inside of the human body using x-rays. This is the largest artificial source of radiation exposure for humans. Medical and dental x-ray imagers use of cobalt-60 or other x-ray sources. A number of radiopharmaceuticals are used, sometimes attached to organic molecules, to act as radioactive tracers or contrast agents in the human body. Positron emitting nucleotides are used for high resolution, short time span imaging in applications known as Positron emission tomography.\nRadiation is also used to treat diseases in radiation therapy.\n\n\n=== Industrial applications ===\nSince some ionizing radiation can penetrate matter, they are used for a variety of measuring methods. X-rays and gamma rays are used in industrial radiography to make images of the inside of solid products, as a means of nondestructive testing and inspection. The piece to be radiographed is placed between the source and a photographic film in a cassette. After a certain exposure time, the film is developed and it shows any internal defects of the material.\nGauges - Gauges use the exponential absorption law of gamma rays\n\nLevel indicators: Source and detector are placed at opposite sides of a container, indicating the presence or absence of material in the horizontal radiation path. Beta or gamma sources are used, depending on the thickness and the density of the material to be measured. The method is used for containers of liquids or of grainy substances\nThickness gauges: if the material is of constant density, the signal measured by the radiation detector depends on the thickness of the material. This is useful for continuous production, like of paper, rubber, etc.Electrostatic control - To avoid the build-up of static electricity in production of paper, plastics, synthetic textiles, etc., a ribbon-shaped source of the alpha emitter 241Am can be placed close to the material at the end of the production line. The source ionizes the air to remove electric charges on the material.\nRadioactive tracers - Since radioactive isotopes behave, chemically, mostly like the inactive element, the behavior of a certain chemical substance can be followed by tracing the radioactivity. Examples:\n\nAdding a gamma tracer to a gas or liquid in a closed system makes it possible to find a hole in a tube.\nAdding a tracer to the surface of the component of a motor makes it possible to measure wear by measuring the activity of the lubricating oil.Oil and Gas Exploration- Nuclear well logging is used to help predict the commercial viability of new or existing wells. The technology involves the use of a neutron or gamma-ray source and a radiation detector which are lowered into boreholes to determine the properties of the surrounding rock such as porosity and lithography.[1]\nRoad Construction - Nuclear moisture/density gauges are used to determine the density of soils, asphalt, and concrete. Typically a cesium-137 source is used.\n\n\n=== Commercial applications ===\nradioluminescence\ntritium illumination: Tritium is used with phosphor in rifle sights to increase nighttime firing accuracy. Some runway markers and building exit signs use the same technology, to remain illuminated during blackouts.\nBetavoltaics.\nSmoke detector: An ionization smoke detector includes a tiny mass of radioactive americium-241, which is a source of alpha radiation. Two ionisation chambers are placed next to each other. Both contain a small source of 241Am that gives rise to a small constant current. One is closed and serves for comparison, the other is open to ambient air; it has a gridded electrode. When smoke enters the open chamber, the current is disrupted as the smoke particles attach to the charged ions and restore them to a neutral electrical state. This reduces the current in the open chamber. When the current drops below a certain threshold, the alarm is triggered.\n\n\n=== Food processing and agriculture ===\nIn biology and agriculture, radiation is used to induce mutations to produce new or improved species. Another use in insect control is the sterile insect technique, where male insects are sterilized by radiation and released, so they have no offspring, to reduce the population.\nIn industrial and food applications, radiation is used for sterilization of tools and equipment. An advantage is that the object may be sealed in plastic before sterilization. An emerging use in food production is the sterilization of food using food irradiation.\n\n \nFood irradiation is the process of exposing food to ionizing radiation in order to destroy microorganisms, bacteria, viruses, or insects that might be present in the food. The radiation sources used include radioisotope gamma ray sources, X-ray generators and electron accelerators. Further applications include sprout inhibition, delay of ripening, increase of juice yield, and improvement of re-hydration. Irradiation is a more general term of deliberate exposure of materials to radiation to achieve a technical goal (in this context 'ionizing radiation' is implied). As such it is also used on non-food items, such as medical hardware, plastics, tubes for gas-pipelines, hoses for floor-heating, shrink-foils for food packaging, automobile parts, wires and cables (isolation), tires, and even gemstones. Compared to the amount of food irradiated, the volume of those every-day applications is huge but not noticed by the consumer.\nThe genuine effect of processing food by ionizing radiation relates to damages to the DNA, the basic genetic information for life. Microorganisms can no longer proliferate and continue their malignant or pathogenic activities. Spoilage causing micro-organisms cannot continue their activities. Insects do not survive or become incapable of procreation. Plants cannot continue the natural ripening or aging process. All these effects are beneficial to the consumer and the food industry, likewise.The amount of energy imparted for effective food irradiation is low compared to cooking the same; even at a typical dose of 10 kGy most food, which is (with regard to warming) physically equivalent to water, would warm by only about 2.5 \u00b0C (4.5 \u00b0F).\nThe specialty of processing food by ionizing radiation is the fact, that the energy density per atomic transition is very high, it can cleave molecules and induce ionization (hence the name) which cannot be achieved by mere heating. This is the reason for new beneficial effects, however at the same time, for new concerns. The treatment of solid food by ionizing radiation can provide an effect similar to heat pasteurization of liquids, such as milk. However, the use of the term, cold pasteurization, to describe irradiated foods is controversial, because pasteurization and irradiation are fundamentally different processes, although the intended end results can in some cases be similar.\nDetractors of food irradiation have concerns about the health hazards of induced radioactivity. Also, a report for the American Council on Science and Health entitled \"Irradiated Foods\" states: \"The types of radiation sources approved for the treatment of foods have specific energy levels well below that which would cause any element in food to become radioactive. Food undergoing irradiation does not become any more radioactive than luggage passing through an airport X-ray scanner or teeth that have been X-rayed.\" Food irradiation is currently permitted by over 40 countries and volumes are estimated to exceed 500,000 metric tons (490,000 long tons; 550,000 short tons) annually worldwide.Food irradiation is essentially a non-nuclear technology; it relies on the use of ionizing radiation which may be generated by accelerators for electrons and conversion into bremsstrahlung, but which may use also gamma-rays from nuclear decay. There is a worldwide industry for processing by ionizing radiation, the majority by number and by processing power using accelerators. Food irradiation is only a niche application compared to medical supplies, plastic materials, raw materials, gemstones, cables and wires, etc.\n\n\n== Accidents ==\n\nNuclear accidents, because of the powerful forces involved, are often very dangerous. Historically, the first incidents involved fatal radiation exposure. Marie Curie died from aplastic anemia which resulted from her high levels of exposure. Two scientists, an American and Canadian respectively, Harry Daghlian and Louis Slotin, died after mishandling the same plutonium mass. Unlike conventional weapons, the intense light, heat, and explosive force is not the only deadly component to a nuclear weapon. Approximately half of the deaths from Hiroshima and Nagasaki died two to five years afterward from radiation exposure.Civilian nuclear and radiological accidents primarily involve nuclear power plants. Most common are nuclear leaks that expose workers to hazardous material. A nuclear meltdown refers to the more serious hazard of releasing nuclear material into the surrounding environment. The most significant meltdowns occurred at Three Mile Island in Pennsylvania and Chernobyl in the Soviet Ukraine. The earthquake and tsunami on March 11, 2011 caused serious damage to three nuclear reactors and a spent fuel storage pond at the Fukushima Daiichi nuclear power plant in Japan. Military reactors that experienced similar accidents were Windscale in the United Kingdom and SL-1 in the United States.\nMilitary accidents usually involve the loss or unexpected detonation of nuclear weapons. The Castle Bravo test in 1954 produced a larger yield than expected, which contaminated nearby islands, a Japanese fishing boat (with one fatality), and raised concerns about contaminated fish in Japan. In the 1950s through 1970s, several nuclear bombs were lost from submarines and aircraft, some of which have never been recovered. The last twenty years have seen a marked decline in such accidents.\n\n\n== See also ==\n\nAtomic age\nLists of nuclear disasters and radioactive incidents\nNuclear power debate\nOutline of nuclear technology\n\n\n== References ==\n\n\n== External links ==\nNuclear Energy Institute \u2013 Beneficial Uses of Radiation\nNuclear Technology\nNational Isotope Development Center \u2013 U.S. Government source of isotopes for basic and applied nuclear science and nuclear technology - production, research, development, distribution, and information", "big science": "Big science is a term used by scientists and historians of science to describe a series of changes in science which occurred in industrial nations during and after World War II, as scientific progress increasingly came to rely on large-scale projects usually funded by national governments or groups of governments. Individual or small group efforts, or Small Science, is still relevant today as theoretical results by individual authors may have a significant impact, but very often the empirical verification requires experiments using constructions, such as the Large Hadron Collider, costing between $5 and $10 billion.\n\n\n== Development ==\n\nWhile science and technology have always been important to and driven by warfare, the increase in military funding of science following the second World War was on a scale wholly unprecedented.  James Conant, in a 1941 letter to Chemical Engineering News, said that World War II \"is a physicist's war rather than a chemist's,\" a phrase that was cemented in the vernacular in post-war discussion of the role that those scientists played in the development of new weapons and tools, notably the proximity fuze, radar, and the atomic bomb.  The bulk of these last two activities took place in a new form of research facility: the government-sponsored laboratory, employing thousands of technicians and scientists, managed by universities (in this case, the University of California and the Massachusetts Institute of Technology).\nIn the shadow of the first atomic weapons, the importance of a strong scientific research establishment was apparent to any country wishing to play a major role in international politics. After the success of the Manhattan Project, governments became the chief patron of science, and the character of the scientific establishment underwent several key changes. This was especially marked in the United States and the Soviet Union during the Cold War, but also to a lesser extent in many other countries.\n\n\n== Definitions ==\n\"Big science\" usually implies one or more of these specific characteristics:\n\nBig budgets: No longer required to rely on philanthropy or industry, scientists were able to use budgets on an unprecedented scale for basic research.\nBig staffs: Similarly, the number of practitioners of science on any one project grew as well, creating difficulty, and often controversy, in the assignment of credit for scientific discoveries (the Nobel Prize system, for example, allows awarding only three individuals in any one topic per year, based on a 19th-century model of the scientific enterprise).\nBig machines: Ernest Lawrence's cyclotron at his Radiation Laboratory in particular ushered in an era of massive machines (requiring massive staffs and budgets) as the tools of basic scientific research. The use of many machines, such as the many sequencers used during the Human Genome Project, might also fall under this definition.\nBig laboratories: Because of the increase in cost to do basic science (with the increase of large machines), centralization of scientific research in large laboratories (such as Lawrence Berkeley National Laboratory or CERN) has become a cost-effective strategy, though questions over facility access have become prevalent.Towards the end of the 20th century, not only projects in basic physics and astronomy, but also in life sciences became big sciences, such as the massive Human Genome Project. The heavy investment of government and industrial interests into academic science has also blurred the line between public and private research, where entire academic departments, even at public universities, are often financed by private companies.  Not all Big Science is related to the military concerns which were at its origins.\n\n\n== Criticism ==\nThe era of Big Science has provoked criticism that it undermines the basic principles of the scientific method.  Increased government funding has often meant increased military funding, which some claim subverts the Enlightenment-era ideal of science as a pure quest for knowledge.  For example, historian Paul Forman has argued that during World War II and the Cold War, the massive scale of defense-related funding prompted a shift in physics from basic to applied research.Many scientists also complain that the requirement for increased funding makes a large part of the scientific activity filling out grant requests and other budgetary bureaucratic activity, and the intense connections between academic, governmental, and industrial interests have raised the question of whether scientists can be completely objective when their research contradicts the interests and intentions of their benefactors.\nIn addition, widespread sharing of scientific knowledge is necessary for rapid progress for both basic and applied sciences. However the sharing of data can be impeded for a number of reasons. For example, scientific findings can be classified by military interests or patented by corporate ones. Grant competitions, while they stimulate interest in a topic, can also increase secretiveness among scientists because application evaluators may value uniqueness more than incremental, collaborative inquiry.\nBig Science is labelled as fragile by essayist Nassim Nicholas Taleb in his books.\n\n\n== Historiography of Big Science ==\nThe popularization of the term \"Big Science\" is usually attributed to an article by Alvin M. Weinberg, then director of Oak Ridge National Laboratory, published in Science in 1961. This was a response to Dwight D. Eisenhower's farewell address, in which the departing U.S. president warned against the dangers of what he called the \"military-industrial complex\" and the potential \"domination of the nation's scholars by Federal employment, project allocations, and the power of money\".  Weinberg compared the large-scale enterprise of science in the 20th century to the wonders of earlier civilization (the pyramids, the palace of Versailles):\n\nWhen history looks at the 20th century, she will see science and technology as its theme; she will find in the monuments of Big Science\u2014the huge rockets, the high-energy accelerators, the high-flux research reactors\u2014symbols of our time just as surely as she finds in Notre Dame a symbol of the Middle Ages. ... We build our monuments in the name of scientific truth, they built theirs in the name of religious truth; we use our Big Science to add to our country's prestige, they used their churches for their cities' prestige; we build to placate what ex-President Eisenhower suggested could become a dominant scientific caste, they built to please the priests of Isis and Osiris.Weinberg's article addressed criticisms of the way in which the era of Big Science could negatively affect science \u2014 such as astronomer Fred Hoyle's contention that excessive money for science would only make science fat and lazy \u2014 and encouraged, in the end, limiting Big Science only to the national laboratory system and preventing its incursion into the university system.\nSince Weinberg's article there have been many historical and sociological studies on the effects of Big Science both in and out of the laboratory.  Soon after that article, Derek J. de Solla Price gave a series of lectures that were published in 1963 as Little Science, Big Science.  The book describes the historical and sociological transition from \"small science\" to \"big science\" and the qualitative differences between the two; it inspired the field of scientometrics as well as new perspectives on large-scale science in other fields.The Harvard historian Peter Galison has written several books addressing the formation of big science. Major themes include the evolution of experimental design, from table-top experiments to today's large-scale collider projects; accompanying changes in standards of evidence; and discourse patterns across researchers whose expertise only partially overlaps. Galison introduced the notion of \"trading zones,\" borrowed from the sociolinguistic study of pidgins, to characterize how such groups learn to interact.\nOther historians have postulated many \"precursors\" to Big Science in earlier times: the Uraniborg of Tycho Brahe (in which massive astronomical instruments were made, often with little practical purpose) and the large cryogenics laboratory established by Heike Kamerlingh Onnes in 1904 have been cited as early examples of Big Science.\n\n\n== See also ==\nOutline of Big Science\n\n\n== References ==\n\n\n== Further reading ==\nGalison, Peter (1994). Big Science: The Growth of Large Scale Research. \nGalison, Peter (1997). Image and Logic: A Material Culture of Microphysics. ISBN 0-226-27917-0. \nWiener, Norbert (1988). The Human Use of Human Beings. Da Capo Press. ISBN 0-306-80320-8.", "design": "Design is the creation of a plan or convention for the construction of an object, system or measurable human interaction (as in architectural blueprints, engineering drawings, business processes, circuit diagrams, and sewing patterns).  Design has different connotations in different fields (see design disciplines below). In some cases, the direct construction of an object (as in pottery, engineering, management, coding, and graphic design) is also considered to use design thinking.\nDesigning often necessitates considering the aesthetic, functional, economic, and sociopolitical dimensions of both the design object and design process. It may involve considerable research, thought, modeling, interactive adjustment, and re-design. Meanwhile, diverse kinds of objects may be designed, including clothing, graphical user interfaces, products, skyscrapers, corporate identities, business processes, and even methods or processes of designing.Thus \"design\" may be a substantive referring to a categorical abstraction of a created thing or things (the design of something), or a verb for the process of creation as is made clear by grammatical context.\n\n\n== Definitions ==\nMore formally design has been defined as follows:\n\n(noun) a specification of an object, manifested by an agent, intended to accomplish goals, in a particular environment, using a set of primitive components, satisfying a set of requirements, subject to constraints; (verb, transitive) to create a design, in an environment (where the designer operates)\nAnother definition for design is \"a roadmap or a strategic approach for someone to achieve a unique expectation. It defines the specifications, plans, parameters, costs, activities, processes and how and what to do within legal, political, social, environmental, safety and economic constraints in achieving that objective.\"Here, a \"specification\" can be manifested as either a plan or a finished product, and \"primitives\" are the elements from which the design object is composed.\nWith such a broad denotation, there is no universal language or unifying institution for designers of all disciplines. This allows for many differing philosophies and approaches toward the subject (see \u00a7 Philosophies and studies of design, below).\nThe person designing is called a designer, which is also a term used for people who work professionally in one of the various design areas usually specifying which area is being dealt with (such as a textile designer, fashion designer, product designer, concept designer, web designer or interior designer). A designer's sequence of activities is called a design process while the scientific study of design is called design science.Another definition of design is planning to manufacture an object, system, component or structure. Thus the word \"design\" can be used as a noun or a verb. In a broader sense, design is an applied art and engineering that integrates with technology.\nWhile the definition of design is fairly broad, design has a myriad of specifications that professionals utilize in their fields.\n\n\n== Design as a process ==\nSubstantial disagreement exists concerning how designers in many fields, whether amateur or professional, alone or in teams, produce designs. Kees Dorst and Judith Dijkhuis, both designers themselves, argued that \"there are many ways of describing design processes\" and discussed \"two basic and fundamentally different ways\", both of which have several names. The prevailing view has been called \"the rational model\", \"technical problem solving\" and \"the reason-centric perspective\". The alternative view has been called \"reflection-in-action\", \"evolutionary design\", \"co-evolution\", and \"the action-centric perspective\".\n\n\n=== The rational model ===\nThe rational model was independently developed by Herbert A. Simon, an American scientist, and Gerhard Pahl and Wolfgang Beitz, two German engineering design theorists. It posits that:\n\ndesigners attempt to optimize a design candidate for known constraints and objectives,\nthe design process is plan-driven,\nthe design process is understood in terms of a discrete sequence of stages.The rational model is based on a rationalist philosophy and underlies the waterfall model, systems development life cycle, and much of the engineering design literature. According to the rationalist philosophy, design is informed by research and knowledge in a predictable and controlled manner.\n\n\n==== Example sequence of stages ====\nTypical stages consistent with the rational model include the following:\n\nPre-production design\nDesign brief or Parti pris \u2013 an early (often the beginning) statement of design goals\nAnalysis \u2013 analysis of current design goals\nResearch \u2013 investigating similar design solutions in the field or related topics\nSpecification \u2013 specifying requirements of a design solution for a product (product design specification) or service.\nProblem solving \u2013 conceptualizing and documenting design solutions\nPresentation \u2013 presenting design solutions\nDesign during production\nDevelopment \u2013 continuation and improvement of a designed solution\nTesting \u2013 in situ testing of a designed solution\nPost-production design feedback for future designs\nImplementation \u2013 introducing the designed solution into the environment\nEvaluation and conclusion \u2013 summary of process and results, including constructive criticism and suggestions for future improvements\nRedesign \u2013 any or all stages in the design process repeated (with corrections made) at any time before, during, or after production.Each stage has many associated best practices.\n\n\n==== Criticism of the rational model ====\nThe rational model has been widely criticized on two primary grounds:\n\nDesigners do not work this way \u2013 extensive empirical evidence has demonstrated that designers do not act as the rational model suggests.\nUnrealistic assumptions \u2013 goals are often unknown when a design project begins, and the requirements and constraints continue to change.\n\n\n=== The action-centric model ===\nThe action-centric perspective is a label given to a collection of interrelated concepts, which are antithetical to the rational model. It posits that:\n\ndesigners use creativity and emotion to generate design candidates,\nthe design process is improvised,\nno universal sequence of stages is apparent \u2013 analysis, design and implementation are contemporary and inextricably linkedThe action-centric perspective is based on an empiricist philosophy and broadly consistent with the agile approach and amethodical development. Substantial empirical evidence supports the veracity of this perspective in describing the actions of real designers. Like the rational model, the action-centric model sees design as informed by research and knowledge. However, research and knowledge are brought into the design process through the judgment and common sense of designers \u2013 by designers \"thinking on their feet\" \u2013 more than through the predictable and controlled process stipulated by the rational model.\n\n\n==== Descriptions of design activities ====\nAt least two views of design activity are consistent with the action-centric perspective. Both involve three basic activities.\nIn the reflection-in-action paradigm, designers alternate between \"framing\", \"making moves\", and \"evaluating moves\". \"Framing\" refers to conceptualizing the problem, i.e., defining goals and objectives. A \"move\" is a tentative design decision. The evaluation process may lead to further moves in the design.In the sensemaking\u2013coevolution\u2013implementation framework, designers alternate between its three titular activities. Sensemaking includes both framing and evaluating moves. Implementation is the process of constructing the design object. Coevolution is \"the process where the design agent simultaneously refines its mental picture of the design object based on its mental picture of the context, and vice versa\".The concept of the design cycle is understood as a circular time structure, which may start with the thinking of an idea, then expressing it by the use of visual or verbal means of communication (design tools), the sharing and perceiving of the expressed idea, and finally starting a new cycle with the critical rethinking of the perceived idea. Anderson points out that this concept emphasizes the importance of the means of expression, which at the same time are means of perception of any design ideas.\n\n\n== Design disciplines ==\n\n\n== Philosophies and studies of design ==\nThere are countless philosophies for guiding design as design values and its accompanying aspects within modern design vary, both between different schools of thought and among practicing designers. Design philosophies are usually for determining design goals. A design goal may range from solving the least significant individual problem of the smallest element, to the most holistic influential utopian goals. Design goals are usually for guiding design. However, conflicts over immediate and minor goals may lead to questioning the purpose of design, perhaps to set better long term or ultimate goals.  John Heskett, a 20th-century British writer on design claimed, \"Design, stripped to its essence, can be defined as the human nature to shape and make our environment in ways without precedent in nature, to serve our needs and give meaning to our lives.\"\n\n\n=== Philosophies for guiding design ===\nDesign philosophies are fundamental guiding principles that dictate how a designer approaches his/her practice. Reflections on material culture and environmental concerns (sustainable design) can guide a design philosophy. One example is the First Things First manifesto which was launched within the graphic design community and states \"We propose a reversal of priorities in favor of more useful, lasting and democratic forms of communication \u2013 a mindshift away from product marketing and toward the exploration and production of a new kind of meaning. The scope of debate is shrinking; it must expand. Consumerism is running uncontested; it must be challenged by other perspectives expressed, in part, through the visual languages and resources of design.\"In The Sciences of the Artificial by polymath Herbert A. Simon, the author asserts design to be a meta-discipline of all professions. \"Engineers are not the only professional designers. Everyone designs who devises courses of action aimed at changing existing situations into preferred ones. The intellectual activity that produces material artifacts is no different fundamentally from the one that prescribes remedies for a sick patient or the one that devises a new sales plan for a company or a social welfare policy for a state. Design, so construed, is the core of all professional training; it is the principal mark that distinguishes the professions from the sciences. Schools of engineering, as well as schools of architecture, business, education, law, and medicine, are all centrally concerned with the process of design.\"\n\n\n=== Approaches to design ===\nA design approach is a general philosophy that may or may not include a guide for specific methods. Some are to guide the overall goal of the design. Other approaches are to guide the tendencies of the designer. A combination of approaches may be used if they don't conflict.\nSome popular approaches include:\n\nSociotechnical system design, a philosophy and tools for participative designing of work arrangements and supporting processes - for organizational purpose, quality, safety, economics and customer requirements in core work processes, the quality of peoples experience at work and the needs of society\nKISS principle, (Keep it Simple Stupid), which strives to eliminate unnecessary complications.\nThere is more than one way to do it (TIMTOWTDI), a philosophy to allow multiple methods of doing the same thing.\nUse-centered design, which focuses on the goals and tasks associated with the use of the artifact, rather than focusing on the end user.\nUser-centered design, which focuses on the needs, wants, and limitations of the end user of the designed artifact.\nCritical design uses designed artifacts as an embodied critique or commentary on existing values, morals, and practices in a culture.\nService design designing or organizing the experience around a product and the service associated with a product's use.\nTransgenerational design, the practice of making products and environments compatible with those physical and sensory impairments associated with human aging and which limit major activities of daily living.\nSpeculative design, the speculative design process doesn't necessarily define a specific problem to solve, but establishes a provocative starting point from which a design process emerges. The result is an evolution of fluctuating iteration and reflection using designed objects to provoke questions and stimulate discussion in academic and research settings.\nParticipatory Design (originally co-operative design, now often co-design) is the practice of collective creativity to design, attempting to actively involve all stakeholders (e.g. employees, partners, customers, citizens, end users) in the design process to help ensure the result meets their needs and is usable. Participatory design is an approach which is focused on processes and procedures of design and is not a design style\n\n\n=== Methods of designing ===\n\nDesign methods is a broad area that focuses on:\n\nExploring possibilities and constraints by focusing critical thinking skills to research and define problem spaces for existing products or services\u2014or the creation of new categories (see also Brainstorming)\nRedefining the specifications of design solutions which can lead to better guidelines for traditional design activities (graphic, industrial, architectural, etc.);\nManaging the process of exploring, defining, creating artifacts continually over time\nPrototyping possible scenarios, or solutions that incrementally or significantly improve the inherited situation\nTrendspotting; understanding the trend process.\n\n\n== Terminology ==\nThe word \"design\" is often considered ambiguous, as it is applied in varying contexts.\n\n\n=== Design and art ===\nToday, the term design is widely associated with the applied arts as initiated by Raymond Loewy and teachings at the Bauhaus and Ulm School of Design (HfG Ulm) in Germany during the 20th century.\nThe boundaries between art and design are blurred, largely due to a range of applications both for the term 'art' and the term 'design'. Applied arts has been used as an umbrella term to define fields of industrial design, graphic design, fashion design, etc. The term 'decorative arts' is a traditional term used in historical discourses to describe craft objects, and also sits within the umbrella of applied arts. In graphic arts (2D image making that ranges from photography to illustration), the distinction is often made between fine art and commercial art, based on the context within which the work is produced and how it is traded.\nTo a degree, some methods for creating work, such as employing intuition, are shared across the disciplines within the applied arts and fine art. Mark Getlein, writer, suggests the principles of design are \"almost instinctive\", \"built-in\", \"natural\", and part of \"our sense of 'rightness'.\" However, the intended application and context of the resulting works will vary greatly.\n\n\n=== Design and engineering ===\nIn engineering, design is a component of the engineering process. Many overlapping methods and processes can be seen when comparing Product design, Industrial design and Engineering. The American Heritage Dictionary defines design as: \"To conceive or fashion in the mind; invent,\" and \"To formulate a plan\", and defines engineering as: \"The application of scientific and mathematical principles to practical ends such as the design, manufacture, and operation of efficient and economical structures, machines, processes, and systems.\". Both are forms of problem-solving with a defined distinction being the application of \"scientific and mathematical principles\". The increasingly scientific focus of engineering in practice, however, has raised the importance of new more \"human-centered\" fields of design. How much science is applied in a design is a question of what is considered \"science\". Along with the question of what is considered science, there is social science versus natural science. Scientists at Xerox PARC made the distinction of design versus engineering at \"moving minds\" versus \"moving atoms\" (probably in contradiction to the origin of term \"engineering - engineer\" from Latin \"in genio\" in meaning of a \"genius\" what assumes existence of a \"mind\" not of an \"atom\").\n\n\n=== Design and production ===\nThe relationship between design and production is one of planning and executing. In theory, the plan should anticipate and compensate for potential problems in the execution process. Design involves problem-solving and creativity. In contrast, production involves a routine or pre-planned process. A design may also be a mere plan that does not include a production or engineering processes although a working knowledge of such processes is usually expected of designers. In some cases, it may be unnecessary or impractical to expect a designer with a broad multidisciplinary knowledge required for such designs to also have a detailed specialized knowledge of how to produce the product.\nDesign and production are intertwined in many creative professional careers, meaning problem-solving is part of execution and the reverse. As the cost of rearrangement increases, the need for separating design from production increases as well. For example, a high-budget project, such as a skyscraper, requires separating (design) architecture from (production) construction. A Low-budget project, such as a locally printed office party invitation flyer, can be rearranged and printed dozens of times at the low cost of a few sheets of paper, a few drops of ink, and less than one hour's pay of a desktop publisher.\nThis is not to say that production never involves problem-solving or creativity, nor that design always involves creativity. Designs are rarely perfect and are sometimes repetitive. The imperfection of a design may task a production position (e.g. production artist, construction worker) with utilizing creativity or problem-solving skills to compensate for what was overlooked in the design process. Likewise, a design may be a simple repetition (copy) of a known preexisting solution, requiring minimal, if any, creativity or problem-solving skills from the designer.\n\n\n=== Process design ===\n\n\"Process design\" (in contrast to \"design process\" mentioned above) refers to the planning of routine steps of a process aside from the expected result. Processes (in general) are treated as a product of design, not the method of design. The term originated with the industrial designing of chemical processes. With the increasing complexities of the information age, consultants and executives have found the term useful to describe the design of business processes as well as manufacturing processes.\n\n\n== See also ==\nDesign elements and principles\nDesign-based learning\nDesign thinking\nEvidence-based design\n\n\n== Footnotes ==\n\n\n== Bibliography ==\nBeck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M., Grenning, J., Highsmith, J., Hunt, A., Jeffries, R., Kern, J., Marick, B., Martin, R.C., Mellor, S., Schwaber, K., Sutherland, J., and Thomas, D. Manifesto for agile software development, 2001.\nBourque, P., and Dupuis, R. (eds.) Guide to the software engineering body of knowledge (SWEBOK). IEEE Computer Society Press, 2004 ISBN 0-7695-2330-7.\nBrooks, F.P. The design of design: Essays from a computer scientist, Addison-Wesley Professional, 2010 ISBN 0-201-36298-8.\nCross, N., Dorst, K., and Roozenburg, N. Research in design thinking, Delft University Press, Delft, 1992 ISBN 90-6275-796-0.\nDorst, K.; Cross, N. (2001). \"Creativity in the design process: Co-evolution of problem-solution\". Design Studies. 22 (2): 425\u2013437. doi:10.1016/0142-694X(94)00012-3. \nDorst, K., and Dijkhuis, J. \"Comparing paradigms for describing design activity,\" Design Studies (16:2) 1995, pp 261\u2013274.\nFaste, R. (2001). \"The Human Challenge in Engineering Design\" (PDF). International Journal of Engineering Education. 17 (4\u20135): 327\u2013331. \nMcCracken, D.D.; Jackson, M.A. (1982). \"Life cycle concept considered harmful\". SIGSOFT Software Engineering Notes. 7 (2): 29\u201332. doi:10.1145/1005937.1005943. \nNewell, A., and Simon, H. Human problem solving, Prentice-Hall, Inc., 1972.\nPahl, G., and Beitz, W. Engineering design: A systematic approach, Springer-Verlag, London, 1996 ISBN 3-540-19917-9.\nPahl, G., Beitz, W., Feldhusen, J., and Grote, K.-H. Engineering design: A systematic approach, (3rd ed.), Springer-Verlag, 2007 ISBN 1-84628-318-3.\nRalph, P. \"Comparing two software design process theories\" International Conference on Design Science Research in Information Systems and Technology (DESRIST 2010), Springer, St. Gallen, Switzerland, 2010, pp. 139\u2013153.\nRoyce, W.W. \"Managing the development of large software systems: Concepts and techniques,\" Proceedings of Wescon, 1970.\nSch\u00f6n, D.A. The reflective practitioner: How professionals think in action, Basic Books, USA, 1983.\nSimon, H.A. The sciences of the artificial, MIT Press, Cambridge, MA, USA, 1996 ISBN 0-262-69191-4.\nTruex, D.; Baskerville, R.; and Travis, J. (2000). \"Amethodical systems development: The deferred meaning of systems development methods\". Accounting, Management and Information Technologies. 10 (1): 53\u201379. doi:10.1016/S0959-8022(99)00009-0.", "artificial intelligence": "Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science  AI research is defined as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".The scope of AI is disputed: as machines become increasingly capable, tasks considered as requiring \"intelligence\" are often removed from the definition, a phenomenon known as the AI effect, leading to the quip, \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from \"artificial intelligence\", having become a routine technology. Capabilities generally classified as AI as of  2017 include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomous cars, intelligent routing in content delivery network and military simulations.\nArtificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding. For most of its history, AI research has been divided into subfields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. \"robotics\" or \"machine learning\"), the use of particular tools (\"logic\" or artificial neural networks), or deep philosophical differences. Subfields have also been based on social factors (particular institutions or the work of particular researchers).The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, artificial neural networks, and methods based on statistics, probability and economics. The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy and many others.\nThe field was founded on the claim that human intelligence \"can be so precisely described that a machine can be made to simulate it\". This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence which are issues that have been explored by myth, fiction and philosophy since antiquity. Some people also consider AI to be a danger to humanity if it progresses unabatedly. Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.In the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science.\n\n\n== History ==\n\nThought-capable artificial beings appeared as storytelling devices in antiquity, and have been common in fiction, as in Mary Shelley's Frankenstein or Karel \u010capek's R.U.R. (Rossum's Universal Robots). These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to  Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church\u2013Turing thesis. Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. Turing proposed that \"if a human could not distinguish between responses from a machine and a human, the machine could be considered \u201cintelligent\". The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".The field of AI research was born at a workshop at Dartmouth College in 1956. Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies (c. 1954) (and by 1959 were reportedly playing better than the average human), solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956) and speaking English. By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. AI's founders were optimistic about the future: Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.In the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas. The success was due to increasing computational power (see Moore's law), greater emphasis on solving specific problems, new ties between AI and other fields (such as statistics, economics and mathematics), and a commitment by researchers to mathematical methods and scientific standards. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997.In 2011, a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012. The Kinect, which provides a 3D body\u2013motion interface for the Xbox 360 and the Xbox One use algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. In the 2017 Future of Go Summit,  AlphaGo  won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years. This marked the completion of a significant milestone in the development of Artificial Intelligence as Go is an extremely complex game, more so than Chess.\nAccording to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects. Clark also presents factual data indicating that error rates in image processing tasks have fallen significantly since 2011. He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. Other cited examples include Microsoft's development of a Skype system that can automatically translate from one language to another and Facebook's system that can describe images to blind people. In a 2017 survey, one in five companies reported they had \"incorporated AI in some offerings or processes\".\n\n\n== Basics ==\nA typical AI perceives its environment and takes actions that maximize its chance of successfully achieving its goals. An AI's intended goal function can be simple (\"1 if the AI wins a game of Go, 0 otherwise\") or complex (\"Do actions mathematically similar to the actions that got you rewards in the past\"). Goals can be explicitly defined, or can be induced. If the AI is programmed for \"reinforcement learning\", goals can be implicitly induced by rewarding some types of behavior and punishing others. Alternatively, an evolutionary system can induce goals by using a \"fitness function\" to mutate and preferentially replicate high-scoring AI systems; this is similar to how animals evolved to innately desire certain goals such as finding food, or how dogs can be bred via artificial selection to possess desired traits. Some AI systems, such as nearest-neighbor, instead reason by analogy; these systems are not generally given goals, except to the degree that goals are somehow implicit in their training data. Such systems can still be benchmarked if the non-goal system is framed as a system whose \"goal\" is to successfully accomplish its narrow classification task.AI often revolves around the use of algorithms. An algorithm is a set of unambiguous instructions that a mechanical computer can execute. A complex algorithm is often built on top of other, simpler, algorithms. A simple example of an algorithm is the following recipe for optimal play at tic-tac-toe:\nIf someone has a \"threat\" (that is, two in a row), take the remaining square. Otherwise,\nif a move \"forks\" to create two threats at once, play that move. Otherwise,\ntake the center square if it is free. Otherwise,\nif your opponent has played in a corner, take the opposite corner. Otherwise,\ntake an empty corner if one exists. Otherwise,\ntake any empty square.Many AI algorithms are capable of learning from data; they can enhance themselves by learning new heuristics (strategies, or \"rules of thumb\", that have worked well in the past), or can themselves write other algorithms. Some of the \"learners\" described below, including Bayesian networks, decision trees, and nearest-neighbor, could theoretically, if given infinite data, time, and memory, learn to approximate any function, including whatever combination of mathematical functions would best describe the entire world. These learners could therefore, in theory, derive all possible knowledge, by considering every possible hypothesis and matching it against the data. In practice, it is almost never possible to consider every possibility, because of the phenomenon of \"combinatorial explosion\", where the amount of time needed to solve a problem grows exponentially. Much of AI research involves figuring out how to identify and avoid considering broad swaths of possibilities that are unlikely to be fruitful. For example, when viewing a map and looking for the shortest driving route from Denver to New York in the East, one can in most cases skip looking at any path through San Francisco or other areas far to the West; thus, an AI wielding an pathfinding algorithm like A* can avoid the combinatorial explosion that would ensue if every possible route had to be ponderously considered in turn.The earliest (and easiest to understand) approach to AI was symbolism (such as formal logic): \"If an otherwise healthy adult has a fever, then they may have influenza\". A second, more general, approach is Bayesian inference: \"If the current patient has a fever, adjust the probability they have influenza in such-and-such way\". The third major approach, extremely popular in routine business AI applications, are analogizers such as SVM and nearest-neighbor: \"After examining the records of known past patients whose temperature, symptoms, age, and other factors mostly match the current patient, X% of those patients turned out to have influenza\". A fourth approach is harder to intuitively understand, but is inspired by how the brain's machinery works: the artificial neural network approach uses artificial \"neurons\" that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to \"reinforce\" connections that seemed to be useful. These four main approaches can overlap with each other and with evolutionary systems; for example, neural nets can learn to make inferences, to generalize, and to make analogies. Some systems implicitly or explicitly use multiple of these approaches, alongside many other AI and non-AI algorithms; the best approach is often different depending on the problem.\n\nLearning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". They can be nuanced, such as \"X% of families have geographically separate species with color variants, so there is an Y% chance that undiscovered black swans exist\". Learners also work on the basis of \"Occam's razor\": The simplest theory that explains the data is the likeliest. Therefore, to be successful, a learner must be designed such that it prefers simpler theories to complex theories, except in cases where the complex theory is proven substantially better. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data, but penalizing the theory in accordance with how complex the theory is. Besides classic overfitting, learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers don't determine the spatial relationship between components of the picture; instead, they learn abstract patterns of pixels that humans are oblivious to, but that linearly correlate with images of certain types of real objects. Faintly superimposing such a pattern on a legitimate image results in an \"adversarial\" image that the system misclassifies.\n\nCompared with humans, existing AI lacks several features of human \"commonsense reasoning\"; most notably, humans have powerful mechanisms for reasoning about \"na\u00efve physics\" such as space, time, and physical interactions. This enables even young children to easily make inferences like \"If I roll this pen off a table, it will fall on the floor\". Humans also have a powerful mechanism of \"folk psychology\" that helps them to interpret natural-language sentences such as \"The city councilmen refused the demonstrators a permit because they advocated violence\". (A generic AI has difficulty inferring whether the councilmen or the demonstrators are the ones alleged to be advocating violence.) This lack of \"common knowledge\" means that AI often makes different mistakes than humans make, in ways that can seem incomprehensible. For example, existing self-driving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do, and instead must use non-human modes of reasoning to avoid accidents.\n\n\n== Problems ==\nThe overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.\n\n\n=== Reasoning, problem solving ===\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.These algorithms proved to be insufficient for solving large reasoning problems, because they experienced a \"combinatorial explosion\": they became exponentially slower as the problems grew larger.  In fact, even humans rarely use the step-by-step deduction that early AI research was able to model. They solve most of their problems using fast, intuitive judgements.\n\n\n=== Knowledge representation ===\n\nKnowledge representation and knowledge engineering are central to classical AI research. Some \"expert systems\" attempt to gather together explicit knowledge possessed by experts in some narrow domain. In addition, some projects attempt to gather the \"commonsense knowledge\" known to the average person into a database containing extensive knowledge about the world. Among the things a comprehensive commonsense knowledge base would contain are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge by acting as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations can be used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.Among the most difficult problems in knowledge representation are:\n\nDefault reasoning and the qualification problem\nMany of the things people know take the form of \"working assumptions\". For example, if a bird comes up in conversation, people typically picture an animal that is fist sized, sings, and flies. None of these things are true about all birds. John McCarthy identified this problem in 1969 as the qualification problem: for any commonsense rule that AI researchers care to represent, there tend to be a huge number of exceptions. Almost nothing is simply true or false in the way that abstract logic requires. AI research has explored a number of solutions to this problem.\nThe breadth of commonsense knowledge\nThe number of atomic facts that the average person knows is very large. Research projects that attempt to build a complete knowledge base of commonsense knowledge (e.g., Cyc) require enormous amounts of laborious ontological engineering\u2014they must be built, by hand, one complicated concept at a time.\nThe subsymbolic form of some commonsense knowledge\nMuch of what people know is not represented as \"facts\" or \"statements\" that they could express verbally. For example, a chess master will avoid a particular chess position because it \"feels too exposed\" or an art critic can take one look at a statue and realize that it is a fake. These are non-conscious and sub-symbolic intuitions or tendencies in the human brain. Knowledge like this informs, supports and provides a context for symbolic, conscious knowledge. As with the related problem of sub-symbolic reasoning, it is hoped that situated AI, computational intelligence, or statistical AI will provide ways to represent this kind of knowledge.\n\n\n=== Planning ===\n\nIntelligent agents must be able to set goals and achieve them. They need a way to visualize the future\u2014a representation of the state of the world and be able to make predictions about how their actions will change it\u2014and be able to make choices that maximize the utility (or \"value\") of available choices.In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions. However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions, but also evaluate its predictions and adapt based on its assessment.Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.\n\n\n=== Learning ===\n\nMachine learning, a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.Unsupervised learning is the ability to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as \"function approximators\" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, \"spam\" or \"not spam\". Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space.\n\n\n=== Natural language processing ===\n\nNatural language processing (NLP) gives machines the ability to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering and machine translation. Many current approaches use word co-occurrence frequencies to construct syntactic representations of text. \"Keyword spotting\" strategies for search are popular and scalable but dumb; a search query for \"dog\" might only match documents with the literal word \"dog\" and miss a document with the word \"poodle\". \"Lexical affinity\" strategies use the occurrence of words such as \"accident\" to assess the sentiment of a document. Modern statistical NLP approaches can combine all these strategies as well as others, and often achieve acceptable accuracy at the page or paragraph level, but continue to lack the semantic understanding required to classify isolated sentences well. Besides the usual difficulties with encoding semantic commonsense knowledge, existing semantic NLP sometimes scales too poorly to be viable in business applications. Beyond semantic NLP, the ultimate goal of \"narrative\" NLP is to embody a full understanding of commonsense reasoning.\n\n\n=== Perception ===\n\nMachine perception is the ability to use input from sensors (such as cameras (visible spectrum or infrared), microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition, facial recognition, and object recognition. Computer vision is the ability to analyze visual input. Such input is usually ambiguous; a giant, fifty-meter-tall pedestrian far away may produce exactly the same pixels as a nearby normal-sized pedestrian, requiring the AI to judge the relative likelihood and reasonableness of different interpretations, for example by using its \"object model\" to assess that fifty-meter pedestrians do not exist.\n\n\n=== Motion and manipulation ===\n\nAI is heavily used in robotics. Advanced robotic arms and other industrial robots, widely used in modern factories, can learn from experience how to move efficiently despite the presence of friction and gear slippage. A modern mobile robot, when given a small, static, and visible environment, can easily determine its location and map its environment; however, dynamic environments, such as (in endoscopy) the interior of a patient's breathing body, pose a greater challenge. Motion planning is the process of breaking down a movement task into \"primitives\" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object. Moravec's paradox generalizes that low-level sensorimotor skills that humans take for granted are, counterintuitively, difficult to program into a robot; the paradox is named after Hans Moravec, who stated in 1988 that \"it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility\". This is attributed to the fact that, unlike checkers, physical dexterity has been a direct target of natural selection for millions of years.\n\n\n=== Social intelligence ===\n\nMoravec's paradox can be extended to many forms of social intelligence. Distributed multi-agent coordination of autonomous vehicles remains a difficult problem. Affective computing is an interdisciplinary umbrella that comprises systems which recognize, interpret, process, or simulate human affects. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal affect analysis (see multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.In the long run, social skills and an understanding of human emotion and game theory would be valuable to a social agent. Being able to predict the actions of others by understanding their motives and emotional states would allow an agent to make better decisions. Some computer systems mimic human emotion and expressions to appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction. Similarly, some virtual assistants are programmed to speak conversationally or even to banter humorously; this tends to give naive users an unrealistic conception of how intelligent existing computer agents actually are.\n\n\n=== General intelligence ===\n\nHistorically, projects such as the Cyc knowledge base (1984\u2013) and the massive Japanese Fifth Generation Computer Systems initiative (1982\u20131992) attempted to cover the breadth of human cognition. These early projects failed to escape the limitations of non-quantitative symbolic logic models and, in retrospect, greatly underestimated the difficulty of cross-domain AI. Nowadays, the vast majority of current AI researchers work instead on tractable \"narrow AI\" applications (such as medical diagnosis or automobile navigation). Many researchers predict that such \"narrow AI\" work in different individual domains will eventually be incorporated into a machine with artificial general intelligence (AGI), combining most of the narrow skills mentioned in this article and at some point even exceeding human ability in most or all these areas. Many advances have general, cross-domain significance. One high-profile example is that DeepMind in the 2010s developed a \"generalized artificial intelligence\" that could learn many diverse Atari games on its own, and later developed a variant of the system which succeeds at sequential learning. Besides transfer learning, hypothetical AGI breakthroughs could include the development of reflective architectures that can engage in decision-theoretic metareasoning, and figuring out how to \"slurp up\" a comprehensive knowledge base from the entire unstructured Web. Some argue that some kind of (currently-undiscovered) conceptually straightforward, but mathematically difficult, \"Master Algorithm\" could lead to AGI. Finally, a few \"emergent\" approaches look to simulating human intelligence extremely closely, and believe that anthropomorphic features like an artificial brain or simulated child development may someday reach a critical point where general intelligence emerges.Many of the problems in this article may also require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered \"AI-complete\", because all of these problems need to be solved simultaneously in order to reach human-level machine performance.\n\n\n== Approaches ==\nThere is no established unifying theory or paradigm that guides AI research. Researchers disagree about many issues. A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurobiology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering?\nCan intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems?\n\n\n=== Cybernetics and brain simulation ===\n\nIn the 1940s and 1950s, a number of researchers explored the connection between neurobiology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter's turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England. By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.\n\n\n=== Symbolic ===\n\nWhen access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and as described below, each one developed its own style of research. John Haugeland named these symbolic approaches to AI \"good old fashioned AI\" or \"GOFAI\". During the 1960s, symbolic approaches had achieved great success at simulating high-level thinking in small demonstration programs. Approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.\nResearchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.\n\n\n==== Cognitive simulation ====\nEconomist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.\n\n\n==== Logic-based ====\nUnlike Simon and Newell, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem-solving, regardless of whether people used the same algorithms. His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning. Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.\n\n\n==== Anti-logic or scruffy ====\nResearchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad-hoc solutions \u2013 they argued that there was no simple and general principle (like logic) that would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford). Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.\n\n\n==== Knowledge-based ====\nWhen computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications. This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software. The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.\n\n\n=== Sub-symbolic ===\nBy the 1980s progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems. Sub-symbolic methods manage to approach intelligence without specific representations of knowledge.\n\n\n==== Embodied intelligence ====\nThis includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive. Their work revived the non-symbolic viewpoint of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.\nWithin developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).\n\n\n==== Computational intelligence and soft computing ====\nInterest in neural networks and \"connectionism\" was revived by David Rumelhart and others in the middle of the 1980s. Artificial neural networks are an example of soft computing --- they are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.\n\n\n=== Statistical learning ===\nMuch of traditional GOFAI got bogged down on ad hoc patches to symbolic computation that worked on their own toy models but failed to generalize to real-world results. However, around the 1990s, AI researchers adopted sophisticated mathematical tools, such as hidden Markov models (HMM), information theory, and normative Bayesian decision theory to compare or to unify competing architectures. The shared mathematical language permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Compared with GOFAI, new \"statistical learning\" techniques such as HMM and neural networks were gaining higher levels of accuracy in many practical domains such as data mining, without necessarily acquiring semantic understanding of the datasets. The increased successes with real-world data led to increasing emphasis on comparing different approaches against shared test data to see which approach performed best in a broader context than that provided by idiosyncratic toy models; AI research was becoming more scientific. Nowadays results of experiments are often rigorously measurable, and are sometimes (with difficulty) reproducible. Different statistical learning techniques have different limitations; for example, basic HMM cannot model the infinite possible combinations of natural language. Critics note that the shift from GOFAI to statistical learning is often also a shift away from Explainable AI. In AGI research, some scholars caution against over-reliance on statistical learning, and argue that continuing research into GOFAI will still be necessary to attain general intelligence.\n\n\n=== Integrating the approaches ===\nIntelligent agent paradigm\nAn intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. The simplest intelligent agents are programs that solve specific problems. More complicated agents include human beings and organizations of human beings (such as firms). The paradigm allows researchers to directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given \"goal function\". An agent that solves a specific problem can use any approach that works \u2013 some agents are symbolic and logical, some are sub-symbolic artificial neural networks and others may use new approaches. The paradigm also gives researchers a common language to communicate with other fields\u2014such as decision theory and economics\u2014that also use concepts of abstract agents. Building a complete agent requires researchers to address realistic problems of integration; for example, because sensory systems give uncertain information about the environment, planning systems must be able to function in the presence of uncertainty. The intelligent agent paradigm became widely accepted during the 1990s.Agent architectures and cognitive architectures\nResearchers have designed systems to build intelligent systems out of interacting intelligent agents in a multi-agent system. A hierarchical control system provides a bridge between sub-symbolic AI at its lowest, reactive levels and traditional symbolic AI at its highest levels, where relaxed time constraints permit planning and world modelling. Some cognitive architectures are custom-built to solve a narrow problem; others, such as Soar, are designed to mimic human cognition and to provide insight into general intelligence. Modern extensions of Soar are hybrid intelligent systems that include both symbolic and sub-symbolic components.\n\n\n== Tools ==\nAI has developed a large number of tools to solve the most difficult problems in computer science. A few of the most general of these methods are discussed below.\n\n\n=== Search and optimization ===\n\nMany problems in AI can be solved in theory by intelligently searching through many possible solutions: Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space. Many learning algorithms use search algorithms based on optimization.\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use \"heuristics\" or \"rules of thumb\" that prioritize choices in favor of those that are more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies heuristics can also serve to entirely eliminate some choices that are unlikely to lead to a goal (called \"pruning the search tree\"). Heuristics supply the program with a \"best guess\" for the path on which the solution lies. Heuristics limit the search for solutions into a smaller sample size.A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are simulated annealing, beam search and random optimization.\n\nEvolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming. Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\n\n=== Logic ===\n\nLogic is used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning and inductive logic programming is a method for learning.Several different forms of logic are used in AI research. Propositional logic involves truth functions such as \"or\" and \"not\". First-order logic adds quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. Fuzzy set theory assigns a \"degree of truth\" (between 0 and 1) to vague statements such as \"Alice is old\" (or rich, or tall, or hungry) that are too linguistically imprecise to be completely true or false. Fuzzy logic is successfully used in control systems to allow experts to contribute vague rules such as \"if you are close to the destination station and moving fast, increase the train's brake pressure\"; these vague rules can then be numerically refined within the system. Fuzzy logic fails to scale well in knowledge bases; many AI researchers question the validity of chaining fuzzy-logic inferences.Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem. Several extensions of logic have been designed to handle specific domains of knowledge, such as: description logics; situation calculus, event calculus and fluent calculus (for representing events and time); causal calculus; belief calculus; and modal logics.Overall, qualitiative symbolic logic is brittle and scales poorly in the presence of noise or other uncertainty. Exceptions to rules are numerous, and it is difficult for logical systems to function in the presence of contradictory rules.\n\n\n=== Probabilistic methods for uncertain reasoning ===\n\nMany problems in AI (in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.Bayesian networks are a very general tool that can be used for a large number of problems: reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters). Compared with symbolic logic, formal Bayesian inference is computationally expensive. For inference to be tractable, most observations must be conditionally independent of one another. Complicated graphs with diamonds or other \"loops\" (undirected cycles) can require a sophisticated method such as Markov Chain Monte Carlo, which spreads an ensemble of random walkers throughout the Bayesian network and attempts to converge to an assessment of the conditional probabilities. Bayesian networks are used on XBox Live to rate and match players; wins and losses are \"evidence\" of how good a player is. AdSense uses a Bayesian network with over 300 million edges to learn which ads to serve.A key concept from the science of economics is \"utility\": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\n\n\n=== Classifiers and statistical learning methods ===\n\nThe simplest AI applications can be divided into two types: classifiers (\"if shiny then diamond\") and controllers (\"if shiny then pick up\"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.A classifier can be trained in various ways; there are many statistical and machine learning approaches. The decision tree is perhaps the most widely used machine learning algorithm. Other widely used classifiers are the neural network,k-nearest neighbor algorithm,kernel methods such as the support vector machine (SVM),Gaussian mixture model, and the extremely popular naive Bayes classifier. Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, the dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as \"naive Bayes\" on most practical data sets.\n\n\n=== Artificial neural networks ===\n\nNeural networks, or neural nets, were inspired by the architecture of neurons in the human brain. A simple \"neuron\" N accepts input from multiple other neurons, each of which, when activated (or \"fired\"), cast a weighted \"vote\" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. The net forms \"concepts\" that are distributed among a subnetwork of shared neurons that tend to fire together; a concept meaning \"leg\" might be coupled with a subnetwork meaning \"foot\" that includes the sound for \"foot\". Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes. Modern neural nets can learn both continuous functions and, surprisingly, digital logical operations. Neural networks' early successes included predicting the stock market and (in 1995) a mostly self-driving car. In the 2010s, advances in neural networks using deep learning thrust AI into widespread public consciousness and contributed to an enormous upshift in corporate AI spending; for example, AI-related M&A in 2017 was over 25 times as large as in 2015.The study of non-learning artificial neural networks began in the decade before the field of AI research was founded, in the work of Walter Pitts and Warren McCullouch. Frank Rosenblatt invented the perceptron, a learning network with a single layer, similar to the old concept of linear regression. Early pioneers also include Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello, and others.\nThe main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks. Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning (\"fire together, wire together\"), GMDH or competitive learning.Today, neural networks are often trained by the backpropagation algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa, and was introduced to neural networks by Paul Werbos.Hierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex.In short, most neural networks use some form of gradient descent on a hand-created neural topology. However, some research groups, such as Uber, argue that simple neuroevolution to mutate new neural network topologies and weights may be competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".\n\n\n==== Deep feedforward neural networks ====\n\nDeep learning is any artificial neural network that can learn a long chain of causal links. For example, a feedforward network with six hidden layers can learn a seven-link causal chain (six hidden layers + output layer) and has a \"credit assignment path\" (CAP) depth of seven. Many deep learning systems need to be able to learn chains ten or more causal links in length. Deep learning has transformed many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing and others.According to one overview, the expression \"Deep Learning\" was introduced to the Machine Learning community by Rina Dechter in 1986 and gained traction after\nIgor Aizenberg and colleagues introduced it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes the learning of a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.Deep learning often uses convolutional neural networks (CNNs), whose origins can be traced back to the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US.\nSince 2011, fast implementations of CNNs on GPUs have\nwon many visual pattern recognition competitions.CNNs with 12 convolutional layers were used in conjunction with reinforcement learning by Deepmind's \"AlphaGo Lee\", the program that beat a top Go champion in 2016.\n\n\n==== Deep recurrent neural networks ====\n\nEarly on, deep learning was also applied to sequence learning with recurrent neural networks (RNNs) which are in theory Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and depends on the length of its input sequence; thus, an RNN is an example of deep learning. RNNs can be trained by gradient descent but suffer from the vanishing gradient problem. In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent supervised learning of deep sequential problems.Numerous researchers now use variants of a deep learning recurrent NN called the long short-term memory (LSTM) network published by Hochreiter & Schmidhuber in 1997. LSTM is often trained by Connectionist Temporal Classification (CTC). At Google, Microsoft and Baidu this approach has revolutionised speech recognition. For example, in 2015, Google's speech recognition experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to billions of smartphone users. Google also used LSTM to improve machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with CNNs also improved automatic image captioning and a plethora of other applications.\n\n\n=== Evaluating progress ===\n\nAI, like electricity or the steam engine, is a general purpose technology. There is no consensus on how to characterize which tasks AI tends to excel at. While projects such as AlphaZero have succeeded in generating their own knowledge from scratch, many other machine learning projects require large training datasets. Researcher Andrew Ng has suggested, as a \"highly imperfect rule of thumb\", that \"almost anything a typical human can do with less than one second of mental thought, we can probably now or in the near future automate using AI.\" Moravec's paradox suggests that AI lags humans at many tasks that the human brain has specifically evolved to perform well.Games provide a well-publicized benchmark for assessing rates of progress. AlphaGo around 2016 brought the era of classical board-game benchmarks to a close. Games of imperfect knowledge provide new challenges to AI in the area of game theory. E-sports such as StarCraft continue to provide additional public benchmarks. There are many competitions and prizes, such as the Imagenet Challenge, to promote research in artificial intelligence. The main areas of competition include general machine intelligence, conversational behavior, data-mining, robotic cars, and robot soccer as well as conventional games.The \"imitation game\" (an interpretation of the 1950 Turing test that assesses whether a computer can imitate a human) is nowadays considered too exploitable to be a meaningful benchmark. A derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). As the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. In contrast to the standard Turing test, CAPTCHA is administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. A computer asks a user to complete a simple test then generates a grade for that test. Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.Proposed \"universal intelligence\" tests aim to compare how well machines, humans, and even non-human animals perform on problem sets that are generic as possible. At an extreme, the test suite can contain every possible problem, weighted by Kolmogorov complexity; unfortunately, these problem sets tend to be dominated by impoverished pattern-matching exercises where a tuned AI can easily exceed human performance levels.\n\n\n== Applications ==\n\nAI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.High-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google search), online assistants (such as Siri), image recognition in photographs, spam filtering, prediction of judicial decisions and targeting online advertisements.With social media sites overtaking TV as a source for news for young people and news organisations increasingly reliant on social media platforms for generating distribution, major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.\n\n\n=== Healthcare ===\n\nArtificial intelligence is breaking into the healthcare industry by assisting doctors. According to Bloomberg Technology, Microsoft has developed AI to help doctors find the right treatments for cancer. There is a great amount of research and drugs developed relating to cancer. In detail, there are more than 800 medicines and vaccines to treat cancer. This negatively affects the doctors, because there are too many options to choose from, making it more difficult to choose the right drugs for the patients. Microsoft is working on a project to develop a machine called \"Hanover\". Its goal is to memorize all the papers necessary to cancer and help predict which combinations of drugs will be most effective for each patient. One project that is being worked on at the moment is fighting myeloid leukemia, a fatal cancer where the treatment has not improved in decades. Another study was reported to have found that artificial intelligence was as good as trained doctors in identifying skin cancers. Another study is using artificial intelligence to try and monitor multiple high-risk patients, and this is done by asking each patient numerous questions based on data acquired from live doctor to patient interactions.According to CNN, a recent study by surgeons at the Children's National Medical Center in Washington successfully demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel during open surgery, and doing so better than a human surgeon, the team claimed. IBM has created its own artificial intelligence computer, the IBM Watson, which has beaten human intelligence (at some levels).  Watson not only won at the game show Jeopardy! against former champions, but was declared a hero after successfully diagnosing a woman who was suffering from leukemia.\n\n\n=== Automotive ===\n\nAdvancements in AI have contributed to the growth of the automotive industry through the creation and evolution of self-driving vehicles. As of 2016, there are over 30 companies utilizing AI into the creation of driverless cars. A few companies involved with AI include Tesla, Google, and Apple.Many components contribute to the functioning of self-driving cars. These vehicles incorporate systems such as braking, lane changing, collision prevention, navigation and mapping. Together, these systems, as well as high performance computers, are integrated into one complex vehicle.Recent developments in autonomous automobiles have made the innovation of self-driving trucks possible, though they are still in the testing phase. The UK government has passed legislation to begin testing of self-driving truck platoons in 2018. Self-driving truck platoons are a fleet of self-driving trucks following the lead of one non-self-driving truck, so the truck platoons aren't entirely autonomous yet. Meanwhile, the Daimler, a German automobile corporation, is testing the Freightliner Inspiration which is a semi-autonomous truck that will only be used on the highway.One main factor that influences the ability for a driver-less automobile to function is mapping. In general, the vehicle would be pre-programmed with a map of the area being driven. This map would include data on the approximations of street light and curb heights in order for the vehicle to be aware of its surroundings. However, Google has been working on an algorithm with the purpose of eliminating the need for pre-programmed maps and instead, creating a device that would be able to adjust to a variety of new surroundings. Some self-driving cars are not equipped with steering wheels or brake pedals, so there has also been research focused on creating an algorithm that is capable of maintaining a safe environment for the passengers in the vehicle through awareness of speed and driving conditions.Another factor that is influencing the ability for a driver-less automobile is the safety of the passenger.  To make a driver-less automobile, engineers must program it to handle high-risk situations. These situations could include a head-on collision with pedestrians.  The car's main goal should be to make a decision that would avoid hitting the pedestrians and saving the passengers in the car.  But there is a possibility the car would need to make a decision that would put someone in danger. In other words, the car would need to decide to save the pedestrians or the passengers. The programing of the car in these situations is crucial to a successful driver-less automobile.\n\n\n=== Finance and economics ===\nFinancial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking can be traced back to 1987 when Security Pacific National Bank in US set-up a Fraud Prevention Task force to counter the unauthorised use of debit cards. Programs like Kasisto and Moneystream are using AI in financial services.\nBanks use artificial intelligence systems today to organize operations, maintain book-keeping, invest in stocks, and manage properties. AI can react to changes overnight or when business is not taking place. In August 2001, robots beat humans in a simulated financial trading competition. AI has also reduced fraud and financial crimes by monitoring behavioral patterns of users for any abnormal changes or anomalies.The use of AI machines in the market in applications such as online trading and decision making has changed major economic theories. For example, AI based buying and selling platforms have changed the law of supply and demand in that it is now possible to easily estimate individualized demand and supply curves and thus individualized pricing. Furthermore, AI machines reduce information asymmetry in the market and thus making markets more efficient while reducing the volume of trades. Furthermore, AI in the markets limits the consequences of behavior in the markets again making markets more efficient. Other theories where AI has had impact include in rational choice, rational expectations, game theory, Lewis turning point, portfolio optimization and counterfactual thinking.\n\n\n=== Video games ===\n\nIn video games, artificial intelligence is routinely used to generate dynamic purposeful behavior in non-player characters (NPCs). In addition, well-understood AI techniques are routinely used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks. Games with more atypical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).\n\n\n=== Military ===\n\nWorldwide annual military spending on robotics rose from 5.1 billion USD in 2010 to 7.5 billion USD in 2015. Military drones capable of autonomous action are widely considered a useful asset. In 2017, Vladimir Putin stated that \"Whoever becomes the leader in (artificial intelligence) will become the ruler of the world\". Many artificial intelligence researchers seek to distance themselves from military applications of AI.\n\n\n=== Audit ===\nFor financial statements audit, AI makes continuous audit possible. AI tools could analyze many sets of different information immediately. The potential benefit would be the overall audit risk will be reduced, the level of assurance will be increased and the time duration of audit will be reduced.\n\n\n=== Advertising ===\nA report by the Guardian newspaper in the UK in 2018 found that online gambling companies were using AI to predict the behavior of customers in order to target them with personalized promotions. Developers of commercial AI platforms are also beginning to appeal more directly to casino operators, offering a range of existing and potential services to help them boost their profits and expand their customer base.\n\n\n=== Art ===\nArtificial Intelligence has inspired numerous creative applications including its usage to produce visual art. The exhibition \"Thinking Machines: Art and Design in the Computer Age, 1959-1989\" at MoMA  provides a good overview of the historical applications of AI for art, architecture, and design. Recent exhibitions showcasing the usage of AI to produce art include the Google-sponsored benefit and auction at the Gray Area Foundation in San Francisco, where artists experimented with the deepdream algorithm  and the exhibition \"Unhuman: Art in the Age of AI,\" which took place in Los Angeles and Frankfurt in the fall of 2017.  In the spring of 2018, the Association of Computing Machinery dedicated a special magazine issue to the subject of computers and art highlighting the role of machine learning in the arts.\n\n\n== Philosophy and ethics ==\n\nThere are three philosophical questions related to AI:\n\nIs artificial general intelligence possible? Can a machine solve any problem that a human being can solve using intelligence? Or are there hard limits to what a machine can accomplish?\nAre intelligent machines dangerous? How can we ensure that machines behave ethically and that they are used ethically?\nCan a machine have a mind, consciousness and mental states in exactly the same sense that human beings do? Can a machine be sentient, and thus deserve certain rights? Can a machine intentionally cause harm?\n\n\n=== The limits of artificial general intelligence ===\n\nCan a machine be intelligent? Can it \"think\"?\n\nAlan Turing's \"polite convention\"\nWe need not decide if a machine can \"think\"; we need only decide if a machine can act as intelligently as a human being. This approach to the philosophical problems associated with artificial intelligence forms the basis of the Turing test.The Dartmouth proposal\n\"Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.\" This conjecture was printed in the proposal for the Dartmouth Conference of 1956, and represents the position of most working AI researchers.Newell and Simon's physical symbol system hypothesis\n\"A physical symbol system has the necessary and sufficient means of general intelligent action.\" Newell and Simon argue that intelligence consists of formal operations on symbols. Hubert Dreyfus argued that, on the contrary, human expertise depends on unconscious instinct rather than conscious symbol manipulation and on having a \"feel\" for the situation rather than explicit symbolic knowledge. (See Dreyfus' critique of AI.)G\u00f6delian arguments\nG\u00f6del himself, John Lucas (in 1961) and Roger Penrose (in a more detailed argument from 1989 onwards) made highly technical arguments that human mathematicians can consistently see the truth of their own \"G\u00f6del statements\" and therefore have computational abilities beyond that of mechanical Turing machines. However, the modern consensus in the scientific and mathematical community is that these \"G\u00f6delian arguments\" fail.The artificial brain argument\nThe brain can be simulated by machines and because brains are intelligent, simulated brains must also be intelligent; thus machines can be intelligent. Hans Moravec, Ray Kurzweil and others have argued that it is technologically feasible to copy the brain directly into hardware and software and that such a simulation will be essentially identical to the original.The AI effect\nMachines are already intelligent, but observers have failed to recognize it. When Deep Blue beat Garry Kasparov in chess, the machine was acting intelligently. However, onlookers commonly discount the behavior of an artificial intelligence program by arguing that it is not \"real\" intelligence after all; thus \"real\" intelligence is whatever intelligent behavior people can do that machines still cannot. This is known as the AI Effect: \"AI is whatever hasn't been done yet.\"\n\n\n=== Potential harm ===\nWidespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to see how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.\n\n\n==== Existential risk ====\n\nPhysicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could \"spell the end of the human race\".\n\nThe development of full artificial intelligence could spell the end of the human race. Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate. Humans, who are limited by slow biological evolution, couldn't compete and would be superseded.\n\nIn his book Superintelligence, Nick Bostrom provides an argument that artificial intelligence will pose a threat to mankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not reflect humanity's \u2013 one example is an AI told to compute as many digits of pi as possible \u2013 it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal.\nConcern over risk from artificial intelligence has led to some high-profile donations and investments. A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1billion to OpenAI a nonprofit company aimed at championing responsible AI development. The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI. In January 2015, Elon Musk donated ten million dollars to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as Google DeepMind and Vicarious to \"just keep an eye on what's going on with artificial intelligence. I think there is potentially a dangerous outcome there.\"For this danger to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching. Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.\n\n\n==== Devaluation of humanity ====\n\nJoseph Weizenbaum wrote that AI applications cannot, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position is now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.\n\n\n==== Decrease in demand for human labor ====\n\nThe relationship between automation and employment is complicated. While automation eliminates old jobs, it also creates new jobs through micro-economic and macro-economic effects. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classifies only 9% of U.S. jobs as \"high risk\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. Author Martin Ford and others go further and argue that a large number of jobs are routine, repetitive and (to an AI) predictable; Ford warns that these jobs may be automated in the next couple of decades, and that many of the new jobs may not be \"accessible to people with average capability\", even with retraining. Economists point out that in the past technology has tended to increase rather than reduce total employment, but acknowledge that \"we're in uncharted territory\" with AI.\n\n\n==== Autonomous weapons ====\n\nCurrently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers and drones.\n\n\n==== AI assisted tyranny ====\n\nUncontrolled progress in AI is a threat to democracy, because it gives bad actors (such as corporations or nations or interest groups) vastly improved powers of surveillance, the dissemination of misinformation and many other forms of propaganda, control, persecution and violations of privacy. Modern face recognition and voice recognition allows computer programs to identity individuals and track their movements. The vast amounts of data available from social media platforms and other sources combined with modern machine learning allows bad actors to influence citizens and to target individuals. More advanced AI in the future will increase the power of these actors. Vladimir Putin said: \"Artificial intelligence is the future not only of Russia but of all of mankind\" and \" Whoever becomes the leader in this sphere will become the ruler of the world.\u201d\n\n\n=== Ethical machines ===\nMachines with intelligence have the potential to use their intelligence to prevent harm and minimize the risks; they may have the ability to use ethical reasoning to better choose their actions in the world. Research in this area includes machine ethics, artificial moral agents, and friendly AI.\n\n\n==== Artificial moral agents ====\nWendell Wallach introduced the concept of artificial moral agents (AMA) in his book Moral Machines For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as \"Does Humanity Want Computers Making Moral Decisions\" and \"Can (Ro)bots Really Be Moral\". For Wallach the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior in contrast to the constraints which society may place on the development of AMAs.\n\n\n==== Machine ethics ====\n\nThe field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making. The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: \"Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems\u2014it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics.\" Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition \"Machine Ethics\" that stems from the AAAI Fall 2005 Symposium on Machine Ethics.\n\n\n==== Malevolent and friendly AI ====\n\nPolitical scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent. He argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of humanity and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\nOne proposal to deal with this is to ensure that the first generally intelligent AI is 'Friendly AI', and will then be able to control subsequently developed AIs. Some question whether this kind of check could really remain in place.\nLeading AI researcher Rodney Brooks writes, \"I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI, and the enormity and complexity of building sentient volitional intelligence.\"\n\n\n=== Machine consciousness, sentience and mind ===\n\nIf an AI system replicates all key aspects of human intelligence, will that system also be sentient \u2013 will it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness.\n\n\n==== Consciousness ====\n\n\n==== Computationalism and functionalism ====\n\nComputationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\n\n\n==== Strong AI hypothesis ====\n\nThe philosophical position that John Searle has named \"strong AI\" states: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the \"mind\" might be.\n\n\n==== Robot rights ====\n\nIf a machine can be created that has intelligence, could it also feel? If it can feel, does it have the same rights as a human? This issue, now known as \"robot rights\", is currently being considered by, for example, California's Institute for the Future, although many critics believe that the discussion is premature. Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights. The subject is profoundly discussed in the 2010 documentary film Plug & Pray.\n\n\n=== Superintelligence ===\n\nAre there limits to how intelligent machines \u2013 or human-machine hybrids \u2013 can be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. \u2018\u2019Superintelligence\u2019\u2019 may also refer to the form or degree of intelligence possessed by such an agent.\n\n\n==== Technological singularity ====\n\nIf research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement. The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario \"singularity\". Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.Ray Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029, and predicts that the singularity will occur in 2045.\n\n\n==== Transhumanism ====\n\nYou awake one morning to find your brain has another lobe functioning. Invisible, this auxiliary lobe answers your questions with information beyond the realm of your own memory, suggests plausible courses of action, and asks questions that help bring out relevant facts. You quickly come to rely on the new lobe so much that you stop wondering how it works. You just use it. This is the dream of artificial intelligence.\n\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, which has roots in Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" (1863), and expanded upon by George Dyson in his book of the same name in 1998.\n\n\n== In fiction ==\n\nThought-capable artificial beings appeared as storytelling devices since antiquity,\nand have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduce the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during layman discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune. In the 1980s artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.\nSeveral works use AI to force us to confront the fundamental of question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's \"R.U.R.\", the films \"A.I. Artificial Intelligence\" and \"Ex Machina\",  as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\n\n== See also ==\n\n\n== Explanatory notes ==\n\n\n== References ==\n\n\n=== AI textbooks ===\n\n\n=== History of AI ===\n\n\n=== Other sources ===\n\n\n== Further reading ==\nDH Autor, \u2018Why Are There Still So Many Jobs? The History and Future of Workplace Automation\u2019 (2015) 29(3) Journal of Economic Perspectives 3.\nTechCast Article Series, John Sagi, \"Framing Consciousness\"\nBoden, Margaret, Mind As Machine, Oxford University Press, 2006\nGopnik, Alison, \"Making AI More Human:  Artificial intelligence has staged a revival by starting to incorporate what we know about how children learn\", Scientific American, vol. 316, no. 6 (June 2017), pp. 60\u201365.\nJohnston, John (2008) The Allure of Machinic Life: Cybernetics, Artificial Life, and the New AI, MIT Press\nMarcus, Gary, \"Am I Human?:  Researchers need new ways to distinguish artificial intelligence from the natural kind\", Scientific American, vol. 316, no. 3 (March 2017), pp. 58\u201363.  Multiple tests of artificial-intelligence efficacy are needed because, \"just as there is no single test of athletic prowess, there cannot be one ultimate test of intelligence.\"  One such test, a \"Construction Challenge\", would test perception and physical action\u2014\"two important elements of intelligent behavior that were entirely absent from the original Turing test.\"  Another proposal has been to give machines the same standardized tests of science and other disciplines that schoolchildren take.  A so far insuperable stumbling block to artificial intelligence is an incapacity for reliable disambiguation.  \"[V]irtually every sentence [that people generate] is ambiguous, often in multiple ways.\"  A prominent example is known as the \"pronoun disambiguation problem\":  a machine has no way of determining to whom or what a pronoun in a sentence\u2014such as \"he\", \"she\" or \"it\"\u2014refers.\nE McGaughey, 'Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy' (2018) SSRN, part 2(3).\nMyers, Courtney Boyd ed. (2009). \"The AI Report\". Forbes June 2009\nRaphael, Bertram (1976). The Thinking Computer. W.H.Freeman and Company. ISBN 0-7167-0723-3. \nSerenko, Alexander (2010). \"The development of an AI journal ranking based on the revealed preference approach\" (PDF). Journal of Informetrics. 4 (4): 447\u2013459. doi:10.1016/j.joi.2010.04.001. \nSerenko, Alexander; Michael Dohan (2011). \"Comparing the expert survey and citation impact journal ranking methods: Example from the field of Artificial Intelligence\" (PDF). Journal of Informetrics. 5 (4): 629\u2013649. doi:10.1016/j.joi.2011.06.002. \nSun, R. & Bookman, L. (eds.), Computational Architectures: Integrating Neural and Symbolic Processes. Kluwer Academic Publishers, Needham, MA. 1994.\nTom Simonite (29 December 2014). \"2014 in Computing: Breakthroughs in Artificial Intelligence\". MIT Technology Review. \n\n\n== External links ==\n\nWhat Is AI? \u2013 An introduction to artificial intelligence by John McCarthy\u2014a co-founder of the field, and the person who coined the term.\nThe Handbook of Artificial Intelligence Volume \u2160 by Avron Barr and Edward A. Feigenbaum (Stanford University)\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy. \nThomason, Richmond. \"Logic and Artificial Intelligence\".  In Zalta, Edward N. Stanford Encyclopedia of Philosophy. \nAI at Curlie (based on DMOZ)\nAITopics \u2013 A large directory of links and other resources maintained by the Association for the Advancement of Artificial Intelligence, the leading organization of academic AI researchers.\nList of AI Conferences \u2013 A list of 225 AI conferences taking place all over the world.\nArtificial Intelligence, BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, Dec. 8, 2005)", "information systems": "An information system (IS) is an organized system for the collection, organization, storage and communication of information.\nMore specifically, it is the study of complementary networks that people and organizations use to collect, filter, process, create and distribute data. Further, \"[a]n information system (IS) is a group of components that interact to produce information.  It focuses on the internal rather than the external.\" Information system can also be described as a combination of hardware, software, data, business process and functions which can be used to increase efficiency and management of an organization. Information Systems is the expression used to describe an Automated System (which may be referred to as a Computerized Information System), be it manual, which covers people, machines or organized methods to collect, process, transmit and disseminate data representing information for the user or client.A computer information system is a system that a branch of Science composed of people and computers that processes or interprets information.\nThe term is also sometimes used in more restricted senses to refer to only the software used to run a computerized database or to refer to only a computer system.\nInformation Systems is an academic study of systems with a specific reference to information and the complementary networks of hardware and software that people and organizations use to collect, filter, process, create and also distribute data. An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks.Any specific information system aims to support operations, management and decision-making. An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes.Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes.Alter argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system whose activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information.As such, information systems inter-relate with data systems on the one hand and activity systems on the other. An information system is a form of communication system in which data represent and are processed as a form of social memory. An information system can also be considered a semi-formal language which supports human decision making and action.\nInformation systems are the primary focus of study for organizational informatics.\n\n\n== Overview ==\nSilver et al. (1995) provided two views on IS that includes software, hardware, data, people, and procedures. Zheng provided another system view of information system which also adds processes and essential system elements like environment, boundary, purpose, and interactions.\nThe Association for Computing Machinery defines \"Information systems specialists [as] focus[ing] on integrating information technology solutions and business processes to meet the information needs of businesses and other enterprises.\"There are various types of information systems, for example: transaction processing systems, decision support systems, knowledge management systems, learning management systems, database management systems, and office information systems. Critical to most information systems are information technologies, which are typically designed to enable humans to perform tasks for which the human brain is not well suited, such as: handling large amounts of information, performing complex calculations, and controlling many simultaneous processes.\nInformation technologies are a very important and malleable resource available to executives. Many companies have created a position of chief information officer (CIO) that sits on the executive board with the chief executive officer (CEO), chief financial officer (CFO), chief operating officer (COO), and chief technical officer (CTO). The CTO may also serve as CIO, and vice versa. The chief information security officer (CISO) focuses on information security management.\nThe six components that must come together in order to produce an information system are: (Information systems are organizational procedures and do not need a computer or software, this data is erroneous) (IE, an accounting system in the 1400s using ledger and ink utilizes an information system)\n\nHardware: The term hardware refers to machinery. This category includes the computer itself, which is often referred to as the central processing unit (CPU), and all of its support equipment. Among the support, equipment are input and output devices, storage devices and communications devices.\nSoftware: The term software refers to computer programs and the manuals (if any) that support them. Computer programs are machine-readable instructions that direct the circuitry within the hardware parts of the system to function in ways that produce useful information from data. Programs are generally stored on some input/output medium, often a disk or tape.\nData: Data are facts that are used by programs to produce useful information. Like programs, data are generally stored in machine-readable form on disk or tape until the computer needs them.\nProcedures: Procedures are the policies that govern the operation of a computer system. \"Procedures are to people what software is to hardware\" is a common analogy that is used to illustrate the role of procedures in a system.\nPeople: Every system needs people if it is to be useful. Often the most overlooked element of the system are the people, probably the component that most influence the success or failure of information systems. This includes \"not only the users, but those who operate and service the computers, those who maintain the data, and those who support the network of computers.\" <Kroenke, D. M. (2015). MIS Essentials. Pearson Education>';'Feedback: it is another component of the IS, that defines that an IS may be provided with a feedback (Although this component isn't necessary to function).Data is the bridge between hardware and people. This means that the data we collect is only data until we involve people. At that point, data is now information.\n\n\n== Types of information system ==\n\nThe \"classic\" view of Information systems found in the textbooks in the 1980s was a pyramid of systems that reflected the hierarchy of the organization, usually transaction processing systems at the bottom of the pyramid, followed by management information systems, decision support systems, and ending with executive information systems at the top. Although the pyramid model remains useful since it was first formulated, a number of new technologies have been developed and new categories of information systems have emerged, some of which no longer fit easily into the original pyramid model.\nSome examples of such systems are:\n\ndata warehouses\nenterprise resource planning\nenterprise systems\nexpert systems\nsearch engines\ngeographic information system\nglobal information system\noffice automation.A computer(-based) information system is essentially an IS using computer technology to carry out some or all of its planned tasks. The basic components of computer-based information systems are:\n\nHardware- these are the devices like the monitor, processor, printer and keyboard, all of which work together to accept, process, show data and information.\nSoftware- are the programs that allow the hardware to process the data.\nDatabases- are the gathering of associated files or tables containing related data.\nNetworks- are a connecting system that allows diverse computers to distribute resources.\nProcedures- are the commands for combining the components above to process information and produce the preferred output.The first four components (hardware, software, database, and network) make up what is known as the information technology platform.\nInformation technology workers could then use these components to create information systems that watch over safety measures, risk and the management of data. These actions are known as information technology services.Certain information systems support parts of organizations, others support entire organizations, and still others, support groups of organizations. Recall that each department or functional area within an organization has its own collection of application programs or information systems. These functional area information systems (FAIS) are supporting pillars for more general IS namely, business intelligence systems and dashboards. As the name suggest, each FAIS support a particular function within the organization, e.g.: accounting IS, finance IS, production-operation management (POM) IS, marketing IS, and human resources IS. In finance and accounting, managers use IT systems to forecast revenues and business activity, to determine the best sources and uses of funds, and to perform audits to ensure that the organization is fundamentally sound and that all financial reports and documents are accurate. Other types of organizational information systems are FAIS, Transaction processing systems, enterprise resource planning, office automation system, management information system, decision support system, expert system, executive dashboard, supply chain management system, and electronic commerce system. Dashboards are a special form of IS that support all managers of the organization. They provide rapid access to timely information and direct access to structured information in the form of reports. Expert systems attempt to duplicate the work of human experts by applying reasoning capabilities, knowledge, and expertise within a specific domain.\n\n\n== Information system development ==\nInformation technology departments in larger organizations tend to strongly influence the development, use, and application of information technology in the organizations.\nA series of methodologies and processes can be used to develop and use an information system. Many developers now use an engineering approach such as the system development life cycle (SDLC), which is a systematic procedure of developing an information system through stages that occur in sequence.\nRecent research aims at enabling and measuring the ongoing, collective development of such systems within an organization by the entirety of human actors themselves.\nAn information system can be developed in house (within the organization) or outsourced. This can be accomplished by outsourcing certain components or the entire system. A specific case is the geographical distribution of the development team (offshoring, global information system).\nA computer-based information system, following a definition of Langefors, is a technologically implemented medium for:\n\nrecording, storing, and disseminating linguistic expressions,\nas well as for drawing conclusions from such expressions.Geographic information systems, land information systems, and disaster information systems are examples of emerging information systems, but they can be broadly considered as spatial information systems.\nSystem development is done in stages which include:\n\nProblem recognition and specification\nInformation gathering\nRequirements specification for the new system\nSystem design\nSystem construction\nSystem implementation\nReview and maintenance.\n\n\n== As an academic discipline ==\n\nThe field of study called information systems encompasses a variety of topics including systems analysis and design, computer networking, information security, database management and decision support systems. Information management deals with the practical and theoretical problems of collecting and analyzing information in a business function area including business productivity tools, applications programming and implementation, electronic commerce, digital media production, data mining, and decision support. Communications and networking deals with the telecommunication technologies.\nInformation systems bridges business and computer science using the theoretical foundations of information and computation to study various business models and related algorithmic processes  on building the IT systems  within a computer science discipline. Computer information system(s) (CIS) is a field studying computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society, whereas IS emphasizes functionality over design.Several IS scholars have debated the nature and foundations of Information Systems which have its roots in other reference disciplines such as Computer Science, Engineering, Mathematics, Management Science, Cybernetics, and others. Information systems also can be defined as a collection of hardware, software, data, people and procedures that work together to produce quality information.\n\n\n=== Differentiating IS from related disciplines ===\n\nSimilar to computer science, other disciplines can be seen as both related and foundation disciplines of IS. The domain of study of IS involves the study of theories and practices related to the social and technological phenomena, which determine the development, use, and effects of information systems in organization and society. But, while there may be considerable overlap of the disciplines at the boundaries, the disciplines are still differentiated by the focus, purpose, and orientation of their activities.In a broad scope, the term Information Systems is a scientific field of study that addresses the range of strategic, managerial, and operational activities involved in the gathering, processing, storing, distributing, and use of information and its associated technologies in society and organizations. The term information systems is also used to describe an organizational function that applies IS knowledge in industry, government agencies, and not-for-profit organizations. Information Systems often refers to the interaction between algorithmic processes and technology. This interaction can occur within or across organizational boundaries. An information system is the technology an organization uses and also the way in which the organizations interact with the technology and the way in which the technology works with the organization\u2019s business processes. Information systems are distinct from information technology (IT) in that an information system has an information technology component that interacts with the processes' components.\nOne problem with that approach is that it prevents the IS field from being interested in non-organizational use of ICT, such as in social networking, computer gaming, mobile personal usage, etc. A different way of differentiating the IS field from its neighbours is to ask, \"Which aspects of reality are most meaningful in the IS field and other fields?\"  This approach, based on philosophy, helps to define not just the focus, purpose and orientation, but also the dignity, destiny and, responsibility of the field among other fields.\nInternational Journal of Information Management, 30, 13-20.\n\n\n== Career pathways ==\nInformation Systems workers enter a number of different careers:\n\nInformation System Strategy\nManagement Information Systems\nProject Management\nEnterprise Architecture\nIS Development\nIS Organization\nIS Consulting\nIS Security\nIS AuditorThere is a wide variety of career paths in the information systems discipline. \"Workers with specialized technical knowledge and strong communications skills will have the best prospects. Workers with management skills and an understanding of business practices and principles will have excellent opportunities, as companies are increasingly looking to technology to drive their revenue.\"Information technology is important to the operation of contemporary businesses, it offers many employment opportunities. The information systems field includes the people in organizations who design and build information systems, the people who use those systems, and the people responsible for managing those systems.\nThe demand for traditional IT staff such as programmers, business analysts, systems analysts, and designer is significant. Many well-paid jobs exist in areas of Information technology. At the top of the list is the chief information officer (CIO).\nThe CIO is the executive who is in charge of the IS function. In most organizations, the CIO works with the chief executive officer (CEO), the chief financial officer (CFO), and other senior executives. Therefore, he or she actively participates in the organization's strategic planning process.\n\n\n== Research ==\nInformation systems research is generally interdisciplinary concerned with the study of the effects of information systems on the behaviour of individuals, groups, and organizations. Hevner et al. (2004) categorized research in IS into two scientific paradigms including behavioural science which is to develop and verify theories that explain or predict human or organizational behavior and design science which extends the boundaries of human and organizational capabilities by creating new and innovative artifacts.\nSalvatore March and Gerald Smith proposed a framework for researching different aspects of Information Technology including outputs of the research (research outputs) and activities to carry out this research (research activities). They identified research outputs as follows:\n\nConstructs which are concepts that form the vocabulary of a domain. They constitute a conceptualization used to describe problems within the domain and to specify their solutions.\nA model which is a set of propositions or statements expressing relationships among constructs.\nA method which is a set of steps (an algorithm or guideline) used to perform a task. Methods are based on a set of underlying constructs and a representation (model) of the solution space.\nAn instantiation is the realization of an artifact in its environment.Also research activities including:\n\nBuild an artifact to perform a specific task.\nEvaluate the artifact to determine if any progress has been achieved.\nGiven an artifact whose performance has been evaluated, it is important to determine why and how the artifact worked or did not work within its environment. Therefore, theorize and justify theories about IT artifacts.Although Information Systems as a discipline has been evolving for over 30 years now, the core focus or identity of IS research is still subject to debate among scholars. There are two main views around this debate: a narrow view focusing on the IT artifact as the core subject matter of IS research, and a broad view that focuses on the interplay between social and technical aspects of IT that is embedded into a dynamic evolving context. A third view calls on IS scholars to pay balanced attention to both the IT artifact and its context.\nSince the study of information systems is an applied field, industry practitioners expect information systems research to generate findings that are immediately applicable in practice. This is not always the case however, as information systems researchers often explore behavioral issues in much more depth than practitioners would expect them to do. This may render information systems research results difficult to understand, and has led to criticism.In the last ten years, the business trend is represented by the considerable increasing of Information Systems Function (ISF) role, especially with regard the enterprise strategies and operations supporting. It became a key-factor to increase productivity and to support new value creation. To study an information system itself, rather than its effects, information systems models are used, such as EATPUT.\nThe international body of Information Systems researchers, the Association for Information Systems (AIS), and its Senior Scholars Forum Subcommittee on Journals (23 April 2007), proposed a 'basket' of journals that the AIS deems as 'excellent', and nominated: Management Information Systems Quarterly (MISQ), Information Systems Research (ISR), Journal of the Association for Information Systems (JAIS), Journal of Management Information Systems (JMIS), European Journal of Information Systems (EJIS), and Information Systems Journal (ISJ).A number of annual information systems conferences are run in various parts of the world, the majority of which are peer reviewed. The AIS directly runs the International Conference on Information Systems (ICIS) and the Americas Conference on Information Systems (AMCIS), while AIS affiliated conferences include the Pacific Asia Conference on Information Systems (PACIS), European Conference on Information Systems (ECIS), the Mediterranean Conference on Information Systems (MCIS), the International Conference on Information Resources Management (Conf-IRM) and the Wuhan International Conference on E-Business (WHICEB). AIS chapter conferences include Australasian Conference on Information Systems (ACIS), Information Systems Research Conference in Scandinavia (IRIS), Information Systems International Conference (ISICO), Conference of the Italian Chapter of AIS (itAIS), Annual Mid-Western AIS Conference (MWAIS) and Annual Conference of the Southern AIS (SAIS). EDSIG,  which is the special interest group on education of the AITP, organizes the Conference on Information Systems and Computing Education and the Conference on Information Systems Applied Research which are both held annually in November.\n\n\n== The impact on economic models ==\nMicroeconomic theory model\nTransaction cost theory\nAgency theory\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nRainer, R. Kelly and Cegielski, Casey G. (2009).  \"Introduction to Information Systems: Enabling and Transforming Business, 3rd Edition\"\nKroenke, David (2008). Using MIS - 2nd Edition.\nLindsay, John (2000). Information Systems \u2013 Fundamentals and Issues. Kingston University, School of Information Systems\nDostal, J. School information systems (Skolni informacni systemy). In Infotech 2007 - modern information and communication technology in education. Olomouc, EU: Votobia, 2007. s. 540 \u2013 546. ISBN 978-80-7220-301-7.\nO'Leary, Timothy and Linda. (2008). Computing Essentials Introductory 2008. McGraw-Hill on Computing2008.com\nImperial College London - Information Systems Engineering degree - Information Systems Engineering\nSage, S.M. \"Information Systems: A brief look into history\", Datamation, 63-69, Nov. 1968.  - Overview of the early history of IS.\nCy18mis546 Group 5 -  http://www.ramblingdata.com.s3-website.us-east-2.amazonaws.com\n\n\n== External links ==\nAssociation for Information Systems (AIS)\nIS History website\nCenter for Information Systems Research - Massachusetts Institute of Technology\nEuropean Research Center for Information Systems\nIndex of Information Systems Journals\nEuropean eCompetence Frame", "processes": "A process is a set of activities that interact to achieve a result.\nThings called a process include:\n\n\n== In arts, entertainment, and media ==\nWriting process, a concept in writing and composition studies\nThe Process, a concept in the movie 3%\n\n\n== In business and management ==\nBusiness process, activities that produce a specific service or product for customers\nBusiness process modeling, activity of representing processes of an enterprise in order to deliver improvements\nManufacturing process management, a collection of technologies and methods used to define how products are to be manufactured.\nProcess architecture, structural design of processes, applies to fields such as computers, business processes, logistics, project management\nProcess costing, a cost allocation procedure of managerial accounting\nProcess management, ensemble of activities of planning and monitoring the performance of a business process or manufacturing processes\nProcess management (Project Management) , a systematic series of activities directed towards causing an end result in engineering activities or project management\nProcess-based management, is a management approach that views a business as a collection of processes\nProcess industry, a category of material-related industry\n\n\n== In law ==\nDue process, the concept that governments must respect the rule of law\nLegal process, the proceedings and records of a legal case\nService of process, the procedure of giving official notice of a legal proceeding\n\n\n== In science and technology ==\nProcess (science), the scientific method\nProcess theory, the scientific study of processes\n\n\n=== In anatomy ===\nProcess (anatomy), a projection or outgrowth of tissue from a larger body\n\n\n=== In biology and psychology ===\nBiological process, a process of a living organism\nCognitive process, such as attention, memory, language use, reasoning, and problem solving\nMental process, a function or processes of the mind\nNeuronal process, also neurite, a projection from the cell body of a neuron\n\n\n=== In chemistry ===\nChemical process, a method or means of changing one or more chemicals or chemical compounds\nUnit process, a step in manufacturing in which chemical reaction takes place\n\n\n=== In computing ===\nProcess (computing), a computer program, or running a program concurrently with other programs\nChild process, created by another process\nParent process\nProcess management (computing), an integral part of any modern-day operating system (OS)\nProcessing (programming language), an open-source language and integrated development environment\n\n\n=== In mathematics ===\nIn probability theory:\nBranching process, a Markov process that models a population\nDiffusion process, a solution to a stochastic differential equation\nEmpirical process, a stochastic process that describes the proportion of objects in a system in a given state\nL\u00e9vy process, a stochastic process with independent, stationary increments\nPoisson process, a point process consisting of randomly located points on some underlying space\nPredictable process, a stochastic process whose value is knowable\nStochastic process, a random process, as opposed to a deterministic process\nWiener process, a continuous-time stochastic process\nProcess calculus, a diverse family of related approaches for formally modeling concurrent systems\nProcess function, a mathematical concept used in thermodynamics\n\n\n=== In thermodynamics ===\nProcess function, a mathematical concept used in thermodynamics\nThermodynamic process, the energetic evolution of a thermodynamic system\nAdiabatic process, which proceeds without transfer of heat or matter between a system and its surroundings\nIsenthalpic process, in which enthalpy stays constant\nIsobaric process, in which the pressure stays constant\nIsochoric process, in which volume stays constant\nIsothermal process, in which temperature stays constant\nPolytropic process, which obeys the equation \n  \n    \n      \n        p\n        \n          v\n          \n            \n            n\n          \n        \n        =\n        C\n      \n    \n    {\\displaystyle pv^{\\,n}=C}\n  \nQuasistatic process, which occurs infinitely slowly, as an approximation\n\n\n== Other uses ==\nProcess (engineering), set of interrelated tasks that transform inputs into outputs.\nFood processing,  transformation of raw ingredients, by physical or chemical means into food\nPraxis (process), in philosophy, the process by which a theory or skill is enacted or realized\nProcess philosophy, which regards change as the cornerstone of reality\nProcess Thinking, a philosophy that focuses on present circumstances\n\n\n== External links ==\n The dictionary definition of process at Wiktionary\n The dictionary definition of processes at Wiktionary\n The dictionary definition of processing at Wiktionary", "automobile": "A car (or automobile) is a wheeled motor vehicle used for transportation.  Most definitions of car say they run primarily on roads, seat one to eight people, have four tires, and mainly transport people rather than goods. Cars came into global use during the 20th century, and developed economies depend on them. The year 1886 is regarded as the birth year of the modern car when German inventor Karl Benz patented his Benz Patent-Motorwagen. Cars became widely available in the early 20th century. One of the first cars that were accessible to the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world.\nCars have controls for driving, parking, passenger comfort and safety, and controlling a variety of lights. Over the decades, additional features and controls have been added to vehicles, making them progressively more complex. Examples include rear reversing cameras, air conditioning, navigation systems, and in car entertainment. Most cars in use in the 2010s are propelled by an internal combustion engine, fueled by the combustion of fossil fuels. This causes air pollution and also contributes to climate change and global warming. Vehicles using alternative fuels such as ethanol flexible-fuel vehicles and natural gas vehicles are also gaining popularity in some countries. Electric cars, which were invented early in the history of the car, began to become commercially available in 2008.\nThere are costs and benefits to car use. The costs include acquiring the vehicle, interest payments (if the car is financed), repairs and maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance. The costs to society include maintaining roads, land use, road congestion, air pollution, public health, health care, and disposing of the vehicle at the end of its life. Road traffic accidents are the largest cause of injury-related deaths worldwide.The benefits include on-demand transportation, mobility, independence, and convenience. The societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from the taxes. The ability for people to move flexibly from place to place has far-reaching implications for the nature of societies. It was estimated in 2014 that the number of cars was over 1.25 billion vehicles, up from the 500 million of 1986. The numbers are increasing rapidly, especially in China, India and other newly industrialized countries.\n\n\n== Etymology ==\nThe word car is believed to originate from the Latin word carrus or carrum (\"wheeled vehicle\"), or the Middle English word carre (meaning \"two-wheel cart\", from Old North French). In turn, these originated from the Gaulish word karros (a Gallic chariot). It originally referred to any wheeled horse-drawn vehicle, such as a cart, carriage, or wagon. \"Motor car\" is attested from 1895, and is the usual formal name for cars in British English. \"Autocar\" is a variant that is also attested from 1895, but that is now considered archaic. It literally means \"self-propelled car\". The term \"horseless carriage\" was used by some to refer to the first cars at the time that they were being built, and is attested from 1895.The word \"automobile\" is a classical compound derived from the Ancient Greek word aut\u00f3s (\u03b1\u1f50\u03c4\u03cc\u03c2), meaning \"self\", and the Latin word mobilis, meaning \"movable\". It entered the English language from French, and was first adopted by the Automobile Club of Great Britain in 1897. Over time, the word \"automobile\" fell out of favour in Britain, and was replaced by \"motor car\". \"Automobile\" remains chiefly North American, particularly as a formal or commercial term. An abbreviated form, \"auto\", was formerly a common way to refer to cars in English, but is now considered old-fashioned. The word is still very common as an adjective in American English, usually in compound formations like \"auto industry\" and \"auto mechanic\". In Dutch and German, two languages historically related to English, the abbreviated form \"auto\" (Dutch) / \"Auto\" (German), as well as the formal full version \"automobiel\" (Dutch) / \"Automobil\" (German) are still used \u2014 in either the short form is the most regular word for \"car\".\n\n\n== History ==\n\nThe first working steam-powered vehicle was designed \u2014 and quite possibly built \u2014 by Ferdinand Verbiest, a Flemish member of a Jesuit mission in China around 1672. It was a 65-cm-long scale-model toy for the Chinese Emperor that was unable to carry a driver or a passenger. It is not known with certainty if Verbiest's model was successfully built or ran.\n\nNicolas-Joseph Cugnot is widely credited with building the first full-scale, self-propelled mechanical vehicle or car in about 1769; he created a steam-powered tricycle.  He also constructed two steam tractors for the French Army, one of which is preserved in the French National Conservatory of Arts and Crafts. His inventions were, however, handicapped by problems with water supply and maintaining steam pressure. In 1801, Richard Trevithick built and demonstrated his Puffing Devil road locomotive, believed by many to be the first demonstration of a steam-powered road vehicle. It was unable to maintain sufficient steam pressure for long periods, and was of little practical use.\nThe development of external combustion engines is detailed as part of the history of the car, but often treated separately from the development of true cars.  A variety of steam-powered road vehicles were used during the first part of the 19th century, including steam cars, steam buses, phaetons, and steam rollers.  Sentiment against them led to the Locomotive Acts of 1865.\nIn 1807, Nic\u00e9phore Ni\u00e9pce and his brother Claude created what was probably the world's first internal combustion engine (which they called a Pyr\u00e9olophore), but they chose to install it in a boat on the river Saone in France. Coincidentally, in 1807 the Swiss inventor Fran\u00e7ois Isaac de Rivaz designed his own 'de Rivaz internal combustion engine' and used it to develop the world's first vehicle to be powered by such an engine. The Ni\u00e9pces' Pyr\u00e9olophore was fuelled by a mixture of Lycopodium powder (dried spores of the Lycopodium plant), finely crushed coal dust and resin that were mixed with oil, whereas de Rivaz used a mixture of hydrogen and oxygen. Neither design was very successful, as was the case with others, such as Samuel Brown, Samuel Morey, and Etienne Lenoir with his hippomobile, who each produced vehicles (usually adapted carriages or carts) powered by internal combustion engines.\n\nIn November 1881, French inventor Gustave Trouv\u00e9 demonstrated the first working (three-wheeled) car powered by electricity at the International Exposition of Electricity, Paris. Although several other German engineers (including Gottlieb Daimler, Wilhelm Maybach, and Siegfried Marcus) were working on the problem at about the same time, Karl Benz generally is acknowledged as the inventor of the modern car.\n\nIn 1879, Benz was granted a patent for his first engine, which had been designed in 1878. Many of his other inventions made the use of the internal combustion engine feasible for powering a vehicle. His first Motorwagen was built in 1885 in Mannheim, Germany. He was awarded the patent for its invention as of his application on 29 January 1886 (under the auspices of his major company, Benz & Cie., which was founded in 1883). Benz began promotion of the vehicle on 3 July 1886, and about 25 Benz vehicles were sold between 1888 and 1893, when his first four-wheeler was introduced along with a model intended for affordability. They also were powered with four-stroke engines of his own design. Emile Roger of France, already producing Benz engines under license, now added the Benz car to his line of products. Because France was more open to the early cars, initially more were built and sold in France through Roger than Benz sold in Germany. In August 1888 Bertha Benz, the wife of Karl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.\n\nIn 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor. During the last years of the nineteenth century, Benz was the largest car company in the world with 572 units produced in 1899 and, because of its size, Benz & Cie., became a joint-stock company. The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the Pr\u00e4sident automobil.\nDaimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt in 1890, and sold their first car in 1892 under the brand name Daimler. It was a horse-drawn stagecoach built by another manufacturer, which they retrofitted with an engine of their design. By 1895 about 30 vehicles had been built by Daimler and Maybach, either at the Daimler works or in the Hotel Hermann, where they set up shop after disputes with their backers. Benz, Maybach and the Daimler team seem to have been unaware of each other's early work. They never worked together; by the time of the merger of the two companies, Daimler and Maybach were no longer part of DMG. Daimler died in 1900 and later that year, Maybach designed an engine named Daimler-Mercedes that was placed in a specially ordered model built to specifications set by Emil Jellinek. This was a production of a small number of vehicles for Jellinek to race and market in his country. Two years later, in 1902, a new model DMG car was produced and the model was named Mercedes after the Maybach engine, which generated 35 hp. Maybach quit DMG shortly thereafter and opened a business of his own. Rights to the Daimler brand name were sold to other manufacturers.\nKarl Benz proposed co-operation between DMG and Benz & Cie. when economic conditions began to deteriorate in Germany following the First World War, but the directors of DMG refused to consider it initially. Negotiations between the two companies resumed several years later when these conditions worsened and, in 1924 they signed an Agreement of Mutual Interest, valid until the year 2000. Both enterprises standardized design, production, purchasing, and sales and they advertised or marketed their car models jointly, although keeping their respective brands. On 28 June 1926, Benz & Cie. and DMG finally merged as the Daimler-Benz company, baptizing all of its cars Mercedes Benz, as a brand honoring the most important model of the DMG cars, the Maybach design later referred to as the 1902 Mercedes-35 hp, along with the Benz name. Karl Benz remained a member of the board of directors of Daimler-Benz until his death in 1929, and at times his two sons also participated in the management of the company.\n\nIn 1890, \u00c9mile Levassor and Armand Peugeot of France began producing vehicles with Daimler engines, and so laid the foundation of the automotive industry in France. In 1891, Auguste Doriot and his Peugeot colleague Louis Rigoulot completed the longest trip by a gasoline-powered vehicle when their self-designed and built Daimler powered Peugeot Type 3 completed 2,100 km (1,300 miles) from Valentigney to Paris and Brest and back again. They were attached to the first Paris\u2013Brest\u2013Paris bicycle race, but finished 6 days after the winning cyclist, Charles Terront.\nThe first design for an American car with a gasoline internal combustion engine was made in 1877 by George Selden of Rochester, New York. Selden applied for a patent for a car in 1879, but the patent application expired because the vehicle was never built. After a delay of sixteen years and a series of attachments to his application, on 5 November 1895, Selden was granted a United States patent (U.S. Patent 549,160) for a two-stroke car engine, which hindered, more than encouraged, development of cars in the United States. His patent was challenged by Henry Ford and others, and overturned in 1911.\nIn 1893, the first running, gasoline-powered American car was built and road-tested by the Duryea brothers of Springfield, Massachusetts. The first public run of the Duryea Motor Wagon took place on 21 September 1893, on Taylor Street in Metro Center Springfield. The Studebaker Automobile Company, subsidiary of a long-established wagon and coach manufacturer, started to build cars in 1897 and commenced sales of electric vehicles in 1902 and gasoline vehicles in 1904.In Britain, there had been several attempts to build steam cars with varying degrees of success, with Thomas Rickett even attempting a production run in 1860. Santler from Malvern is recognized by the Veteran Car Club of Great Britain as having made the first gasoline-powered car in the country in 1894, followed by Frederick William Lanchester in 1895, but these were both one-offs. The first production vehicles in Great Britain came from the Daimler Company, a company founded by Harry J. Lawson in 1896, after purchasing the right to use the name of the engines. Lawson's company made its first car in 1897, and they bore the name Daimler.In 1892, German engineer Rudolf Diesel was granted a patent for a \"New Rational Combustion Engine\". In 1897, he built the first diesel engine. Steam-, electric-, and gasoline-powered vehicles competed for decades, with gasoline internal combustion engines achieving dominance in the 1910s. Although various pistonless rotary engine designs have attempted to compete with the conventional piston and crankshaft design, only Mazda's version of the Wankel engine has had more than very limited success.\nAll in all, it is estimated that over 100,000 patents created the modern automobile and motorcycle.\n\n\n== Mass production ==\n\nLarge-scale, production-line manufacturing of affordable cars was started by Ransom Olds in 1901 at his Oldsmobile factory in Lansing, Michigan and based upon stationary assembly line techniques pioneered by Marc Isambard Brunel at the Portsmouth Block Mills, England, in 1802. The assembly line style of mass production and interchangeable parts had been pioneered in the U.S. by Thomas Blanchard in 1821, at the Springfield Armory in Springfield, Massachusetts. This concept was greatly expanded by Henry Ford, beginning in 1913 with the world's first moving assembly line for cars at the Highland Park Ford Plant.\nAs a result, Ford's cars came off the line in fifteen-minute intervals, much faster than previous methods, increasing productivity eightfold, while using less manpower (from 12.5-man-hours to 1 hour 33 minutes). It was so successful, paint became a bottleneck. Only Japan black would dry fast enough, forcing the company to drop the variety of colors available before 1913, until fast-drying Duco lacquer was developed in 1926. This is the source of Ford's apocryphal remark, \"any color as long as it's black\". In 1914, an assembly line worker could buy a Model T with four months' pay.Ford's complex safety procedures\u2014especially assigning each worker to a specific location instead of allowing them to roam about\u2014dramatically reduced the rate of injury. The combination of high wages and high efficiency is called \"Fordism,\" and was copied by most major industries. The efficiency gains from the assembly line also coincided with the economic rise of the United States. The assembly line forced workers to work at a certain pace with very repetitive motions which led to more output per worker while other countries were using less productive methods.\nIn the automotive industry, its success was dominating, and quickly spread worldwide seeing the founding of Ford France and Ford Britain in 1911, Ford Denmark 1923, Ford Germany 1925; in 1921, Citroen was the first native European manufacturer to adopt the production method. Soon, companies had to have assembly lines, or risk going broke; by 1930, 250 companies which did not, had disappeared.Development of automotive technology was rapid, due in part to the hundreds of small manufacturers competing to gain the world's attention. Key developments included electric ignition and the electric self-starter (both by Charles Kettering, for the Cadillac Motor Company in 1910\u20131911), independent suspension, and four-wheel brakes.\nSince the 1920s, nearly all cars have been mass-produced to meet market needs, so marketing plans often have heavily influenced car design. It was Alfred P. Sloan who established the idea of different makes of cars produced by one company, called the General Motors Companion Make Program, so that buyers could \"move up\" as their fortunes improved.\nReflecting the rapid pace of change, makes shared parts with one another so larger production volume resulted in lower costs for each price range. For example, in the 1930s, LaSalles, sold by Cadillac, used cheaper mechanical parts made by Oldsmobile; in the 1950s, Chevrolet shared hood, doors, roof, and windows with Pontiac; by the 1990s, corporate powertrains and shared platforms (with interchangeable brakes, suspension, and other parts) were common. Even so, only major makers could afford high costs, and even companies with decades of production, such as Apperson, Cole, Dorris, Haynes, or Premier, could not manage: of some two hundred American car makers in existence in 1920, only 43 survived in 1930, and with the Great Depression, by 1940, only 17 of those were left.In Europe, much the same would happen. Morris set up its production line at Cowley in 1924, and soon outsold Ford, while beginning in 1923 to follow Ford's practice of vertical integration, buying Hotchkiss (engines), Wrigley (gearboxes), and Osberton (radiators), for instance, as well as competitors, such as Wolseley: in 1925, Morris had 41% of total British car production. Most British small-car assemblers, from Abbey to Xtra, had gone under. Citroen did the same in France, coming to cars in 1919; between them and other cheap cars in reply such as Renault's 10CV and Peugeot's 5CV, they produced 550,000 cars in 1925, and Mors, Hurtu, and others could not compete. Germany's first mass-manufactured car, the Opel 4PS Laubfrosch (Tree Frog), came off the line at Russelsheim in 1924, soon making Opel the top car builder in Germany, with 37.5% of the market.In Japan, car production was very limited before World War II. Only a handful of companines were producing vehicles in limited numbers, and these were small, three-wheeled for commercial uses, like Daihatsu, or were the result of partnering with European companies, like Isuzu building the Wolseley A-9 in 1922. Mitsubishi was also partnered with Fiat and built the Mitsubishi Model A based on a Fiat vehicle. Toyota, Nissan, Suzuki, Mazda, and Honda began as companies producing non-automotive products before the war, switching to car production during the 1950s. Kiichiro Toyoda's decision to take Toyoda Loom Works into automobile manufacturing would create what would eventually become Toyota Motor Corporation, the largest automobile manufacturer in the world. Subaru, meanwhile, was formed from a conglomerate of six companies who banded together as Fuji Heavy Industries, as a result of having been broken up under keiretsu legislation.\n\n\n== Fuel and propulsion technologies ==\n\nMost cars in use in the 2010s are propelled by an internal combustion engine, fueled by the deflagration (rather than detonation) combustion of hydrocarbon fossil fuels, mostly gasoline (petrol) and diesel, as well as some Autogas and CNG. Hydrocarbon fuels cause air pollution and contribute to climate change and global warming. Rapidly increasing oil prices, concerns about oil dependence, tightening environmental laws and restrictions on greenhouse gas emissions are propelling work on alternative power systems for cars. Efforts to improve or replace existing technologies include the development of hybrid vehicles, plug-in electric vehicles and hydrogen vehicles. Vehicles using alternative fuels such as ethanol flexible-fuel vehicles and natural gas vehicles are also gaining popularity in some countries. Cars for racing or speed records have sometimes employed jet or rocket engines, but these are impractical for common use.\nOil consumption in the twentieth and twenty-first centuries has been abundantly pushed by car growth; the 1985\u20132003 oil glut even fuelled the sales of low-economy vehicles in OECD countries. The BRIC countries are adding to this consumption; in December 2009 China was briefly the largest car market.\n\n\n== User interface ==\n\nCars are equipped with controls used for driving, passenger comfort and safety, normally operated by a combination of the use of feet and hands, and occasionally by voice on 2000s-era cars. These controls include a steering wheel, pedals for operating the brakes and controlling the car's speed (and, in a manual transmission car, a clutch pedal), a shift lever or stick for changing gears, and a number of buttons and dials for turning on lights, ventilation and other functions. Modern cars' controls are now standardised, such as the location for the accelerator and brake, but this was not always the case. Controls are evolving in response to new technologies, for example the electric car and the integration of mobile communications.\nSince the car was first invented, its controls have become fewer and simpler through automation. For example, all cars once had a manual controls for the choke valve, clutch, ignition timing, and a crank instead of an electric starter. However new controls have also been added to vehicles, making them more complex. Examples include air conditioning, navigation systems, and in car entertainment. Another trend is the replacement of physical knob and switches for secondary controls with touchscreen controls such as BMW's iDrive and Ford's MyFord Touch. Another change is that while early cars' pedals were physically linked to the brake mechanism and throttle, in the 2010s, cars have increasingly replaced these physical linkages with electronic controls.\n\n\n== Lighting ==\n\nCars are typically fitted with multiple types of lights. These include headlights, which are used to illuminate the way ahead and make the car visible to other users, so that the vehicle can be used at night; in some jurisdictions, daytime running lights; red brake lights to indicate when the brakes are applied; amber turn signal lights to indicate the turn intentions of the driver; white-coloured reverse lights to illuminate the area behind the car (and indicate that the driver will be or is reversing); and on some vehicles, additional lights (e.g., side marker lights) to increase the visibility of the car. Interior lights on the ceiling of the car are usually fitted for the driver and passengers. Some vehicles also have a trunk light and, more rarely, an engine compartment light.\n\n\n== Weight ==\n\nIn the United States, \"from 1975 to 1980, average [car] weight dropped from 1,842 to 1,464 kg (4,060 to 3,228 lb), likely in response to rising gasoline prices\" and new fuel efficiency standards. The average new car weighed 1,461 kg (3,221 lb) in 1987 but 1,818 kg (4,009 lb) in 2010, due to modern steel safety cages, anti-lock brakes, airbags, and \"more-powerful\u2014if more-efficient\u2014engines.\" Heavier cars are safer for the driver, from an accident perspective, but more dangerous for other vehicles and road users. The weight of a car influences fuel consumption and performance, with more weight resulting in increased fuel consumption and decreased performance. The SmartFortwo, a small city car, weighs 750\u2013795 kg (1,655\u20131,755 lb). Heavier cars include full-size cars, SUVs and extended-length SUVs like the Suburban.\nAccording to research conducted by Julian Allwood of the University of Cambridge, global energy use could be heavily reduced by using lighter cars, and an average weight of 500 kg (1,100 lb) has been said to be well achievable. In some competitions such as the Shell Eco Marathon, average car weights of 45 kg (99 lb) have also been achieved. These cars are only single-seaters (still falling within the definition of a car, although 4-seater cars are more common), but they nevertheless demonstrate the amount by which car weights could still be reduced, and the subsequent lower fuel use (i.e. up to a fuel use of 2560 km/l).\n\n\n== Seating and body style ==\n\nMost cars are designed to carry multiple occupants, often with four or five seats. Cars with five seats typically seat two passengers in the front and three in the rear. Full-size cars and large sport utility vehicles can often carry six, seven, or more occupants depending on the arrangement of the seats. On the other hand, sports cars are most often designed with only two seats. The differing needs for passenger capacity and their luggage or cargo space has resulted in the availability of a large variety of body styles to meet individual consumer requirements that include, among others, the sedan/saloon, hatchback, station wagon/estate, and minivan.\n\n\n== Safety ==\n\nRoad traffic accidents are the largest cause of injury-related deaths worldwide. Mary Ward became one of the first documented car fatalities in 1869 in Parsonstown, Ireland, and Henry Bliss one of the United States' first pedestrian car casualties in 1899 in New York City.\nThere are now standard tests for safety in new cars, such as the EuroNCAP and the US NCAP tests, and insurance-industry-backed tests by the Insurance Institute for Highway Safety (IIHS).Worldwide, road traffic is becoming ever safer, in part due to efforts by the government to implement safety features in cars (e.g., seat belts, air bags, etc.), reduce unsafe driving practices (e.g., speeding, drinking and driving and texting and driving) and make road design more safe by adding features such as speed bumps, which reduce vehicle speed, and roundabouts, which reduce the likelihood of a head-on-collision (as compared with an intersection).\n\n\n== Costs and benefits ==\n\nThe costs of car usage, which may include the cost of: acquiring the vehicle, repairs and auto maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance, are weighed against the cost of the alternatives, and the value of the benefits \u2013 perceived and real \u2013 of vehicle usage. The benefits may include on-demand transportation, mobility, independence and convenience. During the 1920s, cars had another benefit: \"[c]ouples finally had a way to head off on unchaperoned dates, plus they had a private space to snuggle up close at the end of the night.\"Similarly the costs to society of encompassing car use, which may include those of: maintaining roads, land use, air pollution, road congestion, public health, health care, and of disposing of the vehicle at the end of its life, can be balanced against the value of the benefits to society that car use generates. The societal benefits may include: economy benefits, such as job and wealth creation, of car production and maintenance, transportation provision, society wellbeing derived from leisure and travel opportunities, and revenue generation from the tax opportunities. The ability for humans to move flexibly from place to place has far-reaching implications for the nature of societies.\n\n\n== Environmental impact ==\n\nWhile there are different types of fuel that may power cars, most rely on gasoline or diesel. The United States Environmental Protection Agency states that the average vehicle emits 8,887 grams of the greenhouse gas carbon dioxide (CO2) per gallon of gasoline. The average vehicle running on diesel fuel will emit 10,180 grams of carbon dioxide. Many governments are using fiscal policies (such as road tax or the US gas guzzler tax) to influence vehicle purchase decisions, with a low CO2 figure often resulting in reduced taxation. Fuel taxes may act as an incentive for the production of more efficient, hence less polluting, car designs (e.g. hybrid vehicles) and the development of alternative fuels. High fuel taxes may provide a strong incentive for consumers to purchase lighter, smaller, more fuel-efficient cars, or to not drive. On average, today's cars are about 75 percent recyclable, and using recycled steel helps reduce energy use and pollution. In the United States Congress, federally mandated fuel efficiency standards have been debated regularly, passenger car standards have not risen above the 27.5 miles per US gallon (8.6 L/100 km; 33.0 mpg\u2011imp) standard set in 1985. Light truck standards have changed more frequently, and were set at 22.2 miles per US gallon (10.6 L/100 km; 26.7 mpg\u2011imp) in 2007.The manufacture of vehicles is resource intensive, and many manufacturers now report on the environmental performance of their factories, including energy usage, waste and water consumption.The growth in popularity of the car allowed cities to sprawl, therefore encouraging more travel by car resulting in inactivity and obesity, which in turn can lead to increased risk of a variety of diseases.Transportation (of all types including trucks, buses and cars) is a major contributor to air pollution in most industrialised nations. According to the American Surface Transportation Policy Project nearly half of all Americans are breathing unhealthy air. Their study showed air quality in dozens of metropolitan areas has worsened over the last decade.Animals and plants are often negatively impacted by cars via habitat destruction and pollution. Over the lifetime of the average car the \"loss of habitat potential\" may be over 50,000 m2 (540,000 sq ft) based on primary production correlations. Animals are also killed every year on roads by cars, referred to as roadkill. More recent road developments are including significant environmental mitigations in their designs such as green bridges to allow wildlife crossings, and creating wildlife corridors.\nGrowth in the popularity of vehicles and commuting has led to traffic congestion. Brussels was considered Europe's most congested city in 2011 according to TomTom.\n\n\n== Emerging car technologies ==\nCar propulsion technologies that are under development include gasoline/electric and plug-in hybrids, battery electric vehicles, hydrogen cars, biofuels, and various alternative fuels. Research into future alternative forms of power include the development of fuel cells, Homogeneous charge compression ignition (HCCI), stirling engines, and even using the stored energy of compressed air or liquid nitrogen.\nNew materials which may replace steel car bodies include duralumin, fiberglass, carbon fiber, biocomposites, and carbon nanotubes. Telematics technology is allowing more and more people to share cars, on a pay-as-you-go basis, through car share and carpool schemes. Communication is also evolving due to connected car systems.\n\n\n=== Autonomous car ===\n\nFully autonomous vehicles, also known as driverless cars, already exist in prototype (such as the Google driverless car), and are expected to be commercially available around 2020. According to urban designer and futurist Michael E. Arth, driverless electric vehicles\u2014in conjunction with the increased use of virtual reality for work, travel, and pleasure\u2014could reduce the world's 800 million vehicles to a fraction of that number within a few decades. This would be possible if almost all private cars requiring drivers, which are not in use and parked 90% of the time, would be traded for public self-driving taxis that would be in near constant use. This would also allow for getting the appropriate vehicle for the particular need\u2014a bus could come for a group of people, a limousine could come for a special night out, and a Segway could come for a short trip down the street for one person. Children could be chauffeured in supervised safety, DUIs would no longer exist, and 41,000 lives could be saved each year in the US alone.\n\n\n=== Open source development ===\n\nThere have been several projects aiming to develop a car on the principles of open design, an approach to designing in which the plans for the machinery and systems are publicly shared, often without monetary compensation. The projects include OScar, Riversimple (through 40fires.org) and c,mm,n. None of the projects have reached significant success in terms of developing a car as a whole both from hardware and software perspective and no mass production ready open-source based design have been introduced as of late 2009. Some car hacking through on-board diagnostics (OBD) has been done so far.\n\n\n=== Car sharing ===\nCar-share arrangements and carpooling are also increasingly popular, in the US and Europe. For example, in the US, some car-sharing services have experienced double-digit growth in revenue and membership growth between 2006 and 2007. Services like car sharing offering a residents to \"share\" a vehicle rather than own a car in already congested neighborhoods.\n\n\n== Industry ==\n\nThe automotive industry designs, develops, manufactures, markets, and sells the world's motor vehicles. In 2008, more than 70 million motor vehicles, including cars and commercial vehicles were produced worldwide.In 2007, a total of 71.9 million new cars were sold worldwide: 22.9 million in Europe, 21.4 million in the Asia-Pacific Region, 19.4 million in the USA and Canada, 4.4 million in Latin America, 2.4 million in the Middle East and 1.4 million in Africa.  The markets in North America and Japan were stagnant, while those in South America and other parts of Asia grew strongly. Of the major markets, China, Russia, Brazil and India saw the most rapid growth.\nAbout 250 million vehicles are in use in the United States. Around the world, there were about 806 million cars and light trucks on the road in 2007; they burn over 260 billion US gallons (980,000,000 m3) of gasoline and diesel fuel yearly. The numbers are increasing rapidly, especially in China and India. In the opinion of some, urban transport systems based around the car have proved unsustainable, consuming excessive energy, affecting the health of populations, and delivering a declining level of service despite increasing investments. Many of these negative impacts fall disproportionately on those social groups who are also least likely to own and drive cars. The sustainable transport movement focuses on solutions to these problems.\nIn 2008, with rapidly rising oil prices, industries such as the automotive industry, are experiencing a combination of pricing pressures from raw material costs and changes in consumer buying habits. The industry is also facing increasing external competition from the public transport sector, as consumers re-evaluate their private vehicle usage. Roughly half of the US's fifty-one light vehicle plants are projected to permanently close in the coming years, with the loss of another 200,000 jobs in the sector, on top of the 560,000 jobs lost this decade. Combined with robust growth in China, in 2009, this resulted in China becoming the largest car producer and market in the world.\nChina 2009 sales had increased to 13.6 million, a significant increase from one million of domestic car sales in 2000. Since then however, even in China and other BRIC countries, the automotive production is again falling.\n\n\n== Alternatives ==\n\nEstablished alternatives for some aspects of car use include public transit such as buses, trolleybuses, trains, subways, tramways light rail, cycling, and walking. Bike-share systems have been tried in some European cities, including Copenhagen and Amsterdam. Similar programs have been experimented with in a number of US Cities. Additional individual modes of transport, such as personal rapid transit could serve as an alternative to cars if they prove to be socially accepted.\n\n\n== Other meanings ==\nThe term motorcar has formerly also been used in the context of electrified rail systems to denote a car which functions as a small locomotive but also provides space for passengers and baggage.  These locomotive cars were often used on suburban routes by both interurban and intercity railroad systems.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nHalberstam, David (1986). The Reckoning. New York: Morrow. ISBN 0-688-04838-2. \nKay, Jane Holtz (1997). Asphalt nation : how the automobile took over America, and how we can take it back. New York: Crown. ISBN 0-517-58702-5. \nWilliams, Heathcote (1991). Autogeddon. New York: Arcade. ISBN 1-55970-176-5. \nSachs, Wolfgang (1992). For love of the automobile: looking back into the history of our desires. Berkeley: University of California Press. ISBN 0-520-06878-5. \n\n\n== External links ==\nF\u00e9d\u00e9ration Internationale de l'Automobile\nForum for the Automobile and Society", "technocapitalism": "Technocapitalism (a portmanteau word combining \"technology\" and \"capitalism\") refers to changes in capitalism associated with the emergence of new technology sectors, the power of corporations, and new forms of organization.\n\n\n== Corporate power and organization ==\nLuis Suarez-Villa, in his 2009 book Technocapitalism: A Critical Perspective on Technological Innovation and Corporatism argues that it is a new version of capitalism that generates new forms of corporate organization designed to exploit intangibles such as creativity and new knowledge.  The new organizations, which he refers to as experimentalist organizations are deeply grounded in technological research, as opposed to manufacturing and services production.  They are also heavily dependent on the corporate appropriation of research outcomes as intellectual property.\nThis approach is further developed by Suarez-Villa in his 2012 book Globalization and Technocapitalism: The Political Economy of Corporate Power and Technological Domination, in which he relates the emergence of technocapitalism to globalization and to the growing power of technocapitalist corporations.  Taking into account the new relations of power introduced by the corporations that control technocapitalism, he considers new forms of accumulation involving intangibles\u2014such as creativity and new knowledge\u2014along with intellectual property and technological infrastructure.  This perspective on globalization\u2014and the effect of technocapitalism and its corporations\u2014also takes into account the growing global importance of intangibles, the inequalities created between nations at the vanguard of technocapitalism and those that are not, the increasing importance of brain-drain flows between nations, and the rise of what he refers to as a techno-military-corporate complex that is rapidly replacing the old military-industrial complex of the second half of the 20th century.  \nThe concept behind technocapitalism is part of a line of thought that relates science and technology to the evolution of capitalism.  At the core of this idea of the evolution of capitalism is that science and technology are not divorced from society\u2014or that they exist in a vacuum, or in a separate reality of their own\u2014out of reach of social action and human decision.  Science and technology are part of society, and they are subject to the priorities of capitalism as much as any other human endeavor, if not more so.  Prominent scientists in the early 20th century, such as John Bernal, posited that science has a social function, and cannot be seen as something apart from society.  Other scientists at that time, such as John Haldane, related science to social philosophy, and showed how critical approaches to social analysis are very relevant to science, and to our understanding of the need for science.  In our time, this line of thought has encouraged philosophers such as Andrew Feenberg to adopt and apply a critical theory approach to technology and science, providing many important insights on how scientific and technological decisions\u2014and their outcomes\u2014are shaped by society, and by capitalism and its institutions.Another work that is compatible with the explanation of technocapitalism provided above is Harry Braverman's Labor and Monopoly Capital: The Degradation of Work in the Twentieth Century.  In this book, Braverman argued that changes in production technology are influenced by the need of corporate management to control labor and labor processes.  The fact that technology can be placed at the service of corporate power, to control work routines and production processes, in order to diminish workers' rights, is a powerful indicator that technology is neither \"neutral\" nor purely \"functional\", as some technologists believe.  Although Braverman did not use the term technocapitalism in his work, his general approach is part of the line of thought related to this concept.  Following up on Braverman's work but from a different perspective, education philosopher Douglas Kellner used the term in a chapter to discuss the relationship between technology and the capitalist state\u2014along with the prospect for a post-modern theory of crises\u2014from the perspective of the Frankfurt School.The term technocapitalism has been used by one author to denote aspects and ideas that diverge sharply from those explained above. Dinesh D'Souza, writing about Silicon Valley in an article, used the term to describe the corporate environment and venture capital relationships in a high tech-oriented local economy.  His approach to the topic was consonant with that of business journals and the corporate management literature.  Some newspaper articles have also used the term occasionally and in a very general sense, to denote the importance of advanced technologies in the economy.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\nwww.technocapitalism.com", "machines": "A machine uses power to apply forces and control movement to perform an intended action.  Machines can be driven by animals and people, by natural forces such as wind and water, and by chemical, thermal, or electrical power, and include a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement.  They can also include computers and sensors that monitor performance and plan movement, often called mechanical systems.\nRenaissance natural philosophers identified six simple machines which were the elementary devices that put a load into motion, and calculated the ratio of output force to input force, known today as mechanical advantage.Modern machines are complex systems that consist of structural elements, mechanisms and control components and include interfaces for convenient use.  Examples include a wide range of vehicles, such as automobiles, boats and airplanes, appliances in the home and office, building air handling and water handling systems, as well as farm machinery, machine tools and factory automation systems and robots.\n\n\n== Etymology ==\nThe English word machine comes through Middle French from Latin machina, which in turn derives from the Greek (Doric \u03bc\u03b1\u03c7\u03b1\u03bd\u03ac makhana, Ionic \u03bc\u03b7\u03c7\u03b1\u03bd\u03ae mekhane \"contrivance, machine, engine\", a derivation from \u03bc\u1fc6\u03c7\u03bf\u03c2 mekhos \"means, expedient, remedy\"). The word mechanical (Greek: \u03bc\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03cc\u03c2) comes from the same Greek roots. A wider meaning of \"fabric, structure\" is found in classical Latin, but not in Greek usage.  This meaning is found in late medieval French, and is adopted from the French into English in the mid-16th century.\nIn the 17th century, the word could also mean a scheme or plot, a meaning now expressed by the derived machination. The modern meaning develops out of specialized application of the term to stage engines used in theater and to military siege engines, both in the late 16th and early 17th centuries. The OED traces the formal, modern meaning to John Harris' Lexicon Technicum (1704), which has:\n\nMachine, or Engine, in Mechanicks, is whatsoever hath Force sufficient either to raise or stop the Motion of a Body... Simple Machines are commonly reckoned to be Six in Number, viz. the Ballance, Leaver, Pulley, Wheel, Wedge, and Screw... Compound Machines, or Engines, are innumerable.The word engine used as a (near-)synonym both by Harris and in later language derives ultimately (via Old French) from Latin ingenium \"ingenuity, an invention\".\n\n\n== History ==\n\nThe hand axe, made by chipping flint to form a wedge, in the hands of a human transforms force and movement of the tool into a transverse splitting forces and movement of the workpiece.\nThe idea of a simple machine originated with the Greek philosopher Archimedes around the 3rd century BC, who studied the Archimedean simple machines: lever, pulley, and screw.  Archimedes discovered the principle of mechanical advantage in the lever.  Later Greek philosophers defined the classic five simple machines (excluding the inclined plane) and were able to roughly calculate their mechanical advantage.  Heron of Alexandria (ca. 10\u201375 AD) in his work Mechanics lists five mechanisms that can \"set a load in motion\"; lever, windlass, pulley, wedge, and screw, and describes their fabrication and uses.  However, the Greeks' understanding was limited to statics (the balance of forces) and did not include dynamics (the tradeoff between force and distance) or the concept of work.\nDuring the Renaissance the dynamics of the Mechanical Powers, as the simple machines were called, began to be studied from the standpoint of how much useful work they could perform, leading eventually to the new concept of mechanical work.  In 1586 Flemish engineer Simon Stevin derived the mechanical advantage of the inclined plane, and it was included with the other simple machines.   The complete dynamic theory of simple machines was worked out by Italian scientist Galileo Galilei in 1600 in Le Meccaniche (\"On Mechanics\").  He was the first to understand that simple machines do not create energy, they merely transform it.The classic rules of sliding friction in machines were discovered by Leonardo da Vinci (1452\u20131519), but remained unpublished in his notebooks.  They were rediscovered by Guillaume Amontons (1699) and were further developed by Charles-Augustin de Coulomb (1785).James Watt patented his parallel motion linkage in 1782, which made the double acting steam engine practical. The Boulton and Watt steam engine and later designs powered steam locomotives, steam ships, and factories.\nThe Industrial Revolution was a period from 1750 to 1850 where changes in agriculture, manufacturing, mining, transportation, and technology had a profound effect on the social, economic and cultural conditions of the times. It began in the United Kingdom, then subsequently spread throughout Western Europe, North America, Japan, and eventually the rest of the world.\nStarting in the later part of the 18th century, there began a transition in parts of Great Britain's previously manual labour and draft-animal\u2013based economy towards machine-based manufacturing. It started with the mechanisation of the textile industries, the development of iron-making techniques and the increased use of refined coal.\n\n\n== Simple machines ==\n\nThe idea that a machine can be decomposed into simple movable elements led Archimedes to define the lever, pulley and screw as simple machines.  By the time of the Renaissance this list increased to include the wheel and axle, wedge and inclined plane.  The modern approach to characterizing machines focusses on the components that allow movement, known as joints.\nWedge (hand axe):  Perhaps the first example of a device designed to manage power is the hand axe, also see biface and Olorgesailie.  A hand axe is made by chipping stone, generally flint, to form a bifacial edge, or wedge.  A wedge is a simple machine that transforms lateral force and movement of the tool into a transverse splitting force and movement of the workpiece.  The available power is limited by the effort of the person using the tool, but because power is the product of force and movement, the wedge amplifies the force by reducing the movement.  This amplification, or mechanical advantage is the ratio of the input speed to output speed.  For a wedge this is given by 1/tan\u03b1, where \u03b1 is the tip angle.  The faces of a wedge are modeled as straight lines to form a sliding or prismatic joint.\nLever: The lever is another important and simple device for managing power.  This is a body that pivots on a fulcrum.  Because the velocity of a point farther from the pivot is greater than the velocity of a point near the pivot, forces applied far from the pivot are amplified near the pivot by the associated decrease in speed.  If a is the distance from the pivot to the point where the input force is applied and b is the distance to the point where the output force is applied, then a/b is the mechanical advantage of the lever.  The fulcrum of a lever is modeled as a hinged or revolute joint.\nWheel: The wheel is clearly an important early machine, such as the chariot.  A wheel uses the law of the lever to reduce the force needed to overcome friction when pulling a load.  To see this notice that the friction associated with pulling a load on the ground is approximately the same as the friction in a simple bearing that supports the load on the axle of a wheel.  However, the wheel forms a lever that magnifies the pulling force so that it overcomes the frictional resistance in the bearing.\n\nThe classification of simple machines to provide a strategy for the design of new machines was developed by Franz Reuleaux, who collected and studied over 800 elementary machines.  He recognized that the classical simple machines can be separated into the lever, pulley and wheel and axle that are formed by a body rotating about a hinge, and the inclined plane, wedge and screw that are similarly a block sliding on a flat surface.Simple machines are elementary examples of kinematic chains or linkages that are used to model mechanical systems ranging from the steam engine to robot manipulators.  The bearings that form the fulcrum of a lever and that allow the wheel and axle and pulleys to rotate are examples of a kinematic pair called a hinged joint.  Similarly, the flat surface of an inclined plane and wedge are examples of the kinematic pair called a sliding joint.  The screw is usually identified as its own kinematic pair called a helical joint.\nThis realization shows that it is the joints, or the connections that provide movement, that are the primary elements of a machine.  Starting with four types of joints, the rotary joint, sliding joint, cam joint and gear joint, and related connections such as cables and belts, it is possible to understand a machine as an assembly of solid parts that connect these joints called a mechanism .Two levers, or cranks, are combined into a planar four-bar linkage by attaching a link that connects the output of one crank to the input of another.  Additional links can be attached to form a six-bar linkage or in series to form a robot.\n\n\n== Mechanical systems ==\n\nModern machines are systems consisting of (i) a power source and actuators that generate forces and movement, (ii) a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement, (iii) a controller with sensors that compare the output to a performance goal and then directs the actuator input, and (iv) an interface to an operator consisting of levers, switches, and displays. \nThis can be seen in Watt's steam engine (see the illustration) in which the power is provided by steam expanding to drive the piston.  The walking beam, coupler and crank transform the linear movement of the piston into rotation of the output pulley.  Finally, the pulley rotation drives the flyball governor which controls the valve for the steam input to the piston cylinder.\nThe adjective \"mechanical\" refers to skill in the practical application of an art or science, as well as relating to or caused by movement, physical forces, properties or agents such as is dealt with by mechanics.  Similarly Merriam-Webster Dictionary defines \"mechanical\" as relating to machinery or tools.\nPower flow through a machine provides a way to understand the performance of devices ranging from levers and gear trains to automobiles and robotic systems.  The German mechanician Franz Reuleaux wrote, \"a machine is a combination of resistant bodies so arranged that by their means the mechanical forces of nature can be compelled to do work accompanied by certain determinate motion.\" Notice that forces and motion combine to define power.\nMore recently, Uicker et al. stated that a machine is \"a device for applying power or changing its direction.\"  McCarthy and Soh describe a machine as a system that \"generally consists of a power source and a mechanism for the controlled use of this power.\"\n\n\n== Power sources ==\nHuman and animal effort were the original power sources for early machines.  Natural forces such as wind and water powered larger mechanical systems.\nWaterwheel: Waterwheels appeared around the world around 300 BC to use flowing water to generate rotary motion, which was applied to milling grain, and powering lumber, machining and textile operations.  Modern water turbines use water flowing through a dam to drive an electric generator.\nWindmill: Early windmills captured wind power to generate rotary motion for milling operations.  Modern wind turbines also drives a generator. This electricity in turn is used to drive motors forming the actuators of mechanical systems.\nEngine: The word engine derives from \"ingenuity\" and originally referred to contrivances that may or may not be physical devices.  See Merriam-Webster's definition of engine. A steam engine uses heat to boil water contained in a pressure vessel; the expanding steam drives a piston or a turbine. This principle can be seen in the aeolipile of Hero of Alexandria. This is called an external combustion engine.\nAn automobile engine is called an internal combustion engine because it burns fuel (an exothermic chemical reaction) inside a cylinder and uses the expanding gases to drive a piston.  A jet engine uses a turbine to compress air which is burned with fuel so that it expands through a nozzle to provide thrust to an aircraft, and so is also an \"internal combustion engine.\"  Power plant:  The heat from coal and natural gas combustion in a boiler generates steam that drives a steam turbine to rotate an electric generator.  A nuclear power plant uses heat from a nuclear reactor to generate steam and electric power.  This power is distributed through a network of transmission lines for industrial and individual use.\nMotors: Electric motors use either AC or DC electric current to generate rotational movement.  Electric servomotors are the actuators for mechanical systems ranging from robotic systems to modern aircraft.\nFluid Power:  Hydraulic and pneumatic systems use electrically driven pumps to drive water or air respectively into cylinders to power linear movement.\n\n\n== Mechanisms ==\nThe mechanism of a mechanical system is assembled from components called machine elements.  These elements provide structure for the system and control its movement.\nThe structural components are, generally, the frame members, bearings, splines, springs,  seals, fasteners and covers.  The shape, texture and color of covers provide a styling and operational interface between the mechanical system and its users.\nThe assemblies that control movement are also called \"mechanisms.\"   Mechanisms are generally classified as gears and gear trains, which includes belt drives and chain drives, cam and follower mechanisms,  and linkages, though there are other special mechanisms such as clamping linkages, indexing mechanisms, escapements and friction devices such as brakes and clutches.\nThe number of degrees of freedom of a mechanism, or its mobility, depends on the number of links and joints and the types of joints used to construct the mechanism. The general mobility of a mechanism is the difference between the unconstrained freedom of the links and the number of constraints imposed by the joints.  It is described by the Chebychev-Gr\u00fcbler-Kutzbach criterion.\n\n\n=== Structural components ===\nA number of machine elements provide important structural functions such as the frame, bearings, splines, spring and seals.\n\nThe recognition that the frame of a mechanism is an important machine element changed the name three-bar linkage into four-bar linkage.  Frames are generally assembled from truss or beam elements.\nBearings are components designed to manage the interface between moving elements and are the source of friction in machines. In general, bearings are designed for pure rotation or straight line movement.\nSplines and keys are two ways to reliably mount an axle to a wheel, pulley or gear so that torque can be transferred through the connection.\nSprings provides forces that can either hold components of a machine in place or acts as a suspension to support part of a machine.\nSeals are used between mating parts of a machine to ensure fluids, such as water, hot gases, or lubricant do not leak between the mating surfaces.\nFasteners such as screws, bolts, spring clips, and rivets are critical to the assembly of components of a machine. Fasteners are generally considered to be removable. In contrast, joining methods, such as welding, soldering, crimping and the application of adhesives, usually require cutting the parts to disassemble the components\n\n\n== Controllers ==\nControllers combine sensors, logic, and actuators to maintain the performance of components of a machine.  Perhaps the best known is the flyball governor for a steam engine.  Examples of these devices range from a thermostat that as temperature rises opens a valve to cooling water to speed controllers such as the cruise control system in an automobile.  The programmable logic controller replaced relays and specialized control mechanisms with a programmable computer.  Servomotors that accurately position a shaft in response to an electrical command are the actuators that make robotic systems possible.\n\n\n== Computing machines ==\n\nCharles Babbage designed machines to tabulate logarithms and other functions in 1837.  His Difference engine can be considered an advanced mechanical calculator and his Analytical Engine a forerunner of the modern computer, though none were built in Babbage's lifetime.\nThe Arithmometer and the Comptometer are mechanical computers that are precursors to modern digital computers.  Models used to study modern computers are termed State machine and Turing machine.\n\n\n== Molecular machines ==\nThe biological molecule myosin reacts to ATP and ADP to alternately engage with an actin filament and change its shape in a way that exerts a force, and then disengage to reset its shape, or conformation.  This acts as the molecular drive that causes muscle contraction.  Similarly the biological molecule kinesin has two sections that alternately engage and disengage with microtubules causing the molecule to move along the microtubule and transport vesicles within the cell.  These molecules are increasingly considered to be nanomachines.\nResearchers have used DNA to construct nano-dimensioned four-bar linkages.\n\n\n== Impact ==\n\n\n=== Mechanization and automation ===\n\nMechanization  or mechanisation (BE) is providing human operators with machinery that assists them with the muscular requirements of work or displaces muscular work.  In some fields, mechanization includes the use of hand tools.  In modern usage, such as in engineering or economics, mechanization implies machinery more complex than hand tools and would not include simple devices such as an un-geared horse or donkey mill. Devices that cause speed changes or changes to or from reciprocating to rotary motion, using means such as gears, pulleys or sheaves and belts, shafts, cams and cranks, usually are considered machines.  After electrification, when most small machinery was no longer hand powered, mechanization was synonymous with motorized machines.Automation is the use of control systems and information technologies to reduce the need for human work in the production of goods and services. In the scope of industrialization, automation is a step beyond mechanization. Whereas mechanization provides human operators with machinery to assist them with the muscular requirements of work, automation greatly decreases the need for human sensory and mental requirements as well. Automation plays an increasingly important role in the world economy and in daily experience.\n\n\n=== Automata ===\n\nAn automaton (plural: automata or automatons) is a self-operating machine. The word is sometimes used to describe a robot, more specifically an autonomous robot.  A Toy Automaton was patented in 1863.\n\n\n== Mechanics ==\nUsher reports that Hero of Alexandria's treatise on Mechanics focussed on the study of lifting heavy weights.  Today mechanics refers to the mathematical analysis of the forces and movement of a mechanical system, and consists of the study of the kinematics and dynamics of these systems.\n\n\n=== Dynamics of machines ===\nThe dynamic analysis of machines begins with a rigid-body model to determine reactions at the bearings, at which point the elasticity effects are included.  The rigid-body dynamics studies the movement of systems of interconnected bodies under the action of external forces.  The assumption that the bodies are rigid, which means that they do not deform under the action of applied forces, simplifies the analysis by reducing the parameters that describe the configuration of the system to the translation and rotation of reference frames attached to each body.The dynamics of a rigid body system is defined by its equations of motion, which are derived using either Newtons laws of motion or Lagrangian mechanics.  The solution of these equations of motion defines how the configuration of the system of rigid bodies changes as a function of time.  The formulation and solution of rigid body dynamics is an important tool in the computer simulation of mechanical systems.\n\n\n=== Kinematics of machines ===\nThe dynamic analysis of a machine requires the determination of the movement, or kinematics, of its component parts, known as kinematic analysis.  The assumption that the system is an assembly of rigid components allows rotational and translational movement to be modeled mathematically as Euclidean, or rigid, transformations.  This allows the position, velocity and acceleration of all points in a component to be determined from these properties for a reference point, and the angular position, angular velocity and angular acceleration of the component.\n\n\n== Machine design ==\nMachine design refers to the procedures and techniques used to address the three phases of a machine's lifecycle:\n\ninvention, which involves the identification of a need, development of requirements, concept generation, prototype development, manufacturing, and verification testing;\nperformance engineering involves enhancing manufacturing efficiency, reducing service and maintenance demands, adding features and improving effectiveness, and validation testing;\nrecycle is the decommissioning and disposal phase and includes recovery and reuse of materials and components.\n\n\n== See also ==\n\nHistory of technology\nMachine (mechanical)\nTechnology\nAutomaton\n\n\n== References ==\n\n\n== Further reading ==\nOberg, Erik; Franklin D. Jones; Holbrook L. Horton; Henry H. Ryffel (2000).  Christopher J. McCauley; Riccardo Heald; Muhammed Iqbal Hussain, eds. Machinery's Handbook (26th ed.). New York: Industrial Press Inc. ISBN 0-8311-2635-3. \nReuleaux, Franz (1876). The Kinematics of Machinery. Trans. and annotated by A. B. W. Kennedy. New York: reprinted by Dover (1963). \nUicker, J. J.; G. R. Pennock; J. E. Shigley (2003). Theory of Machines and Mechanisms. New York: Oxford University Press. \n\n\n== External links ==\nReuleaux Collection of Mechanisms and Machines at Cornell University", "communication": "Communication (from Latin comm\u016bnic\u0101re, meaning \"to share\") is the act of conveying meanings from one entity or group to another through the use of mutually understood signs and semiotic rules.\nThe main steps inherent to all communication are:\nThe formation of communicative motivation or reason.\nMessage composition (further internal or technical elaboration on what exactly to express).\nMessage encoding (for example, into digital data, written text, speech, pictures, gestures and so on).\nTransmission of the encoded message as a sequence of signals using a specific channel or medium.\nNoise sources such as natural forces and in some cases human activity (both intentional and accidental) begin influencing the quality of signals propagating from the sender to one or more receivers.\nReception of signals and reassembling of the encoded message from a sequence of received signals.\nDecoding of the reassembled encoded message.\nInterpretation and making sense of the presumed original message.The scientific study of communication can be divided into:\n\nInformation theory which studies the quantification, storage, and communication of information in general;\nCommunication studies which concerns human communication;\nBiosemiotics which examines communication in and between living organisms in general.The channel of communication can be visual, auditory, tactile (such as in Braille) and haptic, olfactory, electromagnetic, or biochemical.\nHuman communication is unique for its extensive use of abstract language. Development of civilization has been closely linked with progress in telecommunication.\n\n\n== Non-verbal ==\n\nNonverbal communication describes the processes of conveying a type of information in the form of non-linguistic representations. Examples of nonverbal communication include haptic communication, chronemic communication, gestures, body language, facial expressions, eye contact, and how one dresses. Nonverbal communication also relates to the intent of a message. Examples of intent are voluntary, intentional movements like shaking a hand or winking, as well as involuntary, such as sweating. Speech also contains nonverbal elements known as paralanguage, e.g. rhythm, intonation, tempo, and stress. It affects communication most at the subconscious level and establishes trust. Likewise, written texts include nonverbal elements such as handwriting style, the spatial arrangement of words and the use of emoticons to convey emotion.\nNonverbal communication demonstrates one of Wazlawick's laws: you cannot not communicate. Once proximity has formed awareness, living creatures begin interpreting any signals received. Some of the functions of nonverbal communication in humans are to complement and illustrate, to reinforce and emphasize, to replace and substitute, to control and regulate, and to contradict the denovative message.\nNonverbal cues are heavily relied on to express communication and to interpret others\u2019 communication and can replace or substitute verbal messages. However, non-verbal communication is ambiguous. When verbal messages contradict non-verbal messages, observation of non-verbal behaviour is relied on to judge another\u2019s attitudes and feelings, rather than assuming the truth of the verbal message alone.\nThere are several reasons as to why non-verbal communication plays a vital role in communication:\n\u201cNon-verbal communication is omnipresent.\u201d  They are included in every single communication act. To have total communication, all non-verbal channels such as the body, face, voice, appearance, touch, distance, timing, and other environmental forces must be engaged during face-to-face interaction. Written communication can also have non-verbal attributes. E-mails and web chats allow an individual\u2019s the option to change text font colours, stationary, emoticons, and capitalization in order to capture non-verbal cues into a verbal medium.\n\u201cNon-verbal behaviours are multifunctional.\u201d  Many different non-verbal channels are engaged at the same time in communication acts and allow the chance for simultaneous messages to be sent and received.\n\u201cNon-verbal behaviours may form a universal language system.\u201d  Smiling, crying, pointing, caressing, and glaring are non-verbal behaviours that are used and understood by people regardless of nationality. Such non-verbal signals allow the most basic form of communication when verbal communication is not effective due to language barriers.\n\n\n== Verbal ==\nVerbal communication is the spoken or written conveyance of a message. Human language can be defined as a system of symbols (sometimes known as lexemes) and the grammars (rules) by which the symbols are manipulated. The word \"language\" also refers to common properties of languages. Language learning normally occurs most intensively during human childhood. Most of the thousands of human languages use patterns of sound or gesture for symbols which enable communication with others around them. Languages tend to share certain properties, although there are exceptions. There is no defined line between a language and a dialect. Constructed languages such as Esperanto, programming languages, and various mathematical formalism is not necessarily restricted to the properties shared by human languages.\nAs previously mentioned, language can be characterized as symbolic. Charles Ogden and I.A Richards developed The Triangle of Meaning model to explain the symbol (the relationship between a word), the referent (the thing it describes), and the meaning (the thought associated with the word and the thing)\nThe properties of language are governed by rules. Language follows phonological rules (sounds that appear in a language), syntactic rules (arrangement of words and punctuation in a sentence), semantic rules (the agreed upon meaning of words), and pragmatic rules (meaning derived upon context).\nThe meanings that are attached to words can be literal, or otherwise known as denotative; relating to the topic being discussed, or, the meanings take context and relationships into account, otherwise known as connotative; relating to the feelings, history, and power dynamics of the communicators.\n\n\n== Written communication and its historical development ==\nOver time the forms of and ideas about communication have evolved through the continuing progression of technology. Advances include communications psychology and media psychology, an emerging field of study.\nThe progression of written communication can be divided into three \"information communication revolutions\":\nWritten communication first emerged through the use of pictographs. The pictograms were made in stone, hence written communication was not yet mobile. Pictograms began to develop standardized and simplified forms.\nThe next step occurred when writing began to appear on paper, papyrus, clay, wax, and other media with commonly shared writing systems, leading to adaptable alphabets. Communication became mobile.\nThe final stage is characterized by the transfer of information through controlled waves of electromagnetic radiation (i.e., radio, microwave, infrared) and other electronic signals.Communication is thus a process by which meaning is assigned and conveyed in an attempt to create shared understanding. Gregory Bateson called it \"the replication of tautologies in the universe. This process, which requires a vast repertoire of skills in interpersonal processing, listening, observing, speaking, questioning, analyzing, gestures, and evaluating enables collaboration and cooperation.\n\n\n== Business ==\n\nBusiness communication is used for a wide variety of activities including, but not limited to: strategic communications planning, media relations, public relations (which can include social media, broadcast and written communications, and more), brand management, reputation management, speech-writing, customer-client relations, and internal/employee communications.\nCompanies with limited resources may choose to engage in only a few of these activities, while larger organizations may employ a full spectrum of communications. Since it is difficult to develop such a broad range of skills, communications professionals often specialize in one or two of these areas but usually have at least a working knowledge of most of them. By far, the most important qualifications communications professionals can possess are excellent writing ability, good 'people' skills, and the capacity to think critically and strategically.\n\n\n== Political ==\nCommunication is one of the most relevant tools in political strategies, including persuasion and propaganda. In mass media research and online media research, the effort of the strategist is that of getting a precise decoding, avoiding \"message reactance\", that is, message refusal. The reaction to a message is referred also in terms of approach to a message, as follows:\n\nIn \"radical reading\" the audience rejects the meanings, values, and viewpoints built into the text by its makers. Effect: message refusal.\nIn \"dominant reading\", the audience accepts the meanings, values, and viewpoints built into the text by its makers. Effect: message acceptance.\nIn \"subordinate reading\" the audience accepts, by and large, the meanings, values, and worldview built into the text by its makers. Effect: obey to the message.Holistic approaches are used by communication campaign leaders and communication strategists in order to examine all the options, \"actors\" and channels that can generate change in the semiotic landscape, that is, change in perceptions, change in credibility, change in the \"memetic background\", change in the image of movements, of candidates, players and managers as perceived by key influencers that can have a role in generating the desired \"end-state\".\nThe modern political communication field is highly influenced by the framework and practices of \"information operations\" doctrines that derive their nature from strategic and military studies. According to this view, what is really relevant is the concept of acting on the Information  Environment. The information environment is the aggregate of individuals, organizations, and systems that collect, process, disseminate, or act on information. This environment consists of three interrelated dimensions, which continuously interact with individuals, organizations, and systems. These dimensions are known as physical, informational, and cognitive.\n\n\n== Family ==\nFamily communication is the study of the communication perspective in a broadly defined family, with intimacy and trusting relationship. The main goal of family communication is to understand the interactions of family and the pattern of behaviors of family members in different circumstances. Open and honest communication creates an atmosphere that allows family members to express their differences as well as love and admiration for one another. It also helps to understand the feelings of one another.\nFamily communication study looks at topics such as family rules, family roles or family dialectics and how those factors could affect the communication between family members. Researchers develop theories to understand communication behaviors. Family communication study also digs deep into certain time periods of family life such as marriage, parenthood or divorce and how communication stands in those situations. It is important for family members to understand communication as a trusted way which leads to a well constructed family.\n\n\n== Interpersonal ==\nIn simple terms, interpersonal communication is the communication between one person and another (or others). It is often referred to as face-to-face communication between two (or more) people. Both verbal and nonverbal communication, or body language, play a part in how one person understands another. In verbal interpersonal communication there are two types of messages being sent: a content message and a relational message. Content messages are messages about the topic at hand and relational messages are messages about the relationship itself. This means that relational messages come across in how one says something and it demonstrates a person\u2019s feelings, whether positive or negative, towards the individual they are talking to, indicating not only how they feel about the topic at hand, but also how they feel about their relationship with the other individual.There are many different aspects of interpersonal communication including;\n-\tAudiovisual Perception of Communication Problems \nThe concept follows the idea that our words change what form they take based on the stress level or urgency of the situation.\nIt also explores the concept that stuttering during speech shows the audience that there is a problem or that the situation is more stressful.-\tThe Attachment Theory \nThis is the combined work of John Bowlby and Mary Ainsworth (Ainsworth & Bowlby, 1991)\nThis theory follows the relationships that builds between a mother and child, and the impact it has on their relationships with others.-\tEmotional Intelligence and Triggers \nEmotional Intelligence focuses on the ability to monitor ones own emotions as well as those of others.\nEmotional Triggers focus on events or people that tend to set off intense, emotional reactions within individuals.-\tAttribution Theory \nThis is the study of how individuals explain what causes different events and behaviors.-\tThe Power of Words (Verbal communications) \nVerbal communication focuses heavily on the power of words, and how those words are said.\nIt takes into consideration tone, volume, and choice of words.-\tNonverbal Communication\n\nFocuses heavily on the setting that the words are conveyed in.\nAs well as the physical tone of the words.-\tEthics in Personal Relations \nIt is about a space of mutual responsibility between two individuals, it\u2019s about giving and receiving in a relationship.\nThis theory is explored by Dawn J. Lipthrott in the article What IS Relationship? What is Ethical Partnership?-\tDeception in Communication \nThis concept goes into that everyone lies, and how this can impact relationships.\nThis theory is explored by James Hearn in his article Interpersonal Deception Theory: Ten Lessons for Negotiators-\tConflict in Couples \nThis focuses on the impact that social media has on relationships.\nAs well as how to communicate through conflict.\nThis theory is explored by Amanda Lenhart and Maeve Duggan in their paper Couples, the Internet, and Social Media\n\n\n== Barriers to effectiveness ==\nBarriers to effective communication can retard or distort the message or intention of the message being conveyed. This may result in failure of the communication process or cause an effect that is undesirable. These include filtering, selective perception, information overload, emotions, language, silence, communication apprehension, gender differences and political correctnessThis also includes a lack of expressing \"knowledge-appropriate\" communication, which occurs when a person uses ambiguous or complex legal words, medical jargon, or descriptions of a situation or environment that is not understood by the recipient.\n\nPhysical barriers- Physical barriers are often due to the nature of the environment. An example of this is the natural barrier which exists if staff is located in different buildings or on different sites. Likewise, poor or outdated equipment, particularly the failure of management to introduce new technology, may also cause problems. Staff shortages are another factor which frequently causes communication difficulties for an organization.\nSystem design- System design faults refer to problems with the structures or systems in place in an organization. Examples might include an organizational structure which is unclear and therefore makes it confusing to know whom to communicate with. Other examples could be inefficient or inappropriate information systems, a lack of supervision or training, and a lack of clarity in roles and responsibilities which can lead to staff being uncertain about what is expected of them.\nAttitudinal barriers- Attitudinal barriers come about as a result of problems with staff in an organization. These may be brought about, for example, by such factors as poor management, lack of consultation with employees, personality conflicts which can result in people delaying or refusing to communicate, the personal attitudes of individual employees which may be due to lack of motivation or dissatisfaction at work, brought about by insufficient training to enable them to carry out particular tasks, or simply resistance to change due to entrenched attitudes and ideas.\nAmbiguity of words/phrases- Words sounding the same but having different meaning can convey a different meaning altogether. Hence the communicator must ensure that the receiver receives the same meaning. It is better if such words are avoided by using alternatives whenever possible.\nIndividual linguistic ability- The use of jargon, difficult or inappropriate words in communication can prevent the recipients from understanding the message. Poorly explained or misunderstood messages can also result in confusion. However, research in communication has shown that confusion can lend legitimacy to research when persuasion fails.\nPhysiological barriers- These may result from individuals' personal discomfort, caused\u2014for example\u2014by ill health, poor eyesight or hearing difficulties.\nBypassing-These happens when the communicators (sender and the receiver) do not attach the same symbolic meanings to their words. It is when the sender is expressing a thought or a word but the receiver takes it in a different meaning. For example- ASAP, Rest room\nTechnological multi-tasking and absorbency- With a rapid increase in technologically-driven communication in the past several decades, individuals are increasingly faced with condensed communication in the form of e-mail, text, and social updates. This has, in turn, led to a notable change in the way younger generations communicate and perceive their own self-efficacy to communicate and connect with others. With the ever-constant presence of another \"world\" in one's pocket, individuals are multi-tasking both physically and cognitively as constant reminders of something else happening somewhere else bombard them. Though perhaps too new of an advancement to yet see long-term effects, this is a notion currently explored by such figures as Sherry Turkle.\nFear of being criticized-This is a major factor that prevents good communication. If we exercise simple practices to improve our communication skill, we can become effective communicators. For example, read an article from the newspaper or collect some news from the television and present it in front of the mirror. This will not only boost your confidence but also improve your language and vocabulary.\nGender barriers- Most communicators whether aware or not, often have a set agenda. This is very notable among the different genders. For example, many women are found to be more critical in addressing conflict. It's also been noted that men are more than likely to withdraw from conflict when in comparison to women. This breakdown and comparison not only shows that there are many factors to communication between two specific genders but also room for improvement as well as established guidelines for all.\n\n\n=== Cultural aspects ===\nCultural differences exist within countries (tribal/regional differences, dialects etc.), between religious groups and in organisations or at an organisational level - where companies, teams and units may have different expectations, norms and idiolects. Families and family groups may also experience the effect of cultural barriers to communication within and between different family members or groups. For example: words, colours and symbols have different meanings in different cultures. In most parts of the world, nodding your head means agreement, shaking your head means no, except in some parts of the world.Communication to a great extent is influenced by culture and cultural variables. Understanding cultural aspects of communication refers to having knowledge of different cultures in order to communicate effectively with cross culture people. Cultural aspects of communication are of great relevance in today's world which is now a global village, thanks to globalisation. Cultural aspects of communication are the cultural differences which influences communication across borders. Impact of cultural differences on communication components are explained below:\n1) Verbal communication refers to form of communication which uses spoken and written words for expressing and transferring views and ideas. Language is the most important tool of verbal communication and it is the area where cultural difference play its role. All countries have different languages and to have a better understanding of different culture it is required to have knowledge of languages of different countries.\n2) Non verbal communication is a very wide concept and it includes all the other forms of communication which do not uses written or spoken words. Non verbal communication takes following forms:\n\nParalinguistics are the voice involved in communication other than actual language and involves tones, pitch, vocal cues etc. It also include sounds from throat and all these are greatly influenced by cultural differences across borders.\nProxemics deals with the concept of space element in communication. Proxemics explains four zones of spaces namely intimate personal, social and public. This concept differs with different culture as the permissible space vary in different countries.\nArtifactics studies about the non verbal signals or communication which emerges from personal accessories such as dresses or fashion accessories worn and it varies with culture as people of different countries follow different dressing codes.\nChronemics deal with the time aspects of communication and also include importance given to the time. Some issues explaining this concept are pauses, silences and response lag during an interaction. This aspect of communication is also influenced by cultural differences as it is well known that there is a great difference in the value given by different cultures to time.\nKinesics mainly deals with the body languages such as postures, gestures, head nods, leg movements etc. In different countries, the same gestures and postures are used to convey different messages. Sometimes even a particular kinesic indicating something good in a country may have a negative meaning in any other culture.So in order to have an effective communication across world it is desirable to have a knowledge of cultural variables effecting communication.\nAccording to Michael Walsh and Ghil'ad Zuckermann, Western conversational interaction is typically \"dyadic\", between two particular people, where eye contact is important and the speaker controls the interaction; and \"contained\" in a relatively short, defined time frame. However, traditional Aboriginal conversational interaction is \"communal\", broadcast to many people, eye contact is not important, the listener controls the interaction; and \"continuous\", spread over a longer, indefinite time frame.\n\n\n=== Barriers due to relational distances aspects ===\nArising from research in Risk Communication, the \"4 Distances Model\" (Acronym 4DM, originally by Daniele Trevisani, 1990) highlights the presence of \"relational distances\" in system-to-system or human-to-human communication, a distance whose effect is that of degrading progressively both understanding and agreement. The higher the relational distance, the more communication results become difficult to achieve in terms of effectiveness and expected output. The 4 Distances regard differences in (1) the \"Self's Distance\", acceptance or refusal of other's self-perception of roles (e.g. teacher-student); (2) Communication Codes (linguistic and non verbal) (3) underlying values and world views, and (d) personal experiences (both emotional and objectual). The approach has been applied in several fields including health professions, analysis of critical incidents due to communications misunderstanding in the International Space Station., and in \"Intelligent Decision Support System\" for leadership.\n\n\n== Nonhuman ==\n\nEvery information exchange between living organisms \u2014 i.e. transmission of signals that involve a living sender and receiver can be considered a form of communication; and even primitive creatures such as corals are competent to communicate. Nonhuman communication also include cell signaling, cellular communication, and chemical transmissions between primitive organisms like bacteria and within the plant and fungal kingdoms.\n\n\n=== Animals ===\nThe broad field of animal communication encompasses most of the issues in ethology. Animal communication can be defined as any behavior of one animal that affects the current or future behavior of another animal. The study of animal communication, called zoo semiotics (distinguishable from anthroposemiotics, the study of human communication) has played an important part in the development of ethology, sociobiology, and the study of animal cognition. Animal communication, and indeed the understanding of the animal world in general, is a rapidly growing field, and even in the 21st century so far, a great share of prior understanding related to diverse fields such as personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct, long thought to be well understood, has been revolutionized.\n\n\n=== Plants and fungi ===\nCommunication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. Recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores.\nFungi communicate to coordinate and organize their growth and development such as the formation of Marcelia and fruiting bodies. Fungi communicate with their own and related species as well as with non fungal organisms in a great variety of symbiotic interactions, especially with bacteria, unicellular eukaryote, plants and insects through biochemicals of biotic origin. The biochemicals trigger the fungal organism to react in a specific manner, while if the same chemical molecules are not part of biotic messages, they do not trigger the fungal organism to react. This implies that fungal organisms can differentiate between molecules taking part in biotic messages and similar molecules being irrelevant in the situation. So far five different primary signalling molecules are known to coordinate different behavioral patterns such as filamentation, mating, growth, and pathogenicity. Behavioral coordination and production of signaling substances is achieved through interpretation processes that enables the organism to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out \"noise\", i.e. similar molecules without biotic content.\n\n\n=== Bacteria quorum sensing ===\nCommunication is not a tool used only by humans, plants and animals, but it is also used by microorganisms like bacteria. The process is called quorum sensing. Through quorum sensing, bacteria are able to sense the density of cells, and regulate gene expression accordingly. This can be seen in both gram positive and gram negative bacteria.\nThis was first observed by Fuqua et al. in marine microorganisms like V. harveyi and V. fischeri.\n\n\n== Models ==\n\nThe first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise.\nIn a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emisor/ sender/ encoder to a destination/ receiver/ decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements:\n\nAn information source, which produces a message.\nA transmitter, which encodes the message into signals\nA channel, to which signals are adapted for transmission\nA noise source, which distorts the signal while it propagates through the channel\nA receiver, which 'decodes' (reconstructs) the message from the signal.\nA destination, where the message arrives.Shannon and Weaver argued that there were three levels of problems for communication within this theory.\n\nThe technical problem: how accurately can the message be transmitted?\nThe semantic problem: how precisely is the meaning 'conveyed'?\nThe effectiveness problem: how effectively does the received meaning affect behavior?Daniel Chandler critiques the transmission model by stating:\n\nIt assumes communicators are isolated individuals.\nNo allowance for differing purposes.\nNo allowance for differing interpretations.\nNo allowance for unequal power relations.\nNo allowance for situational contexts.In 1960, David Berlo expanded on Shannon and Weaver's (1949) linear model of communication and created the SMCR Model of Communication. The Sender-Message-Channel-Receiver Model of communication separated the model into clear parts and has been expanded upon by other scholars.\nCommunication is usually described along a few major dimensions: Message (what type of things are communicated), source / emisor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).\nCommunication can be seen as processes of information transmission with three levels of semiotic rules:\n\nPragmatic (concerned with the relations between signs/expressions and their users)\nSemantic (study of relationships between signs and symbols and what they represent) and\nSyntactic (formal properties of signs and symbols).Therefore, communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules. This commonly held rule in some sense ignores autocommunication, including intrapersonal communication via diaries or self-talk, both secondary phenomena that followed the primary acquisition of communicative competences within social interactions.\nIn light of these weaknesses, Barnlund (2008) proposed a transactional model of communication. The basic premise of the transactional model of communication is that individuals are simultaneously engaging in the sending and receiving of messages.\nIn a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of \"communication noise\" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.\nTheories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society. His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society.\n\n\n== Noise ==\nIn any communication model, noise is interference with the decoding of messages sent over a channel by an encoder. There are many examples of noise:\n\nEnvironmental noise. Noise that physically disrupts communication, such as standing next to loud speakers at a party, or the noise from a construction site next to a classroom making it difficult to hear the professor.\nPhysiological-impairment noise. Physical maladies that prevent effective communication, such as actual deafness or blindness preventing messages from being received as they were intended.\nSemantic noise. Different interpretations of the meanings of certain words. For example, the word \"weed\" can be interpreted as an undesirable plant in a yard, or as a euphemism for marijuana.\nSyntactical noise. Mistakes in grammar can disrupt communication, such as abrupt changes in verb tense during a sentence.\nOrganizational noise. Poorly structured communication can prevent the receiver from accurate interpretation. For example, unclear and badly stated directions can make the receiver even more lost.\nCultural noise. Stereotypical assumptions can cause misunderstandings, such as unintentionally offending a non-Christian person by wishing them a \"Merry Christmas\".\nPsychological noise. Certain attitudes can also make communication difficult. For instance, great anger or sadness may cause someone to lose focus on the present moment. Disorders such as autism may also severely hamper effective communication.To face communication noise, redundancy and acknowledgement must often be used. Acknowledgements are messages from the addressee informing the originator that his/her communication has been received and is understood. Message repetition and feedback about message received are necessary in the presence of noise to reduce the probability of misunderstanding.\nThe act of disambiguation regards the attempt of reducing noise and wrong interpretations, when the semantic value or meaning of a sign can be subject to noise, or in presence of multiple meanings, which makes the sense-making difficult. Disambiguation attempts to decrease the likelihood of misunderstanding. This is also a fundamental skill in communication processes activated by counselors, psychotherapists, interpreters, and in coaching sessions based on colloquium. In Information Technology, the disambiguation process and the automatic disambiguation of meanings of words and sentences has also been an interest and concern since the earliest days of computer treatment of language.\n\n\n== As academic discipline ==\n\nThe academic discipline that deals with processes of human communication is communication studies. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting. Communication studies also examines how messages are interpreted through the political, cultural, economic, semiotic, hermeneutic, and social dimensions of their contexts. Statistics, as a quantitative approach to communication science, has also been incorporated into research on communication science in order to help substantiate claims.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nInnis, Harold; Innis, Mary Q. (1975) [1950]. Empire and Communications. Foreword by Marshall McLuhan (Revised ed.). Toronto: University of Toronto Press. ISBN 0-8020-6119-2. OCLC 19403451. \n\n\n== External links ==\n Media related to Communication at Wikimedia Commons\n Quotations related to Communication at Wikiquote", "energy development": "Energy development is the field of activities focused on obtaining sources of energy from natural resources. These activities include production of conventional, alternative and renewable sources of energy, and for the recovery and reuse of energy that would otherwise be wasted. Energy conservation and efficiency measures reduce the demand for energy development, and can have benefits to society with improvements to environmental issues.\nSocieties use energy for transportation, manufacturing, illumination, heating and air conditioning, and communication, for industrial, commercial, and domestic purposes.  Energy resources may be classified as primary resources, where the resource can be used in substantially its original form, or as secondary resources, where the energy source must be converted into a more conveniently usable form. Non-renewable resources are significantly depleted by human use, whereas renewable resources are produced by ongoing processes that can sustain indefinite human exploitation.\nThousands of people are employed in the energy industry. The conventional industry comprises the petroleum industry, the natural gas industry, the electrical power industry, and the nuclear industry. New energy industries include the renewable energy industry, comprising alternative and sustainable manufacture, distribution, and sale of alternative fuels.\n\n\n== Classification of resources ==\n\nEnergy resources may be classified as primary resources, suitable for end use without conversion to another form, or secondary resources, where the usable form of energy required substantial conversion from a primary source.  Examples of primary energy resources are wind power, solar power, wood fuel, fossil fuels such as coal, oil and natural gas, and uranium. Secondary resources are those such as electricity, hydrogen, or other synthetic fuels.\nAnother important classification is based on the time required to regenerate an energy resource.  \"Renewable\" resources are those that recover their capacity in a time significant by human needs. Examples are hydroelectric power or wind power, when the natural phenomena that are the primary source of energy are ongoing and not depleted by human demands.  Non-renewable resources are those that are significantly depleted by human usage and that will not recover their potential significantly during human lifetimes. An example of a non-renewable energy source is coal, which does not form naturally at a rate that would support human use.\n\n\n== Fossil fuels ==\n\nFossil fuel (primary non-renewable fossil) sources burn coal or hydrocarbon fuels, which are the remains of the decomposition of plants and animals. There are three main types of fossil fuels: coal, petroleum, and natural gas. Another fossil fuel, liquefied petroleum gas (LPG), is principally derived from the production of natural gas. Heat from burning fossil fuel is used either directly for space heating and process heating, or converted to mechanical energy for vehicles, industrial processes, or electrical power generation. These fossil fuels are part of the carbon cycle and thus allow stored solar energy to be used today.\nThe use of fossil fuels in the 18th and 19th Century set the stage for the Industrial Revolution.\nFossil fuels make up the bulk of the world's current primary energy sources. In 2005, 81% of the world's energy needs was met from fossil sources. The technology and infrastructure already exist for the use of fossil fuels. Liquid fuels derived from petroleum deliver a great deal of usable energy per unit of weight or volume, which is advantageous when compared with lower energy density sources such as a battery. Fossil fuels are currently economical for decentralised energy use.\n\nEnergy dependence on imported fossil fuels creates energy security risks for dependent countries. Oil dependence in particular has led to war, funding of radicals, monopolization, and socio-political instability.Fossil fuels are non-renewable resources, which will eventually decline in production  and become exhausted.  While the processes that created fossil fuels are ongoing, fuels are consumed far more quickly than the natural rate of replenishment.  Extracting fuels becomes increasingly costly as society consumes the most accessible fuel deposits. Extraction of fossil fuels results in environmental degradation, such as the strip mining and mountaintop removal of coal.\nFuel efficiency is a form of thermal efficiency, meaning the efficiency of a process that converts chemical potential energy contained in a carrier fuel into kinetic energy or work. The fuel economy is the energy efficiency of a particular vehicle, is given as a ratio of distance travelled per unit of fuel consumed. Weight-specific efficiency (efficiency per unit weight) may be stated for freight, and passenger-specific efficiency (vehicle efficiency per passenger). The inefficient atmospheric combustion (burning) of fossil fuels in vehicles, buildings, and power plants contributes to urban heat islands.Conventional production of oil has peaked, conservatively, between 2007 and 2010. In 2010, it was estimated that an investment in non-renewable resources of $8 trillion would be required to maintain current levels of production for 25 years. In 2010, governments subsidized fossil fuels by an estimated $500 billion a year.  Fossil fuels are also a source of greenhouse gas emissions, leading to concerns about global warming if consumption is not reduced.\nThe combustion of fossil fuels leads to the release of pollution into the atmosphere. The fossil fuels are mainly carbon compounds.  During combustion, carbon dioxide is released, and also nitrogen oxides, soot and other fine particulates. Man-made carbon dioxide according to the IPCC contributes to global warming.\nOther emissions from fossil fuel power station include sulfur dioxide, carbon monoxide (CO), hydrocarbons, volatile organic compounds (VOC), mercury, arsenic, lead, cadmium, and other heavy metals including traces of uranium.A typical coal plant generates billions of kilowatt hours per year.\n\n\n== Nuclear ==\n\n\n=== Fission ===\n\nNuclear power is the use of nuclear fission  to generate useful heat and electricity. Fission of uranium produces nearly all economically significant nuclear power. Radioisotope thermoelectric generators form a very small component of energy generation, mostly in specialized applications such as deep space vehicles.\nNuclear power plants, excluding naval reactors, provided about 5.7% of the world's energy and 13% of the world's electricity in 2012.In 2013, the IAEA report that there are 437 operational nuclear power reactors, in 31 countries,  although not every reactor is producing electricity. In addition, there are approximately 140 naval vessels using nuclear propulsion in operation, powered by some 180 reactors. As of 2013, attaining a net energy gain from sustained nuclear fusion reactions, excluding natural fusion power sources such as the Sun, remains an ongoing area of international physics and engineering research. More than 60 years after the first attempts, commercial fusion power production remains unlikely before 2050.There is an ongoing debate about nuclear power.  Proponents, such as the World Nuclear Association, the IAEA and Environmentalists for Nuclear Energy contend that nuclear power is a safe, sustainable energy source that reduces carbon emissions. Opponents, such as Greenpeace International and NIRS, contend that nuclear power poses many threats to people and the environment.Nuclear power plant accidents include the Chernobyl disaster (1986), Fukushima Daiichi nuclear disaster (2011), and the Three Mile Island accident (1979). There have also been some nuclear submarine accidents. In terms of lives lost per unit of energy generated, analysis has determined that nuclear power has caused less fatalities per unit of energy generated than the other major sources of energy generation. Energy production from coal, petroleum, natural gas and hydropower has caused a greater number of fatalities per unit of energy generated due to air pollution and energy accident effects. However, the economic costs of nuclear power accidents is high, and meltdowns can take decades to clean up. The human costs of evacuations of affected populations and lost livelihoods is also significant.Comparing Nuclear's latent cancer deaths, such as cancer with other energy sources immediate deaths per unit of energy generated(GWeyr). This study does not include fossil fuel related cancer and other indirect deaths created by the use of fossil fuel consumption in its \"severe accident\" classification, which would be an accident with more than 5 fatalities.\nNuclear power is a low carbon power generation method of producing electricity, with an analysis of the literature on its total life cycle emission intensity finding that it is similar to renewable sources in a comparison of greenhouse gas(GHG) emissions per unit of energy generated. Since the 1970s, nuclear fuel has displaced about 64 gigatonnes of carbon dioxide equivalent(GtCO2-eq) greenhouse gases, that would have otherwise resulted from the burning of oil, coal or natural gas in fossil-fuel power stations.As of 2012, according to the IAEA, worldwide there were 68 civil nuclear power reactors under construction in 15 countries, approximately 28 of which in the People's Republic of China (PRC), with the most recent nuclear power reactor, as of May 2013, to be connected to the electrical grid, occurring on February 17, 2013 in Hongyanhe Nuclear Power Plant in the PRC. In the United States, two new Generation III reactors are under construction at Vogtle. U.S. nuclear industry officials expect five new reactors to enter service by 2020, all at existing plants. In 2013, four aging, uncompetitive, reactors were permanently closed.Japan's 2011 Fukushima Daiichi nuclear accident, which occurred in a reactor design from the 1960s, prompted a rethink of nuclear safety and nuclear energy policy in many countries. Germany decided to close all its reactors by 2022, and Italy has banned nuclear power. Following Fukushima, in 2011 the International Energy Agency halved its estimate of additional nuclear generating capacity to be built by 2035.Recent experiments in extraction of uranium use polymer ropes that are coated with a substance that selectively absorbs uranium from seawater. This process could make the considerable volume of uranium dissolved in seawater exploitable for energy production.  Since ongoing geologic processes carry uranium to the sea in amounts comparable to the amount that would be extracted by this process, in a sense the sea-borne uranium becomes a sustainable resource.\n\n\n=== Fission economics ===\n\nThe economics of new nuclear power plants is a controversial subject, since there are diverging views on this topic, and multibillion-dollar investments ride on the choice of an energy source. Nuclear power plants typically have high capital costs for building the plant, but low direct fuel costs.\nIn recent years there has been a slowdown of electricity demand growth and financing has become more difficult, which affects large projects such as nuclear reactors, with very large upfront costs and long project cycles which carry a large variety of risks. In Eastern Europe, a number of long-established projects are struggling to find finance, notably Belene in Bulgaria and the additional reactors at Cernavoda in Romania, and some potential backers have pulled out. Where cheap gas is available and its future supply relatively secure, this also poses a major problem for nuclear projects.\n\nAnalysis of the economics of nuclear power must take into account who bears the risks of future uncertainties. To date all operating nuclear power plants were developed by state-owned or regulated utility monopolies where many of the risks associated with construction costs, operating performance, fuel price, and other factors were borne by consumers rather than suppliers. Many countries have now liberalized the electricity market where these risks, and the risk of cheaper competitors emerging before capital costs are recovered, are borne by plant suppliers and operators rather than consumers, which leads to a significantly different evaluation of the economics of new nuclear power plants.Two of the four EPRs under construction (in Finland and France) are significantly behind schedule and substantially over cost. Following the 2011 Fukushima Daiichi nuclear disaster, costs are likely to go up for currently operating and new nuclear power plants, due to increased requirements for on-site spent fuel management and elevated design basis threats. While first of their kind designs, such as the EPRs under construction are behind schedule and over-budget, of the seven South Korean APR-1400s presently under construction worldwide, two are in S.Korea at the Hanul Nuclear Power Plant and four are at the largest nuclear station construction project in the world as of 2016, in the United Arab Emirates at the planned Barakah nuclear power plant. The first reactor, Barakah-1 is 85% completed and on schedule for grid-connection during 2017.\n\n\n== Renewable sources ==\n\nRenewable energy is generally defined as energy that comes from resources which are naturally replenished on a human timescale such as sunlight, wind, rain, tides, waves and geothermal heat. Renewable energy replaces conventional fuels in four distinct areas: electricity generation, hot water/space heating, motor fuels, and rural (off-grid) energy services.About 16% of global final energy consumption presently comes from renewable resources, with 10%  of all energy from traditional biomass, mainly used for heating, and 3.4% from hydroelectricity. New renewables (small hydro, modern biomass, wind, solar, geothermal, and biofuels) account for another 3% and are growing rapidly. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond. Wind power, for example, is growing at the rate of 30% annually, with a worldwide installed capacity of 282,482 megawatts (MW) at the end of 2012.\nRenewable energy resources exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency is resulting in significant energy security, climate change mitigation, and economic benefits. In international public opinion surveys there is strong support for promoting renewable sources such as solar power and wind power.While many renewable energy projects are large-scale, renewable technologies are also suited to rural and remote areas and developing countries, where energy is often crucial in human development. United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity.\n\n\n=== Hydroelectricity ===\n\nHydroelectricity  is electric power generated by hydropower; the force of falling or flowing water. In 2015 hydropower generated 16.6% of the world's total electricity and 70% of all renewable electricity  and is expected to increase about 3.1% each year for the next 25 years.\nHydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. There are now three hydroelectricity plants larger than 10 GW: the Three Gorges Dam in China, Itaipu Dam across the Brazil/Paraguay border, and Guri Dam in Venezuela.The cost of hydroelectricity is relatively low, making it a competitive source of renewable electricity. The average cost of electricity from a hydro plant larger than 10 megawatts is 3 to 5 U.S. cents per kilowatt-hour. Hydro is also a flexible source of electricity since plants can be ramped up and down very quickly to adapt to changing energy demands. However, damming interrupts the flow of rivers and can harm local ecosystems, and building large dams and reservoirs often involves displacing people and wildlife. Once a hydroelectric complex is constructed, the project produces no direct waste, and has a considerably lower output level of the greenhouse gas carbon dioxide than fossil fuel powered energy plants.\n\n\n=== Wind ===\n\nWind power harnesses the power of the wind to propel the blades of wind turbines. These turbines cause the rotation of magnets, which creates electricity. Wind towers are usually built together on wind farms. There are offshore and onshore wind farms. Global wind power capacity has expanded rapidly to 336 GW in June 2014, and wind energy production was around 4% of total worldwide electricity usage, and growing rapidly.Wind power is widely used in Europe, Asia, and the United States. Several countries have achieved relatively high levels of wind power penetration, such as 21% of stationary electricity production in Denmark, 18% in Portugal, 16% in Spain, 14% in Ireland, and 9% in Germany in 2010. By 2011, at times over 50% of electricity in Germany and Spain came from wind and solar power. As of 2011, 83 countries around the world are using wind power on a commercial basis.Many of the world's largest onshore wind farms are located in the United States, China, and India. Most of the world's largest offshore wind farms are located in Denmark, Germany and the United Kingdom. The two largest offshore wind farm are currently the 630 MW London Array and Gwynt y M\u00f4r.\n\n\n=== Solar ===\n\nSolar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, solar photovoltaics, solar thermal electricity, solar architecture and artificial photosynthesis.Solar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert and distribute solar energy. Active solar techniques include the use of photovoltaic panels and solar thermal collectors to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air.\nIn 2011, the International Energy Agency said that \"the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries\u2019 energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared\". More than 100 countries use solar PV.\n\nPhotovoltaics (PV) is a method of generating electrical power by converting solar radiation into direct current electricity using semiconductors that exhibit the photovoltaic effect. Photovoltaic power generation employs solar panels composed of a number of solar cells containing a photovoltaic material. Materials presently used for photovoltaics include monocrystalline silicon, polycrystalline silicon, amorphous silicon, cadmium telluride, and copper indium gallium selenide/sulfide. Due to the increased demand for renewable energy sources, the manufacturing of solar cells and photovoltaic arrays has advanced considerably in recent years.\nSolar photovoltaics is a sustainable energy source. By the end of 2011, a total of 71.1 GW had been installed, sufficient to generate 85 TWh/year. And by end of 2012, the 100 GW installed capacity milestone was achieved. Solar photovoltaics is now, after hydro and wind power, the third most important renewable energy source in terms of globally installed capacity. In 2016, after another year of rapid growth, solar generated 1.3% of global power.Driven by advances in technology and increases in manufacturing scale and sophistication, the cost of photovoltaics has declined steadily since the first solar cells were manufactured, and the levelised cost of electricity (LCOE) from PV is competitive with conventional electricity sources in an expanding list of geographic regions. Net metering and financial incentives, such as preferential feed-in tariffs for solar-generated electricity, have supported solar PV installations in many countries. The Energy Payback Time (EPBT), also known as energy amortization, depends on the location's annual solar insolation and temperature profile, as well as on the used type of PV-technology. For conventional crystalline silicon photovoltaics, the EPBT is higher than for thin-film technologies such as CdTe-PV or CPV-systems. Moreover, the payback time decreased in the recent years due to a number of improvements such as solar cell efficiency and more economic manufacturing processes. As of 2014, photovoltaics recoup on average the energy needed to manufacture them in 0.7 to 2 years. This results in about 95% of net-clean energy produced by a solar rooftop PV system over a 30-year life-time. Installations may be ground-mounted (and sometimes integrated with farming and grazing) or built into the roof or walls of a building (either building-integrated photovoltaics or simply rooftop).\n\n\n=== Biofuels ===\n\nA biofuel is a fuel that contains energy from geologically recent carbon fixation. These fuels are produced from living organisms. Examples of this carbon fixation occur in plants and microalgae. These fuels are made by a biomass conversion (biomass refers to recently living organisms, most often referring to plants or plant-derived materials). This biomass can be converted to convenient energy containing substances in three different ways: thermal conversion, chemical conversion, and biochemical conversion. This biomass conversion can result in fuel in solid, liquid, or gas form. This new biomass can be used for biofuels. Biofuels have increased in popularity because of rising oil prices and the need for energy security.\nBioethanol is an alcohol made by fermentation, mostly from carbohydrates produced in sugar or starch crops such as corn or sugarcane. Cellulosic biomass, derived from non-food sources, such as trees and grasses, is also being developed as a feedstock for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the USA and in Brazil. Current plant design does not provide for converting the lignin portion of plant raw materials to fuel components by fermentation.\nBiodiesel is made from vegetable oils and animal fats. Biodiesel can be used as a fuel for vehicles in its pure form, but it is usually used as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. However, research is underway on producing renewable fuels from decarboxylationIn 2010, worldwide biofuel production reached 105 billion liters (28 billion gallons US), up 17% from 2009, and biofuels provided 2.7% of the world's fuels for road transport, a contribution largely made up of ethanol and biodiesel. Global ethanol fuel production reached 86 billion liters (23 billion gallons US) in 2010, with the United States and Brazil as the world's top producers, accounting together for 90% of global production. The world's largest biodiesel producer is the European Union, accounting for 53% of all biodiesel production in 2010. As of 2011, mandates for blending biofuels exist in 31 countries at the national level and in 29 states or provinces. The International Energy Agency has a goal for biofuels to meet more than a quarter of world demand for transportation fuels by 2050 to reduce dependence on petroleum and coal.\n\n\n=== Geothermal ===\n\nGeothermal energy is thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. The geothermal energy of the Earth's crust originates from the original formation of the planet (20%) and from radioactive decay of minerals (80%).  The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective geothermal originates from the Greek roots \u03b3\u03b7 (ge), meaning earth, and \u03b8\u03b5\u03c1\u03bc\u03bf\u03c2 (thermos), meaning hot.\nEarth's internal heat is thermal energy generated from radioactive decay and continual heat loss from Earth's formation. Temperatures at the core-mantle boundary may reach over 4000 \u00b0C (7,200 \u00b0F). The high temperature and pressure in Earth's interior cause some rock to melt and solid mantle to behave plastically, resulting in portions of mantle convecting upward since it is lighter than the surrounding rock.  Rock and water is heated in the crust, sometimes up to 370 \u00b0C (700 \u00b0F).From hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation. Worldwide, 11,400 megawatts (MW) of geothermal power is online in 24 countries in 2012.  An additional 28 gigawatts of direct geothermal heating capacity is installed for district heating, space heating, spas, industrial processes, desalination and agricultural applications in 2010.Geothermal power is cost effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have dramatically expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.\nThe Earth's geothermal resources are theoretically more than adequate to supply humanity's energy needs, but only a very small fraction may be profitably exploited. Drilling and exploration for deep resources is very expensive. Forecasts for the future of geothermal power depend on assumptions about technology, energy prices, subsidies, and interest rates. Pilot programs like EWEB's customer opt in Green Power Program  show that customers would be willing to pay a little more for a renewable energy source like geothermal. But as a result of government assisted research and industry experience, the cost of generating geothermal power has decreased by 25% over the past two decades. In 2001, geothermal energy cost between two and ten US cents per kWh.\n\n\n=== Oceanic ===\n\nMarine energy or marine power (also sometimes referred to as ocean energy, ocean power, or marine and hydrokinetic energy) refers to the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the world's oceans creates a vast store of kinetic energy, or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries.\nThe term marine energy encompasses both wave power i.e. power from surface waves, and tidal power i.e. obtained from the kinetic energy of large bodies of moving water. Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water.\nThe oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.\n\n\n=== 100% renewable energy ===\n\nThe incentive to use 100% renewable energy, for electricity, transport, or even total primary energy supply globally, has been motivated by global warming and other ecological as well as economic concerns. Renewable energy use has grown much faster than anyone anticipated. The Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply.  Also, Professors S. Pacala and Robert H. Socolow have developed a series of \"stabilization wedges\" that can allow us to maintain our quality of life while avoiding catastrophic climate change, and \"renewable energy sources,\" in aggregate, constitute the largest number of their \"wedges.\" Mark Z. Jacobson says producing all new energy with wind power, solar power,  and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". Jacobson says that energy costs with a wind, solar, water system should be similar to today's energy costs.Similarly, in the United States, the independent National Research Council has noted that \"sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs \u2026 Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand.\" .Critics of the \"100% renewable energy\" approach include Vaclav Smil and James E. Hansen. Smil and Hansen are concerned about the variable output of solar and wind power, but Amory Lovins argues that the electricity grid can cope, just as it routinely backs up nonworking coal-fired and nuclear plants with working ones.Google spent $30 million on their RE<C project to develop renewable energy and stave off catastrophic climate change. The project was cancelled after concluding that a best-case scenario for rapid advances in renewable energy could only result in emissions 55 percent below the fossil fuel projections for 2050.\n\n\n== Increased energy efficiency ==\n\nAlthough increasing the efficiency of energy use is not energy development per se, it may be considered under the topic of energy development since it makes existing energy sources available to do work.Efficient energy use reduces the amount of energy required to provide products and services. For example, insulating a home allows a building to use less heating and cooling energy to maintain a comfortable temperature. Installing fluorescent lamps or natural skylights reduces the amount of energy required for illumination compared to incandescent light bulbs. Compact fluorescent lights use two-thirds less energy and may last 6 to 10 times longer than incandescent lights.  Improvements in energy efficiency are most often achieved by adopting an efficient technology or production process.Reducing energy use may save consumers money,  if the energy savings offsets the cost of an energy efficient technology.  Reducing energy use reduces emissions.  According to the International Energy Agency, improved energy efficiency in buildings, industrial processes and transportation could reduce the world's energy needs in 2050 by one third, and help control global emissions of greenhouse gases.Energy efficiency and renewable energy are said to be the twin pillars of sustainable energy policy. In many countries energy efficiency is also seen to have a national security benefit because it can be used to reduce the level of energy imports from foreign countries and may slow down the rate at which domestic energy resources are depleted.\nIt's been discovered \"that for OECD countries, wind, geothermal, hydro and nuclear have the lowest hazard rates among energy sources in production\".\n\n\n== Transmission ==\n\nWhile new sources of energy are only rarely discovered or made possible by new technology, distribution technology continually evolves. The use of fuel cells in cars, for example, is an anticipated delivery technology. This section presents the various delivery technologies that have been important to historic energy development. They all rely in way on the energy sources listed in the previous section.\n\n\n=== Shipping and pipelines ===\n\nCoal, petroleum and their derivatives are delivered by boat, rail, or road. Petroleum and natural gas may also be delivered by pipeline, and coal via a Slurry pipeline. Fuels such as gasoline and LPG may also be delivered via aircraft. Natural gas pipelines must maintain a certain minimum pressure to function correctly. The higher costs of ethanol transportation and storage are often prohibitive.\n\n\n=== Wired energy transfer ===\n\nElectricity grids are the networks used to transmit and distribute power from production source to end user, when the two may be hundreds of kilometres away.  Sources include electrical generation plants such as a nuclear reactor, coal burning power plant, etc.  A combination of sub-stations and transmission lines are used to maintain a constant flow of electricity. Grids may suffer from transient blackouts and brownouts, often due to weather damage. During certain extreme space weather events solar wind can interfere with transmissions. Grids also have a predefined carrying capacity or load that cannot safely be exceeded. When power requirements exceed what's available, failures are inevitable. To prevent problems, power is then rationed.\nIndustrialised countries such as Canada, the US, and Australia are among the highest per capita consumers of electricity in the world, which is possible thanks to a widespread electrical distribution network. The US grid is one of the most advanced, although infrastructure maintenance is becoming a problem. CurrentEnergy provides a realtime overview of the electricity supply and demand for California, Texas, and the Northeast of the US. African countries with small scale electrical grids have a correspondingly low annual per capita usage of electricity. One of the most powerful power grids in the world supplies power to the state of Queensland, Australia.\n\n\n=== Wireless energy transfer ===\nWireless power transfer is a process whereby electrical energy is transmitted from a power source to an electrical load that does not have a built-in power source, without the use of interconnecting wires. Currently available technology is limited to short distances and relatively low power level.\nOrbiting solar power collectors would require wireless transmission of power to Earth. The proposed method involves creating a large beam of microwave-frequency radio waves, which would be aimed at a collector antenna site on the Earth. Formidable technical challenges exist to ensure the safety and profitability of such a scheme.\n\n\n== Storage ==\n\nEnergy storage is accomplished by devices or physical media that store energy to perform useful operation at a later time. A device that stores energy is sometimes called an accumulator.\nAll forms of energy are either potential energy (e.g. Chemical, gravitational, electrical energy, temperature differential, latent heat, etc.) or kinetic energy (e.g. momentum). Some technologies provide only short-term energy storage, and others can be very long-term such as power to gas using hydrogen or methane and the storage of heat or cold between opposing seasons in deep aquifers or bedrock.  A wind-up clock stores potential energy (in this case mechanical, in the spring tension), a battery stores readily convertible chemical energy to operate a mobile phone, and a hydroelectric dam stores energy in a reservoir as gravitational potential energy. Ice storage tanks store ice (thermal energy in the form of latent heat) at night to meet peak demand for cooling. Fossil fuels such as coal and gasoline store ancient energy derived from sunlight by organisms that later died, became buried and over time were then converted into these fuels. Even food (which is made by the same process as fossil fuels) is a form of energy stored in chemical form.\n\n\n== History ==\n\nSince prehistory, when humanity discovered fire to warm up and roast food, through the Middle Ages in which populations built windmills to grind the wheat, until the modern era in which nations can get electricity splitting the atom. Man has sought endlessly for energy sources.\nExcept nuclear, geothermal and tidal, all other energy sources are from current solar isolation or from fossil remains of plant and animal life that relied upon sunlight. Ultimately, solar energy itself is the result of the Sun's nuclear fusion. Geothermal power from hot, hardened rock above the magma of the Earth's core is the result of the decay of radioactive materials present beneath the Earth's crust, and nuclear fission relies on man-made fission of heavy radioactive elements in the Earth's crust; in both cases these elements were produced in supernova explosions before the formation of the solar system.\nSince the beginning of the Industrial Revolution, the question of the future of energy supplies has been of interest. In 1865, William Stanley Jevons published The Coal Question in which he saw that the reserves of coal were being depleted and that oil was an ineffective replacement. In 1914, U.S. Bureau of Mines stated that the total production was 5.7 billion barrels (910,000,000 m3). In 1956, Geophysicist M. King Hubbert deduces that U.S. oil production would peak between 1965 and 1970 and that oil production will peak \"within half a century\" on the basis of 1956 data. In 1989, predicted peak by Colin Campbell In 2004, OPEC estimated, with substantial investments, it would nearly double oil output by 2025\n\n\n=== Sustainability ===\n\nThe environmental movement has emphasized sustainability of energy use and development. Renewable energy is sustainable in its production; the available supply will not be diminished for the foreseeable future - millions or billions of years.  \"Sustainability\" also refers to the ability of the environment to cope with waste products, especially air pollution.  Sources which have no direct waste products (such as wind, solar, and hydropower) are brought up on this point. With global demand for energy growing, the need to adopt various energy sources is growing. Energy conservation is an alternative or complementary process to energy development.  It reduces the demand for energy by using it efficiently.\n\n\n=== Resilience ===\n\nSome observers contend that idea of \"energy independence\" is an unrealistic and opaque concept. The alternative offer of \"energy resilience\" is a goal aligned with economic, security, and energy realities. The notion of resilience in energy was detailed in the 1982 book Brittle Power: Energy Strategy for National Security. The authors argued that simply switching to domestic energy would not be secure inherently because the true weakness is the interdependent and vulnerable energy infrastructure of the United States. Key aspects such as gas lines and the electrical power grid are centralized and easily susceptible to disruption. They conclude that a \"resilient energy supply\" is necessary for both national security and the environment. They recommend a focus on energy efficiency and renewable energy that is decentralized.In 2008, former Intel Corporation Chairman and CEO Andrew Grove looked to energy resilience, arguing that complete independence is unfeasible given the global market for energy. He describes energy resilience as the ability to adjust to interruptions in the supply of energy. To that end, he suggests the U.S. make greater use of electricity. Electricity can be produced from a variety of sources. A diverse energy supply will be less affected by the disruption in supply of any one source. He reasons that another feature of electrification is that electricity is \"sticky\" \u2013 meaning the electricity produced in the U.S. is to stay there because it cannot be transported overseas.  According to Grove, a key aspect of advancing electrification and energy resilience will be converting the U.S. automotive fleet from gasoline-powered to electric-powered. This, in turn, will require the modernization and expansion of the electrical power grid. As organizations such as The Reform Institute have pointed out, advancements associated with the developing smart grid would facilitate the ability of the grid to absorb vehicles en masse connecting to it to charge their batteries.\n\n\n=== Present and future ===\n\nExtrapolations from current knowledge to the future offer a choice of energy futures. Predictions parallel the Malthusian catastrophe hypothesis. Numerous are complex models based scenarios as pioneered by Limits to Growth. Modeling approaches offer ways to analyze diverse strategies, and hopefully find a road to rapid and sustainable development of humanity. Short term energy crises are also a concern of energy development. Extrapolations lack plausibility, particularly when they predict a continual increase in oil consumption.Energy production usually requires an energy investment. Drilling for oil or building a wind power plant requires energy. The fossil fuel resources that are left are often increasingly difficult to extract and convert. They may thus require increasingly higher energy investments. If investment is greater than the value of the energy produced by the resource, it is no longer an effective energy source.  These resources are no longer an energy source but may be exploited for value as raw materials.  New technology may lower the energy investment required to extract and convert the resources, although ultimately basic physics sets limits that cannot be exceeded.\nBetween 1950 and 1984, as the Green Revolution transformed agriculture around the globe, world grain production increased by 250%. The energy for the Green Revolution was provided by fossil fuels in the form of fertilizers (natural gas), pesticides (oil), and hydrocarbon fueled irrigation. The peaking of world hydrocarbon production (peak oil) may lead to significant changes, and require sustainable methods of production. One vision of a sustainable energy future involves all human structures on the earth's surface (i.e., buildings, vehicles and roads) doing artificial photosynthesis (using sunlight to split water as a source of hydrogen and absorbing carbon dioxide to make fertilizer) efficiently than plants.With contemporary space industry's economic activity and the related private spaceflight, with the manufacturing industries, that go into Earth's orbit or beyond, delivering them to those regions will require further energy development. Researchers have contemplated space-based solar power for collecting solar power for use on Earth. Space-based solar power has been in research since the early 1970s. Space-based solar power would require construction of collector structures in space. The advantage over ground-based solar power is higher intensity of light, and no weather to interrupt power collection.\n\n\n== See also ==\n\nPolicy\nEnergy policy, Energy policy of the United States, Energy policy of China, Energy policy of India, Energy policy of the European Union, Energy policy of the United Kingdom, Energy policy of Russia, Energy policy of Brazil, Energy policy of Canada, Energy policy of the Soviet Union, Energy Industry Liberalization and Privatization (Thailand)General\nSeasonal thermal energy storage (Interseasonal thermal energy storage), Geomagnetically induced current, Energy harvestingFeedstock\nRaw material, Biomaterial, Commodity, Materials science, Recycling, Upcycling, DowncyclingOther\nThorium-based nuclear power, List of oil pipelines, List of natural gas pipelines, Ocean thermal energy conversion, Growth of photovoltaics\n\n\n== References and citations ==\nNotes\nCitations\n\n\n== Sources ==\nArmstrong, Robert C., Catherine Wolfram, Robert Gross, Nathan S. Lewis, and M.V. Ramana et al. The Frontiers of Energy, Nature Energy, Vol 1, 11 January 2016.\nSerra, J. \"Alternative Fuel Resource Development\", Clean and Green Fuels Fund, (2006).\nBilgen, S. and K. Kaygusuz, Renewable Energy for a Clean and Sustainable Future, Energy Sources 26, 1119 (2004).\nEnergy analysis of Power Systems, UIC Nuclear Issues Briefing Paper 57 (2004).\nSilvestre, B. S., Dalcol, P. R. T. Geographical proximity and innovation: Evidences from the Campos Basin oil & gas industrial agglomeration \u2014 Brazil. Technovation (2009), doi:10.1016/j.technovation.2009.01.003\n\n\n== Journals ==\nEnergy Sources, Part A: Recovery, Utilization and Environmental Effects\nEnergy Sources, Part B: Economics, Planning and Policy\nInternational Journal of Green Energy\n\n\n== External links ==\nBureau of Land Management 2012 Renewable Energy Priority Projects\nEnergypedia - a wiki about renewable energies in the context of development cooperation\nHidden Health and Environmental Costs Of Energy Production and Consumption In U.S. \nRECaBS REcalculator Interactive Renewable Energy Calculator \u2014 compare renewable energy to conventional energy sources\nIEA-ECES - International Energy Agency - Energy Conservation through Energy Conservation programme.\nIEA-SHC - International Energy Agency - Solar Heating and Cooling programme.\nSDH - Solar District Heating Platform. (European Union)", "nanotechnology": "Nanotechnology (\"nanotech\") is  manipulation of matter on an atomic, molecular, and supramolecular scale. The earliest, widespread description of nanotechnology  referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology. A more generalized description of nanotechnology was subsequently established by the National Nanotechnology Initiative, which defines nanotechnology as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers. This definition reflects the fact that quantum mechanical effects are important at this quantum-realm scale, and so the definition shifted from a particular technological goal to a research category inclusive of all types of research and technologies that deal with the special properties of matter which occur below the given size threshold. It is therefore common to see the plural form \"nanotechnologies\" as well as \"nanoscale technologies\" to refer to the broad range of research and applications whose common trait is size. Because of the variety of potential applications (including industrial and military), governments have invested billions of dollars in nanotechnology research. Through 2012, the USA has invested $3.7 billion using its National Nanotechnology Initiative, the European Union has invested $1.2 billion, and Japan has invested $750 million.Nanotechnology as defined by size is naturally very broad, including fields of science as diverse as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage, microfabrication, molecular engineering, etc.  The associated research and applications are equally diverse, ranging from extensions of conventional device physics to completely new approaches based upon molecular self-assembly, from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale.\nScientists currently debate the future implications of nanotechnology. Nanotechnology may be able to create many new materials and devices with a vast range of applications, such as in nanomedicine, nanoelectronics, biomaterials energy production, and consumer products. On the other hand, nanotechnology raises many of the same issues as any new technology, including concerns about the toxicity and environmental impact of nanomaterials, and their potential effects on global economics, as well as speculation about various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.\n\n\n== Origins ==\n\nThe concepts that seeded nanotechnology were first discussed in 1959 by renowned physicist Richard Feynman in his talk There's Plenty of Room at the Bottom, in which he described the possibility of synthesis via direct manipulation of atoms. The term \"nano-technology\" was first used by Norio Taniguchi in 1974, though it was not widely known.\n\nInspired by Feynman's concepts, K. Eric Drexler used the term \"nanotechnology\" in his 1986 book Engines of Creation: The Coming Era of Nanotechnology, which proposed the idea of a nanoscale \"assembler\" which would be able to build a copy of itself and of other items of arbitrary complexity with atomic control. Also in 1986, Drexler co-founded The Foresight Institute (with which he is no longer affiliated) to help increase public awareness and understanding of nanotechnology concepts and implications.\nThus, emergence of nanotechnology as a field in the 1980s occurred through convergence of Drexler's theoretical and public work, which developed and popularized a conceptual framework for nanotechnology, and high-visibility experimental advances that drew additional wide-scale attention to the prospects of atomic control of matter. In the 1980s, two major breakthroughs sparked the growth of nanotechnology in modern era.\nFirst, the invention of the scanning tunneling microscope in 1981 which provided unprecedented visualization of individual atoms and bonds, and was successfully used to manipulate individual atoms in 1989. The microscope's developers Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory received a Nobel Prize in Physics in 1986. Binnig, Quate and Gerber also invented the analogous atomic force microscope that year.\n\nSecond, Fullerenes were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry. C60 was not initially described as nanotechnology; the term was used regarding subsequent work with related graphene tubes (called carbon nanotubes and sometimes called Bucky tubes) which suggested potential applications for nanoscale electronics and devices.\nIn the early 2000s, the field garnered increased scientific, political, and commercial attention that led to both controversy and progress. Controversies emerged regarding the definitions and potential implications of nanotechnologies, exemplified by the Royal Society's report on nanotechnology. Challenges were raised regarding the feasibility of applications envisioned by advocates of molecular nanotechnology, which culminated in a public debate between Drexler and Smalley in 2001 and 2003.Meanwhile, commercialization of products based on advancements in nanoscale technologies began emerging. These products are limited to bulk applications of nanomaterials and do not involve atomic control of matter. Some examples include the Silver Nano platform for using silver nanoparticles as an antibacterial agent, nanoparticle-based transparent sunscreens, carbon fiber strengthening using silica nanoparticles, and carbon nanotubes for stain-resistant textiles.Governments moved to promote and fund research into nanotechnology, such as in the U.S. with the National Nanotechnology Initiative, which formalized a size-based definition of nanotechnology and established funding for research on the nanoscale, and in Europe via the European Framework Programmes for Research and Technological Development.\nBy the mid-2000s new and serious scientific attention began to flourish.  Projects emerged to produce nanotechnology roadmaps  which center on atomically precise manipulation of matter and discuss existing and projected capabilities, goals, and applications.\n\n\n== Fundamental concepts ==\nNanotechnology is the engineering of functional systems at the molecular scale. This covers both current work and concepts that are more advanced. In its original sense, nanotechnology refers to the projected ability to construct items from the bottom up, using techniques and tools being developed today to make complete, high performance products.\nOne nanometer (nm) is one billionth, or 10\u22129, of a meter. By comparison, typical carbon-carbon bond lengths, or the spacing between these atoms in a molecule, are in the range 0.12\u20130.15 nm, and a DNA double-helix has a diameter around 2 nm. On the other hand, the smallest cellular life-forms, the bacteria of the genus Mycoplasma, are around 200 nm in length. By convention, nanotechnology is taken as the scale range 1 to 100 nm following the definition used by the National Nanotechnology Initiative in the US. The lower limit is set by the size of atoms (hydrogen has the smallest atoms, which are approximately a quarter of a nm kinetic diameter) since nanotechnology must build its devices from atoms and molecules. The upper limit is more or less arbitrary but is around the size below which  phenomena not observed in larger structures start to become apparent and can be made use of in the nano device. These new phenomena make nanotechnology distinct from devices which are merely miniaturised versions of an equivalent macroscopic device; such devices are on a larger scale and come under the description of microtechnology.To put that scale in another context, the comparative size of a nanometer to a meter is the same as that of a marble to the size of the earth. Or another way of putting it: a nanometer is the amount an average man's beard grows in the time it takes him to raise the razor to his face.Two main approaches are used in nanotechnology. In the \"bottom-up\" approach, materials and devices are built from molecular components which assemble themselves chemically by principles of molecular recognition. In the \"top-down\" approach, nano-objects are constructed from larger entities without atomic-level control.Areas of physics such as nanoelectronics, nanomechanics, nanophotonics and nanoionics have evolved during the last few decades to provide a basic scientific foundation of nanotechnology.\n\n\n=== Larger to smaller: a materials perspective ===\n\nSeveral phenomena become pronounced as the size of the system decreases. These include statistical mechanical effects, as well as quantum mechanical effects, for example the \"quantum size effect\" where the electronic properties of solids are altered with great reductions in particle size. This effect does not come into play by going from macro to micro dimensions. However, quantum effects can become significant when the nanometer size range is reached, typically at distances of 100 nanometers or less, the so-called quantum realm. Additionally, a number of physical (mechanical, electrical, optical, etc.) properties change when compared to macroscopic systems. One example is the increase in surface area to volume ratio altering mechanical, thermal and catalytic properties of materials. Diffusion and reactions at nanoscale, nanostructures materials and nanodevices with fast ion transport are generally referred to nanoionics. Mechanical properties of nanosystems are of interest in the nanomechanics research. The catalytic activity of nanomaterials also opens potential risks in their interaction with biomaterials.\nMaterials reduced to the nanoscale can show different properties compared to what they exhibit on a macroscale, enabling unique applications. For instance, opaque substances can become transparent (copper); stable materials can turn combustible (aluminium); insoluble materials may become soluble (gold). A material such as gold, which is chemically inert at normal scales, can serve as a potent chemical catalyst at nanoscales. Much of the fascination with nanotechnology stems from these quantum and surface phenomena that matter exhibits at the nanoscale.\n\n\n=== Simple to complex: a molecular perspective ===\n\nModern synthetic chemistry has reached the point where it is possible to prepare small molecules to almost any structure. These methods are used today to manufacture a wide variety of useful chemicals such as pharmaceuticals or commercial polymers. This ability raises the question of extending this kind of control to the next-larger level, seeking methods to assemble these single molecules into supramolecular assemblies consisting of many molecules arranged in a well defined manner.\nThese approaches utilize the concepts of molecular self-assembly and/or supramolecular chemistry to automatically arrange themselves into some useful conformation through a bottom-up approach. The concept of molecular recognition is especially important: molecules can be designed so that a specific configuration or arrangement is favored due to non-covalent intermolecular forces. The Watson\u2013Crick basepairing rules are a direct result of this, as is the specificity of an enzyme being targeted to a single substrate, or the specific folding of the protein itself. Thus, two or more components can be designed to be complementary and mutually attractive so that they make a more complex and useful whole.\nSuch bottom-up approaches should be capable of producing devices in parallel and be much cheaper than top-down methods, but could potentially be overwhelmed as the size and complexity of the desired assembly increases. Most useful structures require complex and thermodynamically unlikely arrangements of atoms. Nevertheless, there are many examples of self-assembly based on molecular recognition in biology, most notably Watson\u2013Crick basepairing and enzyme-substrate interactions. The challenge for nanotechnology is whether these principles can be used to engineer new constructs in addition to natural ones.\n\n\n=== Molecular nanotechnology: a long-term view ===\n\nMolecular nanotechnology, sometimes called molecular manufacturing, describes engineered nanosystems (nanoscale machines) operating on the molecular scale. Molecular nanotechnology is especially associated with the molecular assembler, a machine that can produce a desired structure or device atom-by-atom using the principles of mechanosynthesis. Manufacturing in the context of productive nanosystems is not related to, and should be clearly distinguished from, the conventional technologies used to manufacture nanomaterials such as carbon nanotubes and nanoparticles.\nWhen the term \"nanotechnology\" was independently coined and popularized by Eric Drexler (who at the time was unaware of an earlier usage by Norio Taniguchi) it referred to a future manufacturing technology based on molecular machine systems. The premise was that molecular scale biological analogies of traditional machine components demonstrated molecular machines were possible: by the countless examples found in biology, it is known that sophisticated, stochastically optimised biological machines can be produced.\nIt is hoped that developments in nanotechnology will make possible their construction by some other means, perhaps using biomimetic principles. However, Drexler and other researchers have proposed that advanced nanotechnology, although perhaps initially implemented by biomimetic means, ultimately could be based on mechanical engineering principles, namely, a manufacturing technology based on the mechanical functionality of these components (such as gears, bearings, motors, and structural members) that would enable programmable, positional assembly to atomic specification. The physics and engineering performance of exemplar designs were analyzed in Drexler's book Nanosystems.\nIn general it is very difficult to assemble devices on the atomic scale, as one has to position atoms on other atoms of comparable size and stickiness. Another view, put forth by Carlo Montemagno, is that future nanosystems will be hybrids of silicon technology and biological molecular machines. Richard Smalley argued that mechanosynthesis are impossible due to the difficulties in mechanically manipulating individual molecules.\nThis led to an exchange of letters in the ACS publication Chemical & Engineering News in 2003. Though biology clearly demonstrates that molecular machine systems are possible, non-biological molecular machines are today only in their infancy. Leaders in research on non-biological molecular machines are Dr. Alex Zettl and his colleagues at Lawrence Berkeley Laboratories and UC Berkeley.[1] They have constructed at least three distinct molecular devices whose motion is controlled from the desktop with changing voltage: a nanotube nanomotor, a molecular actuator, and a nanoelectromechanical relaxation oscillator. See nanotube nanomotor for more examples.\nAn experiment indicating that positional molecular assembly is possible was performed by Ho and Lee at Cornell University in 1999. They used a scanning tunneling microscope to move an individual carbon monoxide molecule (CO) to an individual iron atom (Fe) sitting on a flat silver crystal, and chemically bound the CO to the Fe by applying a voltage.\n\n\n== Current research ==\n\n\n=== Nanomaterials ===\nThe nanomaterials field includes subfields which develop or study materials having unique properties arising from their nanoscale dimensions.\nInterface and colloid science has given rise to many materials which may be useful in nanotechnology, such as carbon nanotubes and other fullerenes, and various nanoparticles and nanorods. Nanomaterials with fast ion transport are related also to nanoionics and nanoelectronics.\nNanoscale materials can also be used for bulk applications; most present commercial applications of nanotechnology are of this flavor.\nProgress has been made in using these materials for medical applications; see Nanomedicine.\nNanoscale materials such as nanopillars are sometimes used in solar cells which combats the cost of traditional silicon solar cells.\nDevelopment of applications incorporating semiconductor nanoparticles to be used in the next generation of products, such as display technology, lighting, solar cells and biological imaging; see quantum dots.\nRecent application of nanomaterials include a range of biomedical applications, such as tissue engineering, drug delivery, and biosensors.\n\n\n=== Bottom-up approaches ===\nThese seek to arrange smaller components into more complex assemblies.\n\nDNA nanotechnology utilizes the specificity of Watson\u2013Crick basepairing to construct well-defined structures out of DNA and other nucleic acids.\nApproaches from the field of \"classical\" chemical synthesis (Inorganic and organic synthesis) also aim at designing molecules with well-defined shape (e.g. bis-peptides).\nMore generally, molecular self-assembly seeks to use concepts of supramolecular chemistry, and molecular recognition in particular, to cause single-molecule components to automatically arrange themselves into some useful conformation.\nAtomic force microscope tips can be used as a nanoscale \"write head\" to deposit a chemical upon a surface in a desired pattern in a process called dip pen nanolithography. This technique fits into the larger subfield of nanolithography.\nMolecular Beam Epitaxy allows for bottom up assemblies of materials, most notably semiconductor materials commonly used in chip and computing applications, stacks, gating, and nanowire lasers.\n\n\n=== Top-down approaches ===\nThese seek to create smaller devices by using larger ones to direct their assembly.\n\nMany technologies that descended from conventional solid-state silicon methods for fabricating microprocessors are now capable of creating features smaller than 100 nm, falling under the definition of nanotechnology. Giant magnetoresistance-based hard drives already on the market fit this description, as do atomic layer deposition (ALD) techniques. Peter Gr\u00fcnberg and Albert Fert received the Nobel Prize in Physics in 2007 for their discovery of Giant magnetoresistance and contributions to the field of spintronics.\nSolid-state techniques can also be used to create devices known as nanoelectromechanical systems or NEMS, which are related to microelectromechanical systems or MEMS.\nFocused ion beams can directly remove material, or even deposit material when suitable precursor gasses are applied at the same time. For example, this technique is used routinely to create sub-100 nm sections of material for analysis in Transmission electron microscopy.\nAtomic force microscope tips can be used as a nanoscale \"write head\" to deposit a resist, which is then followed by an etching process to remove material in a top-down method.\n\n\n=== Functional approaches ===\nThese seek to develop components of a desired functionality without regard to how they might be assembled.\n\nMagnetic assembly for the synthesis of anisotropic superparamagnetic materials such as recently presented magnetic nano chains.\nMolecular scale electronics seeks to develop molecules with useful electronic properties. These could then be used as single-molecule components in a nanoelectronic device. For an example see rotaxane.\nSynthetic chemical methods can also be used to create synthetic molecular motors, such as in a so-called nanocar.\n\n\n=== Biomimetic approaches ===\nBionics or biomimicry seeks to apply biological methods and systems found in nature, to the study and design of engineering systems and modern technology. Biomineralization is one example of the systems studied.\nBionanotechnology is the use of biomolecules for applications in nanotechnology, including use of viruses and lipid assemblies. Nanocellulose is a potential bulk-scale application.\n\n\n=== Speculative ===\nThese subfields seek to anticipate what inventions nanotechnology might yield, or attempt to propose an agenda along which inquiry might progress. These often take a big-picture view of nanotechnology, with more emphasis on its societal implications than the details of how such inventions could actually be created.\n\nMolecular nanotechnology is a proposed approach which involves manipulating single molecules in finely controlled, deterministic ways. This is more theoretical than the other subfields, and many of its proposed techniques are beyond current capabilities.\nNanorobotics centers on self-sufficient machines of some functionality operating at the nanoscale. There are hopes for applying nanorobots in medicine, but it may not be easy to do such a thing because of several drawbacks of such devices. Nevertheless, progress on innovative materials and methodologies has been demonstrated with some patents granted about new nanomanufacturing devices for future commercial applications, which also progressively helps in the development towards nanorobots with the use of embedded nanobioelectronics concepts.\nProductive nanosystems are \"systems of nanosystems\" which will be complex nanosystems that produce atomically precise parts for other nanosystems, not necessarily using novel nanoscale-emergent properties, but well-understood fundamentals of manufacturing. Because of the discrete (i.e. atomic) nature of matter and the possibility of exponential growth, this stage is seen as the basis of another industrial revolution. Mihail Roco, one of the architects of the USA's National Nanotechnology Initiative, has proposed four states of nanotechnology that seem to parallel the technical progress of the Industrial Revolution, progressing from passive nanostructures to active nanodevices to complex nanomachines and ultimately to productive nanosystems.\nProgrammable matter seeks to design materials whose properties can be easily, reversibly and externally controlled though a fusion of information science and materials science.\nDue to the popularity and media exposure of the term nanotechnology, the words picotechnology and femtotechnology have been coined in analogy to it, although these are only used rarely and informally.\n\n\n=== Dimensionality in nanomaterials ===\nNanomaterials can be classified in 0D, 1D, 2D and 3D nanomaterials. The dimensionality play a major role in determining the characteristic of nanomaterials including physical, chemical and biological characteristics. With the decrease in dimensionality, an increase in surface-to-volume ratio is observed. This indicate that smaller dimensional nanomaterials have higher surface area compared to 3D nanomaterials. Recently, two dimensional (2D) nanomaterials are extensively investigated for electronic, biomedical, drug delivery and biosensor applications.\n\n\n== Tools and techniques ==\n\nThere are several important modern developments. The atomic force microscope (AFM) and the Scanning Tunneling Microscope (STM) are two early versions of scanning probes that launched nanotechnology. There are other types of scanning probe microscopy. Although conceptually similar to the scanning confocal microscope developed by Marvin Minsky in 1961 and the scanning acoustic microscope (SAM) developed by Calvin Quate and coworkers in the 1970s, newer scanning probe microscopes have much higher resolution, since they are not limited by the wavelength of sound or light.\nThe tip of a scanning probe can also be used to manipulate nanostructures (a process called positional assembly). Feature-oriented scanning methodology may be a promising way to implement these nanomanipulations in automatic mode. However, this is still a slow process because of low scanning velocity of the microscope.\nVarious techniques of nanolithography such as optical lithography, X-ray lithography, dip pen nanolithography, electron beam lithography or nanoimprint lithography were also developed. Lithography is a top-down fabrication technique where a bulk material is reduced in size to nanoscale pattern.\nAnother group of nanotechnological techniques include those used for fabrication of nanotubes and nanowires, those used in semiconductor fabrication such as deep ultraviolet lithography, electron beam lithography, focused ion beam machining, nanoimprint lithography, atomic layer deposition, and molecular vapor deposition, and further including molecular self-assembly techniques such as those employing di-block copolymers. The precursors of these techniques preceded the nanotech era, and are extensions in the development of scientific advancements rather than techniques which were devised with the sole purpose of creating nanotechnology and which were results of nanotechnology research.The top-down approach anticipates nanodevices that must be built piece by piece in stages, much as manufactured items are made. Scanning probe microscopy is an important technique both for characterization and synthesis of nanomaterials. Atomic force microscopes and scanning tunneling microscopes can be used to look at surfaces and to move atoms around. By designing different tips for these microscopes, they can be used for carving out structures on surfaces and to help guide self-assembling structures. By using, for example, feature-oriented scanning approach, atoms or molecules can be moved around on a surface with scanning probe microscopy techniques. At present, it is expensive and time-consuming for mass production but very suitable for laboratory experimentation.\nIn contrast, bottom-up techniques build or grow larger structures atom by atom or molecule by molecule. These techniques include chemical synthesis, self-assembly and positional assembly. Dual polarisation interferometry is one tool suitable for characterisation of self assembled thin films. Another variation of the bottom-up approach is molecular beam epitaxy or MBE. Researchers at Bell Telephone Laboratories like John R. Arthur. Alfred Y. Cho, and Art C. Gossard developed and implemented MBE as a research tool in the late 1960s and 1970s. Samples made by MBE were key to the discovery of the fractional quantum Hall effect for which the 1998 Nobel Prize in Physics was awarded. MBE allows scientists to lay down atomically precise layers of atoms and, in the process, build up complex structures. Important for research on semiconductors, MBE is also widely used to make samples and devices for the newly emerging field of spintronics.\nHowever, new therapeutic products, based on responsive nanomaterials, such as the ultradeformable, stress-sensitive Transfersome vesicles, are under development and already approved for human use in some countries.\n\n\n== Applications ==\n\nAs of August 21, 2008, the Project on Emerging Nanotechnologies estimates that over 800 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3\u20134 per week. The project lists all of the products in a publicly accessible online database. Most applications are limited to the use of \"first generation\" passive nanomaterials which includes titanium dioxide in sunscreen, cosmetics, surface coatings, and some food products; Carbon allotropes used to produce gecko tape; silver in food packaging, clothing, disinfectants and household appliances; zinc oxide in sunscreens and cosmetics, surface coatings, paints and outdoor furniture varnishes; and cerium oxide as a fuel catalyst.Further applications allow tennis balls to last longer, golf balls to fly straighter, and even bowling balls to become more durable and have a harder surface. Trousers and socks have been infused with nanotechnology so that they will last longer and keep people cool in the summer. Bandages are being infused with silver nanoparticles to heal cuts faster. Video game consoles and personal computers may become cheaper, faster, and contain more memory thanks to nanotechnology. Also, to build structures for on chip computing with light, for example on chip optical quantum information processing, and picosecond transmission of information.Nanotechnology may have the ability to make existing medical applications cheaper and easier to use in places like the general practitioner's office and at home. Cars are being manufactured with nanomaterials so they may need fewer metals and less fuel to operate in the future.Scientists are now turning to nanotechnology in an attempt to develop diesel engines with cleaner exhaust fumes. Platinum is currently used as the diesel engine catalyst in these engines. The catalyst is what cleans the exhaust fume particles. First a reduction catalyst is employed to take nitrogen atoms from NOx molecules in order to free oxygen. Next the oxidation catalyst oxidizes the hydrocarbons and carbon monoxide to form carbon dioxide and water. Platinum is used in both the reduction and the oxidation catalysts. Using platinum though, is inefficient in that it is expensive and unsustainable. Danish company InnovationsFonden invested DKK 15 million in a search for new catalyst substitutes using nanotechnology. The goal of the project, launched in the autumn of 2014, is to maximize surface area and minimize the amount of material required. Objects tend to minimize their surface energy; two drops of water, for example, will join to form one drop and decrease surface area. If the catalyst's surface area that is exposed to the exhaust fumes is maximized, efficiency of the catalyst is maximized. The team working on this project aims to create nanoparticles that will not merge. Every time the surface is optimized, material is saved. Thus, creating these nanoparticles will increase the effectiveness of the resulting diesel engine catalyst\u2014in turn leading to cleaner exhaust fumes\u2014and will decrease cost. If successful, the team hopes to reduce platinum use by 25%.Nanotechnology also has a prominent role in the fast developing field of Tissue Engineering. When designing scaffolds, researchers attempt to the mimic the nanoscale features of a Cell's microenvironment to direct its differentiation down a suitable lineage. For example, when creating scaffolds to support the growth of bone, researchers may mimic osteoclast resorption pits.Researchers have successfully used DNA origami-based nanobots capable of carrying out logic functions to achieve targeted drug delivery in cockroaches. It is said that the computational power of these nanobots can be scaled up to that of a Commodore 64.\n\n\n== Implications ==\n\nAn area of concern is the effect that industrial-scale manufacturing and use of nanomaterials would have on human health and the environment, as suggested by nanotoxicology research. For these reasons, some groups advocate that nanotechnology be regulated by governments. Others counter that overregulation would stifle scientific research and the development of beneficial innovations. Public health research agencies, such as the National Institute for Occupational Safety and Health are actively conducting research on potential health effects stemming from exposures to nanoparticles.Some nanoparticle products may have unintended consequences. Researchers have discovered that bacteriostatic silver nanoparticles used in socks to reduce foot odor are being released in the wash. These particles are then flushed into the waste water stream and may destroy bacteria which are critical components of natural ecosystems, farms, and waste treatment processes.Public deliberations on risk perception in the US and UK carried out by the Center for Nanotechnology in Society found that participants were more positive about nanotechnologies for energy applications than for health applications, with health applications raising moral and ethical dilemmas such as cost and availability.Experts, including director of the Woodrow Wilson Center's Project on Emerging Nanotechnologies David Rejeski, have testified that successful commercialization depends on adequate oversight, risk research strategy, and public engagement. Berkeley, California is currently the only city in the United States to regulate nanotechnology; Cambridge, Massachusetts in 2008 considered enacting a similar law, but ultimately rejected it. Relevant for both research on and application of nanotechnologies, the insurability of nanotechnology is contested. Without state regulation of nanotechnology, the availability of private insurance for potential damages is seen as necessary to ensure that burdens are not socialised implicitly.\n\n\n=== Health and environmental concerns ===\n\nNanofibers are used in several areas and in different products, in everything from aircraft wings to tennis rackets. Inhaling airborne nanoparticles and nanofibers may lead to a number of pulmonary diseases, e.g. fibrosis.  Researchers have found that when rats breathed in nanoparticles, the particles settled in the brain and lungs, which led to significant increases in biomarkers for inflammation and stress response and that nanoparticles induce skin aging through oxidative stress in hairless mice.A two-year study at UCLA's School of Public Health found lab mice consuming nano-titanium dioxide showed DNA and chromosome damage to a degree \"linked to all the big killers of man, namely cancer, heart disease, neurological disease and aging\".A major study published more recently in Nature Nanotechnology suggests some forms of carbon nanotubes \u2013 a poster child for the \"nanotechnology revolution\" \u2013 could be as harmful as asbestos if inhaled in sufficient quantities. Anthony Seaton of the Institute of Occupational Medicine in Edinburgh, Scotland, who contributed to the article on carbon nanotubes said \"We know that some of them probably have the potential to cause mesothelioma. So those sorts of materials need to be handled very carefully.\" In the absence of specific regulation forthcoming from governments, Paull and Lyons (2008) have called for an exclusion of engineered nanoparticles in food. A newspaper article reports that workers in a paint factory developed serious lung disease and nanoparticles were found in their lungs.\n\n\n== Regulation ==\n\nCalls for tighter regulation of nanotechnology have occurred alongside a growing debate related to the human health and safety risks of nanotechnology. There is significant debate about who is responsible for the regulation of nanotechnology. Some regulatory agencies currently cover some nanotechnology products and processes (to varying degrees) \u2013 by \"bolting on\" nanotechnology to existing regulations \u2013 there are clear gaps in these regimes. Davies (2008) has proposed a regulatory road map describing steps to deal with these shortcomings.Stakeholders concerned by the lack of a regulatory framework to assess and control risks associated with the release of nanoparticles and nanotubes have drawn parallels with bovine spongiform encephalopathy (\"mad cow\" disease), thalidomide, genetically modified food, nuclear energy, reproductive technologies, biotechnology, and asbestosis. Dr. Andrew Maynard, chief science advisor to the Woodrow Wilson Center's Project on Emerging Nanotechnologies, concludes that there is insufficient funding for human health and safety research, and as a result there is currently limited understanding of the human health and safety risks associated with nanotechnology. As a result, some academics have called for stricter application of the precautionary principle, with delayed marketing approval, enhanced labelling and additional safety data development requirements in relation to certain forms of nanotechnology.The Royal Society report identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that \"manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure\" (p. xiii).\nThe Center for Nanotechnology in Society has found that people respond to nanotechnologies differently, depending on application \u2013 with participants in public deliberations more positive about nanotechnologies for energy than health applications \u2013 suggesting that any public calls for nano regulations may differ by technology sector.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\nNanotechnology at Curlie (based on DMOZ)\nWhat is Nanotechnology? (A Vega/BBC/OU Video Discussion)."}