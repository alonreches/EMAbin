{"number theory": "Number theory (or arithmetic or higher arithmetic in older usage), is a branch of pure mathematics devoted primarily to the study of the integers. It is sometimes called \"The Queen of Mathematics\" because of its foundational place in the discipline. Number theorists study prime numbers as well as the properties of objects made out of integers (e.g., rational numbers) or defined as generalizations of the integers (e.g., algebraic integers).\nIntegers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (e.g., the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, e.g., as approximated by the latter (Diophantine approximation).\nThe older term for number theory is arithmetic. By the early twentieth century, it had been superseded by \"number theory\". (The word \"arithmetic\" is used by the general public to mean \"elementary calculations\"; it has also acquired other meanings in mathematical logic, as in Peano arithmetic, and computer science, as in floating point arithmetic.) The use of the term arithmetic for number theory regained some ground in the second half of the 20th century, arguably in part due to French influence. In particular, arithmetical is preferred as an adjective to number-theoretic.\n\n\n== History ==\n\n\n=== Origins ===\n\n\n==== Dawn of arithmetic ====\nThe first historical find of an arithmetical nature is a fragment of a table: the broken clay tablet Plimpton 322 (Larsa, Mesopotamia, ca. 1800 BC) contains a list of \"Pythagorean triples\", i.e., integers \n  \n    \n      \n        (\n        a\n        ,\n        b\n        ,\n        c\n        )\n      \n    \n    {\\displaystyle (a,b,c)}\n   such that \n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        =\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle a^{2}+b^{2}=c^{2}}\n  .\nThe triples are too many and too large to have been obtained by brute force. The heading over the first column reads: \"The takiltum of the diagonal which has been subtracted such that the width...\"\n\nThe table's layout suggests that it was constructed by means of what amounts, in modern language, to the identity\n\n  \n    \n      \n        \n          \n            (\n            \n              \n                \n                  1\n                  2\n                \n              \n              \n                (\n                \n                  x\n                  \u2212\n                  \n                    \n                      1\n                      x\n                    \n                  \n                \n                )\n              \n            \n            )\n          \n          \n            2\n          \n        \n        +\n        1\n        =\n        \n          \n            (\n            \n              \n                \n                  1\n                  2\n                \n              \n              \n                (\n                \n                  x\n                  +\n                  \n                    \n                      1\n                      x\n                    \n                  \n                \n                )\n              \n            \n            )\n          \n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\left({\\frac {1}{2}}\\left(x-{\\frac {1}{x}}\\right)\\right)^{2}+1=\\left({\\frac {1}{2}}\\left(x+{\\frac {1}{x}}\\right)\\right)^{2},}\n  which is implicit in routine Old Babylonian exercises. If some other method was used, the triples were first constructed and then reordered by \n  \n    \n      \n        c\n        \n          /\n        \n        a\n      \n    \n    {\\displaystyle c/a}\n  , presumably for actual use as a \"table\", i.e., with a view to applications.\nIt is not known what these applications may have been, or whether there could have been any; Babylonian astronomy, for example, truly came into its own only later. It has been suggested instead that the table was a source of numerical examples for school problems.While Babylonian number theory\u2014or what survives of Babylonian mathematics that can be called thus\u2014consists of this single, striking fragment, Babylonian algebra (in the secondary-school sense of \"algebra\") was exceptionally well developed. Late Neoplatonic sources state that Pythagoras learned mathematics from the Babylonians. Much earlier sources state that Thales and Pythagoras traveled and studied in Egypt.\nEuclid IX 21\u201434 is very probably Pythagorean; it is very simple material (\"odd times even is even\", \"if an odd number measures [= divides] an even number, then it also measures [= divides] half of it\"), but it is all that is needed to prove that \n  \n    \n      \n        \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {2}}}\n  \nis irrational. Pythagorean mystics gave great importance to the odd and the even.\nThe discovery that \n  \n    \n      \n        \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {2}}}\n   is irrational is credited to the early Pythagoreans (pre-Theodorus). By revealing (in modern terms) that numbers could be irrational, this discovery seems to have provoked the first foundational crisis in mathematical history; its proof or its divulgation are sometimes credited to Hippasus, who was expelled or split from the Pythagorean sect. This forced a distinction between numbers (integers and the rationals\u2014the subjects of arithmetic), on the one hand, and lengths and proportions (which we would identify with real numbers, whether rational or not), on the other hand.\nThe Pythagorean tradition spoke also of so-called polygonal or figurate numbers. While square numbers, cubic numbers, etc., are seen now as more natural than triangular numbers, pentagonal numbers, etc., the study of the sums\nof triangular and pentagonal numbers would prove fruitful in the early modern period (17th to early 19th century).\nWe know of no clearly arithmetical material in ancient Egyptian or Vedic sources, though there is some algebra in both. The Chinese remainder theorem appears as an exercise  in Sunzi Suanjing (3rd, 4th or 5th century CE.) (There is one important step glossed over in Sunzi's solution: it is the problem that was later solved by \u0100ryabha\u1e6da's Ku\u1e6d\u1e6daka \u2013 see below.)\nThere is also some numerical mysticism in Chinese mathematics, but, unlike that of the Pythagoreans, it seems to have led nowhere. Like the Pythagoreans' perfect numbers, magic squares have passed from superstition into recreation.\n\n\n==== Classical Greece and the early Hellenistic period ====\n\nAside from a few fragments, the mathematics of Classical Greece is known to us either through the reports of contemporary non-mathematicians or through mathematical works from the early Hellenistic period. In the case of number theory, this means, by and large, Plato and Euclid, respectively.\nWhile Asian mathematics influenced Greek and Hellenistic learning, it seems to be the case that Greek mathematics is also an indigenous tradition.\nEusebius, PE X, chapter 4 mentions of Pythagoras:\n\n\"In fact the said Pythagoras, while busily studying the wisdom of each nation, visited Babylon, and Egypt, and all Persia, being instructed by the Magi and the priests: and in addition to these he is related to have studied under the Brahmans (these are Indian philosophers); and from some he gathered astrology, from others geometry, and arithmetic and music from others, and different things from different nations, and only from the wise men of Greece did he get nothing, wedded as they were to a poverty and dearth of wisdom: so on the contrary he himself became the author of instruction to the Greeks in the learning which he had procured from abroad.\"\nAristotle claimed that the philosophy of Plato closely followed the teachings of the Pythagoreans, and Cicero repeats this claim: Platonem ferunt didicisse Pythagorea omnia (\"They say Plato learned all things Pythagorean\").Plato had a keen interest in mathematics, and distinguished clearly between arithmetic and calculation. (By arithmetic he meant, in part, theorising on number, rather than what arithmetic or number theory have come to mean.) It is through one of Plato's dialogues\u2014namely, Theaetetus\u2014that we know that Theodorus had proven that \n  \n    \n      \n        \n          \n            3\n          \n        \n        ,\n        \n          \n            5\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \n            17\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {3}},{\\sqrt {5}},\\dots ,{\\sqrt {17}}}\n   are irrational. Theaetetus was, like Plato, a disciple of Theodorus's; he worked on distinguishing different kinds of incommensurables, and was thus arguably a pioneer in the study of number systems. (Book X of Euclid's Elements is described by Pappus as being largely based on Theaetetus's work.)\nEuclid devoted part of his Elements to prime numbers and divisibility, topics that belong unambiguously to number theory and are basic to it (Books VII to IX of Euclid's Elements). In particular, he gave an algorithm for computing the greatest common divisor of two numbers (the Euclidean algorithm; Elements, Prop. VII.2) and the first known proof of the infinitude of primes (Elements, Prop. IX.20).\nIn 1773, Lessing published an epigram he had found in a manuscript during his work as a librarian; it claimed to be a letter sent by Archimedes to Eratosthenes. The epigram proposed what has become known as\nArchimedes's cattle problem; its solution (absent from the manuscript) requires solving an indeterminate quadratic equation (which reduces to what would later be misnamed Pell's equation). As far as we know, such equations were first successfully treated by the Indian school. It is not known whether Archimedes himself had a method of solution.\n\n\n==== Diophantus ====\n\nVery little is known about Diophantus of Alexandria; he probably lived in the third century CE, that is, about five hundred years after Euclid. Six out of the thirteen books of Diophantus's Arithmetica survive in the original Greek; four more books survive in an Arabic translation. The Arithmetica is a collection of worked-out problems where the task is invariably to find rational solutions to a system of polynomial equations, usually of the form \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          z\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle f(x,y)=z^{2}}\n   or \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n        =\n        \n          w\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle f(x,y,z)=w^{2}}\n  . Thus, nowadays, we speak of Diophantine equations when we speak of polynomial equations to which rational or integer solutions must be found.\nOne may say that Diophantus was studying rational points \u2014 i.e., points whose coordinates are rational \u2014 on curves and algebraic varieties; however, unlike the Greeks of the Classical period, who did what we would now call basic algebra in geometrical terms, Diophantus did what we would now call basic algebraic geometry in purely algebraic terms. In modern language, what Diophantus did was to find rational parametrizations of varieties; that is, given an equation of the form (say)\n\n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n        \n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(x_{1},x_{2},x_{3})=0}\n  , his aim was to find (in essence) three rational functions \n  \n    \n      \n        \n          g\n          \n            1\n          \n        \n        ,\n        \n          g\n          \n            2\n          \n        \n        ,\n        \n          g\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle g_{1},g_{2},g_{3}}\n   such that, for all values of \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   and \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  , setting\n\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        =\n        \n          g\n          \n            i\n          \n        \n        (\n        r\n        ,\n        s\n        )\n      \n    \n    {\\displaystyle x_{i}=g_{i}(r,s)}\n   for \n  \n    \n      \n        i\n        =\n        1\n        ,\n        2\n        ,\n        3\n      \n    \n    {\\displaystyle i=1,2,3}\n   gives a solution to \n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n        \n        )\n        =\n        0.\n      \n    \n    {\\displaystyle f(x_{1},x_{2},x_{3})=0.}\n  \nDiophantus also studied the equations of some non-rational curves, for which no rational parametrisation is possible. He managed to find some rational points on these curves (elliptic curves, as it happens, in what seems to be their first known occurrence) by means of what amounts to a tangent construction: translated into coordinate geometry\n(which did not exist in Diophantus's time), his method would be visualised as drawing a tangent to a curve at a known rational point, and then finding the other point of intersection of the tangent with the curve; that other point is a new rational point. (Diophantus also resorted to what could be called a special case of a secant construction.)\nWhile Diophantus was concerned largely with rational solutions, he assumed some results on integer numbers, in particular that every integer is the sum of four squares (though he never stated as much explicitly).\n\n\n==== \u0100ryabha\u1e6da, Brahmagupta, Bh\u0101skara ====\nWhile Greek astronomy probably influenced Indian learning, to the point of introducing trigonometry, it seems to be the case that Indian mathematics is otherwise an indigenous tradition; in particular, there is no evidence that Euclid's Elements reached India before the 18th century.\u0100ryabha\u1e6da (476\u2013550 CE) showed that pairs of simultaneous congruences \n  \n    \n      \n        n\n        \u2261\n        \n          a\n          \n            1\n          \n        \n        \n          \n            mod\n            \n              m\n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle n\\equiv a_{1}{\\bmod {m}}_{1}}\n  , \n  \n    \n      \n        n\n        \u2261\n        \n          a\n          \n            2\n          \n        \n        \n          \n            mod\n            \n              m\n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n\\equiv a_{2}{\\bmod {m}}_{2}}\n   could be solved by a method he called ku\u1e6d\u1e6daka, or pulveriser; this is a procedure close to (a generalisation of) the Euclidean algorithm, which was probably discovered independently in India. \u0100ryabha\u1e6da seems to have had in mind applications to astronomical calculations.Brahmagupta (628 CE) started the systematic study of indefinite quadratic equations\u2014in particular, the misnamed Pell equation, in which Archimedes may have first been interested, and which did not start to be solved in the West until the time of Fermat and Euler. Later Sanskrit authors would follow, using Brahmagupta's technical terminology. A general procedure (the chakravala, or \"cyclic method\") for solving Pell's equation was finally found by Jayadeva (cited in the eleventh century; his work is otherwise lost); the earliest surviving exposition appears in Bh\u0101skara II's B\u012bja-ga\u1e47ita (twelfth century).Indian mathematics remained largely unknown in Europe until the late eighteenth century; Brahmagupta and Bh\u0101skara's work was translated into English in 1817 by Henry Colebrooke.\n\n\n==== Arithmetic in the Islamic golden age ====\n\nIn the early ninth century, the caliph Al-Ma'mun ordered translations of many Greek mathematical works and at least one Sanskrit work (the Sindhind,\nwhich may  or may not be Brahmagupta's Br\u0101hmasphu\u1e6dasiddh\u0101nta).\nDiophantus's main work, the Arithmetica, was translated into Arabic by Qusta ibn Luqa (820\u2013912).\nPart of the treatise al-Fakhri (by al-Karaj\u012b, 953 \u2013 ca. 1029) builds on it to some extent. According to Rashed Roshdi, Al-Karaj\u012b's contemporary Ibn al-Haytham knew what would later be called Wilson's theorem.\n\n\n==== Western Europe in the Middle Ages ====\nOther than a treatise on squares in arithmetic progression by Fibonacci \u2014 who lived and studied in north Africa and Constantinople during his formative years, ca. 1175\u20131200 \u2014 no number theory to speak of was done in western Europe during the Middle Ages. Matters started to change in Europe in the late Renaissance, thanks to a renewed study of the works of Greek antiquity. A catalyst was the textual emendation and translation into Latin of Diophantus's Arithmetica (Bachet, 1621, following a first attempt by Xylander, 1575).\n\n\n=== Early modern number theory ===\n\n\n==== Fermat ====\n\nPierre de Fermat (1601\u20131665) never published his writings; in particular, his work on number theory is contained almost entirely in letters to mathematicians and in private marginal notes. He wrote down nearly no proofs in number theory; he had no models in the area. He did make repeated use of mathematical induction, introducing the method of infinite descent.\nOne of Fermat's first interests was perfect numbers (which appear in Euclid, Elements IX) and amicable numbers; this led him to work on integer divisors, which were from the beginning among the subjects of the\ncorrespondence (1636 onwards) that put him in touch with the mathematical community of the day. He had already studied Bachet's edition of Diophantus carefully; by 1643, his interests had shifted largely to Diophantine problems and sums of squares (also treated by Diophantus).\nFermat's achievements in arithmetic include:\n\nFermat's little theorem (1640), stating that, if a is not divisible by a prime p, then \n  \n    \n      \n        \n          a\n          \n            p\n            \u2212\n            1\n          \n        \n        \u2261\n        1\n        \n          mod\n          \n            p\n          \n        \n        .\n      \n    \n    {\\displaystyle a^{p-1}\\equiv 1{\\bmod {p}}.}\n  \nIf a and b are coprime, then \n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle a^{2}+b^{2}}\n   is not divisible by any prime congruent to \u22121 modulo 4; and every prime congruent to 1 modulo 4 can be written in the form \n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle a^{2}+b^{2}}\n  . These two statements also date from 1640; in 1659, Fermat stated to Huygens that he had proven the latter statement by the method of infinite descent. Fermat and Frenicle also did some work (some of it erroneous) on other quadratic forms.\nFermat posed the problem of solving \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n        \u2212\n        N\n        \n          y\n          \n            2\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle x^{2}-Ny^{2}=1}\n   as a challenge to English mathematicians (1657). The problem was solved in a few months by Wallis and Brouncker. Fermat considered their solution valid, but pointed out they had provided an algorithm without a proof (as had Jayadeva and Bhaskara, though Fermat would never know this). He states that a proof can be found by descent.\nFermat developed methods for (doing what in our terms amounts to) finding points on curves of genus 0 and 1. As in Diophantus, there are many special procedures and what amounts to a tangent construction, but no use of a secant construction.\nFermat states and proves (by descent) in the appendix to Observations on Diophantus (Obs. XLV) that \n  \n    \n      \n        \n          x\n          \n            4\n          \n        \n        +\n        \n          y\n          \n            4\n          \n        \n        =\n        \n          z\n          \n            4\n          \n        \n      \n    \n    {\\displaystyle x^{4}+y^{4}=z^{4}}\n   has no non-trivial solutions in the integers. Fermat also mentioned to his correspondents that \n  \n    \n      \n        \n          x\n          \n            3\n          \n        \n        +\n        \n          y\n          \n            3\n          \n        \n        =\n        \n          z\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle x^{3}+y^{3}=z^{3}}\n   has no non-trivial solutions, and that this could be proven by descent. The first known proof is due to Euler (1753; indeed by descent).Fermat's claim (\"Fermat's last theorem\") to have shown there are no solutions to\n\n  \n    \n      \n        \n          x\n          \n            n\n          \n        \n        +\n        \n          y\n          \n            n\n          \n        \n        =\n        \n          z\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x^{n}+y^{n}=z^{n}}\n   for all \n  \n    \n      \n        n\n        \u2265\n        3\n      \n    \n    {\\displaystyle n\\geq 3}\n   appears only in his annotations on the margin of his copy of Diophantus.\n\n\n==== Euler ====\n\nThe interest of Leonhard Euler (1707\u20131783) in number theory was first spurred in 1729, when a friend of his, the amateur Goldbach, pointed him towards some of Fermat's work on the subject. This has been called the \"rebirth\" of modern number theory, after Fermat's relative lack of success in getting his contemporaries' attention for the subject. Euler's work on number theory includes the following:\nProofs for Fermat's statements. This includes Fermat's little theorem (generalised by Euler to non-prime moduli); the fact that \n  \n    \n      \n        p\n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        \n          y\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle p=x^{2}+y^{2}}\n   if and only if \n  \n    \n      \n        p\n        \u2261\n        1\n        \n          mod\n          \n            4\n          \n        \n      \n    \n    {\\displaystyle p\\equiv 1{\\bmod {4}}}\n  ; initial work towards a proof that every integer is the sum of four squares (the first complete proof is by Joseph-Louis Lagrange (1770), soon improved by Euler himself); the lack of non-zero integer solutions to \n  \n    \n      \n        \n          x\n          \n            4\n          \n        \n        +\n        \n          y\n          \n            4\n          \n        \n        =\n        \n          z\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x^{4}+y^{4}=z^{2}}\n   (implying the case n=4 of Fermat's last theorem, the case n=3 of which Euler also proved by a related method).\nPell's equation, first misnamed by Euler. He wrote on the link between continued fractions and Pell's equation.\nFirst steps towards analytic number theory. In his work of sums of four squares, partitions, pentagonal numbers, and the distribution of prime numbers, Euler pioneered the use of what can be seen as analysis (in particular, infinite series) in number theory. Since he lived before the development of complex analysis, most of his work is restricted to the formal manipulation of power series. He did, however, do some very notable (though not fully rigorous) early work on what would later be called the Riemann zeta function.\nQuadratic forms. Following Fermat's lead, Euler did further research on the question of which primes can be expressed in the form \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n        +\n        N\n        \n          y\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x^{2}+Ny^{2}}\n  , some of it prefiguring quadratic reciprocity. \nDiophantine equations. Euler worked on some Diophantine equations of genus 0 and 1. In particular, he studied Diophantus's work; he tried to systematise it, but the time was not yet ripe for such an endeavour \u2013 algebraic geometry was still in its infancy. He did notice there was a connection between Diophantine problems and elliptic integrals, whose study he had himself initiated.\n\n\n==== Lagrange, Legendre, and Gauss ====\n\nJoseph-Louis Lagrange (1736\u20131813) was the first to give full proofs of some of Fermat's and Euler's work and observations \u2013 for instance, the four-square theorem and the basic theory of the misnamed \"Pell's equation\" (for which an algorithmic solution was found by Fermat and his contemporaries, and also by Jayadeva and Bhaskara II before them.) He also studied quadratic forms in full generality (as opposed to \n  \n    \n      \n        m\n        \n          X\n          \n            2\n          \n        \n        +\n        n\n        \n          Y\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle mX^{2}+nY^{2}}\n  ) \u2014 defining their equivalence relation, showing how to put them in reduced form, etc.\nAdrien-Marie Legendre (1752\u20131833) was the first to state the law of quadratic reciprocity. He also\nconjectured what amounts to the prime number theorem and Dirichlet's theorem on arithmetic progressions. He gave a full treatment of the equation \n  \n    \n      \n        a\n        \n          x\n          \n            2\n          \n        \n        +\n        b\n        \n          y\n          \n            2\n          \n        \n        +\n        c\n        \n          z\n          \n            2\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle ax^{2}+by^{2}+cz^{2}=0}\n   and worked on quadratic forms along the lines later developed fully by Gauss. In his old age, he was the first to prove \"Fermat's last theorem\" for \n  \n    \n      \n        n\n        =\n        5\n      \n    \n    {\\displaystyle n=5}\n   (completing work by Peter Gustav Lejeune Dirichlet, and crediting both him and Sophie Germain).\n\nIn his Disquisitiones Arithmeticae (1798), Carl Friedrich Gauss (1777\u20131855) proved the law of quadratic reciprocity and developed the theory of quadratic forms (in particular, defining their composition). He also introduced some basic notation (congruences) and devoted a section to computational matters, including primality tests. The last section of the Disquisitiones established a link between roots of unity and number theory:\n\nThe theory of the division of the circle...which is treated in sec. 7 does not belong\nby itself to arithmetic, but its principles can only be drawn from higher arithmetic.\nIn this way, Gauss arguably made a first foray towards both \u00c9variste Galois's work and algebraic number theory.\n\n\n=== Maturity and division into subfields ===\n\nStarting early in the nineteenth century, the following developments gradually took place:\n\nThe rise to self-consciousness of number theory (or higher arithmetic) as a field of study.\nThe development of much of modern mathematics necessary for basic modern number theory: complex analysis, group theory, Galois theory\u2014accompanied by greater rigor in analysis and abstraction in algebra.\nThe rough subdivision of number theory into its modern subfields\u2014in particular, analytic and algebraic number theory.Algebraic number theory may be said to start with the study of reciprocity and cyclotomy, but truly came into its own with the development of abstract algebra and early ideal theory and valuation theory; see below. A conventional starting point for analytic number theory is Dirichlet's theorem on arithmetic progressions (1837),  whose proof introduced L-functions and involved some asymptotic analysis and a limiting process on a real variable. The first use of analytic ideas in number theory actually\ngoes back to Euler (1730s),  who used formal power series and non-rigorous (or implicit) limiting arguments. The use of complex analysis in number theory comes later: the work of Bernhard Riemann (1859) on the zeta function is the canonical starting point; Jacobi's four-square theorem (1839), which predates it, belongs to an initially different strand that has by now taken a leading role in analytic number theory (modular forms).The history of each subfield is briefly addressed in its own section below; see the main article of each subfield for fuller treatments. Many of the most interesting questions in each area remain open and are being actively worked on.\n\n\n== Main subdivisions ==\n\n\n=== Elementary tools ===\nThe term elementary generally denotes a method that does not use complex analysis. For example, the prime number theorem was first proven using complex analysis in 1896, but an elementary proof was found only in 1949 by Erd\u0151s and Selberg. The term is somewhat ambiguous: for example, proofs based on complex Tauberian theorems (e.g. Wiener\u2013Ikehara) are often seen as quite enlightening but not elementary, in spite of using Fourier analysis, rather than complex analysis as such. Here as elsewhere, an elementary proof may be longer and more difficult for most readers than a non-elementary one.\nNumber theory has the reputation of being a field many of whose results can be stated to the layperson. At the same time, the proofs of these results are not particularly accessible, in part because the range of tools they use is, if anything, unusually broad within mathematics.\n\n\n=== Analytic number theory ===\n\nAnalytic number theory may be defined\n\nin terms of its tools, as the study of the integers by means of tools from real and complex analysis; or\nin terms of its concerns, as the study within number theory of estimates on size and density, as opposed to identities.Some subjects generally considered to be part of analytic number theory, e.g., sieve theory, are better covered by the second rather than the first definition: some of sieve theory, for instance, uses little analysis, yet it does belong to analytic number theory.\nThe following are examples of problems in analytic number theory: the prime number theorem, the Goldbach conjecture (or the twin prime conjecture, or the Hardy\u2013Littlewood conjectures), the Waring problem and the Riemann hypothesis. Some of the most important tools of analytic number theory are the circle method, sieve methods and L-functions (or, rather, the study of their properties). The theory of modular forms (and, more generally, automorphic forms) also occupies an increasingly central place in the toolbox of analytic number theory.One may ask analytic questions about algebraic numbers, and use analytic means to answer such questions; it is thus that algebraic and analytic number theory intersect. For example, one may define prime ideals (generalizations of prime numbers in the field of algebraic numbers) and ask how many prime ideals there are up to a certain size. This question can be answered by means of an examination of Dedekind zeta functions, which are generalizations of the Riemann zeta function, a key analytic object at the roots of the subject. This is an example of a general procedure in analytic number theory: deriving information about the distribution of a sequence (here, prime ideals or prime numbers) from the analytic behavior of an appropriately constructed complex-valued function.\n\n\n=== Algebraic number theory ===\n\nAn algebraic number is any complex number that is a solution to some polynomial equation \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(x)=0}\n   with rational coefficients; for example, every solution \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   of \n  \n    \n      \n        \n          x\n          \n            5\n          \n        \n        +\n        (\n        11\n        \n          /\n        \n        2\n        )\n        \n          x\n          \n            3\n          \n        \n        \u2212\n        7\n        \n          x\n          \n            2\n          \n        \n        +\n        9\n        =\n        0\n      \n    \n    {\\displaystyle x^{5}+(11/2)x^{3}-7x^{2}+9=0}\n   (say) is an algebraic number. Fields of algebraic numbers are also called algebraic number fields, or shortly number fields. Algebraic number theory studies algebraic number fields. Thus, analytic and algebraic number theory can and do overlap: the former is defined by its methods, the latter by its objects of study.\nIt could be argued that the simplest kind of number fields (viz., quadratic fields) were already studied by Gauss, as the discussion of quadratic forms in Disquisitiones arithmeticae can be restated in terms of ideals and\nnorms in quadratic fields. (A quadratic field consists of all\nnumbers of the form \n  \n    \n      \n        a\n        +\n        b\n        \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle a+b{\\sqrt {d}}}\n  , where\n\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n   are rational numbers and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \nis a fixed rational number whose square root is not rational.)\nFor that matter, the 11th-century chakravala method amounts\u2014in modern terms\u2014to an algorithm for finding the units of a real quadratic number field. However, neither Bh\u0101skara nor Gauss knew of number fields as such.\nThe grounds of the subject as we know it were set in the late nineteenth century, when ideal numbers, the theory of ideals and valuation theory were developed; these are three complementary ways of dealing with the lack of unique factorisation in algebraic number fields. (For example, in the field generated by the rationals\nand \n  \n    \n      \n        \n          \n            \u2212\n            5\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {-5}}}\n  , the number \n  \n    \n      \n        6\n      \n    \n    {\\displaystyle 6}\n   can be factorised both as \n  \n    \n      \n        6\n        =\n        2\n        \u22c5\n        3\n      \n    \n    {\\displaystyle 6=2\\cdot 3}\n   and\n\n  \n    \n      \n        6\n        =\n        (\n        1\n        +\n        \n          \n            \u2212\n            5\n          \n        \n        )\n        (\n        1\n        \u2212\n        \n          \n            \u2212\n            5\n          \n        \n        )\n      \n    \n    {\\displaystyle 6=(1+{\\sqrt {-5}})(1-{\\sqrt {-5}})}\n  ; all of \n  \n    \n      \n        2\n      \n    \n    {\\displaystyle 2}\n  , \n  \n    \n      \n        3\n      \n    \n    {\\displaystyle 3}\n  , \n  \n    \n      \n        1\n        +\n        \n          \n            \u2212\n            5\n          \n        \n      \n    \n    {\\displaystyle 1+{\\sqrt {-5}}}\n   and\n\n  \n    \n      \n        1\n        \u2212\n        \n          \n            \u2212\n            5\n          \n        \n      \n    \n    {\\displaystyle 1-{\\sqrt {-5}}}\n  \nare irreducible, and thus, in a na\u00efve sense, analogous to primes among the integers.) The initial impetus for the development of ideal numbers (by Kummer) seems to have come from the study of higher reciprocity laws,i.e., generalisations of quadratic reciprocity.\nNumber fields are often studied as extensions of smaller number fields: a field L is said to be an extension of a field K if L contains K.\n(For example, the complex numbers C are an extension of the reals R, and the reals R are an extension of the rationals Q.)\nClassifying the possible extensions of a given number field is a difficult and partially open problem. Abelian extensions\u2014that is, extensions L of K such that the Galois group Gal(L/K) of L over K is an abelian group\u2014are relatively well understood.\nTheir classification was the object of the programme of class field theory, which was initiated in the late 19th century (partly by Kronecker and Eisenstein) and carried out largely in 1900\u20141950.\nAn example of an active area of research in algebraic number theory is Iwasawa theory. The Langlands program, one of the main current large-scale research plans in mathematics, is sometimes described as an attempt to generalise class field theory to non-abelian extensions of number fields.\n\n\n=== Diophantine geometry ===\n\nThe central problem of Diophantine geometry is to determine when a Diophantine equation has solutions, and if it does, how many. The approach taken is to think of the solutions of an equation as a geometric object.\nFor example, an equation in two variables defines a curve in the plane. More generally, an equation, or system of equations, in two or more variables defines a curve, a surface or some other such object in n-dimensional space. In Diophantine geometry, one asks whether there are any rational points (points all of whose coordinates are rationals) or\nintegral points (points all of whose coordinates are integers) on the curve or surface. If there are any such points, the next step is to ask how many there are and how they are distributed. A basic question in this direction is: are there finitely\nor infinitely many rational points on a given curve (or surface)? What about integer points?\nAn example here may be helpful. Consider the Pythagorean equation \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n        +\n        \n          y\n          \n            2\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle x^{2}+y^{2}=1}\n  ;\nwe would like to study its rational solutions, i.e., its solutions\n\n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle (x,y)}\n   such that\nx and y are both rational. This is the same as asking for all integer solutions\nto \n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        =\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle a^{2}+b^{2}=c^{2}}\n  ; any solution to the latter equation gives\nus a solution \n  \n    \n      \n        x\n        =\n        a\n        \n          /\n        \n        c\n      \n    \n    {\\displaystyle x=a/c}\n  , \n  \n    \n      \n        y\n        =\n        b\n        \n          /\n        \n        c\n      \n    \n    {\\displaystyle y=b/c}\n   to the former. It is also the\nsame as asking for all points with rational coordinates on the curve\ndescribed by \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n        +\n        \n          y\n          \n            2\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle x^{2}+y^{2}=1}\n  . (This curve happens to be a circle of radius 1 around the origin.)\n\nThe rephrasing of questions on equations in terms of points on curves turns out to be felicitous. The finiteness or not of the number of rational or integer points on an algebraic curve\u2014that is, rational or integer solutions to an equation \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(x,y)=0}\n  , where \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is a polynomial in two variables\u2014turns out to depend crucially on the genus of the curve. The genus can be defined as follows: allow the variables in \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(x,y)=0}\n   to be complex numbers; then \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(x,y)=0}\n   defines a 2-dimensional surface in (projective) 4-dimensional space (since two complex variables can be decomposed into four real variables, i.e., four dimensions). Count\nthe number of (doughnut) holes in the surface; call this number the genus of \n  \n    \n      \n        f\n        (\n        x\n        ,\n        y\n        )\n        =\n        0\n      \n    \n    {\\displaystyle f(x,y)=0}\n  . Other geometrical notions turn out to be just as crucial.\nThere is also the closely linked area of Diophantine approximations: given a number \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , how well can it be approximated by rationals? (We are looking for approximations that are good relative to the amount of space that it takes to write the rational: call \n  \n    \n      \n        a\n        \n          /\n        \n        q\n      \n    \n    {\\displaystyle a/q}\n   (with \n  \n    \n      \n        gcd\n        (\n        a\n        ,\n        q\n        )\n        =\n        1\n      \n    \n    {\\displaystyle \\gcd(a,q)=1}\n  ) a good approximation to \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   if \n  \n    \n      \n        \n          |\n        \n        x\n        \u2212\n        a\n        \n          /\n        \n        q\n        \n          |\n        \n        <\n        \n          \n            1\n            \n              q\n              \n                c\n              \n            \n          \n        \n      \n    \n    {\\displaystyle |x-a/q|<{\\frac {1}{q^{c}}}}\n  , where \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n   is large.) This question is of special interest if \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is an algebraic number. If \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   cannot be well approximated, then some equations do not have integer or rational solutions. Moreover, several concepts (especially that of height) turn out to be crucial both in Diophantine geometry and in the study of Diophantine approximations. This question is also of special interest in transcendental number theory: if a number can be better approximated than any algebraic number, then it is a transcendental number. It is by this argument that \u03c0 and e have been shown to be transcendental.\nDiophantine geometry should not be confused with the geometry of numbers, which is a collection of graphical methods for answering certain questions in algebraic number theory. Arithmetic geometry, on the other hand, is a contemporary term\nfor much the same domain as that covered by the term Diophantine geometry. The term arithmetic geometry is arguably used\nmost often when one wishes to emphasise the connections to modern algebraic geometry (as in, for instance, Faltings's theorem) rather than to techniques in Diophantine approximations.\n\n\n== Recent approaches and subfields ==\nThe areas below date as such from no earlier than the mid-twentieth century, even if they are based on older material. For example, as is explained below, the matter of algorithms in number theory is very old, in some sense older than the concept of proof; at the same time, the modern study of computability dates only from the 1930s and 1940s, and computational complexity theory from the 1970s.\n\n\n=== Probabilistic number theory ===\n\nTake a number at random between one and a million. How likely is it to be prime? This is just another way of asking how many primes there are between one and a million. Further: how many prime divisors will it have, on average? How many divisors will it have altogether, and with what likelihood? What is the probability that it will have many more or many fewer divisors or prime divisors than the average?\nMuch of probabilistic number theory can be seen as an important special case of the study of variables that are almost, but not quite, mutually independent. For example, the event that a random integer between one and a million be divisible by two and the event that it be divisible by three are almost independent, but not quite.\nIt is sometimes said that probabilistic combinatorics uses the fact that whatever happens with probability greater than \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n   must happen sometimes; one may say with equal justice that many applications of probabilistic number theory hinge on the fact that whatever is unusual must be rare. If certain algebraic objects (say, rational or integer solutions to certain equations) can be shown to be in the tail of certain sensibly defined distributions, it follows that there must be few of them; this is a very concrete non-probabilistic statement following from a probabilistic one.\nAt times, a non-rigorous, probabilistic approach leads to a number of heuristic algorithms and open problems, notably Cram\u00e9r's conjecture.\n\n\n=== Arithmetic combinatorics ===\n\nLet A be a set of N integers. Consider the set A + A = { m + n | m, n \u2208 A } consisting of all sums of two elements of A. Is A + A much larger than A? Barely larger? If A + A is barely larger than A, must A have plenty of arithmetic structure, for example, does A resemble an arithmetic progression?\nIf we begin from a fairly \"thick\" infinite set \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , does it contain many elements in arithmetic progression: \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  ,\n\n  \n    \n      \n        a\n        +\n        b\n        ,\n        a\n        +\n        2\n        b\n        ,\n        a\n        +\n        3\n        b\n        ,\n        \u2026\n        ,\n        a\n        +\n        10\n        b\n      \n    \n    {\\displaystyle a+b,a+2b,a+3b,\\ldots ,a+10b}\n  , say? Should it be possible to write large integers as sums of elements of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  ?\nThese questions are characteristic of arithmetic combinatorics. This is a presently coalescing field; it subsumes additive number theory (which concerns itself with certain very specific sets \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   of arithmetic significance, such as the primes or the squares) and, arguably, some of the geometry of numbers,\ntogether with some rapidly developing new material. Its focus on issues of growth and distribution accounts in part for its developing links with ergodic theory, finite group theory, model theory, and other fields. The term additive combinatorics is also used; however, the sets \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   being studied need not be sets of integers, but rather subsets of non-commutative groups, for which the multiplication symbol, not the addition symbol, is traditionally used; they can also be subsets of rings, in which case the growth of \n  \n    \n      \n        A\n        +\n        A\n      \n    \n    {\\displaystyle A+A}\n   and \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \u00b7\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   may be\ncompared.\n\n\n=== Computations in number theory ===\n\nWhile the word algorithm goes back only to certain readers of al-Khw\u0101rizm\u012b, careful descriptions of methods of solution are older than proofs: such methods (that is, algorithms) are as old as any recognisable mathematics\u2014ancient Egyptian, Babylonian, Vedic, Chinese\u2014whereas proofs appeared only with the Greeks of the classical period.\nAn interesting early case is that of what we now call the Euclidean algorithm. In its basic form (namely, as an algorithm for computing the greatest common divisor) it appears as Proposition 2 of Book VII in Elements, together with a proof of correctness. However, in the form that is often used in number theory (namely, as an algorithm for finding integer solutions to an equation \n  \n    \n      \n        a\n        x\n        +\n        b\n        y\n        =\n        c\n      \n    \n    {\\displaystyle ax+by=c}\n  ,\nor, what is the same, for finding the quantities whose existence is assured by the Chinese remainder theorem) it first appears in the works of \u0100ryabha\u1e6da (5th\u20136th century CE) as an algorithm called\nku\u1e6d\u1e6daka (\"pulveriser\"), without a proof of correctness.\nThere are two main questions: \"can we compute this?\" and \"can we compute it rapidly?\". Anyone can test whether a number is prime or, if it is not, split it into prime factors; doing so rapidly is another matter. We now know fast algorithms for testing primality, but, in spite of much work (both theoretical and practical), no truly fast algorithm for factoring.\nThe difficulty of a computation can be useful: modern protocols for encrypting messages (e.g., RSA) depend on functions that are known to all, but whose inverses (a) are known only to a chosen few, and (b) would take one too long a time to figure out on one's own. For example, these functions can be such that their inverses can be computed only if certain large integers are factorized. While many difficult computational problems outside number theory are known, most working encryption protocols nowadays are based on the difficulty of a few number-theoretical problems.\nOn a different note \u2014 some things may not be computable at all; in fact, this can be proven in some instances. For instance, in 1970, it was proven, as a solution to Hilbert's 10th problem, that there is no Turing machine which can solve all Diophantine equations. In particular, this means that, given a computably enumerable set of axioms, there are Diophantine equations for which there is no proof, starting from the axioms, of whether the set of equations has or does not have integer solutions. (We would necessarily be speaking of Diophantine equations for which there are no integer solutions, since, given a Diophantine equation with at least one solution, the solution itself provides a proof of the fact that a solution exists. We cannot prove, of course, that a particular Diophantine equation is of this kind, since this would imply that it has no solutions.)\n\n\n== Applications ==\nThe number-theorist Leonard Dickson (1874\u20131954) said \"Thank God that number theory is unsullied by any application\". Such a view is no longer applicable to number theory. In 1974, Donald Knuth said \"...virtually every theorem in elementary number theory arises in a natural, motivated way in connection with the problem of making computers do high-speed numerical calculations\".\nElementary number theory is taught in discrete mathematics courses for computer scientists; on the other hand, number theory also has applications to the continuous in numerical analysis. As well as the well-known applications to cryptography, there are also applications to many other areas of mathematics.\n\n\n== Prizes ==\nThe American Mathematical Society awards the Cole Prize in Number Theory. Moreover number theory is one of the three mathematical subdisciplines rewarded by the Fermat Prize.\n\n\n== See also ==\n\nAlgebraic function field\nFinite field\np-adic number\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Sources ==\n\nThis article incorporates material from the Citizendium article \"Number theory\", which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License but not under the GFDL.\n\n\n== Further reading ==\nTwo of the most popular introductions to the subject are:\n\nG. H. Hardy; E. M. Wright (2008) [1938]. An introduction to the theory of numbers (rev. by D. R. Heath-Brown and J. H. Silverman, 6th ed.). Oxford University Press. ISBN 978-0-19-921986-5. Retrieved 2016-03-02. Vinogradov, I. M. (2003) [1954]. Elements of Number Theory (reprint of the 1954 ed.). Mineola, NY: Dover Publications. Hardy and Wright's book is a comprehensive classic, though its clarity sometimes suffers due to the authors' insistence on elementary methods (Apostol n.d.).\nVinogradov's main attraction consists in its set of problems, which quickly lead to Vinogradov's own research interests; the text itself is very basic and close to minimal. Other popular first introductions are:\n\nIvan M. Niven; Herbert S. Zuckerman; Hugh L. Montgomery (2008) [1960]. An introduction to the theory of numbers (reprint of the 5th edition 1991 ed.). John Wiley & Sons. ISBN 978-81-265-1811-1. Retrieved 2016-02-28. \nKenneth H. Rosen (2010). Elementary Number Theory (6th ed.). Pearson Education. ISBN 978-0-321-71775-7. Retrieved 2016-02-28. Popular choices for a second textbook include:\n\nBorevich, A. I.; Shafarevich, Igor R. (1966). Number theory. Pure and Applied Mathematics. 20. Boston, MA: Academic Press. ISBN 978-0-12-117850-5. MR 0195803. Serre, Jean-Pierre (1996) [1973]. A course in arithmetic. Graduate texts in mathematics. 7. Springer. ISBN 978-0-387-90040-7. \n\n\n== External links ==\n Media related to Number theory at Wikimedia Commons\nHazewinkel, Michiel, ed. (2001) [1994], \"Number theory\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nNumber Theory Web", "linear programming": "Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).\nMore formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.\nLinear programs are problems that can be expressed in canonical form as\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  minimize\n                \n              \n              \n              \n                \n                  \n                    c\n                  \n                  \n                    \n                      T\n                    \n                  \n                \n                \n                  x\n                \n              \n            \n            \n              \n              \n                \n                  subject to\n                \n              \n              \n              \n                A\n                \n                  x\n                \n                \u2264\n                \n                  b\n                \n              \n            \n            \n              \n              \n                \n                  and\n                \n              \n              \n              \n                \n                  x\n                \n                \u2265\n                \n                  0\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&{\\text{minimize}}&&\\mathbf {c} ^{\\mathrm {T} }\\mathbf {x} \\\\&{\\text{subject to}}&&A\\mathbf {x} \\leq \\mathbf {b} \\\\&{\\text{and}}&&\\mathbf {x} \\geq \\mathbf {0} \\end{aligned}}}\n  where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and \n  \n    \n      \n        (\n        \u22c5\n        \n          )\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle (\\cdot )^{\\mathrm {T} }}\n   is the matrix transpose. The expression to be maximized or minimized is called the objective function (cTx in this case). The inequalities Ax \u2264 b and x \u2265 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector.\nLinear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics, and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.\n\n\n== History ==\n\nThe problem of solving a system of linear inequalities dates back at least as far as Fourier, who in 1827 published a method for solving them, and after whom the method of Fourier\u2013Motzkin elimination is named.\nIn 1939 a linear programming formulation of a problem that is equivalent to the general linear programming problem was given by the Soviet economist Leonid Kantorovich, who also proposed a method for solving it. It is a way he developed, during World War II, to plan expenditures and returns in order to reduce costs of the army and to increase losses imposed on the enemy. Kantorovich's work was initially neglected in the USSR. About the same time as Kantorovich, the Dutch-American economist T. C. Koopmans formulated classical economic problems as linear programs. Kantorovich and Koopmans later shared the 1975 Nobel prize in economics. In 1941, Frank Lauren Hitchcock also formulated transportation  problems as linear programs and gave a solution very similar to the later simplex method. Hitchcock had died in 1957 and the Nobel prize is not awarded posthumously.\nDuring 1946\u20131947, George B. Dantzig independently developed general linear programming formulation to use for planning problems in US Air Force. In 1947, Dantzig also invented the simplex method that for the first time efficiently tackled the linear programming problem in most cases. When Dantzig arranged a meeting with John von Neumann to discuss his simplex method, Neumann immediately conjectured the theory of duality by realizing that the problem he had been working in game theory was equivalent. Dantzig provided formal proof in an unpublished report \"A Theorem on Linear Inequalities\" on January 5, 1948. In the post-war years, many industries applied it in their daily planning.\nDantzig's original example was to find the best assignment of 70 people to 70 jobs. The computing power required to test all the permutations to select the best assignment is vast; the number of possible configurations exceeds the number of particles in the observable universe. However, it takes only a moment to find the optimum solution by posing the problem as a linear program and applying the simplex algorithm. The theory behind linear programming drastically reduces the number of possible solutions that must be checked.\nThe linear programming problem was first shown to be solvable in polynomial time by Leonid Khachiyan in 1979, but a larger theoretical and practical breakthrough in the field came in 1984 when Narendra Karmarkar introduced a new interior-point method for solving linear-programming problems.\n\n\n== Uses ==\nLinear programming is a widely used field of optimization for several reasons. Many practical problems in operations research can be expressed as linear programming problems. Certain special cases of linear programming, such as network flow problems and multicommodity flow problems are considered important enough to have generated much research on specialized algorithms for their solution. A number of algorithms for other types of optimization problems work by solving LP problems as sub-problems. Historically, ideas from linear programming have inspired many of the central concepts of optimization theory, such as duality, decomposition, and the importance of convexity and its generalizations. Likewise, linear programming was heavily used in the early formation of microeconomics and it is currently utilized in company management, such as planning, production, transportation, technology and other issues. Although the modern management issues are ever-changing, most companies would like to maximize profits and minimize costs with limited resources. Therefore, many issues can be characterized as linear programming problems.\n\n\n== Standard form ==\nStandard form is the usual and most intuitive form of describing a linear programming problem. It consists of the following three parts:\n\nA linear function to be maximizede.g. \n  \n    \n      \n        f\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        )\n        =\n        \n          c\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \n          c\n          \n            2\n          \n        \n        \n          x\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle f(x_{1},x_{2})=c_{1}x_{1}+c_{2}x_{2}}\n  Problem constraints of the following forme.g.\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  a\n                  \n                    11\n                  \n                \n                \n                  x\n                  \n                    1\n                  \n                \n                +\n                \n                  a\n                  \n                    12\n                  \n                \n                \n                  x\n                  \n                    2\n                  \n                \n              \n              \n                \u2264\n                \n                  b\n                  \n                    1\n                  \n                \n              \n            \n            \n              \n                \n                  a\n                  \n                    21\n                  \n                \n                \n                  x\n                  \n                    1\n                  \n                \n                +\n                \n                  a\n                  \n                    22\n                  \n                \n                \n                  x\n                  \n                    2\n                  \n                \n              \n              \n                \u2264\n                \n                  b\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  a\n                  \n                    31\n                  \n                \n                \n                  x\n                  \n                    1\n                  \n                \n                +\n                \n                  a\n                  \n                    32\n                  \n                \n                \n                  x\n                  \n                    2\n                  \n                \n              \n              \n                \u2264\n                \n                  b\n                  \n                    3\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{matrix}a_{11}x_{1}+a_{12}x_{2}&\\leq b_{1}\\\\a_{21}x_{1}+a_{22}x_{2}&\\leq b_{2}\\\\a_{31}x_{1}+a_{32}x_{2}&\\leq b_{3}\\\\\\end{matrix}}}\n  Non-negative variablese.g.\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                  \n                    1\n                  \n                \n                \u2265\n                0\n              \n            \n            \n              \n                \n                  x\n                  \n                    2\n                  \n                \n                \u2265\n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{matrix}x_{1}\\geq 0\\\\x_{2}\\geq 0\\end{matrix}}}\n  The problem is usually expressed in matrix form, and then becomes:\n\n  \n    \n      \n        max\n        {\n        \n          \n            c\n          \n          \n            \n              T\n            \n          \n        \n        \n          x\n        \n        \n        \n          |\n        \n        \n        A\n        \n          x\n        \n        \u2264\n        \n          b\n        \n        \u2227\n        \n          x\n        \n        \u2265\n        0\n        }\n      \n    \n    {\\displaystyle \\max\\{\\mathbf {c} ^{\\mathrm {T} }\\mathbf {x} \\;|\\;A\\mathbf {x} \\leq \\mathbf {b} \\land \\mathbf {x} \\geq 0\\}}\n  Other forms, such as minimization problems, problems with constraints on alternative forms, as well as problems involving negative variables can always be rewritten into an equivalent problem in standard form.\n\n\n=== Example ===\nSuppose that a farmer has a piece of farm land, say L km2, to be planted with either wheat or barley or some combination of the two. The farmer has a limited amount of fertilizer, F kilograms, and pesticide, P kilograms. Every square kilometer of wheat requires F1 kilograms of fertilizer and P1 kilograms of pesticide, while every square kilometer of barley requires F2 kilograms of fertilizer and P2 kilograms of pesticide. Let S1 be the selling price of wheat per square kilometer, and S2 be the selling price of barley. If we denote the area of land planted with wheat and barley by x1 and x2 respectively, then profit can be maximized by choosing optimal values for x1 and x2. This problem can be expressed with the following linear programming problem in the standard form:\n\nIn matrix form this becomes:\n\nmaximize \n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    S\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    S\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}S_{1}&S_{2}\\end{bmatrix}}{\\begin{bmatrix}x_{1}\\\\x_{2}\\end{bmatrix}}}\n  \nsubject to \n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  1\n                \n              \n              \n                \n                  \n                    F\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    F\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    P\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    P\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \u2264\n        \n          \n            [\n            \n              \n                \n                  L\n                \n              \n              \n                \n                  F\n                \n              \n              \n                \n                  P\n                \n              \n            \n            ]\n          \n        \n        ,\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \u2265\n        \n          \n            [\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{bmatrix}1&1\\\\F_{1}&F_{2}\\\\P_{1}&P_{2}\\end{bmatrix}}{\\begin{bmatrix}x_{1}\\\\x_{2}\\end{bmatrix}}\\leq {\\begin{bmatrix}L\\\\F\\\\P\\end{bmatrix}},\\,{\\begin{bmatrix}x_{1}\\\\x_{2}\\end{bmatrix}}\\geq {\\begin{bmatrix}0\\\\0\\end{bmatrix}}.}\n  \n\n\n== Augmented form (slack form) ==\nLinear programming problems can be converted into an augmented form in order to apply the common form of the simplex algorithm. This form introduces non-negative slack variables to replace inequalities with equalities in the constraints. The problems can then be written in the following block matrix form:\n\nMaximize \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  :\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  \u2212\n                  \n                    \n                      c\n                    \n                    \n                      T\n                    \n                  \n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    A\n                  \n                \n                \n                  \n                    I\n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  z\n                \n              \n              \n                \n                  \n                    x\n                  \n                \n              \n              \n                \n                  \n                    s\n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  \n                    b\n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}1&-\\mathbf {c} ^{T}&0\\\\0&\\mathbf {A} &\\mathbf {I} \\end{bmatrix}}{\\begin{bmatrix}z\\\\\\mathbf {x} \\\\\\mathbf {s} \\end{bmatrix}}={\\begin{bmatrix}0\\\\\\mathbf {b} \\end{bmatrix}}}\n  \n\n  \n    \n      \n        \n          x\n        \n        \u2265\n        0\n        ,\n        \n          s\n        \n        \u2265\n        0\n      \n    \n    {\\displaystyle \\mathbf {x} \\geq 0,\\mathbf {s} \\geq 0}\n  where \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {s} }\n   are the newly introduced slack variables, \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   are the decision variables, and \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n   is the variable to be maximized.\n\n\n=== Example ===\nThe example above is converted into the following augmented form:\n\nwhere \n  \n    \n      \n        \n          x\n          \n            3\n          \n        \n        ,\n        \n          x\n          \n            4\n          \n        \n        ,\n        \n          x\n          \n            5\n          \n        \n      \n    \n    {\\displaystyle x_{3},x_{4},x_{5}}\n   are (non-negative) slack variables, representing in this example the unused area, the amount of unused fertilizer, and the amount of unused pesticide.\nIn matrix form this becomes:\n\nMaximize \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  :\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  \u2212\n                  \n                    S\n                    \n                      1\n                    \n                  \n                \n                \n                  \u2212\n                  \n                    S\n                    \n                      2\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  1\n                \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    F\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    F\n                    \n                      2\n                    \n                  \n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    P\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    P\n                    \n                      2\n                    \n                  \n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  z\n                \n              \n              \n                \n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      5\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  L\n                \n              \n              \n                \n                  F\n                \n              \n              \n                \n                  P\n                \n              \n            \n            ]\n          \n        \n        ,\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    x\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      3\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      4\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      5\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \u2265\n        0.\n      \n    \n    {\\displaystyle {\\begin{bmatrix}1&-S_{1}&-S_{2}&0&0&0\\\\0&1&1&1&0&0\\\\0&F_{1}&F_{2}&0&1&0\\\\0&P_{1}&P_{2}&0&0&1\\\\\\end{bmatrix}}{\\begin{bmatrix}z\\\\x_{1}\\\\x_{2}\\\\x_{3}\\\\x_{4}\\\\x_{5}\\end{bmatrix}}={\\begin{bmatrix}0\\\\L\\\\F\\\\P\\end{bmatrix}},\\,{\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3}\\\\x_{4}\\\\x_{5}\\end{bmatrix}}\\geq 0.}\n  \n\n\n== Duality ==\n\nEvery linear programming problem, referred to as a primal problem, can be converted into a dual problem, which provides an upper bound to the optimal value of the primal problem. In matrix form, we can express the primal problem as:\n\nMaximize cTx  subject to Ax \u2264 b, x \u2265 0;\nwith the corresponding symmetric dual problem,\nMinimize  bTy  subject to ATy \u2265 c, y \u2265 0.An alternative primal formulation is:\n\nMaximize cTx subject to Ax \u2264 b;\nwith the corresponding asymmetric dual problem,\nMinimize  bTy subject to ATy = c, y \u2265 0.There are two ideas fundamental to duality theory. One is the fact that (for the symmetric dual) the dual of a dual linear program is the original primal linear program. Additionally, every feasible solution for a linear program gives a bound on the optimal value of the objective function of its dual.  The weak duality theorem states that the objective function value of the dual at any feasible solution is always greater than or equal to the objective function value of the primal at any feasible solution. The strong duality theorem states that if the primal has an optimal solution, x*, then the dual also has an optimal solution, y*, and cTx*=bTy*.\nA linear program can also be unbounded or infeasible. Duality theory tells us that if the primal is unbounded then the dual is infeasible by the weak duality theorem. Likewise, if the dual is unbounded, then the primal must be infeasible. However, it is possible for both the dual and the primal to be infeasible. As an example, consider the linear program:\n\n\n=== Example ===\nRevisit the above example of the farmer who may grow wheat and barley with the set provision of some L land, F fertilizer and P pesticide. Assume now that y unit prices for each of these means of production (inputs) are set by a planning board. The planning board's job is to minimize the total cost of procuring the set amounts of inputs while providing the farmer with a floor on the unit price of each of his crops (outputs), S1 for wheat and S2 for barley. This corresponds to the following linear programming problem:\n\nIn matrix form this becomes:\n\nMinimize: \n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  L\n                \n                \n                  F\n                \n                \n                  P\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    y\n                    \n                      L\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      F\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      P\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}L&F&P\\end{bmatrix}}{\\begin{bmatrix}y_{L}\\\\y_{F}\\\\y_{P}\\end{bmatrix}}}\n  \nsubject to: \n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  \n                    F\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    P\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  1\n                \n                \n                  \n                    F\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    P\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    y\n                    \n                      L\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      F\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      P\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \u2265\n        \n          \n            [\n            \n              \n                \n                  \n                    S\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    S\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n        \n        \n          \n            [\n            \n              \n                \n                  \n                    y\n                    \n                      L\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      F\n                    \n                  \n                \n              \n              \n                \n                  \n                    y\n                    \n                      P\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \u2265\n        0.\n      \n    \n    {\\displaystyle {\\begin{bmatrix}1&F_{1}&P_{1}\\\\1&F_{2}&P_{2}\\end{bmatrix}}{\\begin{bmatrix}y_{L}\\\\y_{F}\\\\y_{P}\\end{bmatrix}}\\geq {\\begin{bmatrix}S_{1}\\\\S_{2}\\end{bmatrix}},\\,{\\begin{bmatrix}y_{L}\\\\y_{F}\\\\y_{P}\\end{bmatrix}}\\geq 0.}\n  The primal problem deals with physical quantities. With all inputs available in limited quantities, and assuming the unit prices of all outputs is known, what quantities of outputs to produce so as to maximize total revenue? The dual problem deals with economic values. With floor guarantees on all output unit prices, and assuming the available quantity of all inputs is known, what input unit pricing scheme to set so as to minimize total expenditure?\nTo each variable in the primal space corresponds an inequality to satisfy in the dual space, both indexed by output type. To each inequality to satisfy in the primal space corresponds a variable in the dual space, both indexed by input type.\nThe coefficients that bound the inequalities in the primal space are used to compute the objective in the dual space, input quantities in this example. The coefficients used to compute the objective in the primal space bound the inequalities in the dual space, output unit prices in this example.\nBoth the primal and the dual problems make use of the same matrix. In the primal space, this matrix expresses the consumption of physical quantities of inputs necessary to produce set quantities of outputs. In the dual space, it expresses the creation of the economic values associated with the outputs from set input unit prices.\nSince each inequality can be replaced by an equality and a slack variable, this means each primal variable corresponds to a dual slack variable, and each dual variable corresponds to a primal slack variable.  This relation allows us to speak about complementary slackness.\n\n\n=== Another example ===\nSometimes, one may find it more intuitive to obtain the dual program without looking at the program matrix. Consider the following linear program:\n\nWe have m + n conditions and all variables are non-negative. We shall define m + n dual variables: yj and si. We get:\n\nSince this is a minimization problem, we would like to obtain a dual program that is a lower bound of the primal. In other words, we would like the sum of all right hand side of the constraints to be the maximal under the condition that for each primal variable the sum of its coefficients do not exceed its coefficient in the linear function. For example, x1 appears in n + 1 constraints. If we sum its constraints' coefficients we get a1,1y1 + a1,2y2 + ... + a1,nyn + f1s1. This sum must be at most c1. As a result, we get:\n\nNote that we assume in our calculations steps that the program is in standard form. However, any linear program may be transformed to standard form and it is therefore not a limiting factor.\n\n\n== Variations ==\n\n\n=== Covering/packing dualities ===\nA covering LP is a linear program of the form:\n\nMinimize:  bTy,\nsubject to: ATy \u2265 c, y \u2265 0,such that the matrix A and the vectors b and c are non-negative.\nThe dual of a covering LP is a packing LP, a linear program of the form:\n\nMaximize: cTx,\nsubject to: Ax \u2264 b, x \u2265 0,such that the matrix A and the vectors b and c are non-negative.\n\n\n==== Examples ====\nCovering and packing LPs commonly arise as a linear programming relaxation of a combinatorial problem and are important in the study of approximation algorithms. For example, the LP relaxations of the set packing problem, the independent set problem, and the matching problem are packing LPs. The LP relaxations of the set cover problem, the vertex cover problem, and the dominating set problem are also covering LPs.\nFinding a fractional coloring of a graph is another example of a covering LP. In this case, there is one constraint for each vertex of the graph and one variable for each independent set of the graph.\n\n\n== Complementary slackness ==\nIt is possible to obtain an optimal solution to the dual when only an optimal solution to the primal is known using the complementary slackness theorem. The theorem states:\nSuppose that x = (x1, x2, ... , xn) is primal feasible and that y = (y1, y2, ... , ym) is dual feasible. Let (w1, w2, ..., wm) denote the corresponding primal slack variables, and let (z1, z2, ... , zn) denote the corresponding dual slack variables. Then x and y are optimal for their respective problems if and only if\n\nxj zj = 0, for j = 1, 2, ... , n, and\nwi yi = 0, for i = 1, 2, ... , m.So if the i-th slack variable of the primal is not zero, then the i-th variable of the dual is equal to zero. Likewise, if the j-th slack variable of the dual is not zero, then the j-th variable of the primal is equal to zero.\nThis necessary condition for optimality conveys a fairly simple economic principle.  In standard form (when maximizing), if there is slack in a constrained primal resource (i.e., there are \"leftovers\"), then additional quantities of that resource must have no value.  Likewise, if there is slack in the dual (shadow) price non-negativity constraint requirement, i.e., the price is not zero, then there must be scarce supplies (no \"leftovers\").\n\n\n== Theory ==\n\n\n=== Existence of optimal solutions ===\nGeometrically, the linear constraints define the feasible region, which is a convex polyhedron. A linear function is a convex function, which implies that every local minimum is a global minimum; similarly, a linear function is a concave function, which implies that every local maximum is a global maximum.\nAn optimal solution need not exist, for two reasons. First, if two constraints are inconsistent, then no feasible solution exists: For instance, the constraints x \u2265 2 and x \u2264 1 cannot be satisfied jointly; in this case, we say that the LP is infeasible. Second, when the polytope is unbounded in the direction of the gradient of the objective function (where the gradient of the objective function is the vector of the coefficients of the objective function), then no optimal value is attained because it is always possible to do better than any finite value of the objective function.\n\n\n=== Optimal vertices (and rays) of polyhedra ===\nOtherwise, if a feasible solution exists and if the constraint set is bounded, then the optimum value is always attained on the boundary of the constraint set, by the maximum principle for convex functions (alternatively, by the minimum principle for concave functions) since linear functions are both convex and concave. However, some problems have distinct optimal solutions: For example, the problem of finding a feasible solution to a system of linear inequalities is a linear programming problem in which the objective function is the zero function (that is, the constant function taking the value zero everywhere): For this feasibility problem with the zero-function for its objective-function, if there are two distinct solutions, then every convex combination of the solutions is a solution.\nThe vertices of the polytope are also called basic feasible solutions. The reason for this choice of name is as follows. Let d denote the number of variables. Then the fundamental theorem of linear inequalities implies (for feasible problems) that for every vertex x* of the LP feasible region, there exists a set of d (or fewer) inequality constraints from the LP such that, when we treat those d constraints as equalities, the unique solution is x*. Thereby we can study these vertices by means of looking at certain subsets of the set of all constraints (a discrete set), rather than the continuum of LP solutions. This principle underlies the simplex algorithm for solving linear programs.\n\n\n== Algorithms ==\n\n\n=== Basis exchange algorithms ===\n\n\n==== Simplex algorithm of Dantzig ====\nThe simplex algorithm, developed by George Dantzig in 1947, solves LP problems by constructing a feasible solution at a vertex of the polytope and then walking along a path on the edges of the polytope to vertices with non-decreasing values of the objective function until an optimum is reached for sure. In many practical problems, \"stalling\" occurs: Many pivots are made with no increase in the objective function. In rare practical problems, the usual versions of the simplex algorithm may actually \"cycle\". To avoid cycles, researchers developed new pivoting rules.In practice, the simplex algorithm is quite efficient and can be guaranteed to find the global optimum if certain precautions against cycling are taken. The simplex algorithm has been proved to solve \"random\" problems efficiently, i.e. in a cubic number of steps, which is similar to its behavior on practical problems.However, the simplex algorithm has poor worst-case behavior: Klee and Minty constructed a family of linear programming problems for which the simplex method takes a number of steps exponential in the problem size. In fact, for some time it was not known whether the linear programming problem was solvable in polynomial time, i.e. of complexity class P.\n\n\n==== Criss-cross algorithm ====\nLike the simplex algorithm of Dantzig, the criss-cross algorithm is a basis-exchange algorithm that pivots between bases. However, the criss-cross algorithm need not maintain feasibility, but can pivot rather from a feasible basis to an infeasible basis. The criss-cross algorithm does not have polynomial time-complexity for linear programming. Both algorithms visit all 2D corners of a (perturbed) cube in dimension D, the Klee\u2013Minty cube, in the worst case.\n\n\n=== Interior point ===\nIn contrast to the simplex algorithm, which finds an optimal solution by traversing the edges between vertices on a polyhedral set, interior-point methods move through the interior of the feasible region.\n\n\n==== Ellipsoid algorithm, following Khachiyan ====\nThis is the first worst-case polynomial-time algorithm ever found for linear programming.  To solve a problem which has n variables and can be encoded in L input bits, this algorithm uses O(n4L) pseudo-arithmetic operations on numbers with O(L) digits. Leonid Khachiyan solved this long-standing complexity issue in 1979 with the introduction of the ellipsoid method. The convergence analysis has (real-number) predecessors, notably the iterative methods developed by Naum Z. Shor and the approximation algorithms by Arkadi Nemirovski and D. Yudin.\n\n\n==== Projective algorithm of Karmarkar ====\n\nKhachiyan's algorithm was of landmark importance for establishing the polynomial-time solvability of linear programs.  The algorithm was not a computational break-through, as the simplex method is more efficient for all but specially constructed families of linear programs.\nHowever, Khachiyan's algorithm inspired new lines of research in linear programming. In 1984, N. Karmarkar proposed a projective method for linear programming.  Karmarkar's algorithm improved on Khachiyan's worst-case polynomial bound (giving \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            3.5\n          \n        \n        L\n        )\n      \n    \n    {\\displaystyle O(n^{3.5}L)}\n  ). Karmarkar claimed that his algorithm was much faster in practical LP than the simplex method, a claim that created great interest in interior-point methods. Since Karmarkar's discovery, many interior-point methods have been proposed and analyzed.\n\n\n==== Affine scaling ====\n\nAffine scaling is one of the oldest interior point methods to be developed. It was developed in the Soviet Union in the mid-1960s, but didn't receive much attention until the discovery of Karmarkar's algorithm, after which affine scaling was reinvented multiple times and presented as a simplified version of Karmarkar's. Affine scaling amounts to doing gradient descent steps within the feasible region, while rescaling the problem to make sure the steps move toward the optimum faster.\n\n\n==== Path-following algorithms ====\nFor both theoretical and practical purposes, barrier function or path-following methods have been the most popular interior point methods since the 1990s.\n\n\n=== Comparison of interior-point methods and simplex algorithms ===\nThe current opinion is that the efficiencies of good implementations of simplex-based methods and interior point methods are similar for routine applications of linear programming.  However, for specific types of LP problems, it may be that one type of solver is better than another (sometimes much better), and that the structure of the solutions generated by interior point methods versus simplex-based methods are significantly different with the support set of active variables being typically smaller for the later one.\n\n\n=== Approximate algorithms for covering/packing LPs ===\nCovering and packing LPs can be solved approximately in nearly-linear time. That is, if matrix A is of dimension n\u00d7m and has N non-zero entries, then there exist algorithms that run in time O(N\u00b7(log N)O(1)/\u03b5O(1)) and produce O(1\u00b1\u03b5) approximate solutions to given covering and packing LPs. The best known sequential algorithm of this kind runs in time O(N + (log N)\u00b7(n+m)/\u03b52), and the best known parallel algorithm of this kind runs in O((log N)2/\u03b53) iterations, each requiring only a matrix-vector multiplication which is highly parallelizable.\n\n\n== Open problems and recent work ==\nThere are several open problems in the theory of linear programming, the solution of which would represent fundamental breakthroughs in mathematics and potentially major advances in our ability to solve large-scale linear programs.\n\nDoes LP admit a strongly polynomial-time algorithm?\nDoes LP admit a strongly polynomial-time algorithm to find a strictly complementary solution?\nDoes LP admit a polynomial-time algorithm in the real number (unit cost) model of computation?This closely related set of problems has been cited by Stephen Smale as among the 18 greatest unsolved problems of the 21st century.  In Smale's words, the third version of the problem \"is the main unsolved problem of linear programming theory.\"  While algorithms exist to solve linear programming in weakly polynomial time, such as the ellipsoid methods and interior-point techniques, no algorithms have yet been found that allow strongly polynomial-time performance in the number of constraints and the number of variables.  The development of such algorithms would be of great theoretical interest, and perhaps allow practical gains in solving large LPs as well.\nAlthough the Hirsch conjecture was recently disproved for higher dimensions, it still leaves the following questions open.\n\nAre there pivot rules which lead to polynomial-time simplex variants?\nDo all polytopal graphs have polynomially bounded diameter?These questions relate to the performance analysis and development of simplex-like methods.  The immense efficiency of the simplex algorithm in practice despite its exponential-time theoretical performance hints that there may be variations of simplex that run in polynomial or even strongly polynomial time.  It would be of great practical and theoretical significance to know whether any such variants exist, particularly as an approach to deciding if LP can be solved in strongly polynomial time.\nThe simplex algorithm and its variants fall in the family of edge-following algorithms, so named because they solve linear programming problems by moving from vertex to vertex along edges of a polytope.  This means that their theoretical performance is limited by the maximum number of edges between any two vertices on the LP polytope.  As a result, we are interested in knowing the maximum graph-theoretical diameter of polytopal graphs.  It has been proved that all polytopes have subexponential diameter. The recent disproof of the Hirsch conjecture is the first step to prove whether any polytope has superpolynomial diameter. If any such polytopes exist, then no edge-following variant can run in polynomial time. Questions about polytope diameter are of independent mathematical interest.\nSimplex pivot methods preserve primal (or dual) feasibility.  On the other hand, criss-cross pivot methods do not preserve (primal or dual) feasibility\u2014they may visit primal feasible, dual feasible or primal-and-dual infeasible bases in any order.  Pivot methods of this type have been studied since the 1970s.  Essentially, these methods attempt to find the shortest pivot path on the arrangement polytope under the linear programming problem.  In contrast to polytopal graphs, graphs of arrangement polytopes are known to have small diameter, allowing the possibility of strongly polynomial-time criss-cross pivot algorithm without resolving questions about the diameter of general polytopes.\n\n\n== Integer unknowns ==\nIf all of the unknown variables are required to be integers, then the problem is called an integer programming (IP) or integer linear programming (ILP) problem.  In contrast to linear programming, which can be solved efficiently in the worst case, integer programming problems are in many practical situations (those with bounded variables) NP-hard. 0\u20131 integer programming or binary integer programming (BIP) is the special case of integer programming where variables are required to be 0 or 1 (rather than arbitrary integers). This problem is also classified as NP-hard, and in fact the decision version was one of Karp's 21 NP-complete problems.\nIf only some of the unknown variables are required to be integers, then the problem is called a mixed integer programming (MIP) problem.  These are generally also NP-hard because they are even more general than ILP programs.\nThere are however some important subclasses of IP and MIP problems that are efficiently solvable, most notably problems where the constraint matrix is totally unimodular and the right-hand sides of the constraints are integers or \u2013 more general \u2013 where the system has the total dual integrality (TDI) property.\nAdvanced algorithms for solving integer linear programs include:\n\ncutting-plane method\nBranch and bound\nBranch and cut\nBranch and price\nif the problem has some extra structure, it may be possible to apply delayed column generation.Such integer-programming algorithms are discussed by Padberg and in Beasley.\n\n\n== Integral linear programs ==\nA linear program in real variables is said to be integral if it has at least one optimal solution which is integral. Likewise, a polyhedron \n  \n    \n      \n        P\n        =\n        {\n        x\n        \u2223\n        A\n        x\n        \u2265\n        0\n        }\n      \n    \n    {\\displaystyle P=\\{x\\mid Ax\\geq 0\\}}\n   is said to be integral if for all bounded feasible objective functions c, the linear program \n  \n    \n      \n        {\n        max\n        c\n        x\n        \u2223\n        x\n        \u2208\n        P\n        }\n      \n    \n    {\\displaystyle \\{\\max cx\\mid x\\in P\\}}\n   has an optimum \n  \n    \n      \n        \n          x\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle x^{*}}\n   with integer coordinates. As observed by Edmonds and Giles in 1977, one can equivalently say that the polyhedron \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n   is integral if for every bounded feasible integral objective function c, the optimal value of the linear program \n  \n    \n      \n        {\n        max\n        c\n        x\n        \u2223\n        x\n        \u2208\n        P\n        }\n      \n    \n    {\\displaystyle \\{\\max cx\\mid x\\in P\\}}\n   is an integer.\nIntegral linear programs are of central importance in the polyhedral aspect of combinatorial optimization since they provide an alternate characterization of a problem. Specifically, for any problem, the convex hull of the solutions is an integral polyhedron; if this polyhedron has a nice/compact description, then we can efficiently find the optimal feasible solution under any linear objective. Conversely, if we can prove that a linear programming relaxation is integral, then it is the desired description of the convex hull of feasible (integral) solutions.\nNote that terminology is not consistent throughout the literature, so one should be careful to distinguish the following two concepts,\n\nin an integer linear program, described in the previous section, variables are forcibly constrained to be integers, and this problem is NP-hard in general,\nin an integral linear program, described in this section, variables are not constrained to be integers but rather one has proven somehow that the continuous problem always has an integral optimal value (assuming c is integral), and this optimal value may be found efficiently since all polynomial-size linear programs can be solved in polynomial time.One common way of proving that a polyhedron is integral is to show that it is totally unimodular. There are other general methods including the integer decomposition property and total dual integrality. Other specific well-known integral LPs include the matching polytope, lattice polyhedra, submodular flow polyhedra, and the intersection of 2 generalized polymatroids/g-polymatroids \u2013 e.g. see Schrijver 2003.\nA bounded integral polyhedron is sometimes called a convex lattice polytope, particularly in two dimensions.\n\n\n== Solvers and scripting (programming) languages ==\nPermissive licenses:\n\nCopyleft (reciprocal) licenses:\n\nMINTO (Mixed Integer Optimizer, an integer programming solver which uses branch and bound algorithm) has publicly available source code but is not open source.\nProprietary licenses:\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\nKantorovich, L. V. (1940). \"\u041e\u0431 \u043e\u0434\u043d\u043e\u043c \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u043c \u043c\u0435\u0442\u043e\u0434\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u044d\u043a\u0441\u0442\u0440\u0435\u043c\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\" [A new method of solving some classes of extremal problems]. Doklady Akad Sci SSSR. 28: 211\u2013214. \nF. L. Hitchcock: The distribution of a product from several sources to numerous localities, Journal of Mathematics and Physics, 20, 1941, 224\u2013230.\nG.B Dantzig: Maximization of a linear function of variables subject to linear inequalities, 1947. Published pp. 339\u2013347 in T.C. Koopmans (ed.):Activity Analysis of Production and Allocation, New York-London 1951 (Wiley & Chapman-Hall)\nJ. E. Beasley, editor. Advances in Linear and Integer Programming. Oxford Science, 1996. (Collection of surveys)\nBland, Robert G. (1977). \"New Finite Pivoting Rules for the Simplex Method\". Mathematics of Operations Research. 2 (2): 103\u2013107. doi:10.1287/moor.2.2.103. JSTOR 3689647. \nKarl-Heinz Borgwardt, The Simplex Algorithm: A Probabilistic Analysis, Algorithms and Combinatorics, Volume 1, Springer-Verlag, 1987. (Average behavior on random problems)\nRichard W. Cottle, ed. The Basic George B. Dantzig. Stanford Business Books, Stanford University Press, Stanford, California, 2003. (Selected papers by George B. Dantzig)\nGeorge B. Dantzig and Mukund N. Thapa. 1997. Linear programming 1: Introduction. Springer-Verlag.\nGeorge B. Dantzig and Mukund N. Thapa. 2003. Linear Programming 2: Theory and Extensions. Springer-Verlag. (Comprehensive, covering e.g. pivoting and interior-point algorithms, large-scale problems, decomposition following Dantzig\u2013Wolfe and Benders, and introducing stochastic programming.)\nEdmonds, Jack; Giles, Rick (1977). \"A Min-Max Relation for Submodular Functions on Graphs\". Studies in Integer Programming. Annals of Discrete Mathematics. 1. pp. 185\u2013204. doi:10.1016/S0167-5060(08)70734-9. ISBN 978-0-7204-0765-5. \nFukuda, Komei; Terlaky, Tam\u00e1s (1997).  Thomas M. Liebling and Dominique de Werra, eds. \"Criss-cross methods: A fresh view on pivot algorithms\". Mathematical Programming: Series B. Amsterdam: North-Holland Publishing Co. 79 (1\u20143): 369\u2013395. doi:10.1007/BF02614325. MR 1464775. CS1 maint: Uses editors parameter (link) \nGondzio, Jacek; Terlaky, Tam\u00e1s (1996). \"3 A computational view of interior point methods\".  In J. E. Beasley. Advances in linear and integer programming. Oxford Lecture Series in Mathematics and its Applications. 4. New York: Oxford University Press. pp. 103\u2013144. MR 1438311. Postscript file at website of Gondzio and at McMaster University website of Terlaky. \nMurty, Katta G. (1983). Linear programming. New York: John Wiley & Sons, Inc. pp. xix+482. ISBN 0-471-09725-X. MR 0720547. (comprehensive reference to classical approaches). \nEvar D. Nering and Albert W. Tucker, 1993, Linear Programs and Related Problems, Academic Press. (elementary)\nM. Padberg, Linear Optimization and Extensions, Second Edition, Springer-Verlag, 1999. (carefully written account of primal and dual simplex algorithms and projective algorithms, with an introduction to integer linear programming \u2013 featuring the traveling salesman problem for Odysseus.)\nChristos H. Papadimitriou and Kenneth Steiglitz, Combinatorial Optimization: Algorithms and Complexity, Corrected republication with a new preface, Dover. (computer science)\nMichael J. Todd (February 2002). \"The many facets of linear programming\". Mathematical Programming. 91 (3): 417\u2013436. doi:10.1007/s101070100261.  (Invited survey, from the International Symposium on Mathematical Programming.)\nVanderbei, Robert J. (2001). Linear Programming: Foundations and Extensions. Springer Verlag. \nVazirani, Vijay V. (2001). Approximation Algorithms. Springer-Verlag. ISBN 3-540-65367-8.  (Computer science)\n\n\n== Further reading ==\nA reader may consider beginning with Nering and Tucker, with the first volume of Dantzig and Thapa, or with Williams.\n\nDmitris Alevras and Manfred W. Padberg, Linear Optimization and Extensions: Problems and Solutions, Universitext, Springer-Verlag, 2001. (Problems from Padberg with solutions.)\nMark de Berg, Marc van Kreveld, Mark Overmars, and Otfried Schwarzkopf (2000). Computational Geometry (2nd revised ed.). Springer-Verlag. ISBN 3-540-65620-0. CS1 maint: Multiple names: authors list (link)  Chapter 4: Linear Programming: pp. 63\u201394. Describes a randomized half-plane intersection algorithm for linear programming.\nMichael R. Garey and David S. Johnson (1979). Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. Freeman. ISBN 0-7167-1045-5.  A6: MP1: INTEGER PROGRAMMING, pg.245. (computer science, complexity theory)\nBernd G\u00e4rtner, Ji\u0159\u00ed Matou\u0161ek (2006). Understanding and Using Linear Programming, Berlin: Springer. ISBN 3-540-30697-8 (elementary introduction for mathematicians and computer scientists)\nCornelis Roos, Tam\u00e1s Terlaky, Jean-Philippe Vial, Interior Point Methods for Linear Optimization, Second Edition, Springer-Verlag, 2006. (Graduate level)\nAlexander Schrijver (2003). Combinatorial optimization: polyhedra and efficiency. Springer. \nAlexander Schrijver, Theory of Linear and Integer Programming. John Wiley & sons, 1998, ISBN 0-471-98232-6 (mathematical)\nGerard Sierksma; Yori Zwols (2015). Linear and Integer Optimization: Theory and Practice. CRC Press. ISBN 978-1-498-71016-9. \nGerard Sierksma; Diptesh Ghosh (2010). Networks in Action; Text and Computer Exercises in Network Optimization. Springer. ISBN 978-1-4419-5512-8.  (linear optimization modeling)\nH. P. Williams, Model Building in Mathematical Programming, Third revised Edition, 1990. (Modeling)\nStephen J. Wright, 1997, Primal-Dual Interior-Point Methods, SIAM. (Graduate level)\nYinyu Ye, 1997, Interior Point Algorithms: Theory and Analysis, Wiley. (Advanced graduate-level)\nZiegler, G\u00fcnter M., Chapters 1\u20133 and 6\u20137 in Lectures on Polytopes, Springer-Verlag, New York, 1994. (Geometry)\n\n\n== External links ==\nGuidance On Formulating LP Problems\nMathematical Programming Glossary\nThe Linear Programming FAQ\nBenchmarks For Optimisation Software", "linear algebra": "Linear algebra is the branch of mathematics concerning linear equations such as \n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        =\n        b\n        ,\n      \n    \n    {\\displaystyle a_{1}x_{1}+\\cdots +a_{n}x_{n}=b,}\n  linear functions such as\n\n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        \u21a6\n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        \u2026\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle (x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\ldots +a_{n}x_{n},}\n  and their representations through matrices and vector spaces.Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems, which cannot be modeled with linear algebra, linear algebra is often used as a first-order approximation.\n\n\n== History ==\n\n\n=== From the study of determinants and matrices to modern linear algebra ===\nThe study of linear algebra first emerged from the introduction of determinants, for solving systems of linear equations. Determinants were considered by Leibniz in 1693, and subsequently, in 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's Rule. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for \"womb\". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".In 1882, H\u00fcseyin Tevfik Pasha wrote the book titled \"Linear Algebra\". The first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.\n\n\n=== Educational history ===\nLinear algebra first appeared in American graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s. Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do \"matrix algebra, formerly reserved for college\" in the 1960s.  In France during the 1960s, educators attempted to teach linear algebra through finite-dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum. In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based \"matrix orientation\" as opposed to a theoretical orientation.\nReviews of the teaching of linear algebra call for stress on visualization and geometric interpretation of theoretical ideas, and to include the jewel in the crown of linear algebra, the singular-value decomposition (SVD), as 'so many other disciplines use it'.  To better suit 21st century applications, such as data mining and uncertainty analysis, linear algebra can be based upon the SVD instead of Gaussian Elimination.\n\n\n== Scope of study ==\n\n\n=== Vector spaces ===\n\nThe main structures of linear algebra are vector spaces. A vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations satisfying the following axioms.  \nElements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The operations of addition and multiplication in a vector space must satisfy the following axioms. In the list below, let u, v and w be arbitrary vectors in V, and a and b scalars in F.\n\nThe first four axioms are those of V being an abelian group under vector addition. Elements of a vector space may have various nature; for example, they can be sequences, functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.\n\n\n=== Linear transformations ===\n\nSimilarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map\n\n  \n    \n      \n        T\n        :\n        V\n        \u2192\n        W\n      \n    \n    {\\displaystyle T:V\\to W}\n  that is compatible with addition and scalar multiplication:\n\n  \n    \n      \n        T\n        (\n        u\n        +\n        v\n        )\n        =\n        T\n        (\n        u\n        )\n        +\n        T\n        (\n        v\n        )\n        ,\n        \n        T\n        (\n        a\n        v\n        )\n        =\n        a\n        T\n        (\n        v\n        )\n      \n    \n    {\\displaystyle T(u+v)=T(u)+T(v),\\quad T(av)=aT(v)}\n  for any vectors u,v \u2208 V and a scalar a \u2208 F.\nAdditionally for any vectors u, v \u2208 V and scalars a, b \u2208 F:\n\n  \n    \n      \n        \n        T\n        (\n        a\n        u\n        +\n        b\n        v\n        )\n        =\n        T\n        (\n        a\n        u\n        )\n        +\n        T\n        (\n        b\n        v\n        )\n        =\n        a\n        T\n        (\n        u\n        )\n        +\n        b\n        T\n        (\n        v\n        )\n      \n    \n    {\\displaystyle \\quad T(au+bv)=T(au)+T(bv)=aT(u)+bT(v)}\n  When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.\nLinear transformations have geometric significance. For example, 2 \u00d7 2 real matrices denote standard planar mappings that preserve the origin.\n\n\n=== Subspaces, span, and basis ===\n\nAgain, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors v1, v2, ..., vk:\n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          v\n          \n            1\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        \n          v\n          \n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            k\n          \n        \n        \n          v\n          \n            k\n          \n        \n        ,\n      \n    \n    {\\displaystyle a_{1}v_{1}+a_{2}v_{2}+\\cdots +a_{k}v_{k},}\n  where a1, a2, ..., ak are scalars. The set of all linear combinations of vectors v1, v2, ..., vk is called their span, which forms a subspace.\nA linear combination of any system of vectors with all zero coefficients is the zero vector of V. If this is the only way to express the zero vector as a linear combination of v1, v2, ..., vk then these vectors are linearly independent. Given a set of vectors that span a space, if any vector w is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove w from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space V, which we call a basis of V. Any set of vectors that spans V contains a basis, and any linearly independent set of vectors in V can be extended to a basis. It turns out that if we accept the axiom of choice, every vector space has a basis; nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed.\nAny two bases of a vector space V have the same cardinality, which is called the dimension of V. The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of V has finite number of elements, V is called a finite-dimensional vector space. If V is finite-dimensional and U is a subspace of V, then dim U \u2264 dim V. If U1 and U2 are subspaces of V, then\n\n  \n    \n      \n        dim\n        \u2061\n        (\n        \n          U\n          \n            1\n          \n        \n        +\n        \n          U\n          \n            2\n          \n        \n        )\n        =\n        dim\n        \u2061\n        \n          U\n          \n            1\n          \n        \n        +\n        dim\n        \u2061\n        \n          U\n          \n            2\n          \n        \n        \u2212\n        dim\n        \u2061\n        (\n        \n          U\n          \n            1\n          \n        \n        \u2229\n        \n          U\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\dim(U_{1}+U_{2})=\\dim U_{1}+\\dim U_{2}-\\dim(U_{1}\\cap U_{2})}\n  .One often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic, giving an easy way of characterizing isomorphism.\n\n\n=== Matrix theory ===\n\nA particular basis {v1, v2, ..., vn} of V allows one to construct a coordinate system in V: the vector with coordinates (a1, a2, ..., an) is the linear combination\n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          v\n          \n            1\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        \n          v\n          \n            2\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          v\n          \n            n\n          \n        \n        .\n        \n      \n    \n    {\\displaystyle a_{1}v_{1}+a_{2}v_{2}+\\cdots +a_{n}v_{n}.\\,}\n  The condition that v1, v2, ..., vn span V guarantees that each vector v can be assigned coordinates, whereas the linear independence of v1, v2, ..., vn assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to v). In this way, once a basis of a vector space V over F has been chosen, V may be identified with the coordinate n-space Fn. Under this identification, addition and scalar multiplication of vectors in V correspond to addition and scalar multiplication of their coordinate vectors in Fn. Furthermore, if V and W are an n-dimensional and m-dimensional vector space over F, and a basis of V and a basis of W have been fixed, then any linear transformation T: V \u2192 W may be encoded by an m \u00d7 n matrix A with entries in the field F, called the matrix of T with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.\nThere is an important distinction between the coordinate n-space Rn and a general finite-dimensional vector space V. While Rn has a standard basis {e1, e2, ..., en}, a vector space V typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of V).\nOne major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method.\n\n\n=== Eigenvalues and eigenvectors ===\n\nIn general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation T is to find \"characteristic lines\" that are invariant sets under T. If v is a non-zero vector such that Tv is a scalar multiple of v, then the line through 0 and v is an invariant set under T and v is called a characteristic vector or eigenvector. The scalar \u03bb such that Tv = \u03bbv is called a characteristic value or eigenvalue of T.\nTo find an eigenvector or an eigenvalue, we note that\n\n  \n    \n      \n        T\n        v\n        \u2212\n        \u03bb\n        v\n        =\n        (\n        T\n        \u2212\n        \u03bb\n        \n        \n          I\n        \n        )\n        v\n        =\n        0\n        ,\n      \n    \n    {\\displaystyle Tv-\\lambda v=(T-\\lambda \\,{\\text{I}})v=0,}\n  where I is the identity matrix. For there to be nontrivial solutions to that equation, det(T \u2212 \u03bb I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation T taking a vector space V into itself we can find a basis for V consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if v1, v2, ..., vn are linearly independent eigenvectors of a mapping of n-dimensional spaces T with (not necessarily distinct) eigenvalues \u03bb1, \u03bb2, ..., \u03bbn, and if v = a1v1 + ... + an vn, then,\n\n  \n    \n      \n        T\n        (\n        v\n        )\n        =\n        T\n        (\n        \n          a\n          \n            1\n          \n        \n        \n          v\n          \n            1\n          \n        \n        )\n        +\n        \u22ef\n        +\n        T\n        (\n        \n          a\n          \n            n\n          \n        \n        \n          v\n          \n            n\n          \n        \n        )\n        =\n        \n          a\n          \n            1\n          \n        \n        T\n        (\n        \n          v\n          \n            1\n          \n        \n        )\n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        T\n        (\n        \n          v\n          \n            n\n          \n        \n        )\n        =\n        \n          a\n          \n            1\n          \n        \n        \n          \u03bb\n          \n            1\n          \n        \n        \n          v\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          \u03bb\n          \n            n\n          \n        \n        \n          v\n          \n            n\n          \n        \n        .\n      \n    \n    {\\displaystyle T(v)=T(a_{1}v_{1})+\\cdots +T(a_{n}v_{n})=a_{1}T(v_{1})+\\cdots +a_{n}T(v_{n})=a_{1}\\lambda _{1}v_{1}+\\cdots +a_{n}\\lambda _{n}v_{n}.}\n  Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).\n\n\n=== Inner-product spaces ===\n\nBesides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a map\n\n  \n    \n      \n        \u27e8\n        \u22c5\n        ,\n        \u22c5\n        \u27e9\n        :\n        V\n        \u00d7\n        V\n        \u2192\n        F\n      \n    \n    {\\displaystyle \\langle \\cdot ,\\cdot \\rangle :V\\times V\\rightarrow F}\n  that satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:\nConjugate symmetry:\n  \n    \n      \n        \u27e8\n        u\n        ,\n        v\n        \u27e9\n        =\n        \n          \n            \n              \u27e8\n              v\n              ,\n              u\n              \u27e9\n            \n            \u00af\n          \n        \n        .\n      \n    \n    {\\displaystyle \\langle u,v\\rangle ={\\overline {\\langle v,u\\rangle }}.}\n  Note that in R, it is symmetric.\n\nLinearity in the first argument:\n  \n    \n      \n        \u27e8\n        a\n        u\n        ,\n        v\n        \u27e9\n        =\n        a\n        \u27e8\n        u\n        ,\n        v\n        \u27e9\n        .\n      \n    \n    {\\displaystyle \\langle au,v\\rangle =a\\langle u,v\\rangle .}\n  \n\n  \n    \n      \n        \u27e8\n        u\n        +\n        v\n        ,\n        w\n        \u27e9\n        =\n        \u27e8\n        u\n        ,\n        w\n        \u27e9\n        +\n        \u27e8\n        v\n        ,\n        w\n        \u27e9\n        .\n      \n    \n    {\\displaystyle \\langle u+v,w\\rangle =\\langle u,w\\rangle +\\langle v,w\\rangle .}\n  Positive-definiteness:\n  \n    \n      \n        \u27e8\n        v\n        ,\n        v\n        \u27e9\n        \u2265\n        0\n      \n    \n    {\\displaystyle \\langle v,v\\rangle \\geq 0}\n   with equality only for v = 0.We can define the length of a vector v in V by\n\n  \n    \n      \n        \u2016\n        v\n        \n          \u2016\n          \n            2\n          \n        \n        =\n        \u27e8\n        v\n        ,\n        v\n        \u27e9\n        ,\n      \n    \n    {\\displaystyle \\|v\\|^{2}=\\langle v,v\\rangle ,}\n  and we can prove the Cauchy\u2013Schwarz inequality:\n\n  \n    \n      \n        \n          |\n        \n        \u27e8\n        u\n        ,\n        v\n        \u27e9\n        \n          |\n        \n        \u2264\n        \u2016\n        u\n        \u2016\n        \u22c5\n        \u2016\n        v\n        \u2016\n        .\n      \n    \n    {\\displaystyle |\\langle u,v\\rangle |\\leq \\|u\\|\\cdot \\|v\\|.}\n  In particular, the quantity\n\n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              \u27e8\n              u\n              ,\n              v\n              \u27e9\n              \n                |\n              \n            \n            \n              \u2016\n              u\n              \u2016\n              \u22c5\n              \u2016\n              v\n              \u2016\n            \n          \n        \n        \u2264\n        1\n        ,\n      \n    \n    {\\displaystyle {\\frac {|\\langle u,v\\rangle |}{\\|u\\|\\cdot \\|v\\|}}\\leq 1,}\n  and so we can call this quantity the cosine of the angle between the two vectors.\nTwo vectors are orthogonal if \n  \n    \n      \n        \u27e8\n        u\n        ,\n        v\n        \u27e9\n        =\n        0\n      \n    \n    {\\displaystyle \\langle u,v\\rangle =0}\n  . An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram\u2013Schmidt procedure. Orthonormal bases are particularly nice to deal with, since if v = a1 v1 + ... + an vn, then \n  \n    \n      \n        \n          a\n          \n            i\n          \n        \n        =\n        \u27e8\n        v\n        ,\n        \n          v\n          \n            i\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle a_{i}=\\langle v,v_{i}\\rangle }\n  .\nThe inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying\n\n  \n    \n      \n        \u27e8\n        T\n        u\n        ,\n        v\n        \u27e9\n        =\n        \u27e8\n        u\n        ,\n        \n          T\n          \n            \u2217\n          \n        \n        v\n        \u27e9\n        .\n      \n    \n    {\\displaystyle \\langle Tu,v\\rangle =\\langle u,T^{*}v\\rangle .}\n  If T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V.\n\n\n== Some main useful theorems ==\nA matrix is invertible, or non-singular, if and only if the linear map represented by the matrix is an isomorphism.\nAny vector space over a field F of dimension n is isomorphic to Fn as a vector space over F.\nCorollary: Any two vector spaces over F of the same finite dimension are isomorphic to each other.\nA linear map is an isomorphism if and only if the determinant is nonzero.\n\n\n== Applications ==\nBecause of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.\n\n\n=== Solution of linear systems ===\n\nLinear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:\n\n  \n    \n      \n        \n          \n            \n              \n                2\n                x\n              \n              \n              \n                \n                +\n                \n              \n              \n              \n                y\n              \n              \n              \n                \n                \u2212\n                \n              \n              \n              \n                z\n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                8\n              \n              \n                \n                \n                (\n                \n                  L\n                  \n                    1\n                  \n                \n                )\n              \n            \n            \n              \n                \u2212\n                3\n                x\n              \n              \n              \n                \n                \u2212\n                \n              \n              \n              \n                y\n              \n              \n              \n                \n                +\n                \n              \n              \n              \n                2\n                z\n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                \u2212\n                11\n              \n              \n                \n                \n                (\n                \n                  L\n                  \n                    2\n                  \n                \n                )\n              \n            \n            \n              \n                \u2212\n                2\n                x\n              \n              \n              \n                \n                +\n                \n              \n              \n              \n                y\n              \n              \n              \n                \n                +\n                \n              \n              \n              \n                2\n                z\n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                \u2212\n                3\n              \n              \n                \n                \n                (\n                \n                  L\n                  \n                    3\n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{alignedat}{7}2x&&\\;+\\;&&y&&\\;-\\;&&z&&\\;=\\;&&8&\\qquad (L_{1})\\\\-3x&&\\;-\\;&&y&&\\;+\\;&&2z&&\\;=\\;&&-11&\\qquad (L_{2})\\\\-2x&&\\;+\\;&&y&&\\;+\\;&&2z&&\\;=\\;&&-3&\\qquad (L_{3})\\end{alignedat}}}\n  The Gaussian-elimination algorithm is as follows: eliminate x from all equations below L1, and then eliminate y from all equations below L2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.\nIn the example, x is eliminated from L2 by adding (3/2)L1 to L2. x is then eliminated from L3 by adding L1 to L3. Formally:\n\n  \n    \n      \n        \n          L\n          \n            2\n          \n        \n        +\n        \n          \n            \n              3\n              2\n            \n          \n        \n        \n          L\n          \n            1\n          \n        \n        \u2192\n        \n          L\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle L_{2}+{\\tfrac {3}{2}}L_{1}\\rightarrow L_{2}}\n  \n\n  \n    \n      \n        \n          L\n          \n            3\n          \n        \n        +\n        \n          L\n          \n            1\n          \n        \n        \u2192\n        \n          L\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle L_{3}+L_{1}\\rightarrow L_{3}}\n  The result is:\n\n  \n    \n      \n        \n          \n            \n              \n                2\n                x\n              \n              \n              \n                \n                +\n              \n              \n              \n                y\n              \n              \n              \n                \n                \u2212\n              \n              \n              \n                \n                z\n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                8\n              \n              \n            \n            \n              \n              \n              \n              \n              \n                \n                  \n                    1\n                    2\n                  \n                \n                y\n              \n              \n              \n                \n                +\n              \n              \n              \n                \n                \n                  \n                    1\n                    2\n                  \n                \n                z\n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                1\n              \n              \n            \n            \n              \n              \n              \n              \n              \n                2\n                y\n              \n              \n              \n                \n                +\n              \n              \n              \n                \n                z\n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                5\n              \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{alignedat}{7}2x&&\\;+&&y&&\\;-&&\\;z&&\\;=\\;&&8&\\\\&&&&{\\frac {1}{2}}y&&\\;+&&\\;{\\frac {1}{2}}z&&\\;=\\;&&1&\\\\&&&&2y&&\\;+&&\\;z&&\\;=\\;&&5&\\end{alignedat}}}\n  Now y is eliminated from L3 by adding \u22124L2 to L3:\n\n  \n    \n      \n        \n          L\n          \n            3\n          \n        \n        +\n        \u2212\n        4\n        \n          L\n          \n            2\n          \n        \n        \u2192\n        \n          L\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle L_{3}+-4L_{2}\\rightarrow L_{3}}\n  The result is:\n\n  \n    \n      \n        \n          \n            \n              \n                2\n                x\n              \n              \n              \n                \n                +\n              \n              \n              \n                y\n                \n              \n              \n              \n                \u2212\n              \n              \n              \n                \n                z\n                \n              \n              \n              \n                =\n                \n              \n              \n              \n                8\n              \n              \n            \n            \n              \n              \n              \n              \n              \n                \n                  \n                    1\n                    2\n                  \n                \n                y\n                \n              \n              \n              \n                +\n              \n              \n              \n                \n                \n                  \n                    1\n                    2\n                  \n                \n                z\n                \n              \n              \n              \n                =\n                \n              \n              \n              \n                1\n              \n              \n            \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n                \n                \u2212\n                z\n                \n              \n              \n              \n                \n                =\n                \n              \n              \n              \n                1\n              \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{alignedat}{7}2x&&\\;+&&y\\;&&-&&\\;z\\;&&=\\;&&8&\\\\&&&&{\\frac {1}{2}}y\\;&&+&&\\;{\\frac {1}{2}}z\\;&&=\\;&&1&\\\\&&&&&&&&\\;-z\\;&&\\;=\\;&&1&\\end{alignedat}}}\n  This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.\nThe last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen that\n\n  \n    \n      \n        z\n        =\n        \u2212\n        1\n        \n        (\n        \n          L\n          \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle z=-1\\quad (L_{3})}\n  Then, z can be substituted into L2, which can then be solved to obtain\n\n  \n    \n      \n        y\n        =\n        3\n        \n        (\n        \n          L\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle y=3\\quad (L_{2})}\n  Next, z and y can be substituted into L1, which can be solved to obtain\n\n  \n    \n      \n        x\n        =\n        2\n        \n        (\n        \n          L\n          \n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle x=2\\quad (L_{1})}\n  The system is solved.\nWe can, in general, write any system of linear equations as a matrix equation:\n\n  \n    \n      \n        A\n        x\n        =\n        b\n        .\n      \n    \n    {\\displaystyle Ax=b.}\n  The solution of this system is characterized as follows: first, we find a particular solution x0 of this equation using Gaussian elimination. Then, we compute the solutions of Ax = 0; that is, we find the null space N of A. The solution set of this equation is given by \n  \n    \n      \n        \n          x\n          \n            0\n          \n        \n        +\n        N\n        =\n        {\n        \n          x\n          \n            0\n          \n        \n        +\n        n\n        :\n        n\n        \u2208\n        N\n        }\n      \n    \n    {\\displaystyle x_{0}+N=\\{x_{0}+n:n\\in N\\}}\n  . If the number of variables is equal to the number of equations, then we can characterize when the system has a unique solution: since N is trivial if and only if det A \u2260 0, the equation has a unique solution if and only if det A \u2260 0.\n\n\n=== Least-squares best-fit line ===\nThe least squares method is used to determine the best-fit line for a set of data. This line will minimize the sum of the squares of the residuals.\n\n\n=== Fourier series expansion ===\nFourier series are a representation of a function f: [\u2212\u03c0, \u03c0] \u2192 R as a trigonometric series:\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            \n              a\n              \n                0\n              \n            \n            2\n          \n        \n        +\n        \n          \u2211\n          \n            n\n            =\n            1\n          \n          \n            \u221e\n          \n        \n        \n        [\n        \n          a\n          \n            n\n          \n        \n        cos\n        \u2061\n        (\n        n\n        x\n        )\n        +\n        \n          b\n          \n            n\n          \n        \n        sin\n        \u2061\n        (\n        n\n        x\n        )\n        ]\n        .\n      \n    \n    {\\displaystyle f(x)={\\frac {a_{0}}{2}}+\\sum _{n=1}^{\\infty }\\,[a_{n}\\cos(nx)+b_{n}\\sin(nx)].}\n  This series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.\nThe space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the \"same\" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner product\n\n  \n    \n      \n        \u27e8\n        f\n        ,\n        g\n        \u27e9\n        =\n        \n          \n            1\n            \u03c0\n          \n        \n        \n          \u222b\n          \n            \u2212\n            \u03c0\n          \n          \n            \u03c0\n          \n        \n        f\n        (\n        x\n        )\n        g\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\langle f,g\\rangle ={\\frac {1}{\\pi }}\\int _{-\\pi }^{\\pi }f(x)g(x)\\,dx.}\n  The functions gn(x) = sin(nx)  for n > 0 and hn(x) = cos(nx) for n \u2265 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient ak, we take the inner product with hk:\n\n  \n    \n      \n        \u27e8\n        f\n        ,\n        \n          h\n          \n            k\n          \n        \n        \u27e9\n        =\n        \n          \n            \n              a\n              \n                0\n              \n            \n            2\n          \n        \n        \u27e8\n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            k\n          \n        \n        \u27e9\n        +\n        \n          \u2211\n          \n            n\n            =\n            1\n          \n          \n            \u221e\n          \n        \n        \n        [\n        \n          a\n          \n            n\n          \n        \n        \u27e8\n        \n          h\n          \n            n\n          \n        \n        ,\n        \n          h\n          \n            k\n          \n        \n        \u27e9\n        +\n        \n          b\n          \n            n\n          \n        \n        \u27e8\n         \n        \n          g\n          \n            n\n          \n        \n        ,\n        \n          h\n          \n            k\n          \n        \n        \u27e9\n        ]\n        ,\n      \n    \n    {\\displaystyle \\langle f,h_{k}\\rangle ={\\frac {a_{0}}{2}}\\langle h_{0},h_{k}\\rangle +\\sum _{n=1}^{\\infty }\\,[a_{n}\\langle h_{n},h_{k}\\rangle +b_{n}\\langle \\ g_{n},h_{k}\\rangle ],}\n  and by orthonormality, \n  \n    \n      \n        \u27e8\n        f\n        ,\n        \n          h\n          \n            k\n          \n        \n        \u27e9\n        =\n        \n          a\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\langle f,h_{k}\\rangle =a_{k}}\n  ; that is,\n\n  \n    \n      \n        \n          a\n          \n            k\n          \n        \n        =\n        \n          \n            1\n            \u03c0\n          \n        \n        \n          \u222b\n          \n            \u2212\n            \u03c0\n          \n          \n            \u03c0\n          \n        \n        f\n        (\n        x\n        )\n        cos\n        \u2061\n        (\n        k\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle a_{k}={\\frac {1}{\\pi }}\\int _{-\\pi }^{\\pi }f(x)\\cos(kx)\\,dx.}\n  \n\n\n=== Quantum mechanics ===\nQuantum mechanics is highly inspired by notions in linear algebra. In quantum mechanics, the physical state of a particle is represented by a vector, and observables (such as momentum, energy, and angular momentum) are represented by linear operators on the underlying vector space. More concretely, the wave function of a particle describes its physical state and lies in the vector space L2 (the functions \u03c6: R3 \u2192 C such that \n  \n    \n      \n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          \u222b\n          \n            \u2212\n            \u221e\n          \n          \n            \u221e\n          \n        \n        \n          |\n        \n        \u03d5\n        \n          \n            |\n          \n          \n            2\n          \n        \n        d\n        x\n        d\n        y\n        d\n        z\n      \n    \n    {\\displaystyle \\int _{-\\infty }^{\\infty }\\int _{-\\infty }^{\\infty }\\int _{-\\infty }^{\\infty }|\\phi |^{2}dxdydz}\n   is finite), and it evolves according to the Schr\u00f6dinger equation. Energy is represented as the operator \n  \n    \n      \n        H\n        =\n        \u2212\n        \n          \n            \n              \u210f\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        \n          \u2207\n          \n            2\n          \n        \n        +\n        V\n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n      \n    \n    {\\displaystyle H=-{\\frac {\\hbar ^{2}}{2m}}\\nabla ^{2}+V(x,y,z)}\n  , where V is the potential energy. H is also known as the Hamiltonian operator. The eigenvalues of H represent the possible energies that can be observed. Given a particle in some state \u03c6, we can expand \u03c6 into a linear combination of eigenstates of H. The component of \u03c6 in each eigenstate determines the probability of measuring the corresponding eigenvalue, and the measurement forces the particle to assume that eigenstate (wave function collapse).\n\n\n== Geometric introduction ==\nMany of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E.  When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.\nPoint coordinates in the plane E are ordered pairs of real numbers, (x,y), and a line is defined as the set of points (x,y) that satisfy the linear equation\n\n  \n    \n      \n        \u03bb\n        :\n        a\n        x\n        +\n        b\n        y\n        +\n        c\n        =\n        0\n        ,\n      \n    \n    {\\displaystyle \\lambda :ax+by+c=0,}\n  where a, b and c are not all zero.\nThen,\n\n  \n    \n      \n        \u03bb\n        :\n        \n          \n            [\n            \n              \n                \n                  a\n                \n                \n                  b\n                \n                \n                  c\n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n              \n                \n                  1\n                \n              \n            \n            }\n          \n        \n        =\n        0\n        ,\n      \n    \n    {\\displaystyle \\lambda :{\\begin{bmatrix}a&b&c\\end{bmatrix}}{\\begin{Bmatrix}x\\\\y\\\\1\\end{Bmatrix}}=0,}\n  or\n\n  \n    \n      \n        A\n        \n          x\n        \n        =\n        0\n        ,\n      \n    \n    {\\displaystyle A\\mathbf {x} =0,}\n  where x = (x, y, 1) is the 3\u2009\u00d7\u20091 set of homogeneous coordinates associated with the point (x, y).Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space.  The x\u2212y coordinates in E are obtained from homogeneous coordinates y = (y1, y2, y3) by dividing by the third component (if it is nonzero) to obtain y = (y1/y3, y2/y3, 1).\nThe linear equation, \u03bb, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point \u03b1x1 + \u03b2x2 is also on the line, for any real \u03b1 and \u03b2.\nNow consider the equations of the two lines \u03bb1 and \u03bb2,\n\n  \n    \n      \n        \n          \u03bb\n          \n            1\n          \n        \n        :\n        \n          a\n          \n            1\n          \n        \n        x\n        +\n        \n          b\n          \n            1\n          \n        \n        y\n        +\n        \n          c\n          \n            1\n          \n        \n        =\n        0\n        ,\n        \n        \n          \u03bb\n          \n            2\n          \n        \n        :\n        \n          a\n          \n            2\n          \n        \n        x\n        +\n        \n          b\n          \n            2\n          \n        \n        y\n        +\n        \n          c\n          \n            2\n          \n        \n        =\n        0\n        ,\n      \n    \n    {\\displaystyle \\lambda _{1}:a_{1}x+b_{1}y+c_{1}=0,\\quad \\lambda _{2}:a_{2}x+b_{2}y+c_{2}=0,}\n  which forms a system of linear equations.  The intersection of these two lines is defined by x = (x, y, 1) that satisfy the matrix equation,\n\n  \n    \n      \n        \n          \u03bb\n          \n            1\n            ,\n            2\n          \n        \n        :\n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    a\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n              \n                \n                  1\n                \n              \n            \n            }\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n              \n            \n            }\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\lambda _{1,2}:{\\begin{bmatrix}a_{1}&b_{1}&c_{1}\\\\a_{2}&b_{2}&c_{2}\\end{bmatrix}}{\\begin{Bmatrix}x\\\\y\\\\1\\end{Bmatrix}}={\\begin{Bmatrix}0\\\\0\\end{Bmatrix}},}\n  or using homogeneous coordinates,\n\n  \n    \n      \n        B\n        \n          x\n        \n        =\n        0.\n      \n    \n    {\\displaystyle B\\mathbf {x} =0.}\n  The point of intersection of these two lines is the unique non-zero solution of these equations.  In homogeneous coordinates,\nthe solutions are multiples of the following solution:\n\n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        =\n        \n          \n            |\n            \n              \n                \n                  \n                    b\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    b\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n            |\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        =\n        \u2212\n        \n          \n            |\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    a\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n            |\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n        \n        =\n        \n          \n            |\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    a\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      2\n                    \n                  \n                \n              \n            \n            |\n          \n        \n      \n    \n    {\\displaystyle x_{1}={\\begin{vmatrix}b_{1}&c_{1}\\\\b_{2}&c_{2}\\end{vmatrix}},x_{2}=-{\\begin{vmatrix}a_{1}&c_{1}\\\\a_{2}&c_{2}\\end{vmatrix}},x_{3}={\\begin{vmatrix}a_{1}&b_{1}\\\\a_{2}&b_{2}\\end{vmatrix}}}\n  if the rows of B are linearly independent (i.e., \u03bb1 and \u03bb2 represent distinct lines).\nDivide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns.   Notice that this yields a point in the z = 1 plane only when the 2\u2009\u00d7\u20092 submatrix associated with x3 has a non-zero determinant.\nIt is interesting to consider the case of three lines, \u03bb1, \u03bb2 and \u03bb3, which yield the matrix equation,\n\n  \n    \n      \n        \n          \u03bb\n          \n            1\n            ,\n            2\n            ,\n            3\n          \n        \n        :\n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    a\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    a\n                    \n                      3\n                    \n                  \n                \n                \n                  \n                    b\n                    \n                      3\n                    \n                  \n                \n                \n                  \n                    c\n                    \n                      3\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n              \n                \n                  1\n                \n              \n            \n            }\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n              \n            \n            }\n          \n        \n        .\n      \n    \n    {\\displaystyle \\lambda _{1,2,3}:{\\begin{bmatrix}a_{1}&b_{1}&c_{1}\\\\a_{2}&b_{2}&c_{2}\\\\a_{3}&b_{3}&c_{3}\\end{bmatrix}}{\\begin{Bmatrix}x\\\\y\\\\1\\end{Bmatrix}}={\\begin{Bmatrix}0\\\\0\\\\0\\end{Bmatrix}}.}\n  which in homogeneous form yields,\n\n  \n    \n      \n        C\n        \n          x\n        \n        =\n        0.\n      \n    \n    {\\displaystyle C\\mathbf {x} =0.}\n  Clearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E.  For a solution to exist in the plane E, the coefficient matrix C must have rank 2, which means its determinant must be zero.  Another way to say this is that the columns of the matrix must be linearly dependent.\n\n\n== Introduction to linear transformations ==\nAnother way to approach linear algebra is to consider linear functions on the two-dimensional real plane E=R2.  Here R denotes the set of real numbers.   Let x=(x, y) be an arbitrary vector in E and consider the linear function \u03bb: E\u2192R, given by\n\n  \n    \n      \n        \u03bb\n        :\n        \n          \n            [\n            \n              \n                \n                  a\n                \n                \n                  b\n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n            \n            }\n          \n        \n        =\n        c\n        ,\n      \n    \n    {\\displaystyle \\lambda :{\\begin{bmatrix}a&b\\end{bmatrix}}{\\begin{Bmatrix}x\\\\y\\end{Bmatrix}}=c,}\n  or\n\n  \n    \n      \n        A\n        \n          x\n        \n        =\n        c\n        .\n      \n    \n    {\\displaystyle A\\mathbf {x} =c.}\n  This transformation has the important property that if Ay=d, then\n\n  \n    \n      \n        A\n        (\n        \u03b1\n        \n          x\n        \n        +\n        \u03b2\n        \n          y\n        \n        )\n        =\n        \u03b1\n        A\n        \n          x\n        \n        +\n        \u03b2\n        A\n        \n          y\n        \n        =\n        \u03b1\n        c\n        +\n        \u03b2\n        d\n        .\n      \n    \n    {\\displaystyle A(\\alpha \\mathbf {x} +\\beta \\mathbf {y} )=\\alpha A\\mathbf {x} +\\beta A\\mathbf {y} =\\alpha c+\\beta d.}\n  This shows that the sum of vectors in E map to the sum of their images in R.  This is the defining characteristic of a linear map, or linear transformation.  For this case, where the image space is a real number the map is called a linear functional.Consider the linear functional a little more carefully.  Let i=(1,0) and j =(0,1) be the natural basis vectors on E, so that x=xi+yj.  It is now possible to see that\n\n  \n    \n      \n        A\n        \n          x\n        \n        =\n        A\n        (\n        x\n        \n          i\n        \n        +\n        y\n        \n          j\n        \n        )\n        =\n        x\n        A\n        \n          i\n        \n        +\n        y\n        A\n        \n          j\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  A\n                  \n                    i\n                  \n                \n                \n                  A\n                  \n                    j\n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n            \n            }\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  a\n                \n                \n                  b\n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n            \n            }\n          \n        \n        =\n        c\n        .\n      \n    \n    {\\displaystyle A\\mathbf {x} =A(x\\mathbf {i} +y\\mathbf {j} )=xA\\mathbf {i} +yA\\mathbf {j} ={\\begin{bmatrix}A\\mathbf {i} &A\\mathbf {j} \\end{bmatrix}}{\\begin{Bmatrix}x\\\\y\\end{Bmatrix}}={\\begin{bmatrix}a&b\\end{bmatrix}}{\\begin{Bmatrix}x\\\\y\\end{Bmatrix}}=c.}\n  Thus, the columns of the matrix A are the image of the basis vectors of E in R.\nThis is true for any pair of vectors used to define coordinates in E.   Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in E.  This means a vector x has coordinates (\u03b1,\u03b2), such that x=\u03b1v+\u03b2w.  Then, we have the linear functional\n\n  \n    \n      \n        \u03bb\n        :\n        A\n        \n          x\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  A\n                  \n                    v\n                  \n                \n                \n                  A\n                  \n                    w\n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  \u03b1\n                \n              \n              \n                \n                  \u03b2\n                \n              \n            \n            }\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  d\n                \n                \n                  e\n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  \u03b1\n                \n              \n              \n                \n                  \u03b2\n                \n              \n            \n            }\n          \n        \n        =\n        c\n        ,\n      \n    \n    {\\displaystyle \\lambda :A\\mathbf {x} ={\\begin{bmatrix}A\\mathbf {v} &A\\mathbf {w} \\end{bmatrix}}{\\begin{Bmatrix}\\alpha \\\\\\beta \\end{Bmatrix}}={\\begin{bmatrix}d&e\\end{bmatrix}}{\\begin{Bmatrix}\\alpha \\\\\\beta \\end{Bmatrix}}=c,}\n  where Av=d and Aw=e are the images of the basis vectors  v and w.   This is written in matrix form as\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  a\n                \n                \n                  b\n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    v\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    w\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    w\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  d\n                \n                \n                  e\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{bmatrix}a&b\\end{bmatrix}}{\\begin{bmatrix}v_{1}&w_{1}\\\\v_{2}&w_{2}\\end{bmatrix}}={\\begin{bmatrix}d&e\\end{bmatrix}}.}\n  \n\n\n=== Coordinates relative to a basis ===\nThis leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in E.  Assume that we know the coordinates of the vectors, x, v and w  in the natural basis i=(1,0) and j =(0,1).  Our goal is to find the real numbers \u03b1, \u03b2, so that x=\u03b1v+\u03b2w, that is\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n            \n            }\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    v\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    w\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    w\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  \u03b1\n                \n              \n              \n                \n                  \u03b2\n                \n              \n            \n            }\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{Bmatrix}x\\\\y\\end{Bmatrix}}={\\begin{bmatrix}v_{1}&w_{1}\\\\v_{2}&w_{2}\\end{bmatrix}}{\\begin{Bmatrix}\\alpha \\\\\\beta \\end{Bmatrix}}.}\n  To solve this equation for \u03b1, \u03b2, we compute the linear coordinate functionals \u03c3 and \u03c4 for the basis v, w, which are given by,\n\n  \n    \n      \n        \u03c3\n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    \u03c3\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            1\n            \n              \n                v\n                \n                  1\n                \n              \n              \n                w\n                \n                  2\n                \n              \n              \u2212\n              \n                v\n                \n                  2\n                \n              \n              \n                w\n                \n                  1\n                \n              \n            \n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    w\n                    \n                      2\n                    \n                  \n                \n                \n                  \u2212\n                  \n                    w\n                    \n                      1\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n        \u03c4\n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    \u03c4\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    \u03c4\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            1\n            \n              \n                v\n                \n                  1\n                \n              \n              \n                w\n                \n                  2\n                \n              \n              \u2212\n              \n                v\n                \n                  2\n                \n              \n              \n                w\n                \n                  1\n                \n              \n            \n          \n        \n        \n          \n            [\n            \n              \n                \n                  \u2212\n                  \n                    v\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    v\n                    \n                      1\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sigma ={\\begin{bmatrix}\\sigma _{1}&\\sigma _{2}\\end{bmatrix}}={\\frac {1}{v_{1}w_{2}-v_{2}w_{1}}}{\\begin{bmatrix}w_{2}&-w_{1}\\end{bmatrix}},\\tau ={\\begin{bmatrix}\\tau _{1}&\\tau _{2}\\end{bmatrix}}={\\frac {1}{v_{1}w_{2}-v_{2}w_{1}}}{\\begin{bmatrix}-v_{2}&v_{1}\\end{bmatrix}},}\n  The functionals \u03c3 and \u03c4 compute the components of x along the basis vectors v and w, respectively, that is,\n\n  \n    \n      \n        \u03c3\n        \n          x\n        \n        =\n        \u03b1\n        ,\n        \u03c4\n        \n          x\n        \n        =\n        \u03b2\n        ,\n      \n    \n    {\\displaystyle \\sigma \\mathbf {x} =\\alpha ,\\tau \\mathbf {x} =\\beta ,}\n  which can be written in matrix form as\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    \u03c3\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03c4\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    \u03c4\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n            \n            }\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  \u03b1\n                \n              \n              \n                \n                  \u03b2\n                \n              \n            \n            }\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{1}&\\sigma _{2}\\\\\\tau _{1}&\\tau _{2}\\end{bmatrix}}{\\begin{Bmatrix}x\\\\y\\end{Bmatrix}}={\\begin{Bmatrix}\\alpha \\\\\\beta \\end{Bmatrix}}.}\n  These coordinate functionals have the properties,\n\n  \n    \n      \n        \u03c3\n        \n          v\n        \n        =\n        1\n        ,\n        \u03c3\n        \n          w\n        \n        =\n        0\n        ,\n        \u03c4\n        \n          w\n        \n        =\n        1\n        ,\n        \u03c4\n        \n          v\n        \n        =\n        0.\n      \n    \n    {\\displaystyle \\sigma \\mathbf {v} =1,\\sigma \\mathbf {w} =0,\\tau \\mathbf {w} =1,\\tau \\mathbf {v} =0.}\n  These equations can be assembled into the single matrix equation,\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    \u03c3\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n              \n                \n                  \n                    \u03c4\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    \u03c4\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        \n          \n            [\n            \n              \n                \n                  \n                    v\n                    \n                      1\n                    \n                  \n                \n                \n                  \n                    w\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                \n                \n                  \n                    w\n                    \n                      2\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  1\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{bmatrix}\\sigma _{1}&\\sigma _{2}\\\\\\tau _{1}&\\tau _{2}\\end{bmatrix}}{\\begin{bmatrix}v_{1}&w_{1}\\\\v_{2}&w_{2}\\end{bmatrix}}={\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}}.}\n  Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.\n\n\n=== Inverse image ===\nThe set of points in the plane E that map to the same image in R under the linear functional \u03bb define a line in E. This line is the image of the inverse map, \u03bb\u22121: R\u2192E.  This inverse image is the set of  the points x=(x, y) that solve the equation,\n\n  \n    \n      \n        A\n        \n          x\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  a\n                \n                \n                  b\n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n            \n            }\n          \n        \n        =\n        c\n        .\n      \n    \n    {\\displaystyle A\\mathbf {x} ={\\begin{bmatrix}a&b\\end{bmatrix}}{\\begin{Bmatrix}x\\\\y\\end{Bmatrix}}=c.}\n  Notice that a linear functional operates on known values for x=(x, y) to compute a value c in R, while the inverse image seeks the values for x=(x, y) that yield a specific value c.\nIn order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation\n\n  \n    \n      \n        b\n        y\n        =\n        c\n        \u2212\n        a\n        x\n        .\n      \n    \n    {\\displaystyle by=c-ax.}\n  Solve for y and obtain the inverse image as the set of points,\n\n  \n    \n      \n        \n          x\n        \n        (\n        t\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  0\n                \n              \n              \n                \n                  c\n                  \n                    /\n                  \n                  b\n                \n              \n            \n            }\n          \n        \n        +\n        t\n        \n          \n            {\n            \n              \n                \n                  1\n                \n              \n              \n                \n                  \u2212\n                  a\n                  \n                    /\n                  \n                  b\n                \n              \n            \n            }\n          \n        \n        =\n        \n          p\n        \n        +\n        t\n        \n          h\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {x} (t)={\\begin{Bmatrix}0\\\\c/b\\end{Bmatrix}}+t{\\begin{Bmatrix}1\\\\-a/b\\end{Bmatrix}}=\\mathbf {p} +t\\mathbf {h} .}\n  For convenience the free parameter x has been relabeled t.\nThe vector p defines the intersection of the line with the y-axis, known as the y-intercept.  The vector h satisfies the homogeneous equation,\n\n  \n    \n      \n        A\n        \n          h\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  a\n                \n                \n                  b\n                \n              \n            \n            ]\n          \n        \n        \n          \n            {\n            \n              \n                \n                  1\n                \n              \n              \n                \n                  \u2212\n                  a\n                  \n                    /\n                  \n                  b\n                \n              \n            \n            }\n          \n        \n        =\n        0.\n      \n    \n    {\\displaystyle A\\mathbf {h} ={\\begin{bmatrix}a&b\\end{bmatrix}}{\\begin{Bmatrix}1\\\\-a/b\\end{Bmatrix}}=0.}\n  Notice that if h is a solution to this homogeneous equation, then t h is also a solution.\nThe set of points of a linear functional that map to zero define the kernel of the linear functional.  The line can be considered to be the set of points h in the kernel translated by the vector p.\n\n\n== Generalizations and related topics ==\nSince linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.\nIn multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V\u2217 consisting of linear maps f: V \u2192 F where F is the field of scalars. Multilinear maps T: Vn \u2192 F can be described via tensor products of elements of V\u2217.\nIf, in addition to vector addition and scalar multiplication, there is a bilinear vector product V \u00d7 V \u2192 V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).\nFunctional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as Lp spaces.\nRepresentation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.\nAlgebraic geometry considers the solutions of systems of polynomial equations.\nThere are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to.\n\n\n== See also ==\nLinear equation\nLinear equation over a ring\nSystem of linear equations\nGaussian elimination\nEigenvectors\nFundamental matrix in computer vision\nLinear regression, a statistical estimation method\nList of linear algebra topics\nNumerical linear algebra\nSimplex method, a solution technique for linear programs\nTransformation matrix\n\n\n== Notes ==\n\n\n== Further reading ==\n\n\n=== History ===\nFearnley-Sander, Desmond, \"Hermann Grassmann and the Creation of Linear Algebra\", American Mathematical Monthly 86 (1979), pp. 809\u2013817.\nGrassmann, Hermann (1844), Die lineale Ausdehnungslehre ein neuer Zweig der Mathematik: dargestellt und durch Anwendungen auf die \u00fcbrigen Zweige der Mathematik, wie auch auf die Statik, Mechanik, die Lehre vom Magnetismus und die Krystallonomie erl\u00e4utert, Leipzig: O. Wigand \n\n\n=== Introductory textbooks ===\nBanerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics, Texts in Statistical Science (1st ed.), Chapman and Hall/CRC, ISBN 978-1420095388 \nStrang, Gilbert (May 2016), Introduction to Linear Algebra (5th ed.), Wellesley-Cambridge Press, ISBN 978-09802327-7-6 \nMurty, Katta G. (2014) Computational and Algorithmic Linear Algebra and n-Dimensional Geometry, World Scientific Publishing, ISBN 978-981-4366-62-5. Chapter 1: Systems of Simultaneous Linear Equations\nBretscher, Otto (June 28, 2004), Linear Algebra with Applications (3rd ed.), Prentice Hall, ISBN 978-0-13-145334-0 \nFarin, Gerald; Hansford, Dianne (December 15, 2004), Practical Linear Algebra: A Geometry Toolbox, AK Peters, ISBN 978-1-56881-234-2 \nHefferon, Jim (2008), Linear Algebra \nAnton, Howard (2005), Elementary Linear Algebra (Applications Version) (9th ed.), Wiley International \nLay, David C. (August 22, 2005), Linear Algebra and Its Applications (3rd ed.), Addison Wesley, ISBN 978-0-321-28713-7 \nKolman, Bernard; Hill, David R. (May 3, 2007), Elementary Linear Algebra with Applications (9th ed.), Prentice Hall, ISBN 978-0-13-229654-0 \nLeon, Steven J. (2006), Linear Algebra With Applications (7th ed.), Pearson Prentice Hall, ISBN 978-0-13-185785-8 \nPoole, David (2010), Linear Algebra: A Modern Introduction (3rd ed.), Cengage \u2013 Brooks/Cole, ISBN 978-0-538-73545-2 \nRicardo, Henry (2010), A Modern Introduction To Linear Algebra (1st ed.), CRC Press, ISBN 978-1-4398-0040-9 \nSadun, Lorenzo (2008), Applied Linear Algebra: the decoupling principle (2nd ed.), AMS, ISBN 978-0-8218-4441-0 \n\n\n=== Advanced textbooks ===\nAxler, Sheldon (February 26, 2004), Linear Algebra Done Right (2nd ed.), Springer, ISBN 978-0-387-98258-8 \nBhatia, Rajendra (November 15, 1996), Matrix Analysis, Graduate Texts in Mathematics, Springer, ISBN 978-0-387-94846-1 \nDemmel, James W. (August 1, 1997), Applied Numerical Linear Algebra, SIAM, ISBN 978-0-89871-389-3 \nDym, Harry (2007), Linear Algebra in Action, AMS, ISBN 978-0-8218-3813-6 \nGantmacher, Felix R. (2005), Applications of the Theory of Matrices, Dover Publications, ISBN 978-0-486-44554-0 \nGantmacher, Felix R. (1990), Matrix Theory Vol. 1 (2nd ed.), American Mathematical Society, ISBN 978-0-8218-1376-8 \nGantmacher, Felix R. (2000), Matrix Theory Vol. 2 (2nd ed.), American Mathematical Society, ISBN 978-0-8218-2664-5 \nGelfand, Israel M. (1989), Lectures on Linear Algebra, Dover Publications, ISBN 978-0-486-66082-0 \nGlazman, I. M.; Ljubic, Ju. I. (2006), Finite-Dimensional Linear Analysis, Dover Publications, ISBN 978-0-486-45332-3 \nGolan, Johnathan S. (January 2007), The Linear Algebra a Beginning Graduate Student Ought to Know (2nd ed.), Springer, ISBN 978-1-4020-5494-5 \nGolan, Johnathan S. (August 1995), Foundations of Linear Algebra, Kluwer, ISBN 0-7923-3614-3 \nGolub, Gene H.; Van Loan, Charles F. (October 15, 1996), Matrix Computations, Johns Hopkins Studies in Mathematical Sciences (3rd ed.), The Johns Hopkins University Press, ISBN 978-0-8018-5414-9 \nGreub, Werner H. (October 16, 1981), Linear Algebra, Graduate Texts in Mathematics (4th ed.), Springer, ISBN 978-0-8018-5414-9 \nHoffman, Kenneth; Kunze, Ray (1971), Linear algebra (2nd ed.), Englewood Cliffs, N.J.: Prentice-Hall, Inc., MR 0276251 \nHalmos, Paul R. (August 20, 1993), Finite-Dimensional Vector Spaces, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-90093-3 \nFriedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (November 11, 2002), Linear Algebra (4th ed.), Prentice Hall, ISBN 978-0-13-008451-4 \nHorn, Roger A.; Johnson, Charles R. (February 23, 1990), Matrix Analysis, Cambridge University Press, ISBN 978-0-521-38632-6 \nHorn, Roger A.; Johnson, Charles R. (June 24, 1994), Topics in Matrix Analysis, Cambridge University Press, ISBN 978-0-521-46713-1 \nLang, Serge (March 9, 2004), Linear Algebra, Undergraduate Texts in Mathematics (3rd ed.), Springer, ISBN 978-0-387-96412-6 \nMarcus, Marvin; Minc, Henryk (2010), A Survey of Matrix Theory and Matrix Inequalities, Dover Publications, ISBN 978-0-486-67102-4 \nMeyer, Carl D. (February 15, 2001), Matrix Analysis and Applied Linear Algebra, Society for Industrial and Applied Mathematics (SIAM), ISBN 978-0-89871-454-8, archived from the original on October 31, 2009 \nMirsky, L. (1990), An Introduction to Linear Algebra, Dover Publications, ISBN 978-0-486-66434-7 \nRoman, Steven (March 22, 2005), Advanced Linear Algebra, Graduate Texts in Mathematics (2nd ed.), Springer, ISBN 978-0-387-24766-3 \nShafarevich, I. R.; Remizov, A. O (2012), Linear Algebra and Geometry, Springer, ISBN 978-3-642-30993-9 \nShilov, Georgi E. (June 1, 1977), Linear algebra, Dover Publications, ISBN 978-0-486-63518-7 \nShores, Thomas S. (December 6, 2006), Applied Linear Algebra and Matrix Analysis, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-33194-2 \nSmith, Larry (May 28, 1998), Linear Algebra, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-98455-1 \nTrefethen, Lloyd N.; Bau, David (1997), Numerical Linear Algebra, SIAM, ISBN 978-0-898-71361-9 \n\n\n=== Study guides and outlines ===\nLeduc, Steven A. (May 1, 1996), Linear Algebra (Cliffs Quick Review), Cliffs Notes, ISBN 978-0-8220-5331-6 \nLipschutz, Seymour; Lipson, Marc (December 6, 2000), Schaum's Outline of Linear Algebra (3rd ed.), McGraw-Hill, ISBN 978-0-07-136200-9 \nLipschutz, Seymour (January 1, 1989), 3,000 Solved Problems in Linear Algebra, McGraw\u2013Hill, ISBN 978-0-07-038023-3 \nMcMahon, David (October 28, 2005), Linear Algebra Demystified, McGraw\u2013Hill Professional, ISBN 978-0-07-146579-3 \nZhang, Fuzhen (April 7, 2009), Linear Algebra: Challenging Problems for Students, The Johns Hopkins University Press, ISBN 978-0-8018-9125-0 \n\n\n== External links ==\n\n\n=== Online Resources ===\nInternational Linear Algebra Society\nHazewinkel, Michiel, ed. (2001) [1994], \"Linear algebra\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nLinear Algebra on MathWorld.\nMatrix and Linear Algebra Terms on Earliest Known Uses of Some of the Words of Mathematics\nEarliest Uses of Symbols for Matrices and Vectors on Earliest Uses of Various Mathematical Symbols\n\n\n=== Online books ===\nBeezer, Rob, A First Course in Linear Algebra\nConnell, Edwin H., Elements of Abstract and Linear Algebra\nHefferon, Jim, Linear Algebra\nMatthews, Keith, Elementary Linear Algebra\nSharipov, Ruslan, Course of linear algebra and multidimensional geometry\nTreil, Sergei, Linear Algebra Done Wrong", "string theory": "In physics, string theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. It describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.\nString theory is a broad and varied subject that attempts to address a number of deep questions of fundamental physics. String theory has been applied to a variety of problems in black hole physics, early universe cosmology, nuclear physics, and condensed matter physics, and it has stimulated a number of major developments in pure mathematics. Because string theory potentially provides a unified description of gravity and particle physics, it is a candidate for a theory of everything, a self-contained mathematical model that describes all fundamental forces and forms of matter. Despite much work on these problems, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of its details.\nString theory was first studied in the late 1960s as a theory of the strong nuclear force, before being abandoned in favor of quantum chromodynamics. Subsequently, it was realized that the very properties that made string theory unsuitable as a theory of nuclear physics made it a promising candidate for a quantum theory of gravity. The earliest version of string theory, bosonic string theory, incorporated only the class of particles known as bosons. It later developed into superstring theory, which posits a connection called supersymmetry between bosons and the class of particles called fermions. Five consistent versions of superstring theory were developed before it was conjectured in the mid-1990s that they were all different limiting cases of a single theory in eleven dimensions known as M-theory. In late 1997, theorists discovered an important relationship called the AdS/CFT correspondence, which relates string theory to another type of physical theory called a quantum field theory.\nOne of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. Another issue is that the theory is thought to describe an enormous landscape of possible universes, and this has complicated efforts to develop theories of particle physics based on string theory. These issues have led some in the community to criticize these approaches to physics and question the value of continued research on string theory unification.\n\n\n== Fundamentals ==\n\nIn the twentieth century, two theoretical frameworks emerged for formulating the laws of physics. The first is Albert Einstein's general theory of relativity, a theory that explains the force of gravity and the structure of space and time. The other is quantum mechanics which is a completely different formulation to describe physical phenomena using the known probability principles. By the late 1970s, these two frameworks had proven to be sufficient to explain most of the observed features of the universe, from elementary particles to atoms to the evolution of stars and the universe as a whole.In spite of these successes, there are still many problems that remain to be solved. One of the deepest problems in modern physics is the problem of quantum gravity. The general theory of relativity is formulated within the framework of classical physics, whereas the other fundamental forces are described within the framework of quantum mechanics. A quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, but difficulties arise when one attempts to apply the usual prescriptions of quantum theory to the force of gravity. In addition to the problem of developing a consistent theory of quantum gravity, there are many other fundamental problems in the physics of atomic nuclei, black holes, and the early universe.String theory is a theoretical framework that attempts to address these questions and many others. The starting point for string theory is the idea that the point-like particles of particle physics can also be modeled as one-dimensional objects called strings. String theory describes how strings propagate through space and interact with each other. In a given version of string theory, there is only one kind of string, which may look like a small loop or segment of ordinary string, and it can vibrate in different ways. On distance scales larger than the string scale, a string will look just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In this way, all of the different elementary particles may be viewed as vibrating strings. In string theory, one of the vibrational states of the string gives rise to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.One of the main developments of the past several decades in string theory was the discovery of certain \"dualities\", mathematical transformations that identify one physical theory with another. Physicists studying string theory have discovered a number of these dualities between different versions of string theory, and this has led to the conjecture that all consistent versions of string theory are subsumed in a single framework known as M-theory.Studies of string theory have also yielded a number of results on the nature of black holes and the gravitational interaction. There are certain paradoxes that arise when one attempts to understand the quantum aspects of black holes, and work on string theory has attempted to clarify these issues. In late 1997 this line of work culminated in the discovery of the anti-de Sitter/conformal field theory correspondence or AdS/CFT. This is a theoretical result which relates string theory to other physical theories which are better understood theoretically. The AdS/CFT correspondence has implications for the study of black holes and quantum gravity, and it has been applied to other subjects, including nuclear and condensed matter physics.Since string theory incorporates all of the fundamental interactions, including gravity, many physicists hope that it fully describes our universe, making it a theory of everything. One of the goals of current research in string theory is to find a solution of the theory that reproduces the observed spectrum of elementary particles, with a small cosmological constant, containing dark matter and a plausible mechanism for cosmic inflation. While there has been progress toward these goals, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of details.One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. The scattering of strings is most straightforwardly defined using the techniques of perturbation theory, but it is not known in general how to define string theory nonperturbatively. It is also not clear whether there is any principle by which string theory selects its vacuum state, the physical state that determines the properties of our universe. These problems have led some in the community to criticize these approaches to the unification of physics and question the value of continued research on these problems.\n\n\n=== Strings ===\n\nThe application of quantum mechanics to physical objects such as the electromagnetic field, which are extended in space and time, is known as quantum field theory. In particle physics, quantum field theories form the basis for our understanding of elementary particles, which are modeled as excitations in the fundamental fields.In quantum field theory, one typically computes the probabilities of various physical events using the techniques of perturbation theory. Developed by Richard Feynman and others in the first half of the twentieth century, perturbative quantum field theory uses special diagrams called Feynman diagrams to organize computations. One imagines that these diagrams depict the paths of point-like particles and their interactions.The starting point for string theory is the idea that the point-like particles of quantum field theory can also be modeled as one-dimensional objects called strings. The interaction of strings is most straightforwardly defined by generalizing the perturbation theory used in ordinary quantum field theory. At the level of Feynman diagrams, this means replacing the one-dimensional diagram representing the path of a point particle by a two-dimensional surface representing the motion of a string. Unlike in quantum field theory, string theory does not have a full non-perturbative definition, so many of the theoretical questions that physicists would like to answer remain out of reach.In theories of particle physics based on string theory, the characteristic length scale of strings is assumed to be on the order of the Planck length, or 10\u221235 meters, the scale at which the effects of quantum gravity are believed to become significant. On much larger length scales, such as the scales visible in physics laboratories, such objects would be indistinguishable from zero-dimensional point particles, and the vibrational state of the string would determine the type of particle. One of the vibrational states of a string corresponds to the graviton, a quantum mechanical particle that carries the gravitational force.The original version of string theory was bosonic string theory, but this version described only bosons, a class of particles which transmit forces between the matter particles, or fermions. Bosonic string theory was eventually superseded by theories called superstring theories. These theories describe both bosons and fermions, and they incorporate a theoretical idea called supersymmetry. This is a mathematical relation that exists in certain physical theories between the bosons and fermions. In theories with supersymmetry, each boson has a counterpart which is a fermion, and vice versa.There are several versions of superstring theory: type I, type IIA, type IIB, and two flavors of heterotic string theory (SO(32) and E8\u00d7E8). The different theories allow different types of strings, and the particles that arise at low energies exhibit different symmetries. For example, the type I theory includes both open strings (which are segments with endpoints) and closed strings (which form closed loops), while types IIA, IIB and heterotic include only closed strings.\n\n\n=== Extra dimensions ===\n\nIn everyday life, there are three familiar dimensions of space: height, width and length. Einstein's general theory of relativity treats time as a dimension on par with the three spatial dimensions; in general relativity, space and time are not modeled as separate entities but are instead unified to a four-dimensional spacetime. In this framework, the phenomenon of gravity is viewed as a consequence of the geometry of spacetime.In spite of the fact that the universe is well described by four-dimensional spacetime, there are several reasons why physicists consider theories in other dimensions. In some cases, by modeling spacetime in a different number of dimensions, a theory becomes more mathematically tractable, and one can perform calculations and gain general insights more easily. There are also situations where theories in two or three spacetime dimensions are useful for describing phenomena in condensed matter physics. Finally, there exist scenarios in which there could actually be more than four dimensions of spacetime which have nonetheless managed to escape detection.One notable feature of string theories is that these theories require extra dimensions of spacetime for their mathematical consistency. In bosonic string theory, spacetime is 26-dimensional, while in superstring theory it is 10-dimensional, and in M-theory it is 11-dimensional. In order to describe real physical phenomena using string theory, one must therefore imagine scenarios in which these extra dimensions would not be observed in experiments.\n\nCompactification is one way of modifying the number of dimensions in a physical theory. In compactification, some of the extra dimensions are assumed to \"close up\" on themselves to form circles. In the limit where these curled up dimensions become very small, one obtains a theory in which spacetime has effectively a lower number of dimensions. A standard analogy for this is to consider a multidimensional object such as a garden hose. If the hose is viewed from a sufficient distance, it appears to have only one dimension, its length. However, as one approaches the hose, one discovers that it contains a second dimension, its circumference. Thus, an ant crawling on the surface of the hose would move in two dimensions.Compactification can be used to construct models in which spacetime is effectively four-dimensional. However, not every way of compactifying the extra dimensions produces a model with the right properties to describe nature. In a viable model of particle physics, the compact extra dimensions must be shaped like a Calabi\u2013Yau manifold. A Calabi\u2013Yau manifold is a special space which is typically taken to be six-dimensional in applications to string theory. It is named after mathematicians Eugenio Calabi and Shing-Tung Yau.Another approach to reducing the number of dimensions is the so-called brane-world scenario. In this approach, physicists assume that the observable universe is a four-dimensional subspace of a higher dimensional space. In such models, the force-carrying bosons of particle physics arise from open strings with endpoints attached to the four-dimensional subspace, while gravity arises from closed strings propagating through the larger ambient space. This idea plays an important role in attempts to develop models of real world physics based on string theory, and it provides a natural explanation for the weakness of gravity compared to the other fundamental forces.\n\n\n=== Dualities ===\n\nOne notable fact about string theory is that the different versions of the theory all turn out to be related in highly nontrivial ways. One of the relationships that can exist between different string theories is called S-duality. This is a relationship which says that a collection of strongly interacting particles in one theory can, in some cases, be viewed as a collection of weakly interacting particles in a completely different theory. Roughly speaking, a collection of particles is said to be strongly interacting if they combine and decay often and weakly interacting if they do so infrequently. Type I string theory turns out to be equivalent by S-duality to the SO(32) heterotic string theory. Similarly, type IIB string theory is related to itself in a nontrivial way by S-duality.Another relationship between different string theories is T-duality. Here one considers strings propagating around a circular extra dimension. T-duality states that a string propagating around a circle of radius R is equivalent to a string propagating around a circle of radius 1/R in the sense that all observable quantities in one description are identified with quantities in the dual description. For example, a string has momentum as it propagates around a circle, and it can also wind around the circle one or more times. The number of times the string winds around a circle is called the winding number. If a string has momentum p and winding number n in one description, it will have momentum n and winding number p in the dual description. For example, type IIA string theory is equivalent to type IIB string theory via T-duality, and the two versions of heterotic string theory are also related by T-duality.In general, the term duality refers to a situation where two seemingly different physical systems turn out to be equivalent in a nontrivial way. Two theories related by a duality need not be string theories. For example, Montonen\u2013Olive duality is example of an S-duality relationship between quantum field theories. The AdS/CFT correspondence is example of a duality which relates string theory to a quantum field theory. If two theories are related by a duality, it means that one theory can be transformed in some way so that it ends up looking just like the other theory. The two theories are then said to be dual to one another under the transformation. Put differently, the two theories are mathematically different descriptions of the same phenomena.\n\n\n=== Branes ===\n\nIn string theory and other related theories, a brane is a physical object that generalizes the notion of a point particle to higher dimensions. For instance, a point particle can be viewed as a brane of dimension zero, while a string can be viewed as a brane of dimension one. It is also possible to consider higher-dimensional branes. In dimension p, these are called p-branes. The word brane comes from the word \"membrane\" which refers to a two-dimensional brane.Branes are dynamical objects which can propagate through spacetime according to the rules of quantum mechanics. They have mass and can have other attributes such as charge. A p-brane sweeps out a (p+1)-dimensional volume in spacetime called its worldvolume. Physicists often study fields analogous to the electromagnetic field which live on the worldvolume of a brane.In string theory, D-branes are an important class of branes that arise when one considers open strings. As an open string propagates through spacetime, its endpoints are required to lie on a D-brane. The letter \"D\" in D-brane refers to a certain mathematical condition on the system known as the Dirichlet boundary condition. The study of D-branes in string theory has led to important results such as the AdS/CFT correspondence, which has shed light on many problems in quantum field theory.Branes are frequently studied from a purely mathematical point of view, and they are described as objects of certain categories, such as the derived category of coherent sheaves on a complex algebraic variety, or the Fukaya category of a symplectic manifold. The connection between the physical notion of a brane and the mathematical notion of a category has led to important mathematical insights in the fields of algebraic and symplectic geometry  and representation theory.\n\n\n== M-theory ==\n\nPrior to 1995, theorists believed that there were five consistent versions of superstring theory (type I, type IIA, type IIB, and two versions of heterotic string theory). This understanding changed in 1995 when Edward Witten suggested that the five theories were just special limiting cases of an eleven-dimensional theory called M-theory. Witten's conjecture was based on the work of a number of other physicists, including Ashoke Sen, Chris Hull, Paul Townsend, and Michael Duff. His announcement led to a flurry of research activity now known as the second superstring revolution.\n\n\n=== Unification of superstring theories ===\n\nIn the 1970s, many physicists became interested in supergravity theories, which combine general relativity with supersymmetry. Whereas general relativity makes sense in any number of dimensions, supergravity places an upper limit on the number of dimensions. In 1978, work by Werner Nahm showed that the maximum spacetime dimension in which one can formulate a consistent supersymmetric theory is eleven. In the same year, Eugene Cremmer, Bernard Julia, and Joel Scherk of the \u00c9cole Normale Sup\u00e9rieure showed that supergravity not only permits up to eleven dimensions but is in fact most elegant in this maximal number of dimensions.Initially, many physicists hoped that by compactifying eleven-dimensional supergravity, it might be possible to construct realistic models of our four-dimensional world. The hope was that such models would provide a unified description of the four fundamental forces of nature: electromagnetism, the strong and weak nuclear forces, and gravity. Interest in eleven-dimensional supergravity soon waned as various flaws in this scheme were discovered. One of the problems was that the laws of physics appear to distinguish between clockwise and counterclockwise, a phenomenon known as chirality. Edward Witten and others observed this chirality property cannot be readily derived by compactifying from eleven dimensions.In the first superstring revolution in 1984, many physicists turned to string theory as a unified theory of particle physics and quantum gravity. Unlike supergravity theory, string theory was able to accommodate the chirality of the standard model, and it provided a theory of gravity consistent with quantum effects. Another feature of string theory that many physicists were drawn to in the 1980s and 1990s was its high degree of uniqueness. In ordinary particle theories, one can consider any collection of elementary particles whose classical behavior is described by an arbitrary Lagrangian. In string theory, the possibilities are much more constrained: by the 1990s, physicists had argued that there were only five consistent supersymmetric versions of the theory.Although there were only a handful of consistent superstring theories, it remained a mystery why there was not just one consistent formulation. However, as physicists began to examine string theory more closely, they realized that these theories are related in intricate and nontrivial ways. They found that a system of strongly interacting strings can, in some cases, be viewed as a system of weakly interacting strings. This phenomenon is known as S-duality. It was studied by Ashoke Sen in the context of heterotic strings in four dimensions and by Chris Hull and Paul Townsend in the context of the type IIB theory. Theorists also found that different string theories may be related by T-duality. This duality implies that strings propagating on completely different spacetime geometries may be physically equivalent.At around the same time, as many physicists were studying the properties of strings, a small group of physicists was examining the possible applications of higher dimensional objects. In 1987, Eric Bergshoeff, Ergin Sezgin, and Paul Townsend showed that eleven-dimensional supergravity includes two-dimensional branes. Intuitively, these objects look like sheets or membranes propagating through the eleven-dimensional spacetime. Shortly after this discovery, Michael Duff, Paul Howe, Takeo Inami, and Kellogg Stelle considered a particular compactification of eleven-dimensional supergravity with one of the dimensions curled up into a circle. In this setting, one can imagine the membrane wrapping around the circular dimension. If the radius of the circle is sufficiently small, then this membrane looks just like a string in ten-dimensional spacetime. In fact, Duff and his collaborators showed that this construction reproduces exactly the strings appearing in type IIA superstring theory.Speaking at a string theory conference in 1995, Edward Witten made the surprising suggestion that all five superstring theories were in fact just different limiting cases of a single theory in eleven spacetime dimensions. Witten's announcement drew together all of the previous results on S- and T-duality and the appearance of higher dimensional branes in string theory. In the months following Witten's announcement, hundreds of new papers appeared on the Internet confirming different parts of his proposal. Today this flurry of work is known as the second superstring revolution.Initially, some physicists suggested that the new theory was a fundamental theory of membranes, but Witten was skeptical of the role of membranes in the theory. In a paper from 1996, Ho\u0159ava and Witten wrote \"As it has been proposed that the eleven-dimensional theory is a supermembrane theory but there are some reasons to doubt that interpretation, we will non-committally call it the M-theory, leaving to the future the relation of M to membranes.\" In the absence of an understanding of the true meaning and structure of M-theory, Witten has suggested that the M should stand for \"magic\", \"mystery\", or \"membrane\" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.\n\n\n=== Matrix theory ===\n\nIn mathematics, a matrix is a rectangular array of numbers or other data. In physics, a matrix model is a particular kind of physical theory whose mathematical formulation involves the notion of a matrix in an important way. A matrix model describes the behavior of a set of matrices within the framework of quantum mechanics.One important example of a matrix model is the BFSS matrix model proposed by Tom Banks, Willy Fischler, Stephen Shenker, and Leonard Susskind in 1997. This theory describes the behavior of a set of nine large matrices. In their original paper, these authors showed, among other things, that the low energy limit of this matrix model is described by eleven-dimensional supergravity. These calculations led them to propose that the BFSS matrix model is exactly equivalent to M-theory. The BFSS matrix model can therefore be used as a prototype for a correct formulation of M-theory and a tool for investigating the properties of M-theory in a relatively simple setting.The development of the matrix model formulation of M-theory has led physicists to consider various connections between string theory and a branch of mathematics called noncommutative geometry. This subject is a generalization of ordinary geometry in which mathematicians define new geometric notions using tools from noncommutative algebra. In a paper from 1998, Alain Connes, Michael R. Douglas, and Albert Schwarz showed that some aspects of matrix models and M-theory are described by a noncommutative quantum field theory, a special kind of physical theory in which spacetime is described mathematically using noncommutative geometry. This established a link between matrix models and M-theory on the one hand, and noncommutative geometry on the other hand. It quickly led to the discovery of other important links between noncommutative geometry and various physical theories.\n\n\n== Black holes ==\nIn general relativity, a black hole is defined as a region of spacetime in which the gravitational field is so strong that no particle or radiation can escape. In the currently accepted models of stellar evolution, black holes are thought to arise when massive stars undergo gravitational collapse, and many galaxies are thought to contain supermassive black holes at their centers. Black holes are also important for theoretical reasons, as they present profound challenges for theorists attempting to understand the quantum aspects of gravity. String theory has proved to be an important tool for investigating the theoretical properties of black holes because it provides a framework in which theorists can study their thermodynamics.\n\n\n=== Bekenstein\u2013Hawking formula ===\nIn the branch of physics called statistical mechanics, entropy is a measure of the randomness or disorder of a physical system. This concept was studied in the 1870s by the Austrian physicist Ludwig Boltzmann, who showed that the thermodynamic properties of a gas could be derived from the combined properties of its many constituent molecules. Boltzmann argued that by averaging the behaviors of all the different molecules in a gas, one can understand macroscopic properties such as volume, temperature, and pressure. In addition, this perspective led him to give a precise definition of entropy as the natural logarithm of the number of different states of the molecules (also called microstates) that give rise to the same macroscopic features.In the twentieth century, physicists began to apply the same concepts to black holes. In most systems such as gases, the entropy scales with the volume. In the 1970s, the physicist Jacob Bekenstein suggested that the entropy of a black hole is instead proportional to the surface area of its event horizon, the boundary beyond which matter and radiation is lost to its gravitational attraction. When combined with ideas of the physicist Stephen Hawking, Bekenstein's work yielded a precise formula for the entropy of a black hole. The Bekenstein\u2013Hawking formula expresses the entropy S as\n\n  \n    \n      \n        S\n        =\n        \n          \n            \n              \n                c\n                \n                  3\n                \n              \n              k\n              A\n            \n            \n              4\n              \u210f\n              G\n            \n          \n        \n      \n    \n    {\\displaystyle S={\\frac {c^{3}kA}{4\\hbar G}}}\n  where c is the speed of light, k is Boltzmann's constant, \u0127 is the reduced Planck constant, G is Newton's constant, and A is the surface area of the event horizon.Like any physical system, a black hole has an entropy defined in terms of the number of different microstates that lead to the same macroscopic features. The Bekenstein\u2013Hawking entropy formula gives the expected value of the entropy of a black hole, but by the 1990s, physicists still lacked a derivation of this formula by counting microstates in a theory of quantum gravity. Finding such a derivation of this formula was considered an important test of the viability of any theory of quantum gravity such as string theory.\n\n\n=== Derivation within string theory ===\nIn a paper from 1996, Andrew Strominger and Cumrun Vafa showed how to derive the Beckenstein\u2013Hawking formula for certain black holes in string theory. Their calculation was based on the observation that D-branes\u2014which look like fluctuating membranes when they are weakly interacting\u2014become dense, massive objects with event horizons when the interactions are strong. In other words, a system of strongly interacting D-branes in string theory is indistinguishable from a black hole. Strominger and Vafa analyzed such D-brane systems and calculated the number of different ways of placing D-branes in spacetime so that their combined mass and charge is equal to a given mass and charge for the resulting black hole. Their calculation reproduced the Bekenstein\u2013Hawking formula exactly, including the factor of 1/4. Subsequent work by Strominger, Vafa, and others refined the original calculations and gave the precise values of the \"quantum corrections\" needed to describe very small black holes.The black holes that Strominger and Vafa considered in their original work were quite different from real astrophysical black holes. One difference was that Strominger and Vafa considered only extremal black holes in order to make the calculation tractable. These are defined as black holes with the lowest possible mass compatible with a given charge. Strominger and Vafa also restricted attention to black holes in five-dimensional spacetime with unphysical supersymmetry.Although it was originally developed in this very particular and physically unrealistic context in string theory, the entropy calculation of Strominger and Vafa has led to a qualitative understanding of how black hole entropy can be accounted for in any theory of quantum gravity. Indeed, in 1998, Strominger argued that the original result could be generalized to an arbitrary consistent theory of quantum gravity without relying on strings or supersymmetry. In collaboration with several other authors in 2010, he showed that some results on black hole entropy could be extended to non-extremal astrophysical black holes.\n\n\n== AdS/CFT correspondence ==\n\nOne approach to formulating string theory and studying its properties is provided by the anti-de Sitter/conformal field theory (AdS/CFT) correspondence. This is a theoretical result which implies that string theory is in some cases equivalent to a quantum field theory. In addition to providing insights into the mathematical structure of string theory, the AdS/CFT correspondence has shed light on many aspects of quantum field theory in regimes where traditional calculational techniques are ineffective. The AdS/CFT correspondence was first proposed by Juan Maldacena in late 1997. Important aspects of the correspondence were elaborated in articles by Steven Gubser, Igor Klebanov, and Alexander Markovich Polyakov, and by Edward Witten. By 2010, Maldacena's article had over 7000 citations, becoming the most highly cited article in the field of high energy physics.\n\n\n=== Overview of the correspondence ===\n\nIn the AdS/CFT correspondence, the geometry of spacetime is described in terms of a certain vacuum solution of Einstein's equation called anti-de Sitter space. In very elementary terms, anti-de Sitter space is a mathematical model of spacetime in which the notion of distance between points (the metric) is different from the notion of distance in ordinary Euclidean geometry. It is closely related to hyperbolic space, which can be viewed as a disk as illustrated on the left. This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.One can imagine a stack of hyperbolic disks where each disk represents the state of the universe at a given time. The resulting geometric object is three-dimensional anti-de Sitter space. It looks like a solid cylinder in which any cross section is a copy of the hyperbolic disk. Time runs along the vertical direction in this picture. The surface of this cylinder plays an important role in the AdS/CFT correspondence. As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.\n\nThis construction describes a hypothetical universe with only two space dimensions and one time dimension, but it can be generalized to any number of dimensions. Indeed, hyperbolic space can have more than two dimensions and one can \"stack up\" copies of hyperbolic space to get higher-dimensional models of anti-de Sitter space.An important feature of anti-de Sitter space is its boundary (which looks like a cylinder in the case of three-dimensional anti-de Sitter space). One property of this boundary is that, within a small region on the surface around any given point, it looks just like Minkowski space, the model of spacetime used in nongravitational physics. One can therefore consider an auxiliary theory in which \"spacetime\" is given by the boundary of anti-de Sitter space. This observation is the starting point for AdS/CFT correspondence, which states that the boundary of anti-de Sitter space can be regarded as the \"spacetime\" for a quantum field theory. The claim is that this quantum field theory is equivalent to a gravitational theory, such as string theory, in the bulk anti-de Sitter space in the sense that there is a \"dictionary\" for translating entities and calculations in one theory into their counterparts in the other theory. For example, a single particle in the gravitational theory might correspond to some collection of particles in the boundary theory. In addition, the predictions in the two theories are quantitatively identical so that if two particles have a 40 percent chance of colliding in the gravitational theory, then the corresponding collections in the boundary theory would also have a 40 percent chance of colliding.\n\n\n=== Applications to quantum gravity ===\nThe discovery of the AdS/CFT correspondence was a major advance in physicists' understanding of string theory and quantum gravity. One reason for this is that the correspondence provides a formulation of string theory in terms of quantum field theory, which is well understood by comparison. Another reason is that it provides a general framework in which physicists can study and attempt to resolve the paradoxes of black holes.In 1975, Stephen Hawking published a calculation which suggested that black holes are not completely black but emit a dim radiation due to quantum effects near the event horizon. At first, Hawking's result posed a problem for theorists because it suggested that black holes destroy information. More precisely, Hawking's calculation seemed to conflict with one of the basic postulates of quantum mechanics, which states that physical systems evolve in time according to the Schr\u00f6dinger equation. This property is usually referred to as unitarity of time evolution. The apparent contradiction between Hawking's calculation and the unitarity postulate of quantum mechanics came to be known as the black hole information paradox.The AdS/CFT correspondence resolves the black hole information paradox, at least to some extent, because it shows how a black hole can evolve in a manner consistent with quantum mechanics in some contexts. Indeed, one can consider black holes in the context of the AdS/CFT correspondence, and any such black hole corresponds to a configuration of particles on the boundary of anti-de Sitter space. These particles obey the usual rules of quantum mechanics and in particular evolve in a unitary fashion, so the black hole must also evolve in a unitary fashion, respecting the principles of quantum mechanics. In 2005, Hawking announced that the paradox had been settled in favor of information conservation by the AdS/CFT correspondence, and he suggested a concrete mechanism by which black holes might preserve information.\n\n\n=== Applications to nuclear physics ===\n\nIn addition to its applications to theoretical problems in quantum gravity, the AdS/CFT correspondence has been applied to a variety of problems in quantum field theory. One physical system that has been studied using the AdS/CFT correspondence is the quark\u2013gluon plasma, an exotic state of matter produced in particle accelerators. This state of matter arises for brief instants when heavy ions such as gold or lead nuclei are collided at high energies. Such collisions cause the quarks that make up atomic nuclei to deconfine at temperatures of approximately two trillion kelvins, conditions similar to those present at around 10\u221211 seconds after the Big Bang.The physics of the quark\u2013gluon plasma is governed by a theory called quantum chromodynamics, but this theory is mathematically intractable in problems involving the quark\u2013gluon plasma. In an article appearing in 2005, \u0110\u00e0m Thanh S\u01a1n and his collaborators showed that the AdS/CFT correspondence could be used to understand some aspects of the quark\u2013gluon plasma by describing it in the language of string theory. By applying the AdS/CFT correspondence, S\u01a1n and his collaborators were able to describe the quark gluon plasma in terms of black holes in five-dimensional spacetime. The calculation showed that the ratio of two quantities associated with the quark\u2013gluon plasma, the shear viscosity and volume density of entropy, should be approximately equal to a certain universal constant. In 2008, the predicted value of this ratio for the quark\u2013gluon plasma was confirmed at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory.\n\n\n=== Applications to condensed matter physics ===\n\nThe AdS/CFT correspondence has also been used to study aspects of condensed matter physics. Over the decades, experimental condensed matter physicists have discovered a number of exotic states of matter, including superconductors and superfluids. These states are described using the formalism of quantum field theory, but some phenomena are difficult to explain using standard field theoretic techniques. Some condensed matter theorists including Subir Sachdev hope that the AdS/CFT correspondence will make it possible to describe these systems in the language of string theory and learn more about their behavior.So far some success has been achieved in using string theory methods to describe the transition of a superfluid to an insulator. A superfluid is a system of electrically neutral atoms that flows without any friction. Such systems are often produced in the laboratory using liquid helium, but recently experimentalists have developed new ways of producing artificial superfluids by pouring trillions of cold atoms into a lattice of criss-crossing lasers. These atoms initially behave as a superfluid, but as experimentalists increase the intensity of the lasers, they become less mobile and then suddenly transition to an insulating state. During the transition, the atoms behave in an unusual way. For example, the atoms slow to a halt at a rate that depends on the temperature and on Planck's constant, the fundamental parameter of quantum mechanics, which does not enter into the description of the other phases. This behavior has recently been understood by considering a dual description where properties of the fluid are described in terms of a higher dimensional black hole.\n\n\n== Phenomenology ==\n\nIn addition to being an idea of considerable theoretical interest, string theory provides a framework for constructing models of real world physics that combine general relativity and particle physics. Phenomenology is the branch of theoretical physics in which physicists construct realistic models of nature from more abstract theoretical ideas. String phenomenology is the part of string theory that attempts to construct realistic or semi-realistic models based on string theory.\nPartly because of theoretical and mathematical difficulties and partly because of the extremely high energies needed to test these theories experimentally, there is so far no experimental evidence that would unambiguously point to any of these models being a correct fundamental description of nature. This has led some in the community to criticize these approaches to unification and question the value of continued research on these problems.\n\n\n=== Particle physics ===\nThe currently accepted theory describing elementary particles and their interactions is known as the standard model of particle physics. This theory provides a unified description of three of the fundamental forces of nature: electromagnetism and the strong and weak nuclear forces. Despite its remarkable success in explaining a wide range of physical phenomena, the standard model cannot be a complete description of reality. This is because the standard model fails to incorporate the force of gravity and because of problems such as the hierarchy problem and the inability to explain the structure of fermion masses or dark matter.\nString theory has been used to construct a variety of models of particle physics going beyond the standard model. Typically, such models are based on the idea of compactification. Starting with the ten- or eleven-dimensional spacetime of string or M-theory, physicists postulate a shape for the extra dimensions. By choosing this shape appropriately, they can construct models roughly similar to the standard model of particle physics, together with additional undiscovered particles. One popular way of deriving realistic physics from string theory is to start with the heterotic theory in ten dimensions and assume that the six extra dimensions of spacetime are shaped like a six-dimensional Calabi\u2013Yau manifold. Such compactifications offer many ways of extracting realistic physics from string theory. Other similar methods can be used to construct realistic or semi-realistic models of our four-dimensional world based on M-theory.\n\n\n=== Cosmology ===\n\nThe Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent large-scale evolution. Despite its success in explaining many observed features of the universe including galactic redshifts, the relative abundance of light elements such as hydrogen and helium, and the existence of a cosmic microwave background, there are several questions that remain unanswered. For example, the standard Big Bang model does not explain why the universe appears to be same in all directions, why it appears flat on very large distance scales, or why certain hypothesized particles such as magnetic monopoles are not observed in experiments.Currently, the leading candidate for a theory going beyond the Big Bang is the theory of cosmic inflation. Developed by Alan Guth and others in the 1980s, inflation postulates a period of extremely rapid accelerated expansion of the universe prior to the expansion described by the standard Big Bang theory. The theory of cosmic inflation preserves the successes of the Big Bang while providing a natural explanation for some of the mysterious features of the universe. The theory has also received striking support from observations of the cosmic microwave background, the radiation that has filled the sky since around 380,000 years after the Big Bang.In the theory of inflation, the rapid initial expansion of the universe is caused by a hypothetical particle called the inflaton. The exact properties of this particle are not fixed by the theory but should ultimately be derived from a more fundamental theory such as string theory. Indeed, there have been a number of attempts to identify an inflaton within the spectrum of particles described by string theory, and to study inflation using string theory. While these approaches might eventually find support in observational data such as measurements of the cosmic microwave background, the application of string theory to cosmology is still in its early stages.\n\n\n== Connections to mathematics ==\nIn addition to influencing research in theoretical physics, string theory has stimulated a number of major developments in pure mathematics. Like many developing ideas in theoretical physics, string theory does not at present have a mathematically rigorous formulation in which all of its concepts can be defined precisely. As a result, physicists who study string theory are often guided by physical intuition to conjecture relationships between the seemingly different mathematical structures that are used to formalize different parts of the theory. These conjectures are later proved by mathematicians, and in this way, string theory serves as a source of new ideas in pure mathematics.\n\n\n=== Mirror symmetry ===\n\nAfter Calabi\u2013Yau manifolds had entered physics as a way to compactify extra dimensions in string theory, many physicists began studying these manifolds. In the late 1980s, several physicists noticed that given such a compactification of string theory, it is not possible to reconstruct uniquely a corresponding Calabi\u2013Yau manifold. Instead, two different versions of string theory, type IIA and type IIB, can be compactified on completely different Calabi\u2013Yau manifolds giving rise to the same physics. In this situation, the manifolds are called mirror manifolds, and the relationship between the two physical theories is called mirror symmetry.Regardless of whether Calabi\u2013Yau compactifications of string theory provide a correct description of nature, the existence of the mirror duality between different string theories has significant mathematical consequences. The Calabi\u2013Yau manifolds used in string theory are of interest in pure mathematics, and mirror symmetry allows mathematicians to solve problems in enumerative geometry, a branch of mathematics concerned with counting the numbers of solutions to geometric questions.Enumerative geometry studies a class of geometric objects called algebraic varieties which are defined by the vanishing of polynomials. For example, the Clebsch cubic illustrated on the right is an algebraic variety defined using a certain polynomial of degree three in four variables. A celebrated result of nineteenth-century mathematicians Arthur Cayley and George Salmon states that there are exactly 27 straight lines that lie entirely on such a surface.Generalizing this problem, one can ask how many lines can be drawn on a quintic Calabi\u2013Yau manifold, such as the one illustrated above, which is defined by a polynomial of degree five. This problem was solved by the nineteenth-century German mathematician Hermann Schubert, who found that there are exactly 2,875 such lines. In 1986, geometer Sheldon Katz proved that the number of curves, such as circles, that are defined by polynomials of degree two and lie entirely in the quintic is 609,250.By the year 1991, most of the classical problems of enumerative geometry had been solved and interest in enumerative geometry had begun to diminish. The field was reinvigorated in May 1991 when physicists Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parks showed that mirror symmetry could be used to translate difficult mathematical questions about one Calabi\u2013Yau manifold into easier questions about its mirror. In particular, they used mirror symmetry to show that a six-dimensional Calabi\u2013Yau manifold can contain exactly 317,206,375 curves of degree three. In addition to counting degree-three curves, Candelas and his collaborators obtained a number of more general results for counting rational curves which went far beyond the results obtained by mathematicians.Originally, these results of Candelas were justified on physical grounds. However, mathematicians generally prefer rigorous proofs that do not require an appeal to physical intuition. Inspired by physicists' work on mirror symmetry, mathematicians have therefore constructed their own arguments proving the enumerative predictions of mirror symmetry. Today mirror symmetry is an active area of research in mathematics, and mathematicians are working to develop a more complete mathematical understanding of mirror symmetry based on physicists' intuition. Major approaches to mirror symmetry include the homological mirror symmetry program of Maxim Kontsevich and the SYZ conjecture of Andrew Strominger, Shing-Tung Yau, and Eric Zaslow.\n\n\n=== Monstrous moonshine ===\n\nGroup theory is the branch of mathematics that studies the concept of symmetry. For example, one can consider a geometric shape such as an equilateral triangle. There are various operations that one can perform on this triangle without changing its shape. One can rotate it through 120\u00b0, 240\u00b0, or 360\u00b0, or one can reflect in any of the lines labeled S0, S1, or S2 in the picture. Each of these operations is called a symmetry, and the collection of these symmetries satisfies certain technical properties making it into what mathematicians call a group. In this particular example, the group is known as the dihedral group of order 6 because it has six elements. A general group may describe finitely many or infinitely many symmetries; if there are only finitely many symmetries, it is called a finite group.Mathematicians often strive for a classification (or list) of all mathematical objects of a given type. It is generally believed that finite groups are too diverse to admit a useful classification. A more modest but still challenging problem is to classify all finite simple groups. These are finite groups which may be used as building blocks for constructing arbitrary finite groups in the same way that prime numbers can be used to construct arbitrary whole numbers by taking products. One of the major achievements of contemporary group theory is the classification of finite simple groups, a mathematical theorem which provides a list of all possible finite simple groups.This classification theorem identifies several infinite families of groups as well as 26 additional groups which do not fit into any family. The latter groups are called the \"sporadic\" groups, and each one owes its existence to a remarkable combination of circumstances. The largest sporadic group, the so-called monster group, has over 1053 elements, more than a thousand times the number of atoms in the Earth.\n\nA seemingly unrelated construction is the j-function of number theory. This object belongs to a special class of functions called modular functions, whose graphs form a certain kind of repeating pattern. Although this function appears in a branch of mathematics which seems very different from the theory of finite groups, the two subjects turn out to be intimately related. In the late 1970s, mathematicians John McKay and John Thompson noticed that certain numbers arising in the analysis of the monster group (namely, the dimensions of its irreducible representations) are related to numbers that appear in a formula for the j-function (namely, the coefficients of its Fourier series). This relationship was further developed by John Horton Conway and Simon Norton who called it monstrous moonshine because it seemed so far fetched.In 1992, Richard Borcherds constructed a bridge between the theory of modular functions and finite groups and, in the process, explained the observations of McKay and Thompson. Borcherds' work used ideas from string theory in an essential way, extending earlier results of Igor Frenkel, James Lepowsky, and Arne Meurman, who had realized the monster group as the symmetries of a particular version of string theory. In 1998, Borcherds was awarded the Fields medal for his work.Since the 1990s, the connection between string theory and moonshine has led to further results in mathematics and physics. In 2010, physicists Tohru Eguchi, Hirosi Ooguri, and Yuji Tachikawa discovered connections between a different sporadic group, the Mathieu group M24, and a certain version of string theory. Miranda Cheng, John Duncan, and Jeffrey A. Harvey proposed a generalization of this moonshine phenomenon called umbral moonshine, and their conjecture was proved mathematically by Duncan, Michael Griffin, and Ken Ono. Witten has also speculated that the version of string theory appearing in monstrous moonshine might be related to a certain simplified model of gravity in three spacetime dimensions.\n\n\n== History ==\n\n\n=== Early results ===\nSome of the structures reintroduced by string theory arose for the first time much earlier as part of the program of classical unification started by Albert Einstein. The first person to add a fifth dimension to a theory of gravity was Gunnar Nordstr\u00f6m in 1914, who noted that gravity in five dimensions describes both gravity and electromagnetism in four. Nordstr\u00f6m attempted to unify electromagnetism with his theory of gravitation, which was however superseded by Einstein's general relativity in 1919. Thereafter, German mathematician Theodor Kaluza combined the fifth dimension with general relativity, and only Kaluza is usually credited with the idea. In 1926, the Swedish physicist Oskar Klein gave a physical interpretation of the unobservable extra dimension\u2014it is wrapped into a small circle. Einstein introduced a non-symmetric metric tensor, while much later Brans and Dicke added a scalar component to gravity. These ideas would be revived within string theory, where they are demanded by consistency conditions.\n\nString theory was originally developed during the late 1960s and early 1970s as a never completely successful theory of hadrons, the subatomic particles like the proton and neutron that feel the strong interaction. In the 1960s, Geoffrey Chew and Steven Frautschi discovered that the mesons make families called Regge trajectories with masses related to spins in a way that was later understood by Yoichiro Nambu, Holger Bech Nielsen and Leonard Susskind to be the relationship expected from rotating strings. Chew advocated making a theory for the interactions of these trajectories that did not presume that they were composed of any fundamental particles, but would construct their interactions from self-consistency conditions on the S-matrix. The S-matrix approach was started by Werner Heisenberg in the 1940s as a way of constructing a theory that did not rely on the local notions of space and time, which Heisenberg believed break down at the nuclear scale. While the scale was off by many orders of magnitude, the approach he advocated was ideally suited for a theory of quantum gravity.\nWorking with experimental data, R. Dolen, D. Horn and C. Schmid developed some sum rules for hadron exchange. When a particle and antiparticle scatter, virtual particles can be exchanged in two qualitatively different ways. In the s-channel, the two particles annihilate to make temporary intermediate states that fall apart into the final state particles. In the t-channel, the particles exchange intermediate states by emission and absorption. In field theory, the two contributions add together, one giving a continuous background contribution, the other giving peaks at certain energies. In the data, it was clear that the peaks were stealing from the background\u2014the authors interpreted this as saying that the t-channel contribution was dual to the s-channel one, meaning both described the whole amplitude and included the other.\n\nThe result was widely advertised by Murray Gell-Mann, leading Gabriele Veneziano to construct a scattering amplitude that had the property of Dolen\u2013Horn\u2013Schmid duality, later renamed world-sheet duality. The amplitude needed poles where the particles appear, on straight line trajectories, and there is a special mathematical function whose poles are evenly spaced on half the real line\u2014the gamma function\u2014 which was widely used in Regge theory. By manipulating combinations of gamma functions, Veneziano was able to find a consistent scattering amplitude with poles on straight lines, with mostly positive residues, which obeyed duality and had the appropriate Regge scaling at high energy. The amplitude could fit near-beam scattering data as well as other Regge type fits, and had a suggestive integral representation that could be used for generalization.\nOver the next years, hundreds of physicists worked to complete the bootstrap program for this model, with many surprises. Veneziano himself discovered that for the scattering amplitude to describe the scattering of a particle that appears in the theory, an obvious self-consistency condition, the lightest particle must be a tachyon. Miguel Virasoro and Joel Shapiro found a different amplitude now understood to be that of closed strings, while Ziro Koba and Holger Nielsen generalized Veneziano's integral representation to multiparticle scattering. Veneziano and Sergio Fubini introduced an operator formalism for computing the scattering amplitudes that was a forerunner of world-sheet conformal theory, while Virasoro understood how to remove the poles with wrong-sign residues using a constraint on the states. Claud Lovelace calculated a loop amplitude, and noted that there is an inconsistency unless the dimension of the theory is 26. Charles Thorn, Peter Goddard and Richard Brower went on to prove that there are no wrong-sign propagating states in dimensions less than or equal to 26.\nIn 1969\u201370, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind recognized that the theory could be given a description in space and time in terms of strings. The scattering amplitudes were derived systematically from the action principle by Peter Goddard, Jeffrey Goldstone, Claudio Rebbi, and Charles Thorn, giving a space-time picture to the vertex operators introduced by Veneziano and Fubini and a geometrical interpretation to the Virasoro conditions.\nIn 1971, Pierre Ramond added fermions to the model, which led him to formulate a two-dimensional supersymmetry to cancel the wrong-sign states. John Schwarz and Andr\u00e9 Neveu added another sector to the fermi theory a short time later. In the fermion theories, the critical dimension was 10. Stanley Mandelstam formulated a world sheet conformal theory for both the bose and fermi case, giving a two-dimensional field theoretic path-integral to generate the operator formalism. Michio Kaku and Keiji Kikkawa gave a different formulation of the bosonic string, as a string field theory, with infinitely many particle types and with fields taking values not on points, but on loops and curves.\nIn 1974, Tamiaki Yoneya discovered that all the known string theories included a massless spin-two particle that obeyed the correct Ward identities to be a graviton. John Schwarz and Joel Scherk came to the same conclusion and made the bold leap to suggest that string theory was a theory of gravity, not a theory of hadrons. They reintroduced Kaluza\u2013Klein theory as a way of making sense of the extra dimensions. At the same time, quantum chromodynamics was recognized as the correct theory of hadrons, shifting the attention of physicists and apparently leaving the bootstrap program in the dustbin of history.\nString theory eventually made it out of the dustbin, but for the following decade all work on the theory was completely ignored. Still, the theory continued to develop at a steady pace thanks to the work of a handful of devotees. Ferdinando Gliozzi, Joel Scherk, and David Olive realized in 1977 that the original Ramond and Neveu Schwarz-strings were separately inconsistent and needed to be combined. The resulting theory did not have a tachyon, and was proven to have space-time supersymmetry by John Schwarz and Michael Green in 1984. The same year, Alexander Polyakov gave the theory a modern path integral formulation, and went on to develop conformal field theory extensively. In 1979, Daniel Friedan showed that the equations of motions of string theory, which are generalizations of the Einstein equations of general relativity, emerge from the renormalization group equations for the two-dimensional field theory. Schwarz and Green discovered T-duality, and constructed two superstring theories\u2014IIA and IIB related by T-duality, and type I theories with open strings. The consistency conditions had been so strong, that the entire theory was nearly uniquely determined, with only a few discrete choices.\n\n\n=== First superstring revolution ===\n\nIn the early 1980s, Edward Witten discovered that most theories of quantum gravity could not accommodate chiral fermions like the neutrino. This led him, in collaboration with Luis \u00c1lvarez-Gaum\u00e9, to study violations of the conservation laws in gravity theories with anomalies, concluding that type I string theories were inconsistent. Green and Schwarz discovered a contribution to the anomaly that Witten and Alvarez-Gaum\u00e9 had missed, which restricted the gauge group of the type I string theory to be SO(32). In coming to understand this calculation, Edward Witten became convinced that string theory was truly a consistent theory of gravity, and he became a high-profile advocate. Following Witten's lead, between 1984 and 1986, hundreds of physicists started to work in this field, and this is sometimes called the first superstring revolution.\nDuring this period, David Gross, Jeffrey Harvey, Emil Martinec, and Ryan Rohm discovered heterotic strings. The gauge group of these closed strings was two copies of E8, and either copy could easily and naturally include the standard model. Philip Candelas, Gary Horowitz, Andrew Strominger and Edward Witten found that the Calabi\u2013Yau manifolds are the compactifications that preserve a realistic amount of supersymmetry, while Lance Dixon and others worked out the physical properties of orbifolds, distinctive geometrical singularities allowed in string theory. Cumrun Vafa generalized T-duality from circles to arbitrary manifolds, creating the mathematical field of mirror symmetry. Daniel Friedan, Emil Martinec and Stephen Shenker further developed the covariant quantization of the superstring using conformal field theory techniques. David Gross and Vipul Periwal discovered that string perturbation theory was divergent. Stephen Shenker showed it diverged much faster than in field theory suggesting that new non-perturbative objects were missing.\n\nIn the 1990s, Joseph Polchinski discovered that the theory requires higher-dimensional objects, called D-branes and identified these with the black-hole solutions of supergravity. These were understood to be the new objects suggested by the perturbative divergences, and they opened up a new field with rich mathematical structure. It quickly became clear that D-branes and other p-branes, not just strings, formed the matter content of the string theories, and the physical interpretation of the strings and branes was revealed\u2014they are a type of black hole. Leonard Susskind had incorporated the holographic principle of Gerardus 't Hooft into string theory, identifying the long highly excited string states with ordinary thermal black hole states. As suggested by 't Hooft, the fluctuations of the black hole horizon, the world-sheet or world-volume theory, describes not only the degrees of freedom of the black hole, but all nearby objects too.\n\n\n=== Second superstring revolution ===\nIn 1995, at the annual conference of string theorists at the University of Southern California (USC), Edward Witten gave a speech on string theory that in essence united the five string theories that existed at the time, and giving birth to a new 11-dimensional theory called M-theory. M-theory was also foreshadowed in the work of Paul Townsend at approximately the same time. The flurry of activity that began at this time is sometimes called the second superstring revolution.\n\nDuring this period, Tom Banks, Willy Fischler, Stephen Shenker and Leonard Susskind formulated matrix theory, a full holographic description of M-theory using IIA D0 branes. This was the first definition of string theory that was fully non-perturbative and a concrete mathematical realization of the holographic principle. It is an example of a gauge-gravity duality and is now understood to be a special case of the AdS/CFT correspondence. Andrew Strominger and Cumrun Vafa calculated the entropy of certain configurations of D-branes and found agreement with the semi-classical answer for extreme charged black holes. Petr Ho\u0159ava and Witten found the eleven-dimensional formulation of the heterotic string theories, showing that orbifolds solve the chirality problem. Witten noted that the effective description of the physics of D-branes at low energies is by a supersymmetric gauge theory, and found geometrical interpretations of mathematical structures in gauge theory that he and Nathan Seiberg had earlier discovered in terms of the location of the branes.\nIn 1997, Juan Maldacena noted that the low energy excitations of a theory near a black hole consist of objects close to the horizon, which for extreme charged black holes looks like an anti-de Sitter space. He noted that in this limit the gauge theory describes the string excitations near the branes. So he hypothesized that string theory on a near-horizon extreme-charged black-hole geometry, an anti-de Sitter space times a sphere with flux, is equally well described by the low-energy limiting gauge theory, the N = 4 supersymmetric Yang\u2013Mills theory. This hypothesis, which is called the AdS/CFT correspondence, was further developed by Steven Gubser, Igor Klebanov and Alexander Polyakov, and by Edward Witten, and it is now well-accepted. It is a concrete realization of the holographic principle, which has far-reaching implications for black holes, locality and information in physics, as well as the nature of the gravitational interaction. Through this relationship, string theory has been shown to be related to gauge theories like quantum chromodynamics and this has led to more quantitative understanding of the behavior of hadrons, bringing string theory back to its roots.\n\n\n== Criticism ==\n\n\n=== Number of solutions ===\n\nTo construct models of particle physics based on string theory, physicists typically begin by specifying a shape for the extra dimensions of spacetime. Each of these different shapes corresponds to a different possible universe, or \"vacuum state\", with a different collection of particles and forces. String theory as it is currently understood has an enormous number of vacuum states, typically estimated to be around 10500, and these might be sufficiently diverse to accommodate almost any phenomena that might be observed at low energies.Many critics of string theory have expressed concerns about the large number of possible universes described by string theory. In his book Not Even Wrong, Peter Woit, a lecturer in the mathematics department at Columbia University, has argued that the large number of different physical scenarios renders string theory vacuous as a framework for constructing models of particle physics. According to Woit,\n\nThe possible existence of, say, 10500 consistent different vacuum states for superstring theory probably destroys the hope of using the theory to predict anything. If one picks among this large set just those states whose properties agree with present experimental observations, it is likely there still will be such a large number of these that one can get just about whatever value one wants for the results of any new observation.\nSome physicists believe this large number of solutions is actually a virtue because it may allow a natural anthropic explanation of the observed values of physical constants, in particular the small value of the cosmological constant. The anthropic principle is the idea that some of the numbers appearing in the laws of physics are not fixed by any fundamental principle but must be compatible with the evolution of intelligent life. In 1987, Steven Weinberg published an article in which he argued that the cosmological constant could not have been too large, or else galaxies and intelligent life would not have been able to develop. Weinberg suggested that there might be a huge number of possible consistent universes, each with a different value of the cosmological constant, and observations indicate a small value of the cosmological constant only because humans happen to live in a universe that has allowed intelligent life, and hence observers, to exist.String theorist Leonard Susskind has argued that string theory provides a natural anthropic explanation of the small value of the cosmological constant. According to Susskind, the different vacuum states of string theory might be realized as different universes within a larger multiverse. The fact that the observed universe has a small cosmological constant is just a tautological consequence of the fact that a small value is required for life to exist. Many prominent theorists and critics have disagreed with Susskind's conclusions. According to Woit, \"in this case [anthropic reasoning] is nothing more than an excuse for failure. Speculative scientific ideas fail not just when they make incorrect predictions, but also when they turn out to be vacuous and incapable of predicting anything.\"\n\n\n=== Background independence ===\n\nOne of the fundamental properties of Einstein's general theory of relativity is that it is background independent, meaning that the formulation of the theory does not in any way privilege a particular spacetime geometry.One of the main criticisms of string theory from early on is that it is not manifestly background independent. In string theory, one must typically specify a fixed reference geometry for spacetime, and all other possible geometries are described as perturbations of this fixed one. In his book The Trouble With Physics, physicist Lee Smolin of the Perimeter Institute for Theoretical Physics claims that this is the principal weakness of string theory as a theory of quantum gravity, saying that string theory has failed to incorporate this important insight from general relativity.Others have disagreed with Smolin's characterization of string theory. In a review of Smolin's book, string theorist Joseph Polchinski writes\n\n[Smolin] is mistaking an aspect of the mathematical language being used for one of the physics being described. New physical theories are often discovered using a mathematical language that is not the most suitable for them\u2026 In string theory it has always been clear that the physics is background-independent even if the language being used is not, and the search for more suitable language continues. Indeed, as Smolin belatedly notes, [AdS/CFT] provides a solution to this problem, one that is unexpected and powerful.\nPolchinski notes that an important open problem in quantum gravity is to develop holographic descriptions of gravity which do not require the gravitational field \nto be asymptotically anti-de Sitter. Smolin has responded by saying that the AdS/CFT correspondence, as it is currently understood, may not be strong enough to resolve all concerns about background independence.\n\n\n=== Sociology of science ===\nSince the superstring revolutions of the 1980s and 1990s, string theory has become the dominant paradigm of high energy theoretical physics. Some string theorists have expressed the view that there does not exist an equally successful alternative theory addressing the deep questions of fundamental physics. In an interview from 1987, Nobel laureate David Gross made the following controversial comments about the reasons for the popularity of string theory:\n\nThe most important [reason] is that there are no other good ideas around. That's what gets most people into it. When people started to get interested in string theory they didn't know anything about it. In fact, the first reaction of most people is that the theory is extremely ugly and unpleasant, at least that was the case a few years ago when the understanding of string theory was much less developed. It was difficult for people to learn about it and to be turned on. So I think the real reason why people have got attracted by it is because there is no other game in town. All other approaches of constructing grand unified theories, which were more conservative to begin with, and only gradually became more and more radical, have failed, and this game hasn't failed yet.\nSeveral other high-profile theorists and commentators have expressed similar views, suggesting that there are no viable alternatives to string theory.Many critics of string theory have commented on this state of affairs. In his book criticizing string theory, Peter Woit views the status of string theory research as unhealthy and detrimental to the future of fundamental physics. He argues that the extreme popularity of string theory among theoretical physicists is partly a consequence of the financial structure of academia and the fierce competition for scarce resources. In his book The Road to Reality, mathematical physicist Roger Penrose expresses similar views, stating \"The often frantic competitiveness that this ease of communication engenders leads to bandwagon effects, where researchers fear to be left behind if they do not join in.\" Penrose also claims that the technical difficulty of modern physics forces young scientists to rely on the preferences of established researchers, rather than forging new paths of their own. Lee Smolin expresses a slightly different position in his critique, claiming that string theory grew out of a tradition of particle physics which discourages speculation about the foundations of physics, while his preferred approach, loop quantum gravity, encourages more radical thinking. According to Smolin,\n\nString theory is a powerful, well-motivated idea and deserves much of the work that has been devoted to it. If it has so far failed, the principal reason is that its intrinsic flaws are closely tied to its strengths\u2014and, of course, the story is unfinished, since string theory may well turn out to be part of the truth. The real question is not why we have expended so much energy on string theory but why we haven't expended nearly enough on alternative approaches.\nSmolin goes on to offer a number of prescriptions for how scientists might encourage a greater diversity of approaches to quantum gravity research.\n\n\n== Notes and references ==\n\n\n=== Notes ===\n\n\n=== Citations ===\n\n\n=== Bibliography ===\n\n\n== Further reading ==\n\n\n=== Popularizations ===\n\n\n==== General ====\nGreene, Brian (2003). The Elegant Universe: Superstrings, Hidden Dimensions, and the Quest for the Ultimate Theory. New York: W.W. Norton & Company. ISBN 0-393-05858-1. \nGreene, Brian (2004). The Fabric of the Cosmos: Space, Time, and the Texture of Reality. New York: Alfred A. Knopf. ISBN 0-375-41288-3. \n\n\n==== Critical ====\nPenrose, Roger (2005). The Road to Reality: A Complete Guide to the Laws of the Universe. Knopf. ISBN 0-679-45443-8. \nSmolin, Lee (2006). The Trouble with Physics: The Rise of String Theory, the Fall of a Science, and What Comes Next. New York: Houghton Mifflin Co. ISBN 0-618-55105-0. \nWoit, Peter (2006). Not Even Wrong: The Failure of String Theory And the Search for Unity in Physical Law. London: Jonathan Cape &: New York: Basic Books. ISBN 978-0-465-09275-8. \n\n\n=== Textbooks ===\n\n\n==== For physicists ====\nBecker, Katrin; Becker, Melanie; Schwarz, John (2007). String Theory and M-theory: A Modern Introduction. Cambridge University Press. ISBN 978-0-521-86069-7. \nGreen, Michael; Schwarz, John; Witten, Edward (2012). Superstring theory. Vol. 1: Introduction. Cambridge University Press. ISBN 978-1107029118. \nGreen, Michael; Schwarz, John; Witten, Edward (2012). Superstring theory. Vol. 2: Loop amplitudes, anomalies and phenomenology. Cambridge University Press. ISBN 978-1107029132. \nPolchinski, Joseph (1998). String Theory Vol. 1: An Introduction to the Bosonic String. Cambridge University Press. ISBN 0-521-63303-6. \nPolchinski, Joseph (1998). String Theory Vol. 2: Superstring Theory and Beyond. Cambridge University Press. ISBN 0-521-63304-4. \nZwiebach, Barton (2009). A First Course in String Theory. Cambridge University Press. ISBN 978-0-521-88032-9. \n\n\n==== For mathematicians ====\nDeligne, Pierre; Etingof, Pavel; Freed, Daniel; Jeffery, Lisa; Kazhdan, David; Morgan, John; Morrison, David; Witten, Edward, eds. (1999). Quantum Fields and Strings: A Course for Mathematicians, Vol. 2. American Mathematical Society. ISBN 978-0821819883. \n\n\n== External links ==\nThe Elegant Universe\u2014A three-hour miniseries with Brian Greene by NOVA (original PBS Broadcast Dates: October 28, 8\u201310 p.m. and November 4, 8\u20139 p.m., 2003). Various images, texts, videos and animations explaining string theory.\nNot Even Wrong\u2014A blog critical of string theory\nThe Official String Theory Web Site\nWhy String Theory\u2014An introduction to string theory.\nBedford, James (2012). \"An introduction to string theory\". arXiv:1107.3967\u202f [hep-th]. \nTong, David (2009). \"String theory\". arXiv:0908.0333\u202f [hep-th].", "graph theory": "In mathematics,  graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices, nodes, or points which are connected by edges, arcs, or lines. A graph may be undirected, meaning that there is no distinction between the two vertices associated with each edge, or its edges may be directed from one vertex to another; see Graph (discrete mathematics) for more detailed definitions and for other variations in the types of graph that are commonly considered. Graphs are one of the prime objects of study in discrete mathematics.\nRefer to the glossary of graph theory for basic definitions in graph theory.\n\n\n== Definitions ==\nDefinitions in graph theory vary. The following are some of the more basic ways of defining graphs and related mathematical structures.\n\n\n=== Graph ===\nIn the most common sense of the term, a graph is an ordered pair G = (V, E) comprising a set V of vertices or nodes or points together with a set E of edges or arcs or lines, which are 2-element subsets of V (i.e. an edge is associated with two vertices, and that association takes the form of the unordered pair comprising those two vertices). To avoid ambiguity, this type of graph may be described precisely as undirected and simple.\nOther senses of graph stem from different conceptions of the edge set. In one more generalized notion, V is a set together with a relation of incidence that associates with each edge two vertices. In another generalized notion, E is a multiset of unordered pairs of (not necessarily distinct) vertices. Many authors call this type of object a multigraph or pseudograph.\nAll of these variants and others are described more fully below.\nThe vertices belonging to an edge are called the ends or end vertices of the edge. A vertex may exist in a graph and not belong to an edge.\nV and E are usually taken to be finite, and many of the well-known results are not true (or are rather different) for infinite graphs because many of the arguments fail in the infinite case. The order of a graph is |V |, its number of vertices. The size of a graph is |E|, its number of edges. The degree or valency of a vertex is the number of edges that connect to it, where an edge that connects a vertex to itself (a loop) is counted twice.\nFor an edge {x, y}, graph theorists usually use the somewhat shorter notation xy.\n\n\n== Applications ==\n\nGraphs can be used to model many types of relations and processes in physical, biological, social and information systems. Many practical problems can be represented by graphs. Emphasizing their application to real-world systems, the term network is sometimes defined to mean a graph in which attributes (e.g. names) are associated with the nodes and/or edges.\n\n\n=== Computer science ===\nIn computer science, graphs are used to represent networks of communication, data organization, computational devices, the flow of computation, etc. For instance, the link structure of a website can be represented by a directed graph, in which the vertices represent web pages and directed edges represent links from one page to another. A similar approach can be taken to problems in social media, travel, biology, computer chip design, mapping the progression of neuro-degenerative diseases, and many other fields. The development of algorithms to handle graphs is therefore of major interest in computer science. The transformation of graphs is often formalized and represented by graph rewrite systems. Complementary to graph transformation systems focusing on rule-based in-memory manipulation of graphs are graph databases geared towards transaction-safe, persistent storing and querying of graph-structured data.\n\n\n=== Linguistics ===\nGraph-theoretic methods, in various forms, have proven particularly useful in linguistics, since natural language often lends itself well to discrete structure. Traditionally, syntax and compositional semantics follow tree-based structures, whose expressive power lies in the principle of compositionality, modeled in a hierarchical graph. More contemporary approaches such as head-driven phrase structure grammar model the syntax of natural language using typed feature structures, which are directed acyclic graphs. \nWithin lexical semantics, especially as applied to computers, modeling word meaning is easier when a given word is understood in terms of related words; semantic networks are therefore important in computational linguistics. Still, other methods in phonology (e.g. optimality theory, which uses lattice graphs) and morphology (e.g. finite-state morphology, using finite-state transducers) are common in the analysis of language as a graph. Indeed, the usefulness of this area of mathematics to linguistics has borne organizations such as TextGraphs, as well as various 'Net' projects, such as WordNet, VerbNet, and others.\n\n\n=== Physics and chemistry ===\nGraph theory is also used to study molecules in chemistry and physics. In condensed matter physics, the three-dimensional structure of complicated simulated atomic structures can be studied quantitatively by gathering statistics on graph-theoretic properties related to the topology of the atoms. In chemistry a graph makes a natural model for a molecule, where vertices represent atoms and edges bonds. This approach is especially used in computer processing of molecular structures, ranging from chemical editors to database searching. In statistical physics, graphs can represent local connections between interacting parts of a system, as well as the dynamics of a physical process on such\nsystems. Similarly, in computational neuroscience graphs can be used to represent functional connections between brain areas that interact to give rise to various cognitive processes, where the vertices represent different areas of the brain and the edges represent the connections between those areas. Graph theory plays an important role in electrical modeling of electrical networks, here,  weights are associated with resistance of the wire segments to obtain electrical properties of network structures. Graphs are also used to represent the micro-scale channels of porous media, in which the vertices represent the pores and the edges represent the smaller channels connecting the pores.\n\n\n=== Social sciences ===\nGraph theory is also widely used in sociology as a way, for example, to measure actors' prestige or to explore rumor spreading, notably through the use of social network analysis software. Under the umbrella of social networks are many different types of graphs. Acquaintanceship and friendship graphs describe whether people know each other. Influence graphs model whether certain people can influence the behavior of others. Finally, collaboration graphs model whether two people work together in a particular way, such as acting in a movie together.\n\n\n=== Biology ===\nLikewise, graph theory is useful in biology and conservation efforts where a vertex can represent regions where certain species exist (or inhabit) and the edges represent migration paths or movement between the regions. This information is important when looking at breeding patterns or tracking the spread of disease, parasites or how changes to the movement can affect other species.\n\n\n=== Mathematics ===\nIn mathematics, graphs are useful in geometry and certain parts of topology such as knot theory. Algebraic graph theory has close links with group theory.\n\n\n=== Other topics ===\nA graph structure can be extended by assigning a weight to each edge of the graph. Graphs with weights, or weighted graphs, are used to represent structures in which pairwise connections have some numerical values. For example, if a graph represents a road network, the weights could represent the length of each road. There may be several weights associated with each edge, including distance (as in the previous example), travel time, or monetary cost. Such weighted graphs are commonly used to program GPS's, and travel-planning search engines that compare flight times and costs.\n\n\n== History ==\n\nThe paper written by Leonhard Euler on the Seven Bridges of K\u00f6nigsberg and published in 1736 is regarded as the first paper in the history of graph theory. This paper, as well as the one written by Vandermonde on the knight problem, carried on with the analysis situs initiated by Leibniz. Euler's formula relating the number of edges, vertices, and faces of a convex polyhedron was studied and generalized by Cauchy and L'Huilier, and represents the beginning of the branch of mathematics known as topology.\nMore than one century after Euler's paper on the bridges of K\u00f6nigsberg and while Listing was introducing the concept of topology, Cayley was led by an interest in particular analytical forms arising from differential calculus to study a particular class of graphs, the trees. This study had many implications for theoretical chemistry. The techniques he used mainly concern the enumeration of graphs with particular properties. Enumerative graph theory then arose from the results of Cayley and the fundamental results published by P\u00f3lya between 1935 and 1937. These were generalized by De Bruijn in 1959. Cayley linked his results on trees with contemporary studies of chemical composition. The fusion of ideas from mathematics with those from chemistry began what has become part of the standard terminology of graph theory.\nIn particular, the term \"graph\" was introduced by Sylvester in a paper published in 1878 in Nature, where he draws an analogy between \"quantic invariants\" and \"co-variants\" of algebra and molecular diagrams:\n\"[\u2026] Every invariant and co-variant thus becomes expressible by a graph precisely identical with a Kekul\u00e9an diagram or chemicograph. [\u2026] I give a rule for the geometrical multiplication of graphs, i.e. for constructing a graph to the product of in- or co-variants whose separate graphs are given. [\u2026]\" (italics as in the original).The first textbook on graph theory was written by D\u00e9nes K\u0151nig, and published in 1936. Another book by Frank Harary, published in 1969, was \"considered the world over to be the definitive textbook on the subject\", and enabled mathematicians, chemists, electrical engineers and social scientists to talk to each other. Harary donated all of the royalties to fund the P\u00f3lya Prize.One of the most famous and stimulating problems in graph theory is the four color problem: \"Is it true that any map drawn in the plane may have its regions colored with four colors, in such a way that any two regions having a common border have different colors?\" This problem was first posed by Francis Guthrie in 1852 and its first written record is in a letter of De Morgan addressed to Hamilton the same year. Many incorrect proofs have been proposed, including those by Cayley, Kempe, and others. The study and the generalization of this problem by Tait, Heawood, Ramsey and Hadwiger led to the study of the colorings of the graphs embedded on surfaces with arbitrary genus. Tait's reformulation generated a new class of problems, the factorization problems, particularly studied by Petersen and K\u0151nig. The works of Ramsey on colorations and more specially the results obtained by Tur\u00e1n in 1941 was at the origin of another branch of graph theory, extremal graph theory.\nThe four color problem remained unsolved for more than a century. In 1969 Heinrich Heesch published a method for solving the problem using computers. A computer-aided proof produced in 1976 by Kenneth Appel and Wolfgang Haken makes fundamental use of the notion of \"discharging\" developed by Heesch. The proof involved checking the properties of 1,936 configurations by computer, and was not fully accepted at the time due to its complexity. A simpler proof considering only 633 configurations was given twenty years later by Robertson, Seymour, Sanders and Thomas.The autonomous development of topology from 1860 and 1930 fertilized graph theory back through the works of Jordan, Kuratowski and Whitney. Another important factor of common development of graph theory and topology came from the use of the techniques of modern algebra. The first example of such a use comes from the work of the physicist Gustav Kirchhoff, who published in 1845 his Kirchhoff's circuit laws for calculating the voltage and current in electric circuits.\nThe introduction of probabilistic methods in graph theory, especially in the study of Erd\u0151s and R\u00e9nyi of the asymptotic probability of graph connectivity, gave rise to yet another branch, known as random graph theory, which has been a fruitful source of graph-theoretic results.\n\n\n== Graph drawing ==\n\nGraphs are represented visually by drawing a dot or circle for every vertex, and drawing an arc between two vertices if they are connected by an edge. If the graph is directed, the direction is indicated by drawing an arrow.\nA graph drawing should not be confused with the graph itself (the abstract, non-visual structure) as there are several ways to structure the graph drawing. All that matters is which vertices are connected to which others by how many edges and not the exact layout. In practice, it is often difficult to decide if two drawings represent the same graph. Depending on the problem domain some layouts may be better suited and easier to understand than others.\nThe pioneering work of W. T. Tutte was very influential on the subject of graph drawing. Among other achievements, he introduced the use of linear algebraic methods to obtain graph drawings.\nGraph drawing also can be said to encompass problems that deal with the crossing number and its various generalizations. The crossing number of a graph is the minimum number of intersections between edges that a drawing of the graph in the plane must contain. For a planar graph, the crossing number is zero by definition.\nDrawings on surfaces other than the plane are also studied.\n\n\n== Graph-theoretic data structures ==\n\nThere are different ways to store graphs in a computer system. The data structure used depends on both the graph structure and the algorithm used for manipulating the graph. Theoretically one can distinguish between list and matrix structures but in concrete applications the best structure is often a combination of both. List structures are often preferred for sparse graphs as they have smaller memory requirements. Matrix structures on the other hand provide faster access for some applications but can consume huge amounts of memory.\nList structures include the incidence list, an array of pairs of vertices, and the adjacency list, which separately lists the neighbors of each vertex: Much like the incidence list, each vertex has a list of which vertices it is adjacent to.\nMatrix structures include the incidence matrix, a matrix of 0's and 1's whose rows represent vertices and whose columns represent edges, and the adjacency matrix, in which both the rows and columns are indexed by vertices. In both cases a 1 indicates two adjacent objects and a 0 indicates two non-adjacent objects. The Laplacian matrix is a modified form of the adjacency matrix that incorporates information about the degrees of the vertices, and is useful in some calculations such as Kirchhoff's theorem on the number of spanning trees of a graph.\nThe distance matrix, like the adjacency matrix, has both its rows and columns indexed by vertices, but rather than containing a 0 or a 1 in each cell it contains the length of a shortest path between two vertices.\n\n\n== Problems ==\n\n\n=== Enumeration ===\nThere is a large literature on graphical enumeration: the problem of counting graphs meeting specified conditions. Some of this work is found in Harary and Palmer (1973).\n\n\n=== Subgraphs, induced subgraphs, and minors ===\nA common problem, called the subgraph isomorphism problem, is finding a fixed graph as a subgraph in a given graph. One reason to be interested in such a question is that many graph properties are hereditary for subgraphs, which means that a graph has the property if and only if all subgraphs have it too.\nUnfortunately, finding maximal subgraphs of a certain kind is often an NP-complete problem. For example:\n\nFinding the largest complete subgraph is called the clique problem (NP-complete).A similar problem is finding induced subgraphs in a given graph. Again, some important graph properties are hereditary with respect to induced subgraphs, which means that a graph has a property if and only if all induced subgraphs also have it. Finding maximal induced subgraphs of a certain kind is also often NP-complete. For example:\n\nFinding the largest edgeless induced subgraph or independent set is called the independent set problem (NP-complete).Still another such problem, the minor containment problem, is to find a fixed graph as a minor of a given graph. A minor or subcontraction of a graph is any graph obtained by taking a subgraph and contracting some (or no) edges. Many graph properties are hereditary for minors, which means that a graph has a property if and only if all minors have it too. For example, Wagner's Theorem states: \n\nA graph is planar if it contains as a minor neither the complete bipartite graph K3,3 (see the Three-cottage problem) nor the complete graph K5.A similar problem, the subdivision containment problem, is to find a fixed graph as a subdivision of a given graph. A subdivision or homeomorphism of a graph is any graph obtained by subdividing some (or no) edges. Subdivision containment is related to graph properties such as planarity. For example, Kuratowski's Theorem states:   \n\nA graph is planar if it contains as a subdivision neither the complete bipartite graph K3,3 nor the complete graph K5.Another problem in subdivision containment is Kelmans\u2013Seymour conjecture:\n\nEvery 5-vertex-connected graph that is not planar contains a subdivision of the 5-vertex complete graph K5.Another class of problems has to do with the extent to which various species and generalizations of graphs are determined by their point-deleted subgraphs. For example:\n\nThe reconstruction conjecture\n\n\n=== Graph coloring ===\n\nMany problems and theorems in graph theory have to do with various ways of coloring graphs.  Typically, one is interested in coloring a graph so that no two adjacent vertices have the same color, or with other similar restrictions.  One may also consider coloring edges (possibly so that no two coincident edges are the same color), or other variations.  Among the famous results and conjectures concerning graph coloring are the following:\n\nFour-color theorem\nStrong perfect graph theorem\nErd\u0151s\u2013Faber\u2013Lov\u00e1sz conjecture (unsolved)\nTotal coloring conjecture, also called Behzad's conjecture (unsolved)\nList coloring conjecture (unsolved)\nHadwiger conjecture (graph theory) (unsolved)\n\n\n=== Subsumption and unification ===\nConstraint modeling theories concern families of directed graphs related by a partial order. In these applications, graphs are ordered by specificity, meaning that more constrained graphs\u2014which are more specific and thus contain a greater amount of information\u2014are subsumed by those that are more general. Operations between graphs include evaluating the direction of a subsumption relationship between two graphs, if any, and computing graph unification. The unification of two argument graphs is defined as the most general graph (or the computation thereof) that is consistent with (i.e. contains all of the information in) the inputs, if such a graph exists; efficient unification algorithms are known.\nFor constraint frameworks which are strictly compositional, graph unification is the sufficient satisfiability and combination function. Well-known applications include automatic theorem proving and modeling the elaboration of linguistic structure.\n\n\n=== Route problems ===\nHamiltonian path problem\nMinimum spanning tree\nRoute inspection problem (also called the \"Chinese postman problem\")\nSeven bridges of K\u00f6nigsberg\nShortest path problem\nSteiner tree\nThree-cottage problem\nTraveling salesman problem (NP-hard)\n\n\n=== Network flow ===\nThere are numerous problems arising especially from applications that have to do with various notions of flows in networks, for example:\n\nMax flow min cut theorem\n\n\n=== Visibility problems ===\nMuseum guard problem\n\n\n=== Covering problems ===\nCovering problems in graphs are specific instances of subgraph-finding problems, and they tend to be closely related to the clique problem or the independent set problem.\n\nSet cover problem\nVertex cover problem\n\n\n=== Decomposition problems ===\nDecomposition, defined as partitioning the edge set of a graph (with as many vertices as necessary accompanying the edges of each part of the partition), has a wide variety of question. Often, it is required to decompose a graph into subgraphs isomorphic to a fixed graph; for instance, decomposing a complete graph into Hamiltonian cycles. Other problems specify a family of graphs into which a given graph should be decomposed, for instance, a family of cycles, or decomposing a complete graph Kn into n \u2212 1 specified trees having, respectively, 1, 2, 3, \u2026, n \u2212 1 edges.\nSome specific decomposition problems that have been studied include:\n\nArboricity, a decomposition into as few forests as possible\nCycle double cover, a decomposition into a collection of cycles covering each edge exactly twice\nEdge coloring, a decomposition into as few matchings as possible\nGraph factorization, a decomposition of a regular graph into regular subgraphs of given degrees\n\n\n=== Graph classes ===\nMany problems involve characterizing the members of various classes of graphs. Some examples of such questions are below:\n\nEnumerating the members of a class\nCharacterizing a class in terms of forbidden substructures\nAscertaining relationships among classes (e.g. does one property of graphs imply another)\nFinding efficient algorithms to decide membership in a class\nFinding representations for members of a class\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\nBerge, Claude (1958), Th\u00e9orie des graphes et ses applications, Collection Universitaire de Math\u00e9matiques, II, Paris: Dunod . English edition, Wiley 1961; Methuen & Co, New York 1962; Russian, Moscow 1961; Spanish, Mexico 1962; Roumanian, Bucharest 1969; Chinese, Shanghai 1963; Second printing of the 1962 first English edition, Dover, New York 2001.\nBiggs, N.; Lloyd, E.; Wilson, R. (1986), Graph Theory, 1736\u20131936, Oxford University Press .\nBondy, J.A.; Murty, U.S.R. (2008), Graph Theory, Springer, ISBN 978-1-84628-969-9 .\nBollob\u00e1s, B\u00e9la; Riordan, O.M (2003), Mathematical results on scale-free random graphs in \"Handbook of Graphs and Networks\" (S. Bornholdt and H.G. Schuster (eds)), Wiley VCH, Weinheim, 1st ed. .\nChartrand, Gary (1985), Introductory Graph Theory, Dover, ISBN 0-486-24775-9 .\nGibbons, Alan (1985), Algorithmic Graph Theory, Cambridge University Press .\nReuven Cohen, Shlomo Havlin (2010), Complex Networks: Structure, Robustness and Function, Cambridge University Press .\nGolumbic, Martin (1980), Algorithmic Graph Theory and Perfect Graphs, Academic Press .\nHarary, Frank (1969), Graph Theory, Reading, MA: Addison-Wesley .\nHarary, Frank; Palmer, Edgar M. (1973), Graphical Enumeration, New York, NY: Academic Press .\nMahadev, N.V.R.; Peled, Uri N. (1995), Threshold Graphs and Related Topics, North-Holland .\nMark Newman (2010), Networks: An Introduction, Oxford University Press .\n\n\n== External links ==\nHazewinkel, Michiel, ed. (2001) [1994], \"Graph theory\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nGraph theory tutorial\nA searchable database of small connected graphs\nImage gallery: graphs at the Wayback Machine (archived February 6, 2006)\nConcise, annotated list of graph theory resources for researchers\nrocs \u2014 a graph theory IDE\nThe Social Life of Routers \u2014 non-technical paper discussing graphs of people and computers\nGraph Theory Software \u2014 tools to teach and learn graph theory\nOnline books, and library resources in your library and in other libraries about graph theory\nA list of graph algorithms with references and links to graph library implementations\n\n\n=== Online textbooks ===\nPhase Transitions in Combinatorial Optimization Problems, Section 3: Introduction to Graphs (2006) by Hartmann and Weigt\nDigraphs: Theory Algorithms and Applications 2007 by Jorgen Bang-Jensen and Gregory Gutin\nGraph Theory, by Reinhard Diestel", "mechanics": "Mechanics (Greek \u03bc\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ae) is that area of science concerned with the behaviour of physical bodies when subjected to forces or displacements, and the subsequent effects of the bodies on their environment.\nThe scientific discipline has its origins in Ancient Greece with the writings of Aristotle and Archimedes    (see History of classical mechanics and Timeline of classical mechanics). During the early modern period, scientists such as Galileo, Kepler, and Newton laid the foundation for what is now known as classical mechanics.\nIt is a branch of classical physics that deals with particles that are either at rest or are moving with velocities significantly less than the speed of light. \nIt can also be defined as a branch of science which deals with the motion of and forces on objects. The field is yet less widely understood in terms of quantum theory. \n\n\n== Classical versus quantum ==\nHistorically, classical mechanics came first, and quantum mechanics is a comparatively recent invention. Classical mechanics originated with Isaac Newton's laws of motion in Philosophi\u00e6 Naturalis Principia Mathematica; Quantum Mechanics was discovered in the early 20th century. Both are commonly held to constitute the most certain knowledge that exists about physical nature. \nClassical mechanics has especially often been viewed as a model for other so-called exact sciences. Essential in this respect is the extensive use of mathematics in theories, as well as the decisive role played by experiment in generating and testing them.\nQuantum mechanics is of a bigger  scope, as it encompasses classical mechanics as a sub-discipline which applies under certain restricted circumstances. According to the correspondence principle, there is no contradiction or conflict between the two subjects, each simply pertains to specific situations. The correspondence principle states that the behavior of systems described by quantum theories reproduces classical physics in the limit of large quantum numbers. Quantum mechanics has superseded classical mechanics at the foundation level and is indispensable for the explanation and prediction of processes at the molecular, atomic, and sub-atomic level. However, for macroscopic processes classical mechanics is able to solve problems which are unmanageably difficult in quantum mechanics and hence remains useful and well used.\nModern descriptions of such behavior begin with a careful definition of such quantities as displacement (distance moved), time, velocity, acceleration, mass, and force. Until about 400 years ago, however, motion was explained from a very different point of view. For example, following the ideas of Greek philosopher and scientist Aristotle, scientists reasoned that a cannonball falls down because its natural position is in the Earth; the sun, the moon, and the stars travel in circles around the earth because it is the nature of heavenly objects to travel in perfect circles.\nOften cited as father to modern science, Galileo brought together the ideas of other great thinkers of his time and began to calculate motion in terms of distance traveled from some starting position and the time that it took. He showed that the speed of falling objects increases steadily during the time of their fall. This acceleration is the same for heavy objects as for light ones, provided air friction (air resistance) is discounted. The English mathematician and physicist Isaac Newton improved this analysis by defining force and mass and relating these to acceleration. For objects traveling at speeds close to the speed of light, Newton\u2019s laws were superseded by Albert Einstein\u2019s theory of relativity. [A sentence illustrating the computational complication of Einstein's theory of relativity.] For atomic and subatomic particles, Newton\u2019s laws were superseded by quantum theory. For everyday phenomena, however, Newton\u2019s three laws of motion remain the cornerstone of dynamics, which is the study of what causes motion.\n\n\n== Relativistic versus Newtonian ==\nIn analogy to the distinction between quantum and classical mechanics, Einstein's general and special theories of relativity have expanded the scope of Newton and Galileo's formulation of mechanics. The differences between relativistic and Newtonian mechanics become significant and even dominant as the velocity of a massive body approaches the speed of light. For instance, in Newtonian mechanics, Newton's laws of motion specify that F = ma, whereas in Relativistic mechanics and Lorentz transformations, which were first discovered by Hendrik Lorentz, F = \u03b3ma (where \u03b3 is the Lorentz factor, which is almost equal to 1 for low speeds).\n\n\n== General relativistic versus quantum ==\nRelativistic corrections are also needed for quantum mechanics, although general relativity has not been integrated. The two theories remain incompatible, a hurdle which must be overcome in developing a theory of everything.\n\n\n== History ==\n\n\n=== Antiquity ===\n\nThe main theory of mechanics in antiquity was Aristotelian mechanics. A later developer in this tradition is Hipparchus.\n\n\n=== Medieval age ===\n\nIn the Middle Ages, Aristotle's theories were criticized and modified by a number of figures, beginning with John Philoponus in the 6th century. A central problem was that of projectile motion, which was discussed by Hipparchus and Philoponus. This led to the development of the theory of impetus by 14th-century French priest Jean Buridan, which developed into the modern theories of inertia, velocity, acceleration and momentum. This work and others was developed in 14th-century England by the Oxford Calculators  such as Thomas Bradwardine, who studied and formulated various laws regarding falling bodies.\nOn the question of a body subject to a constant (uniform) force, the 12th-century Jewish-Arab Nathanel (Iraqi, of Baghdad) stated that constant force imparts constant acceleration, while the main properties are uniformly accelerated motion (as of falling bodies) was worked out by the 14th-century Oxford Calculators.\n\n\n=== Early modern age ===\nTwo central figures in the early modern age are Galileo Galilei and Isaac Newton. Galileo's final statement of his mechanics, particularly of falling bodies, is his Two New Sciences (1638). Newton's 1687 Philosophi\u00e6 Naturalis Principia Mathematica provided a detailed mathematical account of mechanics, using the newly developed mathematics of calculus and providing the basis of Newtonian mechanics.There is some dispute over priority of various ideas: Newton's Principia is certainly the seminal work and has been tremendously influential, and the systematic mathematics therein did not and could not have been stated earlier because calculus had not been developed. However, many of the ideas, particularly as pertain to inertia (impetus) and falling bodies had been developed and stated by earlier researchers, both the then-recent Galileo and the less-known medieval predecessors. Precise credit is at times difficult or contentious because scientific language and standards of proof changed, so whether medieval statements are equivalent to modern statements or sufficient proof, or instead similar to modern statements and hypotheses is often debatable.\n\n\n=== Modern age ===\nTwo main modern developments in mechanics are general relativity of Einstein, and quantum mechanics, both developed in the 20th century based in part on earlier 19th-century ideas. The development in the modern continuum mechanics, particularly in the areas of elasticity, plasticity, fluid dynamics, electrodynamics and thermodynamics of deformable media, started in the second half of the 20th century.\n\n\n== Types of mechanical bodies ==\nThe often-used term body needs to stand for a wide assortment of objects, including particles, projectiles, spacecraft, stars, parts of machinery, parts of solids, parts of fluids (gases and liquids), etc.\nOther distinctions between the various sub-disciplines of mechanics, concern the nature of the bodies being described. Particles are bodies with little (known) internal structure, treated as mathematical points in classical mechanics. Rigid bodies have size and shape, but retain a simplicity close to that of the particle, adding just a few so-called degrees of freedom, such as orientation in space.\nOtherwise, bodies may be semi-rigid, i.e. elastic, or non-rigid, i.e. fluid. These subjects have both classical and quantum divisions of study.\nFor instance, the motion of a spacecraft, regarding its orbit and attitude (rotation), is described by the relativistic theory of classical mechanics, while the analogous movements of an atomic nucleus are described by quantum mechanics.\n\n\n== Sub - disciplines ==\nThe following are two lists of various subjects that are studied in mechanics.\nNote that there is also the \"theory of fields\" which constitutes a separate discipline in physics, formally treated as distinct from mechanics, whether classical fields or quantum fields. But in actual practice, subjects belonging to mechanics and fields are closely interwoven. Thus, for instance, forces that act on particles are frequently derived from fields (electromagnetic or gravitational), and particles generate fields by acting as sources. In fact, in quantum mechanics, particles themselves are fields, as described theoretically by the wave function.\n\n\n=== Classical ===\n\nThe following are described as forming classical mechanics:\n\nNewtonian mechanics, the original theory of motion (kinematics) and forces (dynamics).\nAnalytical mechanics is a reformulation of Newtonian mechanics with an emphasis on system energy, rather than on forces. There are two main branches of analytical mechanics:\nHamiltonian mechanics, a theoretical formalism, based on the principle of conservation of energy.\nLagrangian mechanics, another theoretical formalism, based on the principle of the least action.\nClassical statistical mechanics generalizes ordinary classical mechanics to consider systems in an unknown state; often used to derive thermodynamic properties.\nCelestial mechanics, the motion of bodies in space: planets, comets, stars, galaxies, etc.\nAstrodynamics, spacecraft navigation, etc.\nSolid mechanics, elasticity, plasticity, viscoelasticity exhibited by deformable solids.\nFracture mechanics\nAcoustics, sound ( = density variation propagation) in solids, fluids and gases.\nStatics, semi-rigid bodies in mechanical equilibrium\nFluid mechanics, the motion of fluids\nSoil mechanics, mechanical behavior of soils\nContinuum mechanics, mechanics of continua (both solid and fluid)\nHydraulics, mechanical properties of liquids\nFluid statics, liquids in equilibrium\nApplied mechanics, or Engineering mechanics\nBiomechanics, solids, fluids, etc. in biology\nBiophysics, physical processes in living organisms\nRelativistic or Einsteinian mechanics, universal gravitation.\n\n\n=== Quantum ===\nThe following are catgorized as being part of quantum mechanics:\n\nSchr\u00f6dinger wave mechanics, used to describe the movements  of the wavefunction of a single particle.\nMatrix mechanics is an alternative formulatio that allows considering systems with a finite-dimensional state space.\nQuantum statistical mechanics generalizes ordinary quantum mechanics to consider systems in an unknown state; often used to derive thermodynamic properties.\nParticle physics, the motion, structure, and reactions of particles\nNuclear physics, the motion, structure, and reactions of nuclei\nCondensed matter physics, quantum gases, solids, liquids, etc.\n\n\n== Professional organizations ==\nApplied Mechanics Division, American Society of Mechanical Engineers\nFluid Dynamics Division, American Physical Society\nSociety for Experimental Mechanics\nInstitution of Mechanical Engineers is the United Kingdom's qualifying body for Mechanical Engineers and has been the home of Mechanical Engineers for over 150 years.\nInternational Union of Theoretical and Applied Mechanics\n\n\n== See also ==\nApplied mechanics\nDynamics\nEngineering\nIndex of engineering science and mechanics articles\nKinematics\nKinetics\nNon-autonomous mechanics\nStatics\nWiesen Test of Mechanical Aptitude (WTMA)\n\n\n== References ==\n\n\n== Further reading ==\nRobert Stawell Ball (1871) Experimental Mechanics from Google books.\nLandau, L. D.; Lifshitz, E. M. (1972). Mechanics and Electrodynamics, Vol. 1. Franklin Book Company, Inc. ISBN 0-08-016739-X. CS1 maint: Multiple names: authors list (link) \n\n\n== External links ==\niMechanica: the web of mechanics and mechanicians\nMechanics Definition\nMechanics Blog by a Purdue University Professor\nThe Mechanics program at Virginia Tech\nPhysclips: Mechanics with animations and video clips from the University of New South Wales\nU.S. National Committee on Theoretical and Applied Mechanics\nInteractive learning resources for teaching Mechanics\nThe Archimedes Project\nEngineering Fundamental Solid & Fluid Mechanics", "chemistry": "Chemistry is the scientific discipline involved with compounds composed of atoms, i.e. elements, and molecules, i.e. combinations of atoms: their composition, structure, properties, behavior and the changes they undergo during a reaction with other compounds. Chemistry addresses topics such as how atoms and molecules interact via chemical bonds to form new chemical compounds. There are four types of chemical bonds: covalent bonds, in which compounds share one or more electron(s); ionic bonds, in which a compound donates one or more electrons to another compound to produce ions (cations and anions); hydrogen bonds; and Van der Waals force bonds. \nIn the scope of its subject, chemistry occupies an intermediate position between physics and biology.  It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level. Examples include plant chemistry (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the moon (astrophysics), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics).\nThe history of chemistry spans a period from very old times to the present. Since several millennia BC, civilizations were using technologies that would eventually form the basis of the various branches of chemistry. Examples include extracting metals from ores, making pottery and glazes, fermenting beer and wine, extracting chemicals from plants for medicine and perfume, rendering fat into soap, making glass, and making alloys like bronze. Chemistry was preceded by its protoscience, alchemy, which is an intuitive but non-scientific approach to understanding the constituents of matter and their interactions. It was unsuccessful in explaining the nature of matter and its transformations, but, by performing experiments and recording the results, alchemists set the stage for modern chemistry. Chemistry as a body of knowledge distinct from alchemy began to emerge when a clear differentiation was made between them by  Robert Boyle in his work The Sceptical Chymist (1661). While both alchemy and chemistry are concerned with matter and its transformations, the crucial difference was given by the scientific method that chemists employed in their work. Chemistry is considered to have become an established science with the work of Antoine Lavoisier, who developed a law of conservation of mass that demanded careful measurement and quantitative observations of chemical phenomena.  The history of chemistry is intertwined with the history of thermodynamics, especially through the work of Willard Gibbs.\n\n\n== Etymology ==\nThe word chemistry comes from alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism and medicine. It is often seen as linked to the quest to turn lead or another common starting material into gold, though in ancient times the study encompassed many of the questions of modern chemistry being defined as the study of the composition of waters, movement, growth, embodying, disembodying, drawing the spirits from bodies and bonding the spirits within bodies by the early 4th century Greek-Egyptian alchemist Zosimos. An alchemist was called a 'chemist' in popular speech, and later the suffix \"-ry\" was added to this to describe the art of the chemist as \"chemistry\".\nThe modern word alchemy in turn is derived from the Arabic word al-k\u012bm\u012b\u0101 (\u0627\u0644\u0643\u06cc\u0645\u06cc\u0627\u0621).  In origin, the term is borrowed from the Greek \u03c7\u03b7\u03bc\u03af\u03b1 or \u03c7\u03b7\u03bc\u03b5\u03af\u03b1. This may have Egyptian origins since al-k\u012bm\u012b\u0101 is derived from the Greek \u03c7\u03b7\u03bc\u03af\u03b1, which is in turn derived from the word Kemet, which is the ancient name of Egypt in the Egyptian language. Alternately, al-k\u012bm\u012b\u0101 may derive from \u03c7\u03b7\u03bc\u03b5\u03af\u03b1, meaning \"cast together\".\n\n\n== Modern principles ==\n\nThe current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules,  substances,  metals, crystals and other aggregates of matter. This matter can be studied in solid, liquid, or gas states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory.\nThe chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.\n\nA chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws.\nEnergy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are:\n\n\n=== Matter ===\n\nIn chemistry, matter is defined as anything that has rest mass and volume (it takes up space) and is made up of particles. The particles that make up matter have rest mass as well \u2013 not all particles have rest mass, such as the photon. Matter can be a pure chemical substance or a mixture of substances.\n\n\n==== Atom ====\n\nThe atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space occupied by an electron cloud. The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus. In a neutral atom, the negatively charged electrons balance out the positive charge of the protons. The nucleus is dense; the mass of a nucleon is appromixately 1,836 times that of an electron, yet the radius of an atom is about 10,000 times that of its nucleus.The atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential, preferred oxidation state(s), coordination number, and preferred types of bonds to form (e.g., metallic, ionic, covalent).\n\n\n==== Element ====\n\nA chemical element is a pure substance which is composed of a single type of atom, characterized by its particular number of protons in the nuclei of its atoms, known as the atomic number and represented by the symbol Z. The mass number is the sum of the number of protons and neutrons in a nucleus. Although all the nuclei of all atoms belonging to one element will have the same atomic number, they may not necessarily have the same mass number; atoms of an element which have different mass numbers are known as isotopes. For example, all atoms with 6 protons in their nuclei are atoms of the chemical element carbon, but atoms of carbon may have mass numbers of 12 or 13.The standard presentation of the chemical elements is in the periodic table, which orders elements by atomic number. The periodic table is arranged in groups, or columns, and periods, or rows. The periodic table is useful in identifying periodic trends.\n\n\n==== Compound ====\n\nA compound is a pure chemical substance composed of more than one element. The properties of a compound bear little similarity to those of its elements. The standard nomenclature of compounds is set by the International Union of Pure and Applied Chemistry (IUPAC). Organic compounds are named according to the organic nomenclature system. The names for Inorganic compounds are created according to the inorganic nomenclature system. When a compound has more than one component, then they are divided into two classes, the electropositive and the electronegative components. In addition the Chemical Abstracts Service has devised a method to index chemical substances. In this scheme each chemical substance is identifiable by a number known as its CAS registry number.\n\n\n==== Molecule ====\n\nA molecule is the smallest indivisible portion of a pure chemical substance that has its unique set of chemical properties, that is, its potential to undergo a certain set of chemical reactions with other substances. However, this definition only works well for substances that are composed of molecules, which is not true of many substances (see below). Molecules are typically a set of atoms bound together by covalent bonds, such that the structure is electrically neutral and all valence electrons are paired with other electrons either in bonds or in lone pairs.\nThus, molecules exist as electrically neutral units, unlike ions. When this rule is broken, giving the \"molecule\" a charge, the result is sometimes named a molecular ion or a polyatomic ion. However, the discrete and separate nature of the molecular concept usually requires that molecular ions be present only in well-separated form, such as a directed beam in a vacuum in a mass spectrometer. Charged polyatomic collections residing in solids (for example, common sulfate or nitrate ions) are generally not considered \"molecules\" in chemistry. Some molecules contain one or more unpaired electrons, creating radicals. Most radicals are comparatively reactive, but some, such as nitric oxide (NO) can be stable.\n\nThe \"inert\" or noble gas elements (helium, neon, argon, krypton, xenon and radon) are composed of lone atoms as their smallest discrete unit, but the other isolated chemical elements consist of either molecules or networks of atoms bonded to each other in some way. Identifiable molecules compose familiar substances such as water, air, and many organic compounds like alcohol, sugar, gasoline, and the various pharmaceuticals.\nHowever, not all substances or chemical compounds consist of discrete molecules, and indeed most of the solid substances that make up the solid crust, mantle, and core of the Earth are chemical compounds without molecules. These other types of substances, such as ionic compounds and network solids, are organized in such a way as to lack the existence of identifiable molecules per se.  Instead, these substances are discussed in terms of formula units or unit cells as the smallest repeating structure within the substance. Examples of such substances are mineral salts (such as table salt), solids like carbon and diamond, metals, and familiar silica and silicate minerals such as quartz and granite.\nOne of the main characteristics of a molecule is its geometry often called its structure. While the structure of diatomic, triatomic or tetra atomic molecules may be trivial, (linear, angular pyramidal etc.) the structure of polyatomic molecules, that are constituted of more than six atoms (of several elements) can be crucial for its chemical nature.\n\n\n==== Substance and mixture ====\nA chemical substance is a kind of matter with a definite composition and set of properties. A collection of substances is called a mixture. Examples of mixtures are air and alloys.\n\n\n==== Mole and amount of substance ====\n\nThe mole is a unit of measurement that denotes an amount of substance (also called chemical amount). The mole is defined as the number of atoms found in exactly 0.012 kilogram (or 12 grams) of carbon-12, where the carbon-12 atoms are unbound, at rest and in their ground state. The number of entities per mole is known as the Avogadro constant, and is determined empirically to be approximately 6.022\u00d71023 mol\u22121. Molar concentration is the amount of a particular substance per volume of solution, and is commonly reported in mol/dm3.\n\n\n=== Phase ===\n\nIn addition to the specific chemical properties that distinguish different chemical classifications, chemicals can exist in several phases. For the most part, the chemical classifications are independent of these bulk phase classifications; however, some more exotic phases are incompatible with certain chemical properties. A phase is a set of states of a chemical system that have similar bulk structural properties, over a range of conditions, such as pressure or temperature.\nPhysical properties, such as density and refractive index tend to fall within values characteristic of the phase. The phase of matter is defined by the phase transition, which is when energy put into or taken out of the system goes into rearranging the structure of the system, instead of changing the bulk conditions.\nSometimes the distinction between phases can be continuous instead of having a discrete boundary, in this case the matter is considered to be in a supercritical state. When three states meet based on the conditions, it is known as a triple point and since this is invariant, it is a convenient way to define a set of conditions.\nThe most familiar examples of phases are solids, liquids, and gases. Many substances exhibit multiple solid phases. For example, there are three phases of solid iron (alpha, gamma, and delta) that vary based on temperature and pressure. A principal difference between solid phases is the crystal structure, or arrangement, of the atoms. Another phase commonly encountered in the study of chemistry is the aqueous phase, which is the state of substances dissolved in aqueous solution (that is, in water).\nLess familiar phases include plasmas, Bose\u2013Einstein condensates and fermionic condensates and the paramagnetic and ferromagnetic phases of magnetic materials. While most familiar phases deal with three-dimensional systems, it is also possible to define analogs in two-dimensional systems, which has received attention for its relevance to systems in biology.\n\n\n=== Bonding ===\n\nAtoms sticking together in molecules or crystals are said to be bonded with one another. A chemical bond may be visualized as the multipole balance between the positive charges in the nuclei and the negative charges oscillating about them. More than simple attraction and repulsion, the energies and distributions characterize the availability of an electron to bond to another atom.\nA chemical bond can be a covalent bond, an ionic bond, a hydrogen bond or just because of Van der Waals force. Each of these kinds of bonds is ascribed to some potential. These potentials create the interactions which hold atoms together in molecules or crystals. In many simple compounds, valence bond theory, the Valence Shell Electron Pair Repulsion model (VSEPR), and the concept of oxidation number can be used to explain molecular structure and composition.\nAn ionic bond is formed when a metal loses one or more of its electrons, becoming a positively charged cation, and the electrons are then gained by the non-metal atom, becoming a negatively charged anion. The two oppositely charged ions attract one another, and the ionic bond is the electrostatic force of attraction between them. For example, sodium (Na), a metal, loses one electron to become an Na+ cation while chlorine (Cl), a non-metal, gains this electron to become Cl\u2212. The ions are held together due to electrostatic attraction, and that compound sodium chloride (NaCl), or common table salt, is formed.\n\nIn a covalent bond, one or more pairs of valence electrons are shared by two atoms: the resulting electrically neutral group of bonded atoms is termed a molecule. Atoms will share valence electrons in such a way as to create a noble gas electron configuration (eight electrons in their outermost shell) for each atom. Atoms that tend to combine in such a way that they each have eight electrons in their valence shell are said to follow the octet rule. However, some elements like hydrogen and lithium need only two electrons in their outermost shell to attain this stable configuration; these atoms are said to follow the duet rule, and in this way they are reaching the electron configuration of the noble gas helium, which has two electrons in its outer shell.\nSimilarly, theories from classical physics can be used to predict many ionic structures. With more complicated compounds, such as metal complexes, valence bond theory is less applicable and alternative approaches, such as the molecular orbital theory, are generally used. See diagram on electronic orbitals.\n\n\n=== Energy ===\n\nIn the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structures, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants.\nA reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. A reaction is said to be exothermic if the reaction releases heat to the surroundings; in the case of endothermic reactions, the reaction absorbs heat from the surroundings.\nChemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor \n  \n    \n      \n        \n          e\n          \n            \u2212\n            E\n            \n              /\n            \n            k\n            T\n          \n        \n      \n    \n    {\\displaystyle e^{-E/kT}}\n   \u2013 that is the probability of a molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.\nThe activation energy necessary for a chemical reaction to occur can be in the form of heat, light, electricity or mechanical force in the form of ultrasound.A related concept free energy, which also incorporates entropy considerations, is a very useful means for predicting the feasibility of a reaction and determining the state of equilibrium of a chemical reaction, in chemical thermodynamics. A reaction is feasible only if the total change in the Gibbs free energy is negative, \n  \n    \n      \n        \u0394\n        G\n        \u2264\n        0\n        \n      \n    \n    {\\displaystyle \\Delta G\\leq 0\\,}\n  ; if it is equal to zero the chemical reaction is said to be at equilibrium.\nThere exist only limited possible states of energy for electrons, atoms and molecules. These are determined by the rules of quantum mechanics, which require quantization of energy of a bound system. The atoms/molecules in a higher energy state are said to be excited. The molecules/atoms of substance in an excited energy state are often much more reactive; that is, more amenable to chemical reactions.\nThe phase of a substance is invariably determined by its energy and the energy of its surroundings. When the intermolecular forces of a substance are such that the energy of the surroundings is not sufficient to overcome them, it occurs in a more ordered phase like liquid or solid as is the case with water (H2O); a liquid at room temperature because its molecules are bound by hydrogen bonds. Whereas hydrogen sulfide (H2S) is a gas at room temperature and standard pressure, as its molecules are bound by weaker dipole-dipole interactions.\nThe transfer of energy from one chemical substance to another depends on the size of energy quanta emitted from one substance. However, heat energy is often transferred more easily from almost any substance to another because the phonons responsible for vibrational and rotational energy levels in a substance have much less energy than photons invoked for the electronic energy transfer. Thus, because vibrational and rotational energy levels are more closely spaced than electronic energy levels, heat is more easily transferred between substances relative to light or other forms of electronic energy. For example, ultraviolet electromagnetic radiation is not transferred with as much efficacy from one substance to another as thermal or electrical energy.\nThe existence of characteristic energy levels for different chemical substances is useful for their identification by the analysis of spectral lines. Different kinds of spectra are often used in chemical spectroscopy, e.g. IR, microwave, NMR, ESR, etc. Spectroscopy is also used to identify the composition of remote objects \u2013 like stars and distant galaxies \u2013 by analyzing their radiation spectra.\n\nThe term chemical energy is often used to indicate the potential of a chemical substance to undergo a transformation through a chemical reaction or to transform other chemical substances.\n\n\n=== Reaction ===\n\nWhen a chemical substance is transformed as a result of its interaction with another substance or with energy, a chemical reaction is said to have occurred. A chemical reaction is therefore a concept related to the \"reaction\" of a substance when it comes in close contact with another, whether as a mixture or a solution; exposure to some form of energy, or both. It results in some energy exchange between the constituents of the reaction as well as with the system environment, which may be designed vessels\u2014often laboratory glassware.\nChemical reactions can result in the formation or dissociation of molecules, that is, molecules breaking apart to form two or more smaller molecules, or rearrangement of atoms within or across molecules. Chemical reactions usually involve the making or breaking of chemical bonds. Oxidation, reduction, dissociation, acid-base neutralization and molecular rearrangement are some of the commonly used kinds of chemical reactions.\nA chemical reaction can be symbolically depicted through a chemical equation. While in a non-nuclear chemical reaction the number and kind of atoms on both sides of the equation are equal, for a nuclear reaction this holds true only for the nuclear particles viz. protons and neutrons.The sequence of steps in which the reorganization of chemical bonds may be taking place in the course of a chemical reaction is called its mechanism. A chemical reaction can be envisioned to take place in a number of steps, each of which may have a different speed. Many reaction intermediates with variable stability can thus be envisaged during the course of a reaction. Reaction mechanisms are proposed to explain the kinetics and the relative product mix of a reaction. Many physical chemists specialize in exploring and proposing the mechanisms of various chemical reactions. Several empirical rules, like the Woodward\u2013Hoffmann rules often come in handy while proposing a mechanism for a chemical reaction.\nAccording to the IUPAC gold book, a chemical reaction is \"a process that results in the interconversion of chemical species.\" Accordingly, a chemical reaction may be an elementary reaction or a stepwise reaction. An additional caveat is made, in that this definition includes cases where the interconversion of conformers is experimentally observable. Such detectable chemical reactions normally involve sets of molecular entities as indicated by this definition, but it is often conceptually convenient to use the term also for changes involving single molecular entities (i.e. 'microscopic chemical events').\n\n\n=== Ions and salts ===\n\nAn ion is a charged species, an atom or a molecule, that has lost or gained one or more electrons. When an atom loses an electron and thus has more protons than electrons, the atom is a positively charged ion or cation. When an atom gains an electron and thus has more electrons than protons, the atom is a negatively charged ion or anion. Cations and anions can form a crystalline lattice of neutral salts, such as the Na+ and Cl\u2212 ions forming sodium chloride, or NaCl. Examples of polyatomic ions that do not split up during acid-base reactions are hydroxide (OH\u2212) and phosphate (PO43\u2212).\nPlasma is composed of gaseous matter that has been completely ionized, usually through high temperature.\n\n\n=== Acidity and basicity ===\n\nA substance can often be classified as an acid or a base. There are several different theories which explain acid-base behavior. The simplest is Arrhenius theory, which states than an acid is a substance that produces hydronium ions when it is dissolved in water, and a base is one that produces hydroxide ions when dissolved in water. According to Br\u00f8nsted\u2013Lowry acid\u2013base theory, acids are substances that donate a positive hydrogen ion to another substance in a chemical reaction; by extension, a base is the substance which receives that hydrogen ion.\nA third common theory is Lewis acid-base theory, which is based on the formation of new chemical bonds. Lewis theory explains that an acid is a substance which is capable of accepting a pair of electrons from another substance during the process of bond formation, while a base is a substance which can provide a pair of electrons to form a new bond.  According to this theory, the crucial things being exchanged are charges. There are several other ways in which a substance may be classified as an acid or a base, as is evident in the history of this concept.Acid strength is commonly measured by two methods. One measurement, based on the Arrhenius definition of acidity, is pH, which is a measurement of the hydronium ion concentration in a solution, as expressed on a negative logarithmic scale. Thus, solutions that have a low pH have a high hydronium ion concentration, and can be said to be more acidic. The other measurement, based on the Br\u00f8nsted\u2013Lowry definition, is the acid dissociation constant (Ka), which measures the relative ability of a substance to act as an acid under the Br\u00f8nsted\u2013Lowry definition of an acid.  That is, substances with a higher Ka are more likely to donate hydrogen ions in chemical reactions than those with lower Ka values.\n\n\n=== Redox ===\n\nRedox (reduction-oxidation) reactions include all chemical reactions in which atoms have their oxidation state changed by either gaining electrons (reduction) or losing electrons (oxidation). Substances that have the ability to oxidize other substances are said to be oxidative and are known as oxidizing agents, oxidants or oxidizers. An oxidant removes electrons from another substance. Similarly, substances that have the ability to reduce other substances are said to be reductive and are known as reducing agents, reductants, or reducers.\nA reductant transfers electrons to another substance, and is thus oxidized itself. And because it \"donates\" electrons it is also called an electron donor. Oxidation and reduction properly refer to a change in oxidation number\u2014the actual transfer of electrons may never occur. Thus, oxidation is better defined as an increase in oxidation number, and reduction as a decrease in oxidation number.\n\n\n=== Equilibrium ===\n\nAlthough the concept of equilibrium is widely used across sciences, in the context of chemistry, it arises whenever a number of different states of the chemical composition are possible, as for example, in a mixture of several chemical compounds that can react with one another, or when a substance can be present in more than one kind of phase.\nA system of chemical substances at equilibrium, even though having an unchanging composition, is most often not static; molecules of the substances continue to react with one another thus giving rise to a dynamic equilibrium. Thus the concept describes the state in which the parameters such as chemical composition remain unchanged over time.\n\n\n=== Chemical laws ===\n\nChemical reactions are governed by certain laws, which have become fundamental concepts in chemistry. Some of them are:\n\n\n== History ==\n\n\n=== Of definition ===\nThe definition of chemistry has changed over time, as new discoveries and theories add to the functionality of the science. The term \"chymistry\", in the view of noted scientist Robert Boyle in 1661, meant the subject of the material principles of mixed bodies. In 1663 the chemist Christopher Glaser described \"chymistry\" as a scientific art, by which one learns to dissolve bodies, and draw from them the different substances on their composition, and how to unite them again, and exalt them to a higher perfection.The 1730 definition of the word \"chemistry\", as used by Georg Ernst Stahl, meant the art of resolving mixed, compound, or aggregate bodies into their principles; and of composing such bodies from those principles. In 1837, Jean-Baptiste Dumas considered the word \"chemistry\" to refer to the science concerned with the laws and effects of molecular forces. This definition further evolved until, in 1947, it came to mean the science of substances: their structure, their properties, and the reactions that change them into other substances - a characterization accepted by Linus Pauling. More recently, in 1998, Professor Raymond Chang broadened the definition of \"chemistry\" to mean the study of matter and the changes it undergoes.\n\n\n=== Of discipline ===\n\nEarly civilizations, such as the Egyptians Babylonians, Indians amassed practical knowledge concerning the arts of metallurgy, pottery and dyes, but didn't develop a systematic theory.\nA basic chemical hypothesis first emerged in Classical Greece with the theory of four elements as propounded definitively by Aristotle stating that fire, air, earth and water were the fundamental elements from which everything is formed as a combination. Greek atomism dates back to 440 BC, arising in works by philosophers such as Democritus and Epicurus. In 50 BC, the Roman philosopher Lucretius expanded upon the theory in his book De rerum natura (On The Nature of Things). Unlike modern concepts of science, Greek atomism was purely philosophical in nature, with little concern for empirical observations and no concern for chemical experiments.In the Hellenistic world the art of alchemy first proliferated, mingling magic and occultism into the study of natural substances with the ultimate goal of transmuting elements into gold and discovering the elixir of eternal life. Work, particularly the development of distillation, continued in the early Byzantine period with the most famous practitioner being the 4th century Greek-Egyptian Zosimos of Panopolis. Alchemy continued to be developed and practised throughout the Arab world after the Muslim conquests, and from there, and from the Byzantine remnants, diffused into medieval and Renaissance Europe through Latin translations. Some influential Muslim chemists,  Ab\u016b al-Rayh\u0101n al-B\u012br\u016bn\u012b, Avicenna and Al-Kindi refuted the theories of alchemy, particularly the theory of the transmutation of metals; and al-Tusi described a version of the conservation of mass, noting that a body of matter is able to change but is not able to disappear.\n\nThe development of the modern scientific method was slow and arduous, but an early scientific method for chemistry began emerging among early Muslim chemists, beginning with the 9th century Perso-Arab chemist J\u0101bir ibn Hayy\u0101n (known as \"Geber\" in Europe), who is sometimes referred to as \"the father of chemistry\". He introduced a systematic and experimental approach to scientific research based in the laboratory, in contrast to the ancient Greek and Egyptian alchemists whose works were largely allegorical and often unintelligible. Under the influence of the new empirical methods propounded by Sir Francis Bacon and others, a group of chemists at Oxford, Robert Boyle, Robert Hooke and John Mayow began to reshape the old alchemical traditions into a scientific discipline. Boyle in particular is regarded as the founding father of chemistry due to his most important work, the classic chemistry text The Sceptical Chymist where the differentiation is made between the claims of alchemy and the empirical scientific discoveries of the new chemistry. He formulated Boyle's law, rejected the classical \"four elements\" and proposed a mechanistic alternative of atoms and chemical reactions that could be subject to rigorous experiment.\n\nThe theory of phlogiston (a substance at the root of all combustion) was propounded by the German Georg Ernst Stahl in the early 18th century and was only overturned by the end of the century by the French chemist Antoine Lavoisier, the chemical analogue of Newton in physics; who did more than any other to establish the new science on proper theoretical footing, by elucidating the principle of conservation of mass and developing a new system of chemical nomenclature used to this day.Before his work, though, many important discoveries had been made, specifically relating to the nature of 'air' which was discovered to be composed of many different gases. The Scottish chemist Joseph Black (the first experimental chemist) and the Dutchman J. B. van Helmont discovered carbon dioxide, or what Black called 'fixed air' in 1754; Henry Cavendish discovered hydrogen and elucidated its properties and Joseph Priestley and, independently, Carl Wilhelm Scheele isolated pure oxygen.\n\nEnglish scientist John Dalton proposed the modern theory of atoms; that all substances are composed of indivisible 'atoms' of matter and that different atoms have varying atomic weights.\nThe development of the electrochemical theory of chemical combinations occurred in the early 19th century as the result of the work of two scientists in particular, J. J. Berzelius and Humphry Davy, made possible by the prior invention of the voltaic pile by Alessandro Volta. Davy discovered nine new elements including the alkali metals by extracting them from their oxides with electric current.British William Prout first proposed ordering all the elements by their atomic weight as all atoms had a weight that was an exact multiple of the atomic weight of hydrogen. J. A. R. Newlands devised an early table of elements, which was then developed into the modern periodic table of elements in the 1860s by Dmitri Mendeleev and independently by several other scientists including Julius Lothar Meyer. The inert gases, later called the noble gases were discovered by William Ramsay in collaboration with Lord Rayleigh at the end of the century, thereby filling in the basic structure of the table.\n\nAt the turn of the twentieth century the theoretical underpinnings of chemistry were finally understood due to a series of remarkable discoveries that succeeded in probing and discovering the very nature of the internal structure of atoms. In 1897, J. J. Thomson of Cambridge University discovered the electron and soon after the French scientist Becquerel as well as the couple Pierre and Marie Curie investigated the phenomenon of radioactivity. In a series of pioneering scattering experiments Ernest Rutherford at the University of Manchester  discovered the internal structure of the atom and the existence of the proton, classified and explained the different types of radioactivity and successfully transmuted the first element by bombarding nitrogen with alpha particles.\nHis work on atomic structure was improved on by his students, the Danish physicist Niels Bohr and Henry Moseley.  The electronic theory of chemical bonds and molecular orbitals was developed by the American scientists Linus Pauling and Gilbert N. Lewis.\nThe year 2011 was declared by the United Nations as the International Year of Chemistry.  It was an initiative of the International Union of Pure and Applied Chemistry, and of the United Nations Educational, Scientific, and Cultural Organization and involves chemical societies, academics, and institutions worldwide and relied on individual initiatives to organize local and regional activities.\nOrganic chemistry was developed by Justus von Liebig and others, following Friedrich W\u00f6hler's synthesis of urea which proved that living organisms were, in theory, reducible to chemistry. Other crucial 19th century advances were; an understanding of valence bonding (Edward Frankland in 1852) and the application of thermodynamics to chemistry (J. W. Gibbs and Svante Arrhenius in the 1870s).\n\n\n== Practice ==\n\n\n=== Subdisciplines ===\nChemistry is typically divided into several major sub-disciplines. There are also several main cross-disciplinary and more specialized fields of chemistry.\nAnalytical chemistry is the analysis of material samples to gain an understanding of their chemical composition and structure. Analytical chemistry incorporates standardized experimental methods in chemistry. These methods may be used in all subdisciplines of chemistry, excluding purely theoretical chemistry.\nBiochemistry is the study of the chemicals, chemical reactions and chemical interactions that take place in living organisms. Biochemistry and organic chemistry are closely related, as in medicinal chemistry or neurochemistry. Biochemistry is also associated with molecular biology and genetics.\nInorganic chemistry is the study of the properties and reactions of inorganic compounds. The distinction between organic and inorganic disciplines is not absolute and there is much overlap, most importantly in the sub-discipline of organometallic chemistry.\nMaterials chemistry is the preparation, characterization, and understanding of substances with a useful function. The field is a new breadth of study in graduate programs, and it integrates elements from all classical areas of chemistry with a focus on fundamental issues that are unique to materials. Primary systems of study include the chemistry of condensed phases (solids, liquids, polymers) and interfaces between different phases.\nNeurochemistry is the study of neurochemicals; including transmitters, peptides, proteins, lipids, sugars, and nucleic acids; their interactions, and the roles they play in forming, maintaining, and modifying the nervous system.\nNuclear chemistry is the study of how subatomic particles come together and make nuclei. Modern Transmutation is a large component of nuclear chemistry, and the table of nuclides is an important result and tool for this field.\nOrganic chemistry is the study of the structure, properties, composition, mechanisms, and reactions of organic compounds. An organic compound is defined as any compound based on a carbon skeleton.\nPhysical chemistry is the study of the physical and fundamental basis of chemical systems and processes. In particular, the energetics and dynamics of such systems and processes are of interest to physical chemists. Important areas of study include chemical thermodynamics, chemical kinetics, electrochemistry, statistical mechanics, spectroscopy, and more recently, astrochemistry.  Physical chemistry has large overlap with molecular physics. Physical chemistry involves the use of infinitesimal calculus in deriving equations. It is usually associated with quantum chemistry and theoretical chemistry. Physical chemistry is a distinct discipline from chemical physics, but again, there is very strong overlap.\nTheoretical chemistry is the study of chemistry via fundamental theoretical reasoning (usually within mathematics or physics). In particular the application of quantum mechanics to chemistry is called quantum chemistry. Since the end of the Second World War, the development of computers has allowed a systematic development of computational chemistry, which is the art of developing and applying computer programs for solving chemical problems. Theoretical chemistry has large overlap with (theoretical and experimental) condensed matter physics and molecular physics.Other disciplines within chemistry are traditionally grouped by the type of matter being studied or the kind of study. These include inorganic chemistry, the study of inorganic matter; organic chemistry, the study of organic (carbon-based) matter; biochemistry, the study of substances found in biological organisms; physical chemistry, the study of chemical processes using physical concepts such as thermodynamics and quantum mechanics; and analytical chemistry, the analysis of material samples to gain an understanding of their chemical composition and structure. Many more specialized disciplines have emerged in recent years, e.g. neurochemistry the chemical study of the nervous system (see subdisciplines).\nOther fields include agrochemistry, astrochemistry (and cosmochemistry), atmospheric chemistry, chemical engineering, chemical biology, chemo-informatics, electrochemistry, environmental chemistry, femtochemistry, flavor chemistry, flow chemistry, geochemistry, green chemistry, histochemistry, history of chemistry, hydrogenation chemistry, immunochemistry, marine chemistry, materials science, mathematical chemistry, mechanochemistry, medicinal chemistry, molecular biology, molecular mechanics, nanotechnology, natural product chemistry, oenology, organometallic chemistry, petrochemistry, pharmacology, photochemistry, physical organic chemistry, phytochemistry, polymer chemistry, radiochemistry, solid-state chemistry, sonochemistry, supramolecular chemistry, surface chemistry, synthetic chemistry, thermochemistry, and many others.\n\n\n=== Industry ===\n\nThe chemical industry represents an important economic activity worldwide. The global top 50 chemical producers in 2013 had sales of US$980.5 billion  with a profit margin of 10.3%.\n\n\n=== Professional societies ===\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nAtkins, Peter; de Paula, Julio (2009) [1992]. Elements of Physical Chemistry (5th ed.). New York: Oxford University Press. ISBN 978-0-19-922672-6. \nBurrows, Andrew; Holman, John; Parsons, Andrew; Pilling, Gwen; Price, Gareth (2009). Chemistry3. Italy: Oxford University Press. ISBN 978-0-19-927789-6. \nHousecroft, Catherine E.; Sharpe, Alan G. (2008) [2001]. Inorganic Chemistry (3rd ed.). Harlow, Essex: Pearson Education. ISBN 978-0-13-175553-6. \n\n\n== Further reading ==\nPopular readingAtkins, P.W. Galileo's Finger (Oxford University Press) ISBN 0-19-860941-8\nAtkins, P.W. Atkins' Molecules (Cambridge University Press) ISBN 0-521-82397-8\nKean, Sam. The Disappearing Spoon - and other true tales from the Periodic Table (Black Swan) London, 2010 ISBN 978-0-552-77750-6\nLevi, Primo The Periodic Table (Penguin Books) [1975] translated from the Italian by Raymond Rosenthal (1984) ISBN 978-0-14-139944-7\nStwertka, A. A Guide to the Elements (Oxford University Press) ISBN 0-19-515027-9\n\"Dictionary of the History of Ideas\". Archived from the original on March 10, 2008. \n \"Chemistry\". Encyclop\u00e6dia Britannica. 6 (11th ed.). 1911. pp. 33\u201376. Introductory undergraduate text booksAtkins, P.W., Overton, T., Rourke, J., Weller, M. and Armstrong, F. Shriver and Atkins inorganic chemistry (4th edition) 2006 (Oxford University Press) ISBN 0-19-926463-5\nChang, Raymond. Chemistry 6th ed. Boston: James M. Smith, 1998. ISBN 0-07-115221-0.\nClayden, Jonathan; Greeves, Nick; Warren, Stuart; Wothers, Peter (2001). Organic Chemistry (1st ed.). Oxford University Press. ISBN 978-0-19-850346-0. \nVoet and Voet Biochemistry (Wiley) ISBN 0-471-58651-XAdvanced undergraduate-level or graduate text booksAtkins, P.W. Physical Chemistry (Oxford University Press) ISBN 0-19-879285-9\nAtkins, P.W. et al. Molecular Quantum Mechanics (Oxford University Press)\nMcWeeny, R. Coulson's Valence (Oxford Science Publications) ISBN 0-19-855144-4\nPauling, L. The Nature of the chemical bond (Cornell University Press) ISBN 0-8014-0333-2\nPauling, L., and Wilson, E. B. Introduction to Quantum Mechanics with Applications to Chemistry (Dover Publications) ISBN 0-486-64871-0\nSmart and Moore Solid State Chemistry: An Introduction (Chapman and Hall) ISBN 0-412-40040-5\nStephenson, G. Mathematical Methods for Science Students (Longman) ISBN 0-582-44416-0\n\n\n== External links ==\n\nGeneral Chemistry principles, patterns and applications.", "topology": "In mathematics, topology (from the Greek \u03c4\u03cc\u03c0\u03bf\u03c2, place, and \u03bb\u03cc\u03b3\u03bf\u03c2, study) is concerned with the properties of space that are preserved under continuous deformations, such as stretching, crumpling and bending, but not tearing or gluing. This can be studied by considering a collection of subsets, called open sets, that satisfy certain properties, turning the given set into what is known as a topological space. Important topological properties include connectedness and compactness.Topology developed as a field of study out of geometry and set theory, through analysis of concepts such as space, dimension, and transformation. Such ideas go back to Gottfried Leibniz, who in the 17th century envisioned the geometria situs (Greek-Latin for \"geometry of place\") and analysis situs (Greek-Latin for \"picking apart of place\").  Leonhard Euler's Seven Bridges of K\u00f6nigsberg Problem and Polyhedron Formula are arguably the field's first theorems. The term topology was introduced by Johann Benedict Listing in the 19th century, although it was not until the first decades of the 20th century that the idea of a topological space was developed.  By the middle of the 20th century, topology had become a major branch of mathematics.\n\n\n== History ==\n\nTopology, as a well-defined mathematical discipline, originates in the early part of the twentieth century, but some isolated results can be traced back several centuries. Among these are certain questions in geometry investigated by Leonhard Euler. His 1736 paper on the Seven Bridges of K\u00f6nigsberg is regarded as one of the first practical applications of topology. On 14 November 1750 Euler wrote to a friend that he had realised the importance of the edges of a polyhedron. This led to his polyhedron formula, V \u2212 E + F = 2 (where V, E and F respectively indicate the number of vertices, edges and faces of the polyhedron). Some authorities regard this analysis as the first theorem, signalling the birth of topology.Further contributions were made by Augustin-Louis Cauchy, Ludwig Schl\u00e4fli, Johann Benedict Listing, Bernhard Riemann and Enrico Betti. Listing introduced the term \"Topologie\" in Vorstudien zur Topologie, written in his native German, in 1847, having used the word for ten years in correspondence before its first appearance in print. The English form \"topology\" was used in 1883 in Listing's obituary in the journal Nature to distinguish \"qualitative geometry from the ordinary geometry in which quantitative relations chiefly are treated\". The term \"topologist\" in the sense of a specialist in topology was used in 1905 in the magazine Spectator.Their work was corrected, consolidated and greatly extended by Henri Poincar\u00e9. In 1895 he published his ground-breaking paper on Analysis Situs, which introduced the concepts now known as homotopy and homology, which are now considered part of algebraic topology.\nUnifying the work on function spaces of Georg Cantor, Vito Volterra, Cesare Arzel\u00e0, Jacques Hadamard, Giulio Ascoli and others, Maurice Fr\u00e9chet introduced the metric space in 1906. A metric space is now considered a special case of a general topological space, with any given topological space potentially giving rise to many distinct metric spaces. In 1914, Felix Hausdorff coined the term \"topological space\" and gave the definition for what is now called a Hausdorff space. Currently, a topological space is a slight generalization of Hausdorff spaces, given in 1922 by Kazimierz Kuratowski.Modern topology depends strongly on the ideas of set theory, developed by Georg Cantor in the later part of the 19th century. In addition to establishing the basic ideas of set theory, Cantor considered point sets in Euclidean space as part of his study of Fourier series. For further developments, see point-set topology and algebraic topology.\n\n\n== Introduction ==\nTopology can be formally defined as \"the study of qualitative properties of certain objects (called topological spaces) that are invariant under a certain kind of transformation (called a continuous map), especially those properties that are invariant under a certain kind of invertible transformation (called homeomorphism).\"\nTopology is also used to refer to a structure imposed upon a set X, a structure that essentially 'characterizes' the set X as a topological space by taking proper care of properties such as convergence, connectedness and continuity, upon transformation.\nTopological spaces show up naturally in almost every branch of mathematics. This has made topology one of the great unifying ideas of mathematics.\nThe motivating insight behind topology is that some geometric problems depend not on the exact shape of the objects involved, but rather on the way they are put together. For example, the square and the circle have many properties in common: they are both one dimensional objects (from a topological point of view) and both separate the plane into two parts, the part inside and the part outside.\nIn one of the first papers in topology, Leonhard Euler demonstrated that it was impossible to find a route through the town of K\u00f6nigsberg (now Kaliningrad) that would cross each of its seven bridges exactly once. This result did not depend on the lengths of the bridges, nor on their distance from one another, but only on connectivity properties: which bridges connect to which islands or riverbanks. This problem in introductory mathematics called Seven Bridges of K\u00f6nigsberg led to the branch of mathematics known as graph theory.\n\nSimilarly, the hairy ball theorem of algebraic topology says that \"one cannot comb the hair flat on a hairy ball without creating a cowlick.\" This fact is immediately convincing to most people, even though they might not recognize the more formal statement of the theorem, that there is no nonvanishing continuous tangent vector field on the sphere. As with the Bridges of K\u00f6nigsberg, the result does not depend on the shape of the sphere; it applies to any kind of smooth blob, as long as it has no holes.\nTo deal with these problems that do not rely on the exact shape of the objects, one must be clear about just what properties these problems do rely on. From this need arises the notion of homeomorphism. The impossibility of crossing each bridge just once applies to any arrangement of bridges homeomorphic to those in K\u00f6nigsberg, and the hairy ball theorem applies to any space homeomorphic to a sphere.\nIntuitively, two spaces are homeomorphic if one can be deformed into the other without cutting or gluing. A traditional joke is that a topologist cannot distinguish a coffee mug from a doughnut, since a sufficiently pliable doughnut could be reshaped to a coffee cup by creating a dimple and progressively enlarging it, while shrinking the hole into a handle.Homeomorphism can be considered the most basic topological equivalence. Another is homotopy equivalence. This is harder to describe without getting technical, but the essential notion is that two objects are homotopy equivalent if they both result from \"squishing\" some larger object.\n\nAn introductory exercise is to classify the uppercase letters of the English alphabet according to homeomorphism and homotopy equivalence. The result depends partially on the font used. The figures use the sans-serif Myriad font. Homotopy equivalence is a rougher relationship than homeomorphism; a homotopy equivalence class can contain several homeomorphism classes. The simple case of homotopy equivalence described above can be used here to show two letters are homotopy equivalent. For example, O fits inside P and the tail of the P can be squished to the \"hole\" part.\nHomeomorphism classes are:\n\nno holes corresponding with C, G, I, J, L, M, N, S, U, V, W, and Z;\nno holes and three tails corresponding with E, F, T, and Y;\nno holes and four tails corresponding with X;\none hole and no tail corresponding with D and O;\none hole and one tail corresponding with P and Q;\none hole and two tails corresponding with A and R;\ntwo holes and no tail corresponding with B; and\na bar with four tails corresponding with H and K; the \"bar\" on the K is almost too short to see.Homotopy classes are larger, because the tails can be squished down to a point. They are:\n\none hole,\ntwo holes, and\nno holes.To classify the letters correctly, we must show that two letters in the same class are equivalent and two letters in different classes are not equivalent. In the case of homeomorphism, this can be done by selecting points and showing their removal disconnects the letters differently. For example, X and Y are not homeomorphic because removing the center point of the X leaves four pieces; whatever point in Y corresponds to this point, its removal can leave at most three pieces. The case of homotopy equivalence is harder and requires a more elaborate argument showing an algebraic invariant, such as the fundamental group, is different on the supposedly differing classes.\nLetter topology has practical relevance in stencil typography. For instance, Braggadocio font stencils are made of one connected piece of material.\n\n\n== Concepts ==\n\n\n=== Topologies on sets ===\n\nThe term topology also refers to a specific mathematical idea central to the area of mathematics called topology. Informally, a topology tells how elements of a set relate spatially to each other. The same set can have different topologies. For instance, the real line, the complex plane (which is a 1-dimensional complex vector space), and the Cantor set can be thought of as the same set with different topologies.\nFormally, let X be a set and let \u03c4 be a family of subsets of X. Then \u03c4 is called a topology on X if:\n\nBoth the empty set and X are elements of \u03c4.\nAny union of elements of \u03c4 is an element of \u03c4.\nAny intersection of finitely many elements of \u03c4 is an element of \u03c4.If \u03c4 is a topology on X, then the pair (X, \u03c4) is called a topological space. The notation X\u03c4 may be used to denote a set X endowed with the particular topology \u03c4.\nThe members of \u03c4 are called open sets in X. A subset of X is said to be closed if its complement is in \u03c4 (i.e., its complement is open). A subset of X may be open, closed, both (clopen set), or neither. The empty set and X itself are always both closed and open. A subset of X including an open set containing a point x is called a 'neighborhood' of x.\n\n\n=== Continuous functions and homeomorphisms ===\n\nA function or map from one topological space to another is called continuous if the inverse image of any open set is open. If the function maps the real numbers to the real numbers (both spaces with the Standard Topology), then this definition of continuous is equivalent to the definition of continuous in calculus. If a continuous function is one-to-one and onto, and if the inverse of the function is also continuous, then the function is called a homeomorphism and the domain of the function is said to be homeomorphic to the range. Another way of saying this is that the function has a natural extension to the topology. If two spaces are homeomorphic, they have identical topological properties, and are considered topologically the same. The cube and the sphere are homeomorphic, as are the coffee cup and the doughnut. But the circle is not homeomorphic to the doughnut.\n\n\n=== Manifolds ===\n\nWhile topological spaces can be extremely varied and exotic, many areas of topology focus on the more familiar class of spaces known as manifolds. A manifold is a topological space that resembles Euclidean space near each point.  More precisely, each point of an n-dimensional manifold has a neighbourhood that is homeomorphic to the Euclidean space of dimension n.  Lines and circles, but not figure eights, are one-dimensional manifolds.  Two-dimensional manifolds are also called surfaces.  Examples include the plane, the sphere, and the torus, which can all be realized without self-intersection in three dimensions, but also the Klein bottle and real projective plane, which cannot.\n\n\n== Topics ==\n\n\n=== General topology ===\n\nGeneral topology is the branch of topology dealing with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.\nThe fundamental concepts in point-set topology are continuity, compactness, and connectedness. Intuitively, continuous functions take nearby points to nearby points. Compact sets are those that can be covered by finitely many sets of arbitrarily small size. Connected sets are sets that cannot be divided into two pieces that are far apart. The words nearby, arbitrarily small, and far apart can all be made precise by using open sets. If we change the definition of open set, we change what continuous functions, compact sets, and connected sets are. Each choice of definition for open set is called a topology. A set with a topology is called a topological space.\nMetric spaces are an important class of topological spaces where distances can be assigned a number called a metric. Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.\n\n\n=== Algebraic topology ===\n\nAlgebraic topology is a branch of mathematics that uses tools from abstract algebra to study topological spaces. The basic goal is to find algebraic invariants that classify topological spaces up to homeomorphism, though usually most classify up to homotopy equivalence.\nThe most important of these invariants are homotopy groups, homology, and cohomology.\nAlthough algebraic topology primarily uses algebra to study topological problems, using topology to solve algebraic problems is sometimes also possible. Algebraic topology, for example, allows for a convenient proof that any subgroup of a free group is again a free group.\n\n\n=== Differential topology ===\n\nDifferential topology is the field dealing with differentiable functions on differentiable manifolds. It is closely related to differential geometry and together they make up the geometric theory of differentiable manifolds.\nMore specifically, differential topology considers the properties and structures that require only a smooth structure on a manifold to be defined.  Smooth manifolds are 'softer' than manifolds with extra geometric structures, which can act as obstructions to certain types of equivalences and deformations that exist in differential topology. For instance, volume and Riemannian curvature are invariants that can distinguish different geometric structures on the same smooth manifold\u2014that is, one can smoothly \"flatten out\" certain manifolds, but it might require distorting the space and affecting the curvature or volume.\n\n\n=== Geometric topology ===\n\nGeometric topology is a branch of topology that primarily focuses on low-dimensional manifolds (i.e. dimensions 2,3 and 4) and their interaction with geometry, but it also includes some higher-dimensional topology. Some examples of topics in geometric topology are orientability, handle decompositions, local flatness, crumpling and the planar and higher-dimensional Sch\u00f6nflies theorem.\nIn high-dimensional topology, characteristic classes are a basic invariant, and surgery theory is a key theory.\nLow-dimensional topology is strongly geometric, as reflected in the uniformization theorem in 2 dimensions \u2013 every surface admits a constant curvature metric; geometrically, it has one of 3 possible geometries: positive curvature/spherical, zero curvature/flat, negative curvature/hyperbolic \u2013 and the geometrization conjecture (now theorem) in 3 dimensions \u2013 every 3-manifold can be cut into pieces, each of which has one of eight possible geometries.\n2-dimensional topology can be studied as complex geometry in one variable (Riemann surfaces are complex curves) \u2013 by the uniformization theorem every conformal class of metrics is equivalent to a unique complex one, and 4-dimensional topology can be studied from the point of view of complex geometry in two variables (complex surfaces), though not every 4-manifold admits a complex structure.\n\n\n=== Generalizations ===\nOccasionally, one needs to use the tools of topology but a \"set of points\" is not available. In pointless topology one considers instead the lattice of open sets as the basic notion of the theory, while Grothendieck topologies are structures defined on arbitrary categories that allow the definition of sheaves on those categories, and with that the definition of general cohomology theories.\n\n\n== Applications ==\n\n\n=== Biology ===\nKnot theory, a branch of topology, is used in biology to study the effects of certain enzymes on DNA. These enzymes cut, twist, and reconnect the DNA, causing knotting with observable effects such as slower electrophoresis. Topology is also used in evolutionary biology to represent the relationship between phenotype and genotype. Phenotypic forms that appear quite different can be separated by only a few mutations depending on how genetic changes map to phenotypic changes during development. In neuroscience, topological quantities like the Euler characteristic and Betti number have been used to measure the complexity of patterns of activity in neural networks.\n\n\n=== Computer science ===\nTopological data analysis uses techniques from algebraic topology to determine the large scale structure of a set (for instance, determining if a cloud of points is spherical or toroidal). The main method used by topological data analysis is:\n\nReplace a set of data points with a family of simplicial complexes, indexed by a proximity parameter.\nAnalyse these topological complexes via algebraic topology \u2013 specifically, via the theory of persistent homology.\nEncode the persistent homology of a data set in the form of a parameterized version of a Betti number, which is called a barcode.\n\n\n=== Physics ===\nIn physics, topology is used in several areas such as condensed matter physics, quantum field theory and physical cosmology.\nThe topological dependence of mechanical properties in solids is of interest in disciplines of mechanical engineering and materials science. Electrical and mechanical properties depend on the arrangement and network structures of molecules and elementary units in materials. The compressive strength of crumpled topologies is studied in attempts to understand the high strength to weight of such structures that are mostly empty space. Topology is of further significance in Contact mechanics where the dependence of stiffness and friction on the dimensionality of surface structures is the subject of interest with applications in multi-body physics.\nA topological quantum field theory (or topological field theory or TQFT) is a quantum field theory that computes topological invariants.\nAlthough TQFTs were invented by physicists, they are also of mathematical interest, being related to, among other things, knot theory and the theory of four-manifolds in algebraic topology, and to the theory of moduli spaces in algebraic geometry. Donaldson, Jones, Witten, and Kontsevich have all won Fields Medals for work related to topological field theory.\nThe topological classification of Calabi-Yau manifolds has important implications in string theory, as different manifolds can sustain different kinds of strings.In cosmology, topology can be used to describe the overall shape of the universe. This area of research is commonly known as spacetime topology.\n\n\n=== Robotics ===\nThe various possible positions of a robot can be described by a manifold called configuration space. In the area of motion planning, one finds paths between two points in configuration space. These paths represent a motion of the robot's joints and other parts into the desired pose.\n\n\n=== Games and puzzles ===\nTanglement puzzles are based on topological aspects of the puzzle's shapes and components.\n\n\n=== Fiber Art ===\nIn order to create a continuous join of pieces in a modular construction, it is necessary to create an unbroken path in an order which surrounds each piece and traverses each edge only once. This process is an application of the Eulerian path.\n\n\n== See also ==\n\nEquivariant topology\nGeneral topology\nList of algebraic topology topics\nList of examples in general topology\nList of general topology topics\nList of geometric topology topics\nList of topology topics\nPublications in topology\nTopoisomer\nTopology glossary\nTopological geometry\nTopological order\nTopological space\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Bibliography ===\nAleksandrov, P. S. (1969) [1956], \"Chapter XVIII Topology\",  in Aleksandrov, A.D.; Kolmogorov, A.N.; Lavrent'ev, M.A., Mathematics / Its Content, Methods and Meaning (2nd ed.), The M.I.T. Press \nCroom, Fred H. (1989), Principles of Topology, Saunders College Publishing, ISBN 0-03-029804-0 \nRicheson, D. (2008), Euler's Gem: The Polyhedron Formula and the Birth of Topology, Princeton University Press \n\n\n== Further reading ==\nRyszard Engelking, General Topology, Heldermann Verlag, Sigma Series in Pure Mathematics, December 1989, ISBN 3-88538-006-4.\nBourbaki; Elements of Mathematics: General Topology, Addison\u2013Wesley (1966).\nBreitenberger, E. (2006). \"Johann Benedict Listing\".  In James, I. M. History of Topology. North Holland. ISBN 978-0-444-82375-5. \nKelley, John L. (1975). General Topology. Springer-Verlag. ISBN 0-387-90125-6. \nBrown, Ronald (2006). Topology and Groupoids. Booksurge. ISBN 1-4196-2722-8.  (Provides a well motivated, geometric account of general topology, and shows the use of groupoids in discussing van Kampen's theorem, covering spaces, and orbit spaces.)\nWac\u0142aw Sierpi\u0144ski, General Topology, Dover Publications, 2000, ISBN 0-486-41148-6\nPickover, Clifford A. (2006). The M\u00f6bius Strip: Dr. August M\u00f6bius's Marvelous Band in Mathematics, Games, Literature, Art, Technology, and Cosmology. Thunder's Mouth Press. ISBN 1-56025-826-8.  (Provides a popular introduction to topology and geometry)\nGemignani, Michael C. (1990) [1967], Elementary Topology (2nd ed.), Dover Publications Inc., ISBN 0-486-66522-4 \n\n\n== External links ==\nHazewinkel, Michiel, ed. (2001) [1994], \"Topology, general\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nElementary Topology: A First Course Viro, Ivanov, Netsvetaev, Kharlamov.\nTopology at Curlie (based on DMOZ)\nThe Topological Zoo at The Geometry Center.\nTopology Atlas\nTopology Course Lecture Notes Aisling McCluskey and Brian McMaster, Topology Atlas.\nTopology Glossary\nMoscow 1935: Topology moving towards America, a historical essay by Hassler Whitney.", "research": "Research comprises \"creative and systematic work undertaken to increase the stock of knowledge, including knowledge of humans, culture and society, and the use of this stock of knowledge to devise new applications.\" It is used to establish or confirm facts, reaffirm the results of previous work, solve new or existing problems, support theorems, or develop new theories. A research project may also be an expansion on past work in the field. Research projects can be used to develop further knowledge on a topic, or in the example of a school research project, they can be used to further a student's research prowess to prepare them for future jobs or reports.  To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects or the project as a whole.  The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, or the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life, technological, etc.\n\n\n== Etymology ==\n\nThe word research is derived from the Middle French \"recherche\", which means \"to go about seeking\", the term itself being derived from the Old French term \"recerchier\" a compound word from \"re-\" + \"cerchier\", or \"sercher\", meaning 'search'. The earliest recorded use of the term was in 1577.\n\n\n== Definitions ==\nResearch has been defined in a number of different ways, and while there are similarities, there does not appear to be a single, all-encompassing definition that is embraced by all who engage in it. \nOne definition of research is used by the OECD, \"Any creative systematic activity undertaken in order to increase the stock of knowledge, including knowledge of man, culture and society, and the use of this knowledge to devise new applications.\"Another definition of research is given by John W. Creswell, who states that  \"[r]esearch is a process of steps used to collect and analyze information to increase our understanding of a topic or issue\". It consists of three steps: pose a question, collect data to answer the question, and present an answer to the question.The Merriam-Webster Online Dictionary defines research in more detail as \"studious inquiry or examination;  especially : investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws\"\n\n\n== Forms of research ==\n\nOriginal research is research that is not exclusively based on a summary, review or synthesis of earlier publications on the subject of research. This material is of a primary source character. The purpose of the original research is to produce new knowledge, rather than to present the existing knowledge in a new form (e.g., summarized or classified).Original research can take a number of forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced, or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher.The degree of originality of the research is among major criteria for articles to be published in academic journals and usually established by means of peer review. Graduate students are commonly required to perform original research as part of a dissertation.Scientific research is a systematic way of gathering data and harnessing curiosity. This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research is funded by public authorities, by charitable organizations and by private groups, including many companies. Scientific research can be subdivided into different classifications according to their academic and application disciplines. Scientific research is a widely used criterion for judging the standing of an academic institution, but some argue that such is an inaccurate assessment of the institution, because the quality of research does not tell about the quality of teaching (these do not necessarily correlate).Research in the humanities involves different methods such as for example hermeneutics and semiotics. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead, explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural, or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past. Other studies aim to merely examine the occurrence of behaviours in societies and communities, without particularly looking for reasons or motivations to explain these. These studies may be qualitative or quantitative, and can use a variety of approaches, such as queer theory or feminist theory.Artistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth.\n\n\n=== Scientific research ===\n\nGenerally, research is understood to follow a certain structural process. Though step order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied:\n\nObservations and formation of the topic: Consists of the subject area of one's interest and following that subject area to conduct subject related research. The subject area should not be randomly chosen since it requires reading a vast amount of literature on the topic to determine the gap in the literature the researcher intends to narrow. A keen interest in the chosen subject area is advisable. The research will have to be justified by linking its importance to already existing knowledge about the topic.\nHypothesis: A testable prediction which designates the relationship between two or more variables.\nConceptual definition: Description of a concept by relating it to other concepts.\nOperational definition:  Details in regards to defining the variables and how they will be measured/assessed in the study.\nGathering of data: Consists of identifying a population and selecting samples, gathering information from or about these samples by using specific research instruments. The instruments used for data collection must be valid and reliable.\nAnalysis of data: Involves breaking down the individual pieces of data to draw conclusions about it.\nData Interpretation: This can be represented through tables, figures, and pictures, and then described in words.\nTest, revising of hypothesis\nConclusion, reiteration if necessaryA common misconception is that a hypothesis will be proven (see, rather, null hypothesis). Generally, a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment.  If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true.\nA useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which states no relationship or difference between the independent or dependent variables.\n\n\n=== Historical research ===\n\nThe historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research:\nIdentification of origin date\nEvidence of localization\nRecognition of authorship\nAnalysis of data\nIdentification of integrity\nAttribution of credibility\n\n\n=== Artistic research ===\nThe controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines. One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis.Artistic research has been defined by the University of Dance and Circus (Dans och Cirkush\u00f6gskolan, DOCH), Stockholm in the following manner - \"Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods, and criticality. Through presented documentation, the insights gained shall be placed in a context.\" Artistic research aims to enhance knowledge and understanding with presentation of the arts. A more simple understanding by Julian Klein defines Artistic Research as any kind of research employing the artistic mode of perception.  For a survey of the central problematics of today's Artistic Research, see Giaco Schiesser.According to artist Hakan Topal, in artistic research, \"perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities\". Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research.The Society for Artistic Research (SAR) publishes the triannual Journal for Artistic Research (JAR), an international, online, open access, and peer-reviewed journal for the identification, publication, and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the Research Catalogue (RC), a searchable, documentary database of artistic research, to which anyone can contribute.\nPatricia Leavy addresses eight arts-based research (ABR) genres: narrative inquiry, fiction-based research, poetry, music, dance, theatre, film, and visual art.In 2016 ELIA (European League of the Institutes of the Arts) launched The Florence Principles' on the Doctorate in the Arts. The Florence Principles relating to the Salzburg Principles and the Salzburg Recommendations of EUA (European University Association) name seven points of attention to specify the Doctorate / PhD in the Arts compared to a scientific doctorate / PhD The Florence Principles have been endorsed and are supported also by AEC, CILECT, CUMULUS and SAR.\n\n\n== Steps in conducting research ==\nResearch is often conducted using the hourglass model structure of research. The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are:\nIdentification of research problem\nLiterature review\nSpecifying the purpose of research\nDetermining specific research questions\nSpecification of a conceptual framework, sometimes including a set of hypotheses\nChoice of a methodology (for data collection)\nData collection\nVerifying data\nAnalyzing and interpreting the data\nReporting and evaluating research\nCommunicating the research findings and, possibly, recommendationsThe steps generally represent the overall process; however, they should be viewed as an ever-changing iterative process rather than a fixed set of steps. Most research begins with a general statement of the problem, or rather, the purpose for engaging in the study. The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified.  A gap in the current literature, as identified by a researcher, then engenders a research question.  The research question may be parallel to the hypothesis.  The hypothesis is the supposition to be tested.  The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as empirical research. The results of the data analysis in rejecting or failing to reject the null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the reverse approach: starting with articulating findings and discussion of them, moving \"up\" to identification of a research problem that emerges in the findings and literature review. The reverse approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings have fully emerged and been interpreted.\nRudolph Rummel says, \"... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results.\"Plato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrased in the following way, \"If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\"\n\n\n== Research methods ==\n\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\n\nExploratory research, which helps to identify and define a problem or question.\nConstructive research, which tests theories and proposes solutions to a problem or question.\nEmpirical research, which tests the feasibility of a solution using empirical evidence.There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\n\nQualitative research\nThis involves understanding human behavior and the reasons that govern such behavior, by asking a broad question, collecting data in the form of words, images, video etc that is analyzed, and searching for themes. This type of research aims to investigate a question without attempting to quantifiably measure variables or look to potential relationships between variables. It is viewed as more restrictive in testing hypotheses because it can be expensive and time-consuming and typically limited to a single set of research subjects. Qualitative research is often used as a method of exploratory research as a basis for later quantitative research hypotheses.  Qualitative research is linked with the philosophical and theoretical stance of social constructionism.Social media posts are used for qualitative research.\nQuantitative research\nThis involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive). Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables.  Quantitative research is linked with the philosophical and theoretical stance of positivism.The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that are easy to summarize, compare, and generalize. Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest.\nIf the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment).  If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants.In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data.  Primary data is data collected specifically for the research, such as through interviews or questionnaires.  Secondary data is data that already exists, such as census data, which can be re-used for the research.  It is good ethical research practice to use secondary data wherever possible.Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common. This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.Big data has brought big impacts on research methods so that now many researchers do not put much effort into data collection; furthermore, methods to analyze easily available huge amounts of data have also been developed.\nNon-empirical researchNon-empirical (theoretical) research is an approach that involves the development of theory as opposed to using observation and experimentation. As such, non-empirical research seeks solutions to problems using existing knowledge as its source. This, however, does not mean that new ideas and innovations cannot be found within the pool of existing and established knowledge. Non-empirical research is not an absolute alternative to empirical research because they may be used together to strengthen a research approach. Neither one is less effective than the other since they have their particular purpose in science. Typically empirical research produces observations that need to be explained; then theoretical research tries to explain them, and in so doing generates empirically testable hypotheses; these hypotheses are then tested empirically, giving more observations that may need further explanation; and so on. See Scientific method.\nA simple example of a non-empirical task is the prototyping of a new drug using a differentiated application of existing knowledge; another is the development of a business process in the form of a flow chart and texts where all the ingredients are from established knowledge. Much of cosmological research is theoretical in nature. Mathematics research does not rely on externally available data; rather, it seeks to prove theorems about mathematical objects.\n\n\n== Research ethics ==\nResearch ethics involves the application of fundamental ethical principles to a variety of topics involving research, including scientific research. These principles include deontology, consequentialism, virtue ethics and value (ethics). Ethical issues may arise in the design and implementation of research involving human experimentation or animal experimentation, such as: various aspects of academic scandal, including scientific misconduct (such as fraud, fabrication of data and plagiarism), whistleblowing; regulation of research, etc.  Research ethics is most developed as a concept in medical research. The key agreement here is the 1964 Declaration of Helsinki. The Nuremberg Code is a former agreement, but with many still important notes. Research in the social sciences presents a different set of issues than those in medical research and can involve issues of researcher and participant safety, empowerment and access to justice.When research involves human subjects, obtaining informed consent from them is essential.\n\n\n== Problems in research ==\n\n\n=== Methods of research ===\nIn many disciplines, Western methods of conducting research are predominant. Researchers are overwhelmingly taught Western methods of data collection and study. The increasing participation of indigenous peoples as researchers has brought increased attention to the lacuna in culturally-sensitive methods of data collection. Non-Western methods of data collection may not be the most accurate or relevant for research on non-Western societies. For example, \"Hua Oranga\" was created as a criterion for psychological evaluation in M\u0101ori populations, and is based on dimensions of mental health important to the M\u0101ori people \u2013 \"taha wairua (the spiritual dimension), taha hinengaro (the mental dimension), taha tinana (the physical dimension), and taha whanau (the family dimension)\".\n\n\n=== Linguicism ===\nPeriphery scholars face the challenges of exclusion and linguicism in research and academic publication. As the great majority of mainstream academic journals are written in English, multilingual periphery scholars often must translate their work to be accepted to elite Western-dominated journals. Multilingual scholars' influences from their native communicative styles can be assumed to be incompetence instead of difference.\n\n\n=== Publication Peer Review ===\nPeer review is a form of self-regulation by qualified members of a profession within the relevant field. Peer review methods are employed to maintain standards of quality, improve performance, and provide credibility. In academia, scholarly peer review is often used to determine an academic paper's suitability for publication. Usually, the peer review process involves experts in the same field who are consulted by editors to give a review of the scholarly works produced by a colleague of theirs from an unbiased and impartial point of view, and this is usually done free of charge. The tradition of peer reviews being done for free has however brought many pitfalls which are also indicative of why most peer reviewers decline many invitations to review. It was observed that publications from periphery countries rarely rise to the same elite status as those of North America and Europe, because limitations on the availability of resources including high-quality paper and sophisticated image-rendering software and printing tools render these publications less able to satisfy standards currently carrying formal or informal authority in the publishing industry. These limitations in turn result in the under-representation of scholars from periphery nations among the set of publications holding prestige status relative to the quantity and quality of those scholars' research efforts, and this under-representation in turn results in disproportionately reduced acceptance of the results of their efforts as contributions to the body of knowledge available worldwide.\n\n\n=== Influence of the open-access movement ===\nThe open access movement assumes that all information generally deemed useful should be free and belongs to a \"public domain\", that of \"humanity\". This idea gained prevalence as a result of Western colonial history and ignores alternative conceptions of knowledge circulation. For instance, most indigenous communities consider that access to certain information proper to the group should be determined by relationships.There is alleged to be a double standard in the Western knowledge system. On the one hand, \"digital right management\" used to restrict access to personal information on social networking platforms is celebrated as a protection of privacy, while simultaneously when similar functions are utilised by cultural groups (i.e. indigenous communities) this is denounced as \"access control\" and reprehended as censorship.\n\n\n=== Future perspectives ===\nEven though Western dominance seems to be prominent in research, some scholars, such as Simon Marginson, argue for \"the need [for] a plural university world\". Marginson argues that the East Asian Confucian model could take over the Western model.\nThis could be due to changes in funding for research both in the East and the West. Focussed on emphasizing educational achievement, East Asian cultures, mainly in China and South Korea, have encouraged the increase of funding for research expansion. In contrast, in the Western academic world, notably in the United Kingdom as well as in some state governments in the United States, funding cuts for university research have occurred, which some say may lead to the future decline of Western dominance in research.\n\n\n== Professionalisation ==\n\nIn several national and private academic systems, the professionalisation of research has resulted in formal job titles.\n\n\n=== In Russia ===\nIn present-day Russia, the former Soviet Union and in some post-Soviet states the term researcher (Russian: \u041d\u0430\u0443\u0447\u043d\u044b\u0439 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a, nauchny sotrudnik) is both a generic term for a person who carried out scientific research, as well as a job position within the frameworks of the USSR Academy of Sciences, Soviet universities, and in other research-oriented establishments. The term is also sometimes translated as research fellow, research associate, etc.\nThe following ranks are known:\n\nJunior Researcher (Junior Research Associate)\nResearcher (Research Associate)\nSenior Researcher (Senior Research Associate)\nLeading Researcher (Leading Research Associate)\nChief Researcher (Chief Research Associate)\n\n\n== Publishing ==\n\nAcademic publishing is a system that is necessary for academic scholars to peer review the work and make it available for a wider audience. The system varies widely by field and is also always changing, if often slowly. Most academic work is published in journal article or book form. There is also a large body of research that exists in either a thesis or dissertation form. These forms of research can be found in databases explicitly for theses and dissertations. In publishing, STM publishing is an abbreviation for academic publications in science, technology, and medicine.\nMost established academic fields have their own scientific journals and other outlets for publication, though many academic journals are somewhat interdisciplinary, and publish work from several distinct fields or subfields. The kinds of publications that are accepted as contributions of knowledge or research vary greatly between fields, from the print to the electronic format. A study suggests that researchers should not give great consideration to findings that are not replicated frequently. It has also been suggested that all published studies should be subjected to some measure for assessing the validity or reliability of its procedures to prevent the publication of unproven findings. Business models are different in the electronic environment. Since about the early 1990s, licensing of electronic resources, particularly journals, has been very common. Presently, a major trend, particularly with respect to scholarly journals, is open access. There are two main forms of open access: open access publishing, in which the articles or the whole journal is freely available from the time of publication, and self-archiving, where the author makes a copy of their own work freely available on the web.\n\n\n== Research funding ==\n\nMost funding for scientific research comes from three major sources: corporate research and development departments; private foundations, for example, the Bill and Melinda Gates Foundation; and government research councils such as the National Institutes of Health in the USA and the Medical Research Council in the UK. These are managed primarily through universities and in some cases through military contractors. Many senior researchers (such as group leaders) spend a significant amount of their time applying for grants for research funds. These grants are necessary not only for researchers to carry out their research but also as a source of merit.\nThe Social Psychology Network provides a comprehensive list of U.S. Government and private foundation funding sources.\n\n\n== See also ==\nEuropean Charter for Researchers\nList of words ending in ology\nUndergraduate research\nInternet research\nList of countries by research and development spending\nAdvertising research\nMarket research\nMarketing research\nOpen research\nOperations research\nParticipatory action research\nPsychological research methods\nResearch-intensive cluster\nScholarly research\nSecondary research\nSociety for Artistic Research\nSocial research\nTimeline of the history of scientific method\n\n\n== References ==\n\n\n== Further reading ==\nCohen, N.; Arieli, T. (2011). \"Field research in conflict environments: Methodological challenges and snowball sampling\". Journal of Peace Research. 48 (4): 423\u2013436. doi:10.1177/0022343311405698. \nSoeters, Joseph; Shields, Patricia and Rietjens, Sebastiaan. 2014. Handbook of Research Methods in Military Studies New York: Routledge.\nTalja, Sanna and Pamela J. Mckenzie (2007). Editor's Introduction: Special Issue on Discursive Approaches to Information Seeking in Context, The University of Chicago Press.\n\n\n== External links ==\n The dictionary definition of research at Wiktionary\n Quotations related to Research at Wikiquote\n Media related to Research at Wikimedia Commons", "calculus": "Calculus (from Latin calculus, literally 'small pebble', used for counting and calculations, as on an abacus), is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations. \nIt has two major branches, differential calculus (concerning instantaneous rates of change and slopes of curves), and integral calculus (concerning accumulation of quantities and the areas under and between curves). These two branches are related to each other by the fundamental theorem of calculus. Both branches make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit. \nGenerally, modern calculus is considered to have been developed in the 17th century by Isaac Newton and Gottfried Wilhelm Leibniz. Today, calculus has widespread uses in science, engineering, and economics.Calculus is a part of modern mathematics education. A course in calculus is a gateway to other, more advanced courses in mathematics devoted to the study of functions and limits, broadly called mathematical analysis. Calculus has historically been called \"the calculus of infinitesimals\", or \"infinitesimal calculus\". The term calculus (plural calculi) is also used for naming specific methods of calculation or notation as well as some theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.\n\n\n== History ==\n\nModern calculus was developed in 17th-century Europe by Isaac Newton and Gottfried Wilhelm Leibniz (independently of each other, first publishing around the same time) but elements of it appeared in ancient Greece, then in China and the Middle East, and still later again in medieval Europe and in India.\n\n\n=== Ancient ===\n\nThe ancient period introduced some of the ideas that led to integral calculus, but does not seem to have developed these ideas in a rigorous and systematic way. Calculations of volume and area, one goal of integral calculus, can be found in the Egyptian Moscow papyrus (13th dynasty, c.\u20091820 BC), but the formulas are simple instructions, with no indication as to method, and some of them lack major components.From the age of Greek mathematics, Eudoxus (c.\u2009408\u2013355 BC) used the method of exhaustion, which foreshadows the concept of the limit, to calculate areas and volumes, while Archimedes (c.\u2009287\u2013212 BC) developed this idea further, inventing heuristics which resemble the methods of integral calculus.The method of exhaustion was later discovered independently in China by Liu Hui in the 3rd century AD in order to find the area of a circle. In the 5th century AD, Zu Gengzhi, son of Zu Chongzhi, established a method that would later be called Cavalieri's principle to find the volume of a sphere.\n\n\n=== Medieval ===\nIn the Middle East, Hasan Ibn al-Haytham, Latinized as Alhazen (c.\u2009965 \u2013 c.\u20091040 CE) derived a formula for the sum of fourth powers. He used the results to carry out what would now be called an integration of this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid.In the 14th century, Indian mathematicians gave a non-rigorous method, resembling differentiation, applicable to some trigonometric functions. Madhava of Sangamagrama and the Kerala School of Astronomy and Mathematics thereby stated components of calculus. A complete theory encompassing these components is now well-known in the Western world as the Taylor series or infinite series approximations. However, they were not able to \"combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the great problem-solving tool we have today\".\n\n\n=== Modern ===\n\nIn Europe, the foundational work was a treatise written by Bonaventura Cavalieri, who argued that volumes and areas should be computed as the sums of the volumes and areas of infinitesimally thin cross-sections. The ideas were similar to Archimedes' in The Method, but this treatise is believed to have been lost in the 13th century, and was only rediscovered in the early 20th century, and so would have been unknown to Cavalieri. Cavalieri's work was not well respected since his methods could lead to erroneous results, and the infinitesimal quantities he introduced were disreputable at first.\nThe formal study of calculus brought together Cavalieri's infinitesimals with the calculus of finite differences developed in Europe at around the same time. Pierre de Fermat, claiming that he borrowed from Diophantus, introduced the concept of adequality, which represented equality up to an infinitesimal error term. The combination was achieved by John Wallis, Isaac Barrow, and James Gregory, the latter two proving the second fundamental theorem of calculus around 1670.\n\nThe product rule and chain rule, the notions of higher derivatives and Taylor series, and of analytic functions were introduced by Isaac Newton in an idiosyncratic notation which he used to solve problems of mathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on a cycloid, and many other problems discussed in his Principia Mathematica (1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of the Taylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.\n\nThese ideas were arranged into a true calculus of infinitesimals by Gottfried Wilhelm Leibniz, who was originally accused of plagiarism by Newton. He is now regarded as an independent inventor of and contributor to calculus. His contribution was to provide a clear set of rules for working with infinitesimal quantities, allowing the computation of second and higher derivatives, and providing the product rule and chain rule, in their differential and integral forms. Unlike Newton, Leibniz paid a lot of attention to the formalism, often spending days determining appropriate symbols for concepts.\nToday, Leibniz and Newton are usually both given credit for independently inventing and developing calculus. Newton was the first to apply calculus to general physics and Leibniz developed much of the notation used in calculus today. The basic insights that both Newton and Leibniz provided were the laws of differentiation and integration, second and higher derivatives, and the notion of an approximating polynomial series. By Newton's time, the fundamental theorem of calculus was known.\nWhen Newton and Leibniz first published their results, there was great controversy over which mathematician (and therefore which country) deserved credit. Newton derived his results first (later to be published in his Method of Fluxions), but Leibniz published his \"Nova Methodus pro Maximis et Minimis\" first. Newton claimed Leibniz stole ideas from his unpublished notes, which Newton had shared with a few members of the Royal Society. This controversy divided English-speaking mathematicians from continental European mathematicians for many years, to the detriment of English mathematics. A careful examination of the papers of Leibniz and Newton shows that they arrived at their results independently, with Leibniz starting first with integration and Newton with differentiation.  It is Leibniz, however, who gave the new discipline its name. Newton called his calculus \"the science of fluxions\".\nSince the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal and integral calculus was written in 1748 by Maria Gaetana Agnesi.\n\n\n=== Foundations ===\nIn calculus, foundations refers to the rigorous development of the subject from axioms and definitions.  In early calculus the use of infinitesimal quantities was thought unrigorous, and was fiercely criticized by a number of authors, most notably Michel Rolle and Bishop Berkeley. Berkeley famously described infinitesimals as the ghosts of departed quantities in his book The Analyst in 1734.  Working out a rigorous foundation for calculus occupied mathematicians for much of the century following Newton and Leibniz, and is still to some extent an active area of research today.\nSeveral mathematicians, including Maclaurin, tried to prove the soundness of using infinitesimals, but it would not be until 150 years later when, due to the work of Cauchy and Weierstrass, a way was finally found to avoid mere \"notions\" of infinitely small quantities. The foundations of differential and integral calculus had been laid. In Cauchy's Cours d'Analyse, we find a broad range of foundational approaches, including a definition of continuity in terms of infinitesimals, and a (somewhat imprecise) prototype of an (\u03b5, \u03b4)-definition of limit in the definition of differentiation. In his work Weierstrass formalized the concept of limit and eliminated infinitesimals (although his definition can actually validate nilsquare infinitesimals). Following the work of Weierstrass, it eventually became common to base calculus on limits instead of infinitesimal quantities, though the subject is still occasionally called \"infinitesimal calculus\". Bernhard Riemann used these ideas to give a precise definition of the integral. It was also during this period that the ideas of calculus were generalized to Euclidean space and the complex plane.\nIn modern mathematics, the foundations of calculus are included in the field of real analysis, which contains full definitions and proofs of the theorems of calculus.  The reach of calculus has also been greatly extended. Henri Lebesgue invented measure theory and used it to define integrals of all but the most pathological functions.  Laurent Schwartz introduced distributions, which can be used to take the derivative of any function whatsoever.\nLimits are not the only rigorous approach to the foundation of calculus. Another way is to use Abraham Robinson's non-standard analysis. Robinson's approach, developed in the 1960s, uses technical machinery from mathematical logic to augment the real number system with infinitesimal and infinite numbers, as in the original Newton-Leibniz conception. The resulting numbers are called hyperreal numbers, and they can be used to give a Leibniz-like development of the usual rules of calculus. There is also smooth infinitesimal analysis, which differs from non-standard analysis in that it mandates neglecting higher power infinitesimals during derivations.\n\n\n=== Significance ===\nWhile many of the ideas of calculus had been developed earlier in Greece, China, India, Iraq, Persia, and Japan, the use of calculus began in Europe, during the 17th century, when Isaac Newton and Gottfried Wilhelm Leibniz built on the work of earlier mathematicians to introduce its basic principles. The development of calculus was built on earlier concepts of instantaneous motion and area underneath curves.\nApplications of differential calculus include computations involving velocity and acceleration, the slope of a curve, and optimization. Applications of integral calculus include computations involving area, volume, arc length, center of mass, work, and pressure. More advanced applications include power series and Fourier series.\nCalculus is also used to gain a more precise understanding of the nature of space, time, and motion. For centuries, mathematicians and philosophers wrestled with paradoxes involving division by zero or sums of infinitely many numbers. These questions arise in the study of motion and area. The ancient Greek philosopher Zeno of Elea gave several famous examples of such paradoxes. Calculus provides tools, especially the limit and the infinite series, that resolve the paradoxes.\n\n\n== Principles ==\n\n\n=== Limits and infinitesimals ===\n\nCalculus is usually developed by working with very small quantities. Historically, the first method of doing so was by infinitesimals. These are objects which can be treated like real numbers but which are, in some sense, \"infinitely small\".  For example, an infinitesimal number could be greater than 0, but less than any number in the sequence 1, 1/2, 1/3, ... and thus less than any positive real number. From this point of view, calculus is a collection of techniques for manipulating infinitesimals.  The symbols \n  \n    \n      \n        d\n        x\n      \n    \n    {\\displaystyle dx}\n   and \n  \n    \n      \n        d\n        y\n      \n    \n    {\\displaystyle dy}\n   were taken to be infinitesimal, and the derivative \n  \n    \n      \n        d\n        y\n        \n          /\n        \n        d\n        x\n      \n    \n    {\\displaystyle dy/dx}\n   was simply their ratio. \nThe infinitesimal approach fell out of favor in the 19th century because it was difficult to make the notion of an infinitesimal precise. However, the concept was revived in the 20th century with the introduction of non-standard analysis and smooth infinitesimal analysis, which provided solid foundations for the manipulation of infinitesimals.\nIn the 19th century, infinitesimals were replaced by the epsilon, delta approach to limits. Limits describe the value of a function at a certain input in terms of its values at nearby inputs. They capture small-scale behavior in the context of the real number system. In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by very small numbers, and the infinitely small behavior of the function is found by taking the limiting behavior for smaller and smaller numbers. Limits were the first way to provide rigorous foundations for calculus, and for this reason they are the standard approach.\n\n\n=== Differential calculus ===\n\nDifferential calculus is the study of the definition, properties, and applications of the derivative of a function. The process of finding the derivative is called differentiation. Given a function and a point in the domain, the derivative at that point is a way of encoding the small-scale behavior of the function near that point. By finding the derivative of a function at every point in its domain, it is possible to produce a new function, called the derivative function or just the derivative of the original function. In formal terms, the derivative is a linear operator which takes a function as its input and produces a second function as its output. This is more abstract than many of the processes studied in elementary algebra, where functions usually input a number and output another number. For example, if the doubling function is given the input three, then it outputs six, and if the squaring function is given the input three, then it outputs nine. The derivative, however, can take the squaring function as an input. This means that the derivative takes all the information of the squaring function\u2014such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on\u2014and uses this information to produce another function. The function produced by deriving the squaring function turns out to be the doubling function.\nIn more explicit terms the \"doubling function\" may be denoted by g(x) = 2x and the \"squaring function\" by f(x) = x2. The \"derivative\" now takes the function f(x), defined by the expression \"x2\", as an input, that is all the information \u2014such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on\u2014 and uses this information to output another function, the function g(x) = 2x, as will turn out.\nThe most common symbol for a derivative is an apostrophe-like mark called prime. Thus, the derivative of a function called f is denoted by f\u2032, pronounced \"f prime\". For instance, if f(x) = x2 is the squaring function, then f\u2032(x) = 2x is its derivative (the doubling function g from above). This notation is known as Lagrange's notation.\nIf the input of the function represents time, then the derivative represents change with respect to time. For example, if f is a function that takes a time as input and gives the position of a ball at that time as output, then the derivative of f is how the position is changing in time, that is, it is the velocity of the ball.\nIf a function is linear (that is, if the graph of the function is a straight line), then the function can be written as y = mx + b, where x is the independent variable, y is the dependent variable, b is the y-intercept, and:\n\n  \n    \n      \n        m\n        =\n        \n          \n            rise\n            run\n          \n        \n        =\n        \n          \n            \n              \n                change in \n              \n              y\n            \n            \n              \n                change in \n              \n              x\n            \n          \n        \n        =\n        \n          \n            \n              \u0394\n              y\n            \n            \n              \u0394\n              x\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle m={\\frac {\\text{rise}}{\\text{run}}}={\\frac {{\\text{change in }}y}{{\\text{change in }}x}}={\\frac {\\Delta y}{\\Delta x}}.}\n  This gives an exact value for the slope of a straight line. If the graph of the function is not a straight line, however, then the change in y divided by the change in x varies. Derivatives give an exact meaning to the notion of change in output with respect to change in input. To be concrete, let f be a function, and fix a point a in the domain of f. (a, f(a)) is a point on the graph of the function. If h is a number close to zero, then a + h is a number close to a. Therefore, (a + h, f(a + h)) is close to (a, f(a)). The slope between these two points is\n\n  \n    \n      \n        m\n        =\n        \n          \n            \n              f\n              (\n              a\n              +\n              h\n              )\n              \u2212\n              f\n              (\n              a\n              )\n            \n            \n              (\n              a\n              +\n              h\n              )\n              \u2212\n              a\n            \n          \n        \n        =\n        \n          \n            \n              f\n              (\n              a\n              +\n              h\n              )\n              \u2212\n              f\n              (\n              a\n              )\n            \n            h\n          \n        \n        .\n      \n    \n    {\\displaystyle m={\\frac {f(a+h)-f(a)}{(a+h)-a}}={\\frac {f(a+h)-f(a)}{h}}.}\n  This expression is called a difference quotient. A line through two points on a curve is called a secant line, so m is the slope of the secant line between (a, f(a)) and (a + h, f(a + h)). The secant line is only an approximation to the behavior of the function at the point a because it does not account for what happens between a and a + h. It is not possible to discover the behavior at a by setting h to zero because this would require dividing by zero, which is undefined. The derivative is defined by taking the limit as h tends to zero, meaning that it considers the behavior of f for all small values of h and extracts a consistent value for the case when h equals zero:\n\n  \n    \n      \n        \n          lim\n          \n            h\n            \u2192\n            0\n          \n        \n        \n          \n            \n              f\n              (\n              a\n              +\n              h\n              )\n              \u2212\n              f\n              (\n              a\n              )\n            \n            \n              h\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\lim _{h\\to 0}{f(a+h)-f(a) \\over {h}}.}\n  Geometrically, the derivative is the slope of the tangent line to the graph of f at a. The tangent line is a limit of secant lines just as the derivative is a limit of difference quotients. For this reason, the derivative is sometimes called the slope of the function f.\nHere is a particular example, the derivative of the squaring function at the input 3. Let f(x) = x2 be the squaring function.\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  f\n                  \u2032\n                \n                (\n                3\n                )\n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    \u2192\n                    0\n                  \n                \n                \n                  \n                    \n                      (\n                      3\n                      +\n                      h\n                      \n                        )\n                        \n                          2\n                        \n                      \n                      \u2212\n                      \n                        3\n                        \n                          2\n                        \n                      \n                    \n                    \n                      h\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    \u2192\n                    0\n                  \n                \n                \n                  \n                    \n                      9\n                      +\n                      6\n                      h\n                      +\n                      \n                        h\n                        \n                          2\n                        \n                      \n                      \u2212\n                      9\n                    \n                    \n                      h\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    \u2192\n                    0\n                  \n                \n                \n                  \n                    \n                      6\n                      h\n                      +\n                      \n                        h\n                        \n                          2\n                        \n                      \n                    \n                    \n                      h\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    \u2192\n                    0\n                  \n                \n                (\n                6\n                +\n                h\n                )\n              \n            \n            \n              \n              \n                \n                =\n                6\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}f'(3)&=\\lim _{h\\to 0}{(3+h)^{2}-3^{2} \\over {h}}\\\\&=\\lim _{h\\to 0}{9+6h+h^{2}-9 \\over {h}}\\\\&=\\lim _{h\\to 0}{6h+h^{2} \\over {h}}\\\\&=\\lim _{h\\to 0}(6+h)\\\\&=6\\end{aligned}}}\n  The slope of the tangent line to the squaring function at the point (3, 9) is 6, that is to say, it is going up six times as fast as it is going to the right. The limit process just described can be performed for any point in the domain of the squaring function. This defines the derivative function of the squaring function, or just the derivative of the squaring function for short. A computation similar to the one above shows that the derivative of the squaring function is the doubling function.\n\n\n=== Leibniz notation ===\n\nA common notation, introduced by Leibniz, for the derivative in the example above is\n\n  \n    \n      \n        \n          \n            \n              \n                y\n              \n              \n                \n                =\n                \n                  x\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  \n                    \n                      d\n                      y\n                    \n                    \n                      d\n                      x\n                    \n                  \n                \n              \n              \n                \n                =\n                2\n                x\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}y&=x^{2}\\\\{\\frac {dy}{dx}}&=2x.\\end{aligned}}}\n  In an approach based on limits, the symbol dy/dx is to be interpreted not as the quotient of two numbers but as a shorthand for the limit computed above. Leibniz, however, did intend it to represent the quotient of two infinitesimally small numbers, dy being the infinitesimally small change in y caused by an infinitesimally small change dx applied to x. We can also think of d/dx as a differentiation operator, which takes a function as an input and gives another function, the derivative, as the output. For example:\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n        (\n        \n          x\n          \n            2\n          \n        \n        )\n        =\n        2\n        x\n        .\n      \n    \n    {\\displaystyle {\\frac {d}{dx}}(x^{2})=2x.}\n  In this usage, the dx in the denominator is read as \"with respect to x\". Another example of correct notation could be:\n\n  \n    \n      \n        \n          \n            \n              \n                g\n                (\n                t\n                )\n                =\n                \n                  t\n                  \n                    2\n                  \n                \n                +\n                2\n                t\n                +\n                4\n              \n            \n            \n              \n            \n            \n              \n                \n                  \n                    d\n                    \n                      d\n                      t\n                    \n                  \n                \n                g\n                (\n                t\n                )\n                =\n                2\n                t\n                +\n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}g(t)=t^{2}+2t+4\\\\\\\\{d \\over dt}g(t)=2t+2\\end{aligned}}}\n  \nEven when calculus is developed using limits rather than infinitesimals, it is common to manipulate symbols like dx and dy as if they were real numbers; although it is possible to avoid such manipulations, they are sometimes notationally convenient in expressing operations such as the total derivative.\n\n\n=== Integral calculus ===\n\nIntegral calculus is the study of the definitions, properties, and applications of two related concepts, the indefinite integral and the definite integral. The process of finding the value of an integral is called integration. In technical language, integral calculus studies two related linear operators.\nThe indefinite integral, also known as the antiderivative, is the inverse operation to the derivative. F is an indefinite integral of f when f is a derivative of F.  (This use of lower- and upper-case letters for a function and its indefinite integral is common in calculus.)\nThe definite integral inputs a function and outputs a number, which gives the algebraic sum of areas between the graph of the input and the x-axis. The technical definition of the definite integral involves the limit of a sum of areas of rectangles, called a Riemann sum.\nA motivating example is the distances traveled in a given time.\n\n  \n    \n      \n        \n          D\n          i\n          s\n          t\n          a\n          n\n          c\n          e\n        \n        =\n        \n          S\n          p\n          e\n          e\n          d\n        \n        \u22c5\n        \n          T\n          i\n          m\n          e\n        \n      \n    \n    {\\displaystyle \\mathrm {Distance} =\\mathrm {Speed} \\cdot \\mathrm {Time} }\n  If the speed is constant, only multiplication is needed, but if the speed changes, a more powerful method of finding the distance is necessary. One such method is to approximate the distance traveled by breaking up the time into many short intervals of time, then multiplying the time elapsed in each interval by one of the speeds in that interval, and then taking the sum (a Riemann sum) of the approximate distance traveled in each interval. The basic idea is that if only a short time elapses, then the speed will stay more or less the same. However, a Riemann sum only gives an approximation of the distance traveled. We must take the limit of all such Riemann sums to find the exact distance traveled.\n\nWhen velocity is constant, the total distance traveled over the given time interval can be computed by multiplying velocity and time.  For example, travelling a steady 50 mph for 3 hours results in a total distance of 150 miles.  In the diagram on the left, when constant velocity and time are graphed, these two values form a rectangle with height equal to the velocity and width equal to the time elapsed.  Therefore, the product of velocity and time also calculates the rectangular area under the (constant) velocity curve.  This connection between the area under a curve and distance traveled can be extended to any irregularly shaped region exhibiting a fluctuating velocity over a given time period. If f(x) in the diagram on the right represents speed as it varies over time, the distance traveled (between the times represented by a and b) is the area of the shaded region s.\nTo approximate that area, an intuitive method would be to divide up the distance between a and b into a number of equal segments, the length of each segment represented by the symbol \u0394x. For each small segment, we can choose one value of the function f(x). Call that value h. Then the area of the rectangle with base \u0394x and height h gives the distance (time \u0394x multiplied by speed h) traveled in that segment.   Associated with each segment is the average value of the function above it, f(x) = h. The sum of all such rectangles gives an approximation of the area between the axis and the curve, which is an approximation of the total distance traveled. A smaller value for \u0394x will give more rectangles and in most cases a better approximation, but for an exact answer we need to take a limit as \u0394x approaches zero.\nThe symbol of integration is \n  \n    \n      \n        \u222b\n      \n    \n    {\\displaystyle \\int }\n  , an elongated S (the S stands for \"sum\"). The definite integral is written as:\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx.}\n  and is read \"the integral from a to b of f-of-x with respect to x.\"  The Leibniz notation dx is intended to suggest dividing the area under the curve into an infinite number of rectangles, so that their width \u0394x becomes the infinitesimally small dx.  In a formulation of the calculus based on limits, the notation\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        \u22ef\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int _{a}^{b}\\cdots \\,dx}\n  is to be understood as an operator that takes a function as an input and gives a number, the area, as an output. The terminating differential, dx, is not a number, and is not being multiplied by f(x), although, serving as a reminder of the \u0394x limit definition, it can be treated as such in symbolic manipulations of the integral. Formally, the differential indicates the variable over which the function is integrated and serves as a closing bracket for the integration operator.\nThe indefinite integral, or antiderivative, is written:\n\n  \n    \n      \n        \u222b\n        f\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int f(x)\\,dx.}\n  Functions differing by only a constant have the same derivative, and it can be shown that the antiderivative of a given function is actually a family of functions differing only by a constant. Since the derivative of the function y = x2 + C, where C is any constant, is y\u2032 = 2x, the antiderivative of the latter given by:\n\n  \n    \n      \n        \u222b\n        2\n        x\n        \n        d\n        x\n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        C\n        .\n      \n    \n    {\\displaystyle \\int 2x\\,dx=x^{2}+C.}\n  The unspecified constant C present in the indefinite integral or antiderivative is known as the constant of integration.\n\n\n=== Fundamental theorem ===\n\nThe fundamental theorem of calculus states that differentiation and integration are inverse operations. More precisely, it relates the values of antiderivatives to definite integrals. Because it is usually easier to compute an antiderivative than to apply the definition of a definite integral, the fundamental theorem of calculus provides a practical way of computing definite integrals. It can also be interpreted as a precise statement of the fact that differentiation is the inverse of integration.\nThe fundamental theorem of calculus states: If a function f is continuous on the interval [a, b] and if F is a function whose derivative is f on the interval (a, b), then\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        F\n        (\n        b\n        )\n        \u2212\n        F\n        (\n        a\n        )\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx=F(b)-F(a).}\n  Furthermore, for every x in the interval (a, b),\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n        \n          \u222b\n          \n            a\n          \n          \n            x\n          \n        \n        f\n        (\n        t\n        )\n        \n        d\n        t\n        =\n        f\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {d}{dx}}\\int _{a}^{x}f(t)\\,dt=f(x).}\n  This realization, made by both Newton and Leibniz, who based their results on earlier work by Isaac Barrow, was key to the proliferation of analytic results after their work became known. The fundamental theorem provides an algebraic method of computing many definite integrals\u2014without performing limit processes\u2014by finding formulas for antiderivatives. It is also a prototype solution of a differential equation. Differential equations relate an unknown function to its derivatives, and are ubiquitous in the sciences.\n\n\n== Applications ==\n\nCalculus is used in every branch of the physical sciences, actuarial science, computer science, statistics, engineering, economics, business, medicine, demography, and in other fields wherever a problem can be mathematically modeled and an optimal solution is desired. It allows one to go from (non-constant) rates of change to the total change or vice versa, and many times in studying a problem we know one and are trying to find the other.\nPhysics makes particular use of calculus; all concepts in classical mechanics and electromagnetism are related through calculus. The mass of an object of known density, the moment of inertia of objects, as well as the total energy of an object within a conservative field can be found by the use of calculus. An example of the use of calculus in mechanics is Newton's second law of motion: historically stated it expressly uses the term \"change of motion\" which implies the derivative saying The change of momentum of a body is equal to the resultant force acting on the body and is in the same direction. Commonly expressed today as Force = Mass \u00d7 acceleration, it implies differential calculus because acceleration is the time derivative of velocity or second time derivative of trajectory or spatial position. Starting from knowing how an object is accelerating, we use calculus to derive its path.\nMaxwell's theory of electromagnetism and Einstein's theory of general relativity are also expressed in the language of differential calculus. Chemistry also uses calculus in determining reaction rates and radioactive decay. In biology, population dynamics starts with reproduction and death rates to model population changes.\nCalculus can be used in conjunction with other mathematical disciplines. For example, it can be used with linear algebra to find the \"best fit\" linear approximation for a set of points in a domain. Or it can be used in probability theory to determine the probability of a continuous random variable from an assumed density function. In analytic geometry, the study of graphs of functions, calculus is used to find high points and low points (maxima and minima), slope, concavity and inflection points.\nGreen's Theorem, which gives the relationship between a line integral around a simple closed curve C and a double integral over the plane region D bounded by C, is applied in an instrument known as a planimeter, which is used to calculate the area of a flat surface on a drawing. For example, it can be used to calculate the amount of area taken up by an irregularly shaped flower bed or swimming pool when designing the layout of a piece of property.\nDiscrete Green's Theorem, which gives the relationship between a double integral of a function around a simple closed rectangular curve C and a linear combination of the antiderivative's values at corner points along the edge of the curve, allows fast calculation of sums of values in rectangular domains.  For example, it can be used to efficiently calculate sums of rectangular domains in images, in order to rapidly extract features and detect object; another algorithm that could be used is the summed area table.\nIn the realm of medicine, calculus can be used to find the optimal branching angle of a blood vessel so as to maximize flow. From the decay laws for a particular drug's elimination from the body, it is used to derive dosing laws. In nuclear medicine, it is used to build models of radiation transport in targeted tumor therapies.\nIn economics, calculus allows for the determination of maximal profit by providing a way to easily calculate both marginal cost and marginal revenue.\nCalculus is also used to find approximate solutions to equations; in practice it is the standard way to solve differential equations and do root finding in most applications. Examples are methods such as Newton's method, fixed point iteration, and linear approximation. For instance, spacecraft use a variation of the Euler method to approximate curved courses within zero gravity environments.\n\n\n== Varieties ==\nOver the years, many reformulations of calculus have been investigated for different purposes.\n\n\n=== Non-standard calculus ===\n\nImprecise calculations with infinitesimals were widely replaced with the rigorous (\u03b5, \u03b4)-definition of limit starting in the 1870s. Meanwhile, calculations with infinitesimals persisted and often led to correct results. This led Abraham Robinson to investigate if it were possible to develop a number system with infinitesimal quantities over which the theorems of calculus were still valid. In 1960, building upon the work of Edwin Hewitt and Jerzy \u0141o\u015b, he succeeded in developing non-standard analysis. The theory of non-standard analysis is rich enough to be applied in many branches of mathematics.  As such, books and articles dedicated solely to the traditional theorems of calculus often go by the title non-standard calculus.\n\n\n=== Smooth infinitesimal analysis ===\n\nThis is another reformulation of the calculus in terms of infinitesimals. Based on the ideas of F. W. Lawvere and employing the methods of category theory, it views all functions as being continuous and incapable of being expressed in terms of discrete entities.  One aspect of this formulation is that the law of excluded middle does not hold in this formulation.\n\n\n=== Constructive analysis ===\n\nConstructive mathematics is a branch of mathematics that insists that proofs of the existence of a number, function, or other mathematical object should give a construction of the object. As such constructive mathematics also rejects the law of excluded middle. Reformulations of calculus in a constructive framework are generally part of the subject of constructive analysis.\n\n\n== See also ==\n\n\n=== Lists ===\nGlossary of calculus\nList of calculus topics\nList of derivatives and integrals in alternative calculi\nList of differentiation identities\nPublications in calculus\nTable of integrals\n\n\n=== Other related topics ===\nCalculus of finite differences\nCalculus with polynomials\nComplex analysis\nDifferential equation\nDifferential geometry\nElementary Calculus: An Infinitesimal Approach\nFourier series\nIntegral equation\nMathematical analysis\nMultiplicative calculus\nMultivariable calculus\nNon-classical analysis\nNon-Newtonian calculus\nNon-standard analysis\nNon-standard calculus\nPrecalculus (mathematical education)\nProduct integral\nStochastic calculus\nTaylor series\n\n\n== References ==\n\n\n== Further reading ==\n\n\n=== Books ===\n\n\n=== Online books ===\n\n\n== External links ==\n\nHazewinkel, Michiel, ed. (2001) [1994], \"Calculus\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nWeisstein, Eric W. \"Calculus\". MathWorld. \nTopics on Calculus at PlanetMath.org.\nCalculus Made Easy (1914) by Silvanus P. Thompson Full text in PDF\nCalculus on In Our Time at the BBC\nCalculus.org: The Calculus page at University of California, Davis \u2013 contains resources and links to other sites\nCOW: Calculus on the Web at Temple University \u2013 contains resources ranging from pre-calculus and associated algebra\nEarliest Known Uses of Some of the Words of Mathematics: Calculus & Analysis\nOnline Integrator (WebMathematica) from Wolfram Research\nThe Role of Calculus in College Mathematics from ERICDigests.org\nOpenCourseWare Calculus from the Massachusetts Institute of Technology\nInfinitesimal Calculus \u2013 an article on its historical development, in Encyclopedia of Mathematics, ed. Michiel Hazewinkel.\nDaniel Kleitman, MIT. \"Calculus for Beginners and Artists\". \nCalculus Problems and Solutions by D. A. Kouba\nDonald Allen's notes on calculus\nCalculus training materials at imomath.com\n(in English) (in Arabic) The Excursion of Calculus, 1772", "trigonometry": "Trigonometry (from Greek trig\u014dnon, \"triangle\" and metron, \"measure\") is a branch of mathematics that studies relationships involving lengths and angles of triangles. The field emerged in the Hellenistic  world during the 3rd century BC from applications of geometry to astronomical studies.The 3rd-century astronomers first noted that the lengths of the sides of a right-angle triangle and the angles between those sides have fixed relationships: that is, if at least the length of one side and the value of one angle is known, then all other angles and lengths can be determined algorithmically. These calculations soon came to be defined as the trigonometric functions and today are pervasive in both pure and applied mathematics: fundamental methods of analysis such as the Fourier transform, for example, or the wave equation, use trigonometric functions to understand cyclical phenomena across many applications in fields as diverse as physics, mechanical and electrical engineering, music and acoustics, astronomy, ecology, and biology. Trigonometry is also the foundation of surveying.\nTrigonometry is most simply associated with planar right-angle triangles (each of which is a two-dimensional triangle with one angle equal to 90 degrees). The applicability to non-right-angle triangles exists, but, since any non-right-angle triangle (on a flat plane) can be bisected to create two right-angle triangles, most problems can be reduced to calculations on right-angle triangles. Thus the majority of applications relate to right-angle triangles. One exception to this is spherical trigonometry, the study of triangles on spheres, surfaces of constant positive curvature, in elliptic geometry (a fundamental part of astronomy and navigation). Trigonometry on surfaces of negative curvature is part of hyperbolic geometry.\nTrigonometry basics are often taught in schools, either as a separate course or as a part of a precalculus course.\n\n\n== History ==\n\nA thick ring shell at the Indus Valley Civilization site of Lothal, with four slits each in two margins served as a compass to measure angles on plane surfaces or in the horizon in multiples of 40 degrees, up to 360 degrees. Such shell instruments were probably invented to measure 8\u201312 whole sections of the horizon and sky, explaining the slits on the lower and upper margins. Archaeologists consider this as evidence that the Lothal experts had achieved an 8\u201312 fold division of horizon and sky, as well as an instrument for measuring angles and perhaps the position of stars, and for navigation.Sumerian astronomers studied angle measure, using a division of circles into 360 degrees. They, and later the Babylonians, studied the ratios of the sides of similar triangles and discovered some properties of these ratios but did not turn that into a systematic method for finding sides and angles of triangles. The ancient Nubians used a similar method.In the 3rd century BC, Hellenistic mathematicians such as Euclid and Archimedes studied the properties of chords and inscribed angles in circles, and they proved theorems that are equivalent to modern trigonometric formulae, although they presented them geometrically rather than algebraically. In 140 BC, Hipparchus (from Nicaea, Asia Minor) gave the first tables of chords, analogous to modern tables of sine values, and used them to solve problems in trigonometry and spherical trigonometry. In the 2nd century AD, the Greco-Egyptian astronomer Ptolemy (from Alexandria, Egypt) constructed detailed trigonometric tables (Ptolemy's table of chords) in Book 1, chapter 11 of his Almagest. Ptolemy used chord length to define his trigonometric functions, a minor difference from the sine convention we use today. (The value we call sin(\u03b8) can be found by looking up the chord length for twice the angle of interest (2\u03b8) in Ptolemy's table, and then dividing that value by two.) Centuries passed before more detailed tables were produced, and Ptolemy's treatise remained in use for performing trigonometric calculations in astronomy throughout the next 1200 years in the medieval Byzantine, Islamic, and, later, Western European worlds.\nThe modern sine convention is first attested in the Surya Siddhanta, and its properties were further documented by the 5th century (AD) Indian mathematician and astronomer Aryabhata. These Greek and Indian works were translated and expanded by medieval Islamic mathematicians. By the 10th century, Islamic mathematicians were using all six trigonometric functions, had tabulated their values, and were applying them to problems in spherical geometry. At about the same time, Chinese mathematicians developed trigonometry independently, although it was not a major field of study for them. Knowledge of trigonometric functions and methods reached Western Europe via Latin translations of Ptolemy's Greek Almagest as well as the works of Persian and Arabic astronomers such as Al Battani and Nasir al-Din al-Tusi. One of the earliest works on trigonometry by a northern European mathematician is De Triangulis by the 15th century German mathematician Regiomontanus, who was encouraged to write, and provided with a copy of the Almagest, by the Byzantine Greek scholar cardinal Basilios Bessarion with whom he lived for several years. At the same time, another translation of the Almagest from Greek into Latin was completed by the Cretan George of Trebizond. Trigonometry was still so little known in 16th-century northern Europe that Nicolaus Copernicus devoted two chapters of De revolutionibus orbium coelestium to explain its basic concepts.\nDriven by the demands of navigation and the growing need for accurate maps of large geographic areas, trigonometry grew into a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his Trigonometria in 1595. Gemma Frisius described for the first time the method of triangulation still used today in surveying. It was Leonhard Euler who fully incorporated complex numbers into trigonometry. The works of the Scottish mathematicians James Gregory in the 17th century and Colin Maclaurin in the 18th century were influential in the development of trigonometric series. Also in the 18th century, Brook Taylor defined the general Taylor series.\n\n\n== Overview ==\n\nIf one angle of a triangle is 90 degrees and one of the other angles is known, the third is thereby fixed, because the three angles of any triangle add up to 180 degrees. The two acute angles therefore add up to 90 degrees: they are complementary angles. The shape of a triangle is completely determined, except for similarity, by the angles. Once the angles are known, the ratios of the sides are determined, regardless of the overall size of the triangle. If the length of one of the sides is known, the other two are determined. These ratios are given by the following trigonometric functions of the known angle A, where a,  b and c refer to the lengths of the sides in the accompanying figure:\n\nSine function (sin), defined as the ratio of the side opposite the angle to the hypotenuse.\n  \n    \n      \n        sin\n        \u2061\n        A\n        =\n        \n          \n            \n              opposite\n            \n            \n              hypotenuse\n            \n          \n        \n        =\n        \n          \n            a\n            \n              \n              c\n              \n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\sin A={\\frac {\\textrm {opposite}}{\\textrm {hypotenuse}}}={\\frac {a}{\\,c\\,}}\\,.}\n  Cosine function (cos), defined as the ratio of the adjacent leg to the hypotenuse.\n  \n    \n      \n        cos\n        \u2061\n        A\n        =\n        \n          \n            \n              adjacent\n            \n            \n              hypotenuse\n            \n          \n        \n        =\n        \n          \n            b\n            \n              \n              c\n              \n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\cos A={\\frac {\\textrm {adjacent}}{\\textrm {hypotenuse}}}={\\frac {b}{\\,c\\,}}\\,.}\n  Tangent function (tan), defined as the ratio of the opposite leg to the adjacent leg.\n  \n    \n      \n        tan\n        \u2061\n        A\n        =\n        \n          \n            \n              opposite\n            \n            \n              adjacent\n            \n          \n        \n        =\n        \n          \n            a\n            \n              \n              b\n              \n            \n          \n        \n        =\n        \n          \n            a\n            \n              \n              c\n              \n            \n          \n        \n        \u2217\n        \n          \n            c\n            \n              \n              b\n              \n            \n          \n        \n        =\n        \n          \n            a\n            \n              \n              c\n              \n            \n          \n        \n        \n          /\n        \n        \n          \n            b\n            \n              \n              c\n              \n            \n          \n        \n        =\n        \n          \n            \n              sin\n              \u2061\n              A\n            \n            \n              cos\n              \u2061\n              A\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\tan A={\\frac {\\textrm {opposite}}{\\textrm {adjacent}}}={\\frac {a}{\\,b\\,}}={\\frac {a}{\\,c\\,}}*{\\frac {c}{\\,b\\,}}={\\frac {a}{\\,c\\,}}/{\\frac {b}{\\,c\\,}}={\\frac {\\sin A}{\\cos A}}\\,.}\n  The hypotenuse is the side opposite to the 90 degree angle in a right triangle; it is the longest side of the triangle and one of the two sides adjacent to angle A. The adjacent leg is the other side that is adjacent to angle A. The opposite side is the side that is opposite to angle A. The terms perpendicular and base are sometimes used for the opposite and adjacent sides respectively. Many people find it easy to remember what sides of the right triangle are equal to sine, cosine, or tangent, by memorizing the word SOH-CAH-TOA (see below under Mnemonics).\nThe reciprocals of these functions are named the cosecant (csc or cosec), secant (sec), and cotangent (cot), respectively:\n\n  \n    \n      \n        csc\n        \u2061\n        A\n        =\n        \n          \n            1\n            \n              sin\n              \u2061\n              A\n            \n          \n        \n        =\n        \n          \n            \n              hypotenuse\n            \n            \n              opposite\n            \n          \n        \n        =\n        \n          \n            c\n            a\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\csc A={\\frac {1}{\\sin A}}={\\frac {\\textrm {hypotenuse}}{\\textrm {opposite}}}={\\frac {c}{a}},}\n  \n  \n    \n      \n        sec\n        \u2061\n        A\n        =\n        \n          \n            1\n            \n              cos\n              \u2061\n              A\n            \n          \n        \n        =\n        \n          \n            \n              hypotenuse\n            \n            \n              adjacent\n            \n          \n        \n        =\n        \n          \n            c\n            b\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sec A={\\frac {1}{\\cos A}}={\\frac {\\textrm {hypotenuse}}{\\textrm {adjacent}}}={\\frac {c}{b}},}\n  \n  \n    \n      \n        cot\n        \u2061\n        A\n        =\n        \n          \n            1\n            \n              tan\n              \u2061\n              A\n            \n          \n        \n        =\n        \n          \n            \n              adjacent\n            \n            \n              opposite\n            \n          \n        \n        =\n        \n          \n            \n              cos\n              \u2061\n              A\n            \n            \n              sin\n              \u2061\n              A\n            \n          \n        \n        =\n        \n          \n            b\n            a\n          \n        \n        .\n      \n    \n    {\\displaystyle \\cot A={\\frac {1}{\\tan A}}={\\frac {\\textrm {adjacent}}{\\textrm {opposite}}}={\\frac {\\cos A}{\\sin A}}={\\frac {b}{a}}.}\n  The inverse functions are called the arcsine, arccosine, and arctangent, respectively. There are arithmetic relations between these functions, which are known as trigonometric identities. The cosine, cotangent, and cosecant are so named because they are respectively the sine, tangent, and secant of the complementary angle abbreviated to \"co-\".\nWith these functions, one can answer virtually all questions about arbitrary triangles by using the law of sines and the law of cosines. These laws can be used to compute the remaining angles and sides of any triangle as soon as two sides and their included angle or two angles and a side or three sides are known. These laws are useful in all branches of geometry, since every polygon may be described as a finite combination of triangles.\n\n\n=== Extending the definitions ===\n\nThe above definitions only apply to angles between 0 and 90 degrees (0 and \u03c0/2 radians). Using the unit circle, one can extend them to all positive and negative arguments (see trigonometric function). The trigonometric functions are periodic, with a period of 360 degrees or 2\u03c0 radians. That means their values repeat at those intervals. The tangent and cotangent functions also have a shorter period, of 180 degrees or \u03c0 radians.\nThe trigonometric functions can be defined in other ways besides the geometrical definitions above, using tools from calculus and infinite series. With these definitions the trigonometric functions can be defined for complex numbers. The complex exponential function is particularly useful.\n\n  \n    \n      \n        \n          e\n          \n            x\n            +\n            i\n            y\n          \n        \n        =\n        \n          e\n          \n            x\n          \n        \n        (\n        cos\n        \u2061\n        y\n        +\n        i\n        sin\n        \u2061\n        y\n        )\n        .\n      \n    \n    {\\displaystyle e^{x+iy}=e^{x}(\\cos y+i\\sin y).}\n  See Euler's and De Moivre's formulas.\n\n\t\t\n\t\t\n\n\n=== Mnemonics ===\n\nA common use of mnemonics is to remember facts and relationships in trigonometry. For example, the sine, cosine, and tangent ratios in a right triangle can be remembered by representing them and their corresponding sides as strings of letters. For instance, a mnemonic is SOH-CAH-TOA:\nSine = Opposite \u00f7 Hypotenuse\nCosine = Adjacent \u00f7 Hypotenuse\nTangent = Opposite \u00f7 AdjacentOne way to remember the letters is to sound them out phonetically (i.e., SOH-CAH-TOA, which is pronounced 'so-k\u0259-toe-uh' ). Another method is to expand the letters into a sentence, such as \"Some Old Hippie Caught Another Hippie Trippin' On Acid\".\n\n\n=== Calculating trigonometric functions ===\n\nTrigonometric functions were among the earliest uses for mathematical tables. Such tables were incorporated into mathematics textbooks and students were taught to look up values and how to interpolate between the values listed to get higher accuracy. Slide rules had special scales for trigonometric functions.\nToday, scientific calculators have buttons for calculating the main trigonometric functions (sin, cos, tan, and sometimes cis and their inverses). Most allow a choice of angle measurement methods: degrees, radians, and sometimes gradians. Most computer programming languages provide function libraries that include the trigonometric functions. The floating point unit hardware incorporated into the microprocessor chips used in most personal computers has built-in instructions for calculating trigonometric functions.\n\n\n== Applications ==\n\nThere is an enormous number of uses of trigonometry and trigonometric functions. For instance, the technique of triangulation is used in astronomy to measure the distance to nearby stars, in geography to measure distances between landmarks, and in satellite navigation systems. The sine and cosine functions are fundamental to the theory of periodic functions, such as those that describe sound and light waves.\nFields that use trigonometry or trigonometric functions include astronomy (especially for locating apparent positions of celestial objects, in which spherical trigonometry is essential) and hence navigation (on the oceans, in aircraft, and in space), music theory, audio synthesis, acoustics, optics, electronics, biology, medical imaging (CT scans and ultrasound), pharmacy, chemistry, number theory (and hence cryptology), seismology, meteorology, oceanography, many physical sciences, land surveying and geodesy, architecture, image compression, phonetics, economics, electrical engineering, mechanical engineering, civil engineering, computer graphics, cartography, crystallography and game development.\n\n\n== Pythagorean identities ==\nThe following identities are related to the Pythagorean theorem and hold for any value:\n\n  \n    \n      \n        \n          sin\n          \n            2\n          \n        \n        \u2061\n        A\n        +\n        \n          cos\n          \n            2\n          \n        \n        \u2061\n        A\n        =\n        1\n         \n      \n    \n    {\\displaystyle \\sin ^{2}A+\\cos ^{2}A=1\\ }\n  \n  \n    \n      \n        \n          tan\n          \n            2\n          \n        \n        \u2061\n        A\n        +\n        1\n        =\n        \n          sec\n          \n            2\n          \n        \n        \u2061\n        A\n         \n      \n    \n    {\\displaystyle \\tan ^{2}A+1=\\sec ^{2}A\\ }\n  \n  \n    \n      \n        \n          cot\n          \n            2\n          \n        \n        \u2061\n        A\n        +\n        1\n        =\n        \n          csc\n          \n            2\n          \n        \n        \u2061\n        A\n         \n      \n    \n    {\\displaystyle \\cot ^{2}A+1=\\csc ^{2}A\\ }\n  \n\n\n== Angle transformation formulae ==\n\n  \n    \n      \n        sin\n        \u2061\n        (\n        A\n        \u00b1\n        B\n        )\n        =\n        sin\n        \u2061\n        A\n         \n        cos\n        \u2061\n        B\n        \u00b1\n        cos\n        \u2061\n        A\n         \n        sin\n        \u2061\n        B\n      \n    \n    {\\displaystyle \\sin(A\\pm B)=\\sin A\\ \\cos B\\pm \\cos A\\ \\sin B}\n  \n  \n    \n      \n        cos\n        \u2061\n        (\n        A\n        \u00b1\n        B\n        )\n        =\n        cos\n        \u2061\n        A\n         \n        cos\n        \u2061\n        B\n        \u2213\n        sin\n        \u2061\n        A\n         \n        sin\n        \u2061\n        B\n      \n    \n    {\\displaystyle \\cos(A\\pm B)=\\cos A\\ \\cos B\\mp \\sin A\\ \\sin B}\n  \n  \n    \n      \n        tan\n        \u2061\n        (\n        A\n        \u00b1\n        B\n        )\n        =\n        \n          \n            \n              tan\n              \u2061\n              A\n              \u00b1\n              tan\n              \u2061\n              B\n            \n            \n              1\n              \u2213\n              tan\n              \u2061\n              A\n               \n              tan\n              \u2061\n              B\n            \n          \n        \n      \n    \n    {\\displaystyle \\tan(A\\pm B)={\\frac {\\tan A\\pm \\tan B}{1\\mp \\tan A\\ \\tan B}}}\n  \n  \n    \n      \n        cot\n        \u2061\n        (\n        A\n        \u00b1\n        B\n        )\n        =\n        \n          \n            \n              cot\n              \u2061\n              A\n               \n              cot\n              \u2061\n              B\n              \u2213\n              1\n            \n            \n              cot\n              \u2061\n              B\n              \u00b1\n              cot\n              \u2061\n              A\n            \n          \n        \n      \n    \n    {\\displaystyle \\cot(A\\pm B)={\\frac {\\cot A\\ \\cot B\\mp 1}{\\cot B\\pm \\cot A}}}\n  \n\n\n== Common formulae ==\n\nCertain equations involving trigonometric functions are true for all angles and are known as trigonometric identities. Some identities equate an expression to a different expression involving the same angles. These are listed in List of trigonometric identities. Triangle identities that relate the sides and angles of a given triangle are listed below.\nIn the following identities, A, B and C are the angles of a triangle and a, b and c are the lengths of sides of the triangle opposite the respective angles (as shown in the diagram).\n\n\n=== Law of sines ===\nThe law of sines (also known as the \"sine rule\") for an arbitrary triangle states:\n\n  \n    \n      \n        \n          \n            a\n            \n              sin\n              \u2061\n              A\n            \n          \n        \n        =\n        \n          \n            b\n            \n              sin\n              \u2061\n              B\n            \n          \n        \n        =\n        \n          \n            c\n            \n              sin\n              \u2061\n              C\n            \n          \n        \n        =\n        2\n        R\n        =\n        \n          \n            \n              a\n              b\n              c\n            \n            \n              2\n              \u0394\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {a}{\\sin A}}={\\frac {b}{\\sin B}}={\\frac {c}{\\sin C}}=2R={\\frac {abc}{2\\Delta }},}\n  where \n  \n    \n      \n        \u0394\n      \n    \n    {\\displaystyle \\Delta }\n   is the area of the triangle and R is the radius of the circumscribed circle of the triangle:\n\n  \n    \n      \n        R\n        =\n        \n          \n            \n              a\n              b\n              c\n            \n            \n              (\n              a\n              +\n              b\n              +\n              c\n              )\n              (\n              a\n              \u2212\n              b\n              +\n              c\n              )\n              (\n              a\n              +\n              b\n              \u2212\n              c\n              )\n              (\n              b\n              +\n              c\n              \u2212\n              a\n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle R={\\frac {abc}{\\sqrt {(a+b+c)(a-b+c)(a+b-c)(b+c-a)}}}.}\n  Another law involving sines can be used to calculate the area of a triangle. Given two sides a and b and the angle between the sides C, the area of the triangle is given by half the product of the lengths of two sides and the sine of the angle between the two sides:\n\n  \n    \n      \n        \n          \n            Area\n          \n        \n        =\n        \u0394\n        =\n        \n          \n            1\n            2\n          \n        \n        a\n        b\n        sin\n        \u2061\n        C\n        .\n      \n    \n    {\\displaystyle {\\mbox{Area}}=\\Delta ={\\frac {1}{2}}ab\\sin C.}\n  \n\n\n=== Law of cosines ===\nThe law of cosines (known as the cosine formula, or the \"cos rule\") is an extension of the Pythagorean theorem to arbitrary triangles:\n\n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n        =\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        \u2212\n        2\n        a\n        b\n        cos\n        \u2061\n        C\n        ,\n        \n      \n    \n    {\\displaystyle c^{2}=a^{2}+b^{2}-2ab\\cos C,\\,}\n  or equivalently:\n\n  \n    \n      \n        cos\n        \u2061\n        C\n        =\n        \n          \n            \n              \n                a\n                \n                  2\n                \n              \n              +\n              \n                b\n                \n                  2\n                \n              \n              \u2212\n              \n                c\n                \n                  2\n                \n              \n            \n            \n              2\n              a\n              b\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle \\cos C={\\frac {a^{2}+b^{2}-c^{2}}{2ab}}.\\,}\n  The law of cosines may be used to prove Heron's formula, which is another method that may be used to calculate the area of a triangle. This formula states that if a triangle has sides of lengths a, b, and c, and if the semiperimeter is\n\n  \n    \n      \n        s\n        =\n        \n          \n            1\n            2\n          \n        \n        (\n        a\n        +\n        b\n        +\n        c\n        )\n        ,\n      \n    \n    {\\displaystyle s={\\frac {1}{2}}(a+b+c),}\n  then the area of the triangle is:\n\n  \n    \n      \n        \n          \n            Area\n          \n        \n        =\n        \u0394\n        =\n        \n          \n            s\n            (\n            s\n            \u2212\n            a\n            )\n            (\n            s\n            \u2212\n            b\n            )\n            (\n            s\n            \u2212\n            c\n            )\n          \n        \n        =\n        \n          \n            \n              a\n              b\n              c\n            \n            \n              4\n              R\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mbox{Area}}=\\Delta ={\\sqrt {s(s-a)(s-b)(s-c)}}={\\frac {abc}{4R}}}\n  ,where R is the radius of the circumcircle of the triangle.\n\n\n=== Law of tangents ===\nThe law of tangents:\n\n  \n    \n      \n        \n          \n            \n              a\n              \u2212\n              b\n            \n            \n              a\n              +\n              b\n            \n          \n        \n        =\n        \n          \n            \n              tan\n              \u2061\n              \n                [\n                \n                  \n                    \n                      \n                        1\n                        2\n                      \n                    \n                  \n                  (\n                  A\n                  \u2212\n                  B\n                  )\n                \n                ]\n              \n            \n            \n              tan\n              \u2061\n              \n                [\n                \n                  \n                    \n                      \n                        1\n                        2\n                      \n                    \n                  \n                  (\n                  A\n                  +\n                  B\n                  )\n                \n                ]\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {a-b}{a+b}}={\\frac {\\tan \\left[{\\tfrac {1}{2}}(A-B)\\right]}{\\tan \\left[{\\tfrac {1}{2}}(A+B)\\right]}}}\n  \n\n\n=== Euler's formula ===\nEuler's formula, which states that \n  \n    \n      \n        \n          e\n          \n            i\n            x\n          \n        \n        =\n        cos\n        \u2061\n        x\n        +\n        i\n        sin\n        \u2061\n        x\n      \n    \n    {\\displaystyle e^{ix}=\\cos x+i\\sin x}\n  , produces the following analytical identities for sine, cosine, and tangent in terms of e and the imaginary unit i:\n\n  \n    \n      \n        sin\n        \u2061\n        x\n        =\n        \n          \n            \n              \n                e\n                \n                  i\n                  x\n                \n              \n              \u2212\n              \n                e\n                \n                  \u2212\n                  i\n                  x\n                \n              \n            \n            \n              2\n              i\n            \n          \n        \n        ,\n        \n        cos\n        \u2061\n        x\n        =\n        \n          \n            \n              \n                e\n                \n                  i\n                  x\n                \n              \n              +\n              \n                e\n                \n                  \u2212\n                  i\n                  x\n                \n              \n            \n            2\n          \n        \n        ,\n        \n        tan\n        \u2061\n        x\n        =\n        \n          \n            \n              i\n              (\n              \n                e\n                \n                  \u2212\n                  i\n                  x\n                \n              \n              \u2212\n              \n                e\n                \n                  i\n                  x\n                \n              \n              )\n            \n            \n              \n                e\n                \n                  i\n                  x\n                \n              \n              +\n              \n                e\n                \n                  \u2212\n                  i\n                  x\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sin x={\\frac {e^{ix}-e^{-ix}}{2i}},\\qquad \\cos x={\\frac {e^{ix}+e^{-ix}}{2}},\\qquad \\tan x={\\frac {i(e^{-ix}-e^{ix})}{e^{ix}+e^{-ix}}}.}\n  \n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nBoyer, Carl B. (1991). A History of Mathematics (Second ed.). John Wiley & Sons, Inc. ISBN 0-471-54397-7. \nHazewinkel, Michiel, ed. (2001) [1994], \"Trigonometric functions\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nChristopher M. Linton (2004). From Eudoxus to Einstein: A History of Mathematical Astronomy . Cambridge University Press.\nWeisstein, Eric W. \"Trigonometric Addition Formulas\". MathWorld. \n\n\n== External links ==\n\nKhan Academy: Trigonometry, free online micro lectures\nTrigonometry by Alfred Monroe Kenyon and Louis Ingold, The Macmillan Company, 1914. In images, full text presented.\nBenjamin Banneker's Trigonometry Puzzle at Convergence\nDave's Short Course in Trigonometry by David Joyce of Clark University\nTrigonometry, by Michael Corral, Covers elementary trigonometry, Distributed under GNU Free Documentation License", "geometry ": "Geometry (from the Ancient Greek: \u03b3\u03b5\u03c9\u03bc\u03b5\u03c4\u03c1\u03af\u03b1; geo- \"earth\", -metron \"measurement\") is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space. A mathematician who works in the field of geometry is called a geometer.\nGeometry arose independently in a number of early cultures as a practical way for dealing with lengths, areas, and volumes. Geometry began to see elements of formal mathematical science emerging in the West as early as the 6th century BC. By the 3rd century BC, geometry was put into an axiomatic form by Euclid, whose treatment, Euclid's Elements, set a standard for many centuries to follow. Geometry arose independently in India, with texts providing rules for geometric constructions appearing as early as the 3rd century BC. Islamic scientists preserved Greek ideas and expanded on them during the Middle Ages. By the early 17th century, geometry had been put on a solid analytic footing by mathematicians such as Ren\u00e9 Descartes and Pierre de Fermat. Since then, and into modern times, geometry has expanded into non-Euclidean geometry and manifolds, describing spaces that lie beyond the normal range of human experience.While geometry has evolved significantly throughout the years, there are some general concepts that are more or less fundamental to geometry. These include the concepts of points, lines, planes, surfaces, angles, and curves, as well as the more advanced notions of manifolds and topology or metric.Geometry has applications to many fields, including art, architecture, physics, as well as to other branches of mathematics.\n\n\n== Overview ==\nContemporary geometry has many subfields:\n\nEuclidean geometry is geometry in its classical sense. The mandatory educational curriculum of the majority of nations includes the study of points, lines, planes, angles, triangles, congruence, similarity, solid figures, circles, and analytic geometry. Euclidean geometry also has applications in computer science, crystallography, and various branches of modern mathematics.\nDifferential geometry uses techniques of calculus and linear algebra to study problems in geometry. It has applications in physics, including in general relativity.\nTopology is the field concerned with the properties of geometric objects that are unchanged by continuous mappings. In practice, this often means dealing with large-scale properties of spaces, such as connectedness and compactness.\nConvex geometry investigates convex shapes in the Euclidean space and its more abstract analogues, often using techniques of real analysis. It has close connections to convex analysis, optimization and functional analysis and important applications in number theory.\nAlgebraic geometry studies geometry through the use of multivariate polynomials and other algebraic techniques. It has applications in many areas, including cryptography and string theory.\nDiscrete geometry is concerned mainly with questions of relative position of simple geometric objects, such as points, lines and circles. It shares many methods and principles with combinatorics.\n\n\n== History ==\n\nThe earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC. Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian Rhind Papyrus (2000\u20131800 BC) and Moscow Papyrus (c. 1890 BC), the Babylonian clay tablets such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum. Later clay tablets (350\u201350 BC) demonstrate that Babylonian astronomers implemented trapezoid procedures for computing Jupiter's position and motion within time-velocity space. These geometric procedures anticipated the Oxford Calculators, including the mean speed theorem, by 14 centuries. South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.In the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore.  He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.  Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem,  though the statement of the theorem has a long history. Eudoxus (408\u2013c. 355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures, as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose Elements, widely considered the most successful and influential textbook of all time, introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework. The Elements was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today. Archimedes (c. 287\u2013212 BC) of Syracuse used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of Pi. He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution.\n\nIndian mathematicians also made many important contributions in geometry. The Satapatha Brahmana (3rd century BC) contains rules for ritual geometric constructions that are similar to the Sulba Sutras. According to (Hayashi 2005, p. 363), the \u015aulba S\u016btras contain \"the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples, which are particular cases of Diophantine equations.\nIn the Bakhshali manuscript, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also \"employs a decimal place value system with a dot for zero.\" Aryabhata's Aryabhatiya (499) includes the computation of areas and volumes.\nBrahmagupta wrote his astronomical work Br\u0101hma Sphu\u1e6da Siddh\u0101nta in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: \"basic operations\" (including cube roots, fractions, ratio and proportion, and barter) and \"practical mathematics\" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain). In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).In the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry. Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra. Th\u0101bit ibn Qurra (known as Thebit in Latin) (836\u2013901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry. Omar Khayy\u00e1m (1048\u20131131) found geometric solutions to cubic equations.  The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were early results in hyperbolic geometry, and along with their alternative postulates, such as Playfair's axiom, these works had a considerable influence on the development of non-Euclidean geometry among later European geometers, including Witelo (c. 1230\u2013c. 1314), Gersonides (1288\u20131344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri.In the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by Ren\u00e9 Descartes (1596\u20131650) and Pierre de Fermat (1601\u20131665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591\u20131661). Projective geometry is a geometry without measurement or parallel lines, just the study of how points are related to each other.\nTwo developments in geometry in the 19th century changed the way it had been studied previously. These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, J\u00e1nos Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen Programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826\u20131866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincar\u00e9, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of \"space\" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.\n\n\n== Important concepts in geometry ==\nThe following are some of the most important concepts in geometry.\n\n\n=== Axioms ===\n\nEuclid took an abstract approach to geometry in his Elements, one of the most influential books ever written. Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes. He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known as axiomatic or synthetic geometry. At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792\u20131856), J\u00e1nos Bolyai (1802\u20131860), Carl Friedrich Gauss (1777\u20131855) and others led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862\u20131943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.\n\n\n=== Points ===\n\nPoints are considered fundamental objects in Euclidean geometry. They have been defined in a variety of ways, including Euclid's definition as 'that which has no part' and through the use of algebra or nested sets. In many areas of geometry, such as analytic geometry, differential geometry, and topology, all objects are considered to be built up from points. However, there has been some study of geometry without reference to points.\n\n\n=== Lines ===\n\nEuclid described a line as \"breadthless length\" which \"lies equally with respect to the points on itself\". In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it. In differential geometry, a  geodesic is a generalization of the notion of a line  to curved spaces.\n\n\n=== Planes ===\n\nA plane is a flat, two-dimensional surface that extends infinitely far. Planes are used in every area of geometry. For instance, planes can be studied as a topological surface without reference to distances or angles; it can be studied as an affine space, where collinearity and ratios can be studied but not distances; it can be studied as the complex plane using techniques of complex analysis; and so on.\n\n\n=== Angles ===\n\nEuclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other. In modern terms, an angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle.\n\nIn Euclidean geometry, angles are used to study polygons and triangles, as well as forming an object of study in their own right. The study of the angles of a triangle or of angles in a unit circle forms the basis of trigonometry.In differential geometry and calculus, the angles between plane curves or space curves or surfaces can be calculated using the derivative.\n\n\n=== Curves ===\n\nA curve is a 1-dimensional object that may be straight (like a line) or not; curves in 2-dimensional space are called plane curves and those in 3-dimensional space are called space curves.In topology, a curve is defined by a function from an interval of the real numbers to another space. In differential geometry, the same definition is used, but the defining function is required to be differentiable  Algebraic geometry studies algebraic curves, which are defined as algebraic varieties of dimension one.\n\n\n=== Surfaces ===\n\nA surface is a two-dimensional object, such as a sphere or paraboloid. In differential geometry and topology, surfaces are described by two-dimensional 'patches' (or neighborhoods) that are assembled by diffeomorphisms or homeomorphisms, respectively. In algebraic geometry, surfaces are described by polynomial equations.\n\n\n=== Manifolds ===\n\nA manifold is a generalization of the concepts of curve and surface. In topology, a manifold is a topological space where every point has a neighborhood that is homeomorphic to Euclidean space. In differential geometry, a differentiable manifold is a space where each neighborhood is diffeomorphic to Euclidean space.Manifolds are used extensively in physics, including in general relativity and string theory\n\n\n=== Topologies and metrics ===\n\nA topology is a mathematical structure on a set that tells how elements of the set relate spatially to each other. The best-known examples of topologies come from metrics, which are ways of measuring distances between points. For instance, the Euclidean metric measures the distance between points in the Euclidean plane, while the hyperbolic metric measures the distance in the hyperbolic plane. Other important examples of metrics include the Lorentz metric of special relativity and the semi-Riemannian metrics of general relativity.\n\n\n=== Compass and straightedge constructions ===\n\nClassical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments allowed in geometric constructions are the compass and straightedge. Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using parabolas and other curves, as well as mechanical devices, were found.\n\n\n=== Dimension ===\n\nWhere the traditional geometry allowed dimensions 1 (a line), 2 (a plane) and 3 (our ambient world conceived of as three-dimensional space), mathematicians have used higher dimensions for nearly two centuries. Dimension has gone through stages of being any natural number n, possibly infinite with the introduction of Hilbert space, and any positive real number in fractal geometry. Dimension theory is a technical area, initially within general topology, that discusses definitions; in common with most mathematical ideas, dimension is now defined rather than an intuition. Connected topological manifolds have a well-defined dimension; this is a theorem (invariance of domain) rather than anything a priori.\nThe issue of dimension still matters to geometry, in the absence of complete answers to classic questions. Dimensions 3 of space and 4 of space-time are special cases in geometric topology. Dimension 10 or 11 is a key number in string theory. Research may bring a satisfactory geometric reason for the significance of 10 and 11 dimensions. \n\n\n=== Symmetry ===\n\nThe theme of symmetry in geometry is nearly as old as the science of geometry itself. Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers and were investigated in detail before the time of Euclid. Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of M. C. Escher. Nonetheless, it was not until the second half of 19th century that the unifying role of symmetry in foundations of geometry was recognized. Felix Klein's Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry is. Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines. However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein's idea to 'define a geometry via its symmetry group' proved most influential. Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory, the latter in Lie theory and Riemannian geometry.\nA different type of symmetry is the principle of duality in projective geometry (see Duality (projective geometry)) among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange point with plane, join with meet, lies in with contains, and you will get an equally true theorem. A similar and closely related form of duality exists between a vector space and its dual space.\n\n\n=== Non-Euclidean geometry ===\n\nIn the nearly two thousand years since Euclid, while the range of geometrical questions asked and answered inevitably expanded, the basic understanding of space remained essentially the same. Immanuel Kant argued that there is only one, absolute, geometry, which is known to be true a priori by an inner faculty of mind: Euclidean geometry was synthetic a priori.  This dominant view was overturned by the revolutionary discovery of non-Euclidean geometry in the works of Bolyai, Lobachevsky, and Gauss (who never published his theory). They demonstrated that ordinary Euclidean space is only one possibility for development of geometry. A broad vision of the subject of geometry was then expressed  by Riemann in his 1867 inauguration lecture \u00dcber die Hypothesen, welche der Geometrie zu Grunde liegen (On the hypotheses on which geometry is based), published only after his death. Riemann's new idea of space proved crucial in Einstein's general relativity theory, and Riemannian geometry, that considers very general spaces in which the notion of length is defined, is a mainstay of modern geometry.\n\n\n== Contemporary geometry ==\n\n\n=== Euclidean geometry ===\n\nEuclidean geometry has become closely connected with computational geometry, computer graphics, convex geometry, incidence geometry, finite geometry, discrete geometry, and some areas of combinatorics. Attention was given to further work on Euclidean geometry and the Euclidean groups by crystallography and the work of H. S. M. Coxeter, and can be seen in theories of Coxeter groups and polytopes. Geometric group theory is an expanding area of the theory of more general discrete groups, drawing on geometric models and algebraic techniques.\n\n\n=== Differential geometry ===\nDifferential geometry has been of increasing importance to mathematical physics due to Einstein's general relativity postulation that the universe is curved. Contemporary differential geometry is intrinsic, meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point, and not a priori parts of some ambient flat Euclidean space.\n\n\n=== Topology and geometry ===\n\nThe field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms. This has often been expressed in the form of the dictum 'topology is rubber-sheet geometry'. Contemporary geometric topology and differential topology, and particular subfields such as Morse theory, would be counted by most mathematicians as part of geometry. Algebraic topology and general topology have gone their own ways.\n\n\n=== Algebraic geometry ===\n\nThe field of algebraic geometry is the modern incarnation of the Cartesian geometry of co-ordinates. From late 1950s through mid-1970s it had undergone major foundational development, largely due to work of Jean-Pierre Serre and Alexander Grothendieck. This led to the introduction of schemes and greater emphasis on topological methods, including various cohomology theories. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.\nThe study of low-dimensional algebraic varieties, algebraic curves, algebraic surfaces and algebraic varieties of dimension 3 (\"algebraic threefolds\"), has been far advanced. Gr\u00f6bner basis theory and real algebraic geometry are among more applied subfields of modern algebraic geometry. Arithmetic geometry is an active field combining algebraic geometry and number theory. Other directions of research involve moduli spaces and complex geometry. Algebro-geometric methods are commonly applied in string and brane  theory.\n\n\n== Applications ==\nGeometry has found applications in many fields, some of which are described below.\n\n\n=== Art ===\n\nMathematics and art are related in a variety of ways. For instance, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry.\n\n\n=== Architecture ===\n\nMathematics and architecture are related, since, as with other arts, architects use mathematics for several reasons. Apart from the mathematics needed when engineering buildings, architects use geometry: to define the spatial form of a building; from the Pythagoreans of the sixth century BC onwards, to create forms considered harmonious, and thus to lay out buildings and their surroundings according to mathematical, aesthetic and sometimes religious principles; to decorate buildings with mathematical objects such as tessellations; and to meet environmental goals, such as to minimise wind speeds around the bases of tall buildings.\n\n\n=== Physics ===\n\nThe field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, have served as an important source of geometric problems throughout history.\nModern geometry has many ties to physics as is exemplified by the links between pseudo-Riemannian geometry and general relativity. One of the youngest physical theories, string theory, is also very geometric in flavour.\n\n\n=== Other fields of mathematics ===\nGeometry has also had a large effect on other areas of mathematics. For instance, the introduction of coordinates by Ren\u00e9 Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century.  The subject of geometry was further enriched by the study of the intrinsic structure of geometric objects that originated with Euler and Gauss and led to the creation of topology and differential geometry.\n\nAn important area of application is number theory. In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths, which contradicted their philosophical views, made them abandon abstract numbers in favor of concrete geometric quantities, such as length and area of figures. Since the 19th century, geometry has been used for solving problems in number theory, for example through the geometry of numbers or, more recently, scheme theory, which is used in Wiles's proof of Fermat's Last Theorem.\nWhile the visual nature of geometry makes it initially more accessible than other mathematical areas such as algebra or number theory, geometric language is also used in contexts far removed from its traditional, Euclidean provenance (for example, in fractal geometry and algebraic geometry).Analytic geometry applies methods of algebra to geometric questions, typically by relating geometric curves to algebraic equations. These ideas played a key role in the development of calculus in the 17th century and led to the discovery of many new properties of plane curves. Modern algebraic geometry considers similar questions on a vastly more abstract level.\nLeonhard Euler, in studying problems like the Seven Bridges of K\u00f6nigsberg, considered the most fundamental properties of geometric figures based solely on shape, independent of their metric properties. Euler called this new branch of geometry geometria situs (geometry of place), but it is now known as topology. Topology grew out of geometry, but turned into a large independent discipline. It does not differentiate between objects that can be continuously deformed into each other. The objects may nevertheless retain some geometry, as in the case of hyperbolic knots.\n\n\n== See also ==\n\n\n=== Lists ===\nList of geometers\nCategory:Algebraic geometers\nCategory:Differential geometers\nCategory:Geometers\nCategory:Topologists\nList of formulas in elementary geometry\nList of geometry topics\nList of important publications in geometry\nList of mathematics articles\n\n\n=== Related topics ===\nDescriptive geometry\nFinite geometry\nFlatland, a book written by Edwin Abbott Abbott about two- and three-dimensional space, to understand the concept of four dimensions\nInteractive geometry software\n\n\n=== Other fields ===\nMolecular geometry\n\n\n== Notes ==\n\n\n== Sources ==\nBoyer, C. B. (1991) [1989]. A History of Mathematics (Second edition, revised by Uta C. Merzbach ed.). New York: Wiley. ISBN 0-471-54397-7. \nCooke, Roger (2005), The History of Mathematics:, New York: Wiley-Interscience, 632 pages, ISBN 0-471-44459-6 \nHayashi, Takao (2003), \"Indian Mathematics\",  in Grattan-Guinness, Ivor, Companion Encyclopedia of the History and Philosophy of the Mathematical Sciences, 1, Baltimore, MD: The Johns Hopkins University Press, 976 pages, pp. 118\u2013130, ISBN 0-8018-7396-7 \nHayashi, Takao (2005), \"Indian Mathematics\",  in Flood, Gavin, The Blackwell Companion to Hinduism, Oxford: Basil Blackwell, 616 pages, pp. 360\u2013375, ISBN 978-1-4051-3251-0 \nNikolai I. Lobachevsky, Pangeometry, translator and editor: A. Papadopoulos,  Heritage of European Mathematics Series, Vol. 4, European Mathematical Society, 2010.\n\n\n== Further reading ==\nJay Kappraff, A Participatory Approach to Modern Geometry, 2014, World Scientific Publishing, ISBN 978-981-4556-70-5.\nLeonard Mlodinow, Euclid's Window \u2013 The Story of Geometry from Parallel Lines to Hyperspace, UK edn. Allen Lane, 1992.\n\n\n== External links ==\n\nA geometry course from Wikiversity\nUnusual Geometry Problems\nThe Math Forum \u2014 Geometry\nThe Math Forum \u2014 K\u201312 Geometry\nThe Math Forum \u2014 College Geometry\nThe Math Forum \u2014 Advanced Geometry\nNature Precedings \u2014 Pegs and Ropes Geometry at Stonehenge\nThe Mathematical Atlas \u2014 Geometric Areas of Mathematics\n\"4000 Years of Geometry\", lecture by Robin Wilson given at Gresham College, 3 October 2007 (available for MP3 and MP4 download as well as a text file)\nFinitism in Geometry at the Stanford Encyclopedia of Philosophy\nThe Geometry Junkyard\nInteractive geometry reference with hundreds of applets\nDynamic Geometry Sketches (with some Student Explorations)\nGeometry classes at Khan Academy", "game theory": "Game theory is \"the study of mathematical models of conflict and cooperation between intelligent rational decision-makers\". Game theory is mainly used in economics, political science, and psychology, as well as in logic and computer science. Originally, it addressed zero-sum games, in which one person's gains result in losses for the other participants. Today, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.\nModern game theory began with the idea regarding the existence of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book Theory of Games and Economic Behavior, co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.\nThis theory was developed extensively in the 1950s by many scholars. Game theory was later explicitly applied to biology in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. As of  2014, with the Nobel Memorial Prize in Economic Sciences going to game theorist Jean Tirole, eleven game theorists have won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of game theory to biology.\nGame theory is not concerned with playing games for entertainment, fun or similar purposes - that is the field of Game studies.\n\n\n== History ==\n\nEarly discussions of examples of two-person games occurred long before the rise of modern, mathematical game theory. The first known discussion of game theory occurred in a letter written by Charles Waldegrave, an active Jacobite, and uncle to James Waldegrave, a British diplomat, in 1713. In this letter, Waldegrave provides a minimax mixed strategy solution to a two-person version of the card game le Her, and the problem is now known as Waldegrave problem. James Madison made what we now recognize as a game-theoretic analysis of the ways states can be expected to behave under different systems of taxation. In his 1838 Recherches sur les principes math\u00e9matiques de la th\u00e9orie des richesses (Researches into the Mathematical Principles of the Theory of Wealth), Antoine Augustin Cournot considered a duopoly and presents a solution that is a restricted version of the Nash equilibrium.\nIn 1913, Ernst Zermelo published \u00dcber eine Anwendung der Mengenlehre auf die Theorie des Schachspiels (On an Application of Set Theory to the Theory of the Game of Chess). It proved that the optimal chess strategy is strictly determined. This paved the way for more general theorems.In 1938, the Danish mathematical economist Frederik Zeuthen proved that the mathematical model had a winning strategy by using Brouwer's fixed point theorem. In his 1938 book Applications aux Jeux de Hasard and earlier notes, \u00c9mile Borel proved a minimax theorem for two-person zero-sum matrix games only when the pay-off matrix was symmetric. Borel conjectured that non-existence of mixed-strategy equilibria in two-person zero-sum games would occur, a conjecture that was proved false.\nGame theory did not really exist as a unique field until John von Neumann published the paper On the Theory of Games of Strategy  in 1928. Von Neumann's original proof used Brouwer's fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by his 1944 book Theory of Games and Economic Behavior co-authored with Oskar Morgenstern. The second edition of this book provided an axiomatic theory of utility, which reincarnated Daniel Bernoulli's old theory of utility (of the money) as an independent discipline. Von Neumann's work in game theory culminated in this 1944 book. This foundational work contains the method for finding mutually consistent solutions for two-person zero-sum games. During the following time period, work on game theory was primarily focused on cooperative game theory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.In 1950, the first mathematical discussion of the prisoner's dilemma appeared, and an experiment was undertaken by notable mathematicians Merrill M. Flood and Melvin Dresher, as part of the RAND Corporation's investigations into game theory. RAND pursued the studies because of possible applications to global nuclear strategy. Around this same time, John Nash developed a criterion for mutual consistency of players' strategies, known as Nash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every n-player, non-zero-sum (not just 2-player zero-sum) non-cooperative game has what is now known as a Nash equilibrium.\nGame theory experienced a flurry of activity in the 1950s, during which time the concepts of the core, the extensive form game, fictitious play, repeated games, and the Shapley value were developed. In addition, the first applications of game theory to philosophy and political science occurred during this time.\nIn 1979 Robert Axelrod tried setting up computer programs as players and found that in tournaments between them the winner was often a simple \"tit-for-tat\" program that cooperates on the first step, then on subsequent steps just does whatever its opponent did on the previous step. The same winner was also often obtained by natural selection; a fact widely taken to explain cooperation phenomena in evolutionary biology and the social sciences.\n\n\n=== Prize-winning achievements ===\nIn 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium (later he would introduce trembling hand perfection as well). In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory.\nIn the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection, and common knowledge were introduced and analyzed.\nIn 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing an equilibrium coarsening, correlated equilibrium, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.\nIn 2007, Leonid Hurwicz, together with Eric Maskin and Roger Myerson, was awarded the Nobel Prize in Economics \"for having laid the foundations of mechanism design theory\". Myerson's contributions include the notion of proper equilibrium, and an important graduate text: Game Theory, Analysis of Conflict. Hurwicz introduced and formalized the concept of incentive compatibility.\nIn 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics \"for the theory of stable allocations and the practice of market design\" and, in 2014, the Nobel went to game theorist Jean Tirole.\n\n\n== Game types ==\n\n\n=== Cooperative / Non-cooperative ===\n\nA game is cooperative if the players are able to form binding commitments externally enforced (e.g. through contract law). A game is non-cooperative if players cannot form alliances or if all agreements need to be self-enforcing (e.g. through credible threats).[1]Cooperative games are often analysed through the framework of cooperative game theory, which focuses on predicting which coalitions will form, the joint actions that groups take and the resulting collective payoffs. It is opposed to the traditional non-cooperative game theory which focuses on predicting individual players' actions and payoffs and analyzing Nash equilibria.[2][3]Cooperative game theory provides a high-level approach as it only describes the structure, strategies and payoffs of coalitions, whereas non-cooperative game theory also looks at how bargaining procedures will affect the distribution of payoffs within each coalition. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation. While it would thus be optimal to have all games expressed under a non-cooperative framework, in many instances insufficient information is available to accurately model the formal procedures available to the players during the strategic bargaining process, or the resulting model would be of too high complexity to offer a practical tool in the real world. In such cases, cooperative game theory provides a simplified approach that allows analysis of the game at large without having to make any assumption about bargaining powers.\n\n\n=== Symmetric / Asymmetric ===\n\nA symmetric game is a game where the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them. If the identities of the players can be changed without changing the payoff to the strategies, then a game is symmetric. Many of the commonly studied 2\u00d72 games are symmetric. The standard representations of chicken, the prisoner's dilemma, and the stag hunt are all symmetric games. Some scholars would consider certain asymmetric games as examples of these games as well. However, the most common payoffs for each of these games are symmetric.\nMost commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, the ultimatum game and similarly the dictator game have different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured to the right is asymmetric despite having identical strategy sets for both players.\n\n\n=== Zero-sum / Non-zero-sum ===\n\nZero-sum games are a special case of constant-sum games, in which choices by players can neither increase nor decrease the available resources. In zero-sum games the total benefit to all players in the game, for every combination of strategies, always adds to zero (more informally, a player benefits only at the equal expense of others). Poker exemplifies a zero-sum game (ignoring the possibility of the house's cut), because one wins exactly the amount one's opponents lose. Other zero-sum games include matching pennies and most classical board games including Go and chess.\nMany games studied by game theorists (including the famed prisoner's dilemma) are non-zero-sum games, because the outcome has net results greater or less than zero. Informally, in non-zero-sum games, a gain by one player does not necessarily correspond with a loss by another.\nConstant-sum games correspond to activities like theft and gambling, but not to the fundamental economic situation in which there are potential gains from trade. It is possible to transform any game into a (possibly asymmetric) zero-sum game by adding a dummy player (often called \"the board\") whose losses compensate the players' net winnings.\n\n\n=== Simultaneous / Sequential ===\n\nSimultaneous games are games where both players move simultaneously, or if they do not move simultaneously, the later players are unaware of the earlier players' actions (making them effectively simultaneous). Sequential games (or dynamic games) are games where later players have some knowledge about earlier actions. This need not be perfect information about every action of earlier players; it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while he does not know which of the other available actions the first player actually performed.\nThe difference between simultaneous and sequential games is captured in the different representations discussed above. Often, normal form is used to represent simultaneous games, while extensive form is used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games; see subgame perfection.\nIn short, the differences between sequential and simultaneous games are as follows:\n\n\n=== Perfect information and imperfect information ===\n\nAn important subset of sequential games consists of games of perfect information. A game is one of perfect information if all players know the moves previously made by all other players. Most games studied in game theory are imperfect-information games. Examples of perfect-information games include tic-tac-toe, checkers, infinite chess, and Go.Many card games are games of imperfect information, such as poker and bridge. Perfect information is often confused with complete information, which is a similar concept. Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken. Games of incomplete information can be reduced, however, to games of imperfect information by introducing \"moves by nature\".\n\n\n=== Combinatorial games ===\nGames in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess and go. Games that involve imperfect information may also have a strong combinatorial character, for instance backgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve particular problems and answer general questions.Games of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic (and sometimes non-constructive) proof methods to solve games of certain types, including \"loopy\" games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional (or \"economic\") game theory. A typical game that has been solved this way is hex. A related field of study, drawing from computational complexity theory, is game complexity, which is concerned with estimating the computational difficulty of finding optimal strategies.Research in artificial intelligence has addressed both perfect and imperfect information games that have very complex combinatorial structures (like chess, go, or backgammon) for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, like alpha-beta pruning or use of artificial neural networks trained by reinforcement learning, which make games more tractable in computing practice.\n\n\n=== Infinitely long games ===\n\nGames, as studied by economists and real-world game players, are generally finished in finitely many moves. Pure mathematicians are not so constrained, and set theorists in particular study games that last for infinitely many moves, with the winner (or other payoff) not known until after all those moves are completed.\nThe focus of attention is usually not so much on the best way to play such a game, but whether one player has a winning strategy. (It can be proven, using the axiom of choice, that there are games \u2013 even with perfect information and where the only outcomes are \"win\" or \"lose\" \u2013 for which neither player has a winning strategy.) The existence of such strategies, for cleverly designed games, has important consequences in descriptive set theory.\n\n\n=== Discrete and continuous games ===\nMuch of game theory is concerned with finite, discrete games, that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however. Continuous games allow players to choose a strategy from a continuous strategy set. For instance, Cournot competition is typically modeled with players' strategies being any non-negative quantities, including fractional quantities.\n\n\n=== Differential games ===\nDifferential games such as the continuous pursuit and evasion game are continuous games where the evolution of the players' state variables is governed by differential equations. The problem of finding an optimal strategy in a differential game is closely related to the optimal control theory. In particular, there are two types of strategies: the open-loop strategies are found using the Pontryagin maximum principle while the closed-loop strategies are found using Bellman's Dynamic Programming method.\nA particular case of differential games are the games with a random time horizon. In such games, the terminal time is a random variable with a given probability distribution function. Therefore, the players maximize the mathematical expectation of the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval.\n\n\n=== Evolutionary game theory ===\nEvolutionary game theory studies players who adjust their strategies over time according to rules that are not necessarily rational or farsighted.  In general, the evolution of strategies over time according to such rules is modeled as a Markov chain with a state variable such as the current strategy profile or how the game has been played in the recent past. Such rules may feature imitation, optimization or survival of the fittest.\nIn biology, such models can represent (biological) evolution, in which offspring adopt their parents' strategies and parents who play more successful strategies (i.e. corresponding to higher payoffs) have a greater number of offspring. In the social sciences, such models typically represent strategic adjustment by players who play a game many times within their lifetime and, consciously or unconsciously, occasionally adjust their strategies.\n\n\n=== Stochastic outcomes (and relation to other fields) ===\nIndividual decision problems with stochastic outcomes are sometimes considered \"one-player games\". These situations are not considered game theoretical by some authors. They may be modeled using similar tools within the related disciplines of decision theory, operations research, and areas of artificial intelligence, particularly AI planning (with uncertainty) and multi-agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. using Markov decision processes (MDP).Stochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes \"chance moves\" (\"moves by nature\"). This player is not typically considered a third player in what is otherwise a two-player game, but merely serves to provide a roll of the dice where required by the game.\nFor some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and the minimax solution is that the latter considers the worst-case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely (but costly) events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen. (See Black swan theory for more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)\nGeneral models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The \"gold standard\" is considered to be partially observable stochastic game (POSG), but few realistic problems are computationally feasible in POSG representation.\n\n\n=== Metagames ===\nThese are games the play of which is the development of the rules for another game, the target or subject game. Metagames seek to maximize the utility value of the rule set developed. The theory of metagames is related to mechanism design theory.\nThe term metagame analysis is also used to refer to a practical approach developed by Nigel Howard. whereby a situation is framed as a strategic game in which stakeholders try to realise their objectives by means of the options available to them. Subsequent developments have led to the formulation of confrontation analysis.\n\n\n=== Pooling games ===\nThese are games prevailing over all forms of society. Pooling games are repeated plays with changing payoff table in general over an experienced path and their equilibrium strategies usually take a form of evolutionary social convention and economic convention. Pooling game theory emerges to formally recognize the interaction between optimal choice in one play and the emergence of forthcoming payoff table update path, identify the invariance existence and robustness, and predict variance over time. The theory is based upon topological transformation classification of payoff table update over time to predict variance and invariance, and is also within the jurisdiction of the computational law of reachable optimality for ordered system.\n\n\n=== Mean field game theory ===\nMean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature by Boyan Jovanovic and Robert W. Rosenthal, in the engineering literature by Peter E. Caines and by mathematician Pierre-Louis Lions and Jean-Michel Lasry.\n\n\n== Representation of games ==\n\nThe games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the players of the game, the information and actions available to each player at each decision point, and the payoffs for each outcome. (Eric Rasmusen refers to these four \"essential elements\" by the acronym \"PAPI\".) A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game\u2014a stable state in which either one outcome occurs or a set of outcomes occur with known probability.\nMost cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games.\n\n\n=== Extensive form ===\n\nThe extensive form can be used to formalize games with a time sequencing of moves. Games here are played on trees (as pictured here). Here each vertex (or node) represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi-player generalization of a decision tree. To solve any extensive form game, backward induction must be used. It involves working backwards up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached.The game pictured consists of two players.  The way this particular game is structured (i.e., with sequential decision making and perfect information), Player 1 \"moves\" first by choosing either F or U (Fair or Unfair). Next in the sequence, Player 2, who has now seen Player 1's move, chooses to play either A or R. Once Player 2 has made his/ her choice, the game is considered finished and each player gets their respective payoff.  Suppose that Player 1 chooses U and then Player 2 chooses A: Player 1 then gets a payoff of \"eight\" (which in real-world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players) and Player 2 gets a payoff of \"two\".\nThe extensive form can also capture simultaneous-move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set (i.e. the players do not know at which point they are), or a closed line is drawn around them. (See example in the imperfect information section.)\n\n\n=== Normal form ===\n\nThe normal (or strategic form) game is usually represented by a matrix which shows the players, strategies, and payoffs (see the example to the right). More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players; one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player (Player 1 in our example); the second is the payoff for the column player (Player 2 in our example). Suppose that Player 1 plays Up and that Player 2 plays Left. Then Player 1 gets a payoff of 4, and Player 2 gets 3.\nWhen a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form.\nEvery extensive-form game has an equivalent normal-form game, however the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical.\n\n\n=== Characteristic function form ===\n\nIn games that possess removable utility, separate rewards are not given; rather, the characteristic function decides the payoff of each unity. The idea is that the unity that is 'empty', so to speak, does not receive a reward at all.\nThe origin of this form is to be found in John von Neumann and Oskar Morgenstern's book; when looking at these instances, they guessed that when a union \n  \n    \n      \n        \n          C\n        \n      \n    \n    {\\displaystyle \\mathbf {C} }\n   appears, it works against the fraction\n\n  \n    \n      \n        \n          (\n          \n            \n              \n                N\n              \n              \n                C\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left({\\frac {\\mathbf {N} }{\\mathbf {C} }}\\right)}\n  \nas if two individuals were playing a normal game. The balanced payoff of C is a basic function. Although there are differing examples that help determine coalitional amounts from normal games, not all appear that in their function form can be derived from such.\nFormally, a characteristic function is seen as: (N,v), where N represents the group of people and \n  \n    \n      \n        v\n        :\n        \n          2\n          \n            N\n          \n        \n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle v:2^{N}\\to \\mathbf {R} }\n   is a normal utility.\nSuch characteristic functions have expanded to describe games where there is no removable utility.\n\n\n== General and applied uses ==\nAs a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was by Antoine Augustin Cournot in 1838 with his solution of the Cournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.\nAlthough pre-twentieth century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name \"game theory\", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his book Evolution and the Theory of Games.In addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior. In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic arguments of this type can be found as far back as Plato.\n\n\n=== Description and modeling ===\n\nThe primary use of game theory is to describe and model how human populations behave. Some scholars believe that by finding the equilibria of games they can predict how actual human populations will behave when confronted with situations analogous to the game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real world situations. Game theorists usually assume players act rationally, but in practice, human behavior often deviates from this model. Game theorists respond by comparing their assumptions to those used in physics. Thus while their assumptions do not always hold, they can treat game theory as a reasonable scientific ideal akin to the models used by physicists. However, empirical work has shown that in some classic games, such as the centipede game, guess 2/3 of the average game, and the dictator game, people regularly do not play Nash equilibria. There is an ongoing debate regarding the importance of these experiments and whether the analysis of the experiments fully captures all aspects of the relevant situation.Some game theorists, following the work of John Maynard Smith and George R. Price, have turned to evolutionary game theory in order to resolve these issues. These models presume either no rationality or bounded rationality on the part of players. Despite the name, evolutionary game theory does not necessarily presume natural selection in the biological sense. Evolutionary game theory includes both biological as well as cultural evolution and also models of individual learning (for example, fictitious play dynamics).\n\n\n=== Prescriptive or normative analysis ===\nSome scholars, like Leonard Savage, see game theory not as a predictive tool for the behavior of human beings, but as a suggestion for how people ought to behave. Since a strategy, corresponding to a Nash equilibrium of a game constitutes one's best response to the actions of the other players \u2013 provided they are in (the same) Nash equilibrium \u2013 playing a strategy that is part of a Nash equilibrium seems appropriate. This normative use of game theory has also come under criticism.\n\n\n=== Economics and business ===\nGame theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers & acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems; and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.This research usually focuses on particular sets of strategies known as \"solution concepts\" or \"equilibria\". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.The payoffs of the game are generally taken to represent the utility of individual players.\nA prototypical paper on game theory in economics begins by presenting a game that is an abstraction of a particular economic situation. One or more solution concepts are chosen, and the author demonstrates which strategy sets in the presented game are equilibria of the appropriate type. Naturally one might wonder to what use this information should be put. Economists and business professors suggest two primary uses (noted above): descriptive and prescriptive.\n\n\n=== Political science ===\nThe application of game theory to political science is focused in the overlapping areas of fair division, political economy, public choice, war bargaining, positive political theory, and social choice theory. In each of these areas, researchers have developed game-theoretic models in which the players are often voters, states, special interest groups, and politicians.\nEarly examples of game theory applied to political science are provided by Anthony Downs. In his book An Economic Theory of Democracy, he applies the Hotelling firm location model to the political process. In the Downsian model, political candidates commit to ideologies on a one-dimensional policy space. Downs first shows how the political candidates will converge to the ideology preferred by the median voter if voters are fully informed, but then argues that voters choose to remain rationally ignorant which allows for candidate divergence. Game Theory was applied in 1962 to the Cuban missile crisis during the presidency of John F. Kennedy.It has also been proposed that game theory explains the stability of any form of political government.  Taking the simplest case of a monarchy, for example, the king, being only one person, does not and cannot maintain his authority by personally exercising physical control over all or even any significant number of his subjects.  Sovereign control is instead explained by the recognition by each citizen that all other citizens expect each other to view the king (or other established government) as the person whose orders will be followed.  Coordinating communication among citizens to replace the sovereign is effectively barred, since conspiracy to replace the sovereign is generally punishable as a crime.  Thus, in a process that can be modeled by variants of the prisoner's dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.A game-theoretic explanation for democratic peace is that public and open debate in democracies send clear and reliable information regarding their intentions to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a non-democracy.On the other hand, game theory predicts that two countries may still go to war even if their leaders are cognizant of the costs of fighting. War may result from asymmetric information; two countries may have incentives to mis-represent the amount of military resources they have on hand, rendering them unable to settle disputes agreeably without resorting to fighting. Moreover, war may arise because of commitment problems: if two countries wish to settle a dispute via peaceful means, but each wishes to go back on the terms of that settlement, they may have no choice but to resort to warfare. Finally, war may result from issue indivisibilities.Game theory could also help predict a nation's responses when there is a new rule or law to be applied to that nation. One example would be Peter John Wood's (2013) research when he looked into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reduce green house gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma to the nations.\n\n\n=== Biology ===\n\nUnlike those in economics, the payoffs for games in biology are often interpreted as corresponding to fitness. In addition, the focus has been less on equilibria that correspond to a notion of rationality and more on ones that would be maintained by evolutionary forces. The best known equilibrium in biology is known as the evolutionarily stable strategy (ESS), first introduced in (Smith & Price 1973). Although its initial motivation did not involve any of the mental requirements of the Nash equilibrium, every ESS is a Nash equilibrium.\nIn biology, game theory has been used as a model to understand many different phenomena. It was first used to explain the evolution (and stability) of the approximate 1:1 sex ratios. (Fisher 1930) suggested that the 1:1 sex ratios are a result of evolutionary forces acting on individuals who could be seen as trying to maximize their number of grandchildren.\nAdditionally, biologists have used evolutionary game theory and the ESS to explain the emergence of animal communication. The analysis of signaling games and other communication games has provided insight into the evolution of communication among animals. For example, the mobbing behavior of many species, in which a large number of prey animals attack a larger predator, seems to be an example of spontaneous emergent organization. Ants have also been shown to exhibit feed-forward behavior akin to fashion (see Paul Ormerod's Butterfly Economics).\nBiologists have used the game of chicken to analyze fighting behavior and territoriality.According to Maynard Smith, in the preface to Evolution and the Theory of Games, \"paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed\". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.One such phenomenon is known as biological altruism. This is a situation in which an organism appears to act in a way that benefits other organisms and is detrimental to itself. This is distinct from traditional notions of altruism because such actions are not conscious, but appear to be evolutionary adaptations to increase overall fitness. Examples can be found in species ranging from vampire bats that regurgitate blood they have obtained from a night's hunting and give it to group members who have failed to feed, to worker bees that care for the queen bee for their entire lives and never mate, to vervet monkeys that warn group members of a predator's approach, even when it endangers that individual's chance of survival. All of these actions increase the overall fitness of a group, but occur at a cost to the individual.\nEvolutionary game theory explains this altruism with the idea of kin selection. Altruists discriminate between the individuals they help and favor relatives. Hamilton's rule explains the evolutionary rationale behind this selection with the equation c < b \u00d7 r, where the cost c to the altruist must be less than the benefit b to the recipient multiplied by the coefficient of relatedness r. The more closely related two organisms are causes the incidences of altruism to increase because they share many of the same alleles. This means that the altruistic individual, by ensuring that the alleles of its close relative are passed on through survival of its offspring, can forgo the option of having offspring itself because the same number of alleles are passed on. For example, helping a sibling (in diploid animals) has a coefficient of \u00bd, because (on average) an individual shares \u00bd of the alleles in its sibling's offspring. Ensuring that enough of a sibling\u2019s offspring survive to adulthood precludes the necessity of the altruistic individual producing offspring. The coefficient values depend heavily on the scope of the playing field; for example if the choice of whom to favor includes all genetic living things, not just all relatives, we assume the discrepancy between all humans only accounts for approximately 1% of the diversity in the playing field, a co-efficient that was \u00bd in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time the playing field becomes larger still, and the discrepancies smaller.\n\n\n=== Computer science and logic ===\nGame theory has come to play an increasingly important role in logic and in computer science. Several logical theories have a basis in game semantics. In addition, computer scientists have used games to model interactive computations. Also, game theory provides a theoretical basis to the field of multi-agent systems.\nSeparately, game theory has played a role in online algorithms; in particular, the k-server problem, which has in the past been referred to as games with moving costs and request-answer games. Yao's principle is a game-theoretic technique for proving lower bounds on the computational complexity of randomized algorithms, especially online algorithms.\nThe emergence of the internet has motivated the development of algorithms for finding equilibria in games, markets, computational auctions, peer-to-peer systems, and security and information markets. Algorithmic game theory and within it algorithmic mechanism design combine computational algorithm design and analysis of complex systems with economic theory.\n\n\n=== Philosophy ===\nGame theory has been put to several uses in philosophy. Responding to two papers by W.V.O. Quine (1960, 1967), Lewis (1969) used game theory to develop a philosophical account of convention. In so doing, he provided the first analysis of common knowledge and employed it in analyzing play in coordination games. In addition, he first suggested that one can understand meaning in terms of signaling games. This later suggestion has been pursued by several philosophers since Lewis. Following Lewis (1969) game-theoretic account of conventions, Edna Ullmann-Margalit (1977) and Bicchieri (2006) have developed theories of social norms that define them as Nash equilibria that result from transforming a mixed-motive game into a coordination game.Game theory has also challenged philosophers to think in terms of interactive epistemology: what it means for a collective to have common beliefs or knowledge, and what are the consequences of this knowledge for the social outcomes resulting from the interactions of agents. Philosophers who have worked in this area include Bicchieri (1989, 1993), Skyrms (1990), and Stalnaker (1999).In ethics, some (most notably David Gauthier, Gregory Kavka, and Jean Hampton) authors have attempted to pursue Thomas Hobbes' project of deriving morality from self-interest. Since games like the prisoner's dilemma present an apparent conflict between morality and self-interest, explaining why cooperation is required by self-interest is an important component of this project. This general strategy is a component of the general social contract view in political philosophy (for examples, see Gauthier (1986) and Kavka (1986)).Other authors have attempted to use evolutionary game theory in order to explain the emergence of human attitudes about morality and corresponding animal behaviors. These authors look at several games including the prisoner's dilemma, stag hunt, and the Nash bargaining game as providing an explanation for the emergence of attitudes about morality (see, e.g., Skyrms (1996, 2004) and Sober and Wilson (1999)).\n\n\n== In popular culture ==\nBased on the 1998 book by Sylvia Nasar, the life story of game theorist and mathematician John Nash was turned into the 2001 biopic A Beautiful Mind, starring Russell Crowe as Nash.\nThe 1959 military science fiction novel Starship Troopers by Robert A. Heinlein mentioned \"games theory\" and \"theory of games\". In the 1997 film of the same name, the character Carl Jenkins referred to his military intelligence assignment as being assigned to \"games and theory\".\nThe 1964 film Dr. Strangelove satirizes game theoretic ideas about deterrence theory. For example, nuclear deterrence depends on the threat to retaliate catastrophically if a nuclear attack is detected. A game theorist might argue that such threats can fail to be credible, in the sense that they can lead to subgame imperfect equilibria. The movie takes this idea one step further, with the Russians irrevocably committing to a catastrophic nuclear response without making the threat public.\nThe 1980s power pop band Game Theory was founded by singer/songwriter Scott Miller, who described the band's name as alluding to \"the study of calculating the most appropriate action given an adversary... to give yourself the minimum amount of failure.\"\nLiar Game, a 2005 Japanese manga and 2007 television series, presents the main characters in each episode with a game or problem that is typically drawn from game theory, as demonstrated by the strategies applied by the characters.\n\n\n== See also ==\n\nLists\n\nList of cognitive biases\nList of emerging technologies\nList of games in game theory\nOutline of artificial intelligence\n\n\n== Notes ==\n\n\n== References and further reading ==\n\n\n=== Textbooks and general references ===\nAumann, Robert J (1987), \"game theory\", The New Palgrave: A Dictionary of Economics, 2, pp. 460\u201382 .\nCamerer, Colin (2003), \"Introduction\", Behavioral Game Theory: Experiments in Strategic Interaction, Russell Sage Foundation, pp. 1\u201325, ISBN 978-0-691-09039-9 , Description.\nDutta, Prajit K. (1999), Strategies and games: theory and practice, MIT Press, ISBN 978-0-262-04169-0 . Suitable for undergraduate and business students.\nFernandez, L F.; Bierman, H S. (1998), Game theory with economic applications, Addison-Wesley, ISBN 978-0-201-84758-1 . Suitable for upper-level undergraduates.\nGibbons, Robert D. (1992), Game theory for applied economists, Princeton University Press, ISBN 978-0-691-00395-5 . Suitable for advanced undergraduates.\nPublished in Europe as Gibbons, Robert (2001), A Primer in Game Theory, London: Harvester Wheatsheaf, ISBN 978-0-7450-1159-2 .\nGintis, Herbert (2000), Game theory evolving: a problem-centered introduction to modeling strategic behavior, Princeton University Press, ISBN 978-0-691-00943-8 \nGreen, Jerry R.; Mas-Colell, Andreu; Whinston, Michael D. (1995), Microeconomic theory, Oxford University Press, ISBN 978-0-19-507340-9 . Presents game theory in formal way suitable for graduate level.\nJoseph E. Harrington (2008) Games, strategies, and decision making, Worth, ISBN 0-7167-6630-2. Textbook suitable for undergraduates in applied fields; numerous examples, fewer formalisms in concept presentation.\nHoward, Nigel (1971), Paradoxes of Rationality: Games, Metagames, and Political Behavior, Cambridge, MA: The MIT Press, ISBN 978-0-262-58237-7 \nIsaacs, Rufus (1999), Differential Games: A Mathematical Theory With Applications to Warfare and Pursuit, Control and Optimization, New York: Dover Publications, ISBN 978-0-486-40682-4 \nMiller, James H. (2003), Game theory at work: how to use game theory to outthink and outmaneuver your competition, New York: McGraw-Hill, ISBN 978-0-07-140020-6 . Suitable for a general audience.\nOsborne, Martin J. (2004), An introduction to game theory, Oxford University Press, ISBN 978-0-19-512895-6 . Undergraduate textbook.\nOsborne, Martin J.; Rubinstein, Ariel (1994), A course in game theory, MIT Press, ISBN 978-0-262-65040-3 . A modern introduction at the graduate level.\nShoham, Yoav; Leyton-Brown, Kevin (2009), Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations, New York: Cambridge University Press, ISBN 978-0-521-89943-7, retrieved 8 March 2016 \nRoger McCain's Game Theory: A Nontechnical Introduction to the Analysis of Strategy (Revised Edition)\nWebb, James N. (2007), Game theory: decisions, interaction and evolution, Undergraduate mathematics, Springer, ISBN 1-84628-423-6  Consistent treatment of game types usually claimed by different applied fields, e.g. Markov decision processes.\n\n\n=== Historically important texts ===\nAumann, R.J. and Shapley, L.S. (1974), Values of Non-Atomic Games, Princeton University Press\nCournot, A. Augustin (1838), \"Recherches sur les principles mathematiques de la th\u00e9orie des richesses\", Libraire des sciences politiques et sociales, Paris: M. Rivi\u00e8re & C.ie \nEdgeworth, Francis Y. (1881), Mathematical Psychics, London: Kegan Paul \nFarquharson, Robin (1969), Theory of Voting, Blackwell (Yale U.P. in the U.S.), ISBN 0-631-12460-8 \nLuce, R. Duncan; Raiffa, Howard (1957), Games and decisions: introduction and critical survey, New York: Wiley reprinted edition: R. Duncan Luce ; Howard Raiffa (1989), Games and decisions: introduction and critical survey, New York: Dover Publications, ISBN 978-0-486-65943-5 Maynard Smith, John (1982), Evolution and the theory of games, Cambridge University Press, ISBN 978-0-521-28884-2 \nMaynard Smith, John; Price, George R. (1973), \"The logic of animal conflict\", Nature, 246 (5427): 15\u201318, Bibcode:1973Natur.246...15S, doi:10.1038/246015a0 \nNash, John (1950), \"Equilibrium points in n-person games\", Proceedings of the National Academy of Sciences of the United States of America, 36 (1): 48\u201349, Bibcode:1950PNAS...36...48N, doi:10.1073/pnas.36.1.48, PMC 1063129\u202f, PMID 16588946 \nShapley, L.S. (1953), A Value for n-person Games, In: Contributions to the Theory of Games volume II, H. W. Kuhn and A. W. Tucker (eds.)\nShapley, L.S. (1953), Stochastic Games, Proceedings of National Academy of Science Vol. 39, pp. 1095\u20131100.\nvon Neumann, John (1928), \"Zur Theorie der Gesellschaftsspiele\", Mathematische Annalen, 100 (1): 295\u2013320, doi:10.1007/bf01448847  English translation: \"On the Theory of Games of Strategy,\" in A. W. Tucker and R. D. Luce, ed. (1959), Contributions to the Theory of Games, v. 4, p. 42. Princeton University Press.\nvon Neumann, John; Morgenstern, Oskar (1944), Theory of games and economic behavior, Princeton University Press \nZermelo, Ernst (1913), \"\u00dcber eine Anwendung der Mengenlehre auf die Theorie des Schachspiels\", Proceedings of the Fifth International Congress of Mathematicians, 2: 501\u20134 \n\n\n=== Other print references ===\nBen David, S.; Borodin, Allan; Karp, Richard; Tardos, G.; Wigderson, A. (1994), \"On the Power of Randomization in On-line Algorithms\" (PDF), Algorithmica, 11 (1): 2\u201314, doi:10.1007/BF01294260 \nDowns, Anthony (1957), An Economic theory of Democracy, New York: Harper \nGauthier, David (1986), Morals by agreement, Oxford University Press, ISBN 978-0-19-824992-4 \nAllan Gibbard, \"Manipulation of voting schemes: a general result\", Econometrica, Vol. 41, No. 4 (1973), pp. 587\u2013601.\nGrim, Patrick; Kokalis, Trina; Alai-Tafti, Ali; Kilb, Nicholas; St Denis, Paul (2004), \"Making meaning happen\", Journal of Experimental & Theoretical Artificial Intelligence, 16 (4): 209\u2013243, doi:10.1080/09528130412331294715 \nHarper, David; Maynard Smith, John (2003), Animal signals, Oxford University Press, ISBN 978-0-19-852685-8 \nLewis, David (1969), Convention: A Philosophical Study , ISBN 978-0-631-23257-5 (2002 edition)\nMcDonald, John (1950\u20131996), Strategy in Poker, Business & War, W. W. Norton, ISBN 0-393-31457-X . A layman's introduction.\nPapayoanou, Paul (2010), Game Theory for Business: A Primer in Strategic Gaming, Probabilistic, ISBN 978-0964793873 .\nQuine, W.v.O (1967), \"Truth by Convention\", Philosophica Essays for A.N. Whitehead, Russel and Russel Publishers, ISBN 978-0-8462-0970-6 \nQuine, W.v.O (1960), \"Carnap and Logical Truth\", Synthese, 12 (4): 350\u2013374, doi:10.1007/BF00485423 \nMark A. Satterthwaite, \"Strategy-proofness and Arrow's Conditions: Existence and Correspondence Theorems for Voting Procedures and Social Welfare Functions\", Journal of Economic Theory 10 (April 1975), 187\u2013217.\nSiegfried, Tom (2006), A Beautiful Math, Joseph Henry Press, ISBN 0-309-10192-1 \nSkyrms, Brian (1990), The Dynamics of Rational Deliberation, Harvard University Press, ISBN 0-674-21885-X \nSkyrms, Brian (1996), Evolution of the social contract, Cambridge University Press, ISBN 978-0-521-55583-8 \nSkyrms, Brian (2004), The stag hunt and the evolution of social structure, Cambridge University Press, ISBN 978-0-521-53392-8 \nSober, Elliott; Wilson, David Sloan (1998), Unto others: the evolution and psychology of unselfish behavior, Harvard University Press, ISBN 978-0-674-93047-6 \nThrall, Robert M.; Lucas, William F. (1963), \"\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -person games in partition function form\", Naval Research Logistics Quarterly, 10 (4): 281\u2013298, doi:10.1002/nav.3800100126 \nDolev, Shlomi; Panagopoulou, Panagiota; Rabie, Mikael; Schiller, Elad Michael; Spirakis, Paul (2011), \"Rationality authority for provable rational behavior\", Acm Podc: 289\u2013290, doi:10.1145/1993806.1993858, ISBN 9781450307192 \nChastain, E. (2014), \"Algorithms, games, and evolution\", Proceedings of the National Academy of Sciences, 111: 10620\u201310623, Bibcode:2014PNAS..11110620C, doi:10.1073/pnas.1406556111, PMC 4115542\u202f \n\n\n== External links ==\nJames Miller (2015): Introductory Game Theory Videos.\nHazewinkel, Michiel, ed. (2001) [1994], \"Games, theory of\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nPaul Walker: History of Game Theory Page.\nDavid Levine: Game Theory. Papers, Lecture Notes and much more stuff.\nAlvin Roth:\"Game Theory and Experimental Economics page\". Archived from the original on 15 August 2000. Retrieved 13 September 2003.   \u2014 Comprehensive list of links to game theory information on the Web\nAdam Kalai: Game Theory and Computer Science \u2014 Lecture notes on Game Theory and Computer Science\nMike Shor: Game Theory .net \u2014 Lecture notes, interactive illustrations and other information.\nJim Ratliff's Graduate Course in Game Theory (lecture notes).\nDon Ross: Review Of Game Theory in the Stanford Encyclopedia of Philosophy.\nBruno Verbeek and Christopher Morris: Game Theory and Ethics\nElmer G. Wiens: Game Theory \u2014 Introduction, worked examples, play online two-person zero-sum games.\nMarek M. Kaminski: Game Theory and Politics \u2014 Syllabuses and lecture notes for game theory and political science.\nWeb sites on game theory and social interactions\nKesten Green's Conflict Forecasting at the Wayback Machine (archived 11 April 2011) \u2014 See Papers for evidence on the accuracy of forecasts from game theory and other methods.\nMcKelvey, Richard D., McLennan, Andrew M., and Turocy, Theodore L. (2007) Gambit: Software Tools for Game Theory.\nBenjamin Polak: Open Course on Game Theory at Yale videos of the course\nBenjamin Moritz, Bernhard K\u00f6nsgen, Danny Bures, Ronni Wiersch, (2007) Spieltheorie-Software.de: An application for Game Theory implemented in JAVA.\nAntonin Kucera: Stochastic Two-Player Games.\nYu-Chi Ho: What is Mathematical Game Theory; What is Mathematical Game Theory (#2); What is Mathematical Game Theory (#3); What is Mathematical Game Theory (#4)-Many person game theory; What is Mathematical Game Theory ?( #5) \u2013 Finale, summing up, and my own view", "empirical method": "Empirical research is research using empirical evidence. It is a way of gaining knowledge by means of direct and indirect observation or experience. Empiricism values such research more than other kinds. Empirical evidence (the record of one's direct observations or experiences) can be analyzed quantitatively or qualitatively. Quantifying the evidence or making sense of it in qualitative form, a researcher can answer empirical questions, which should be clearly defined and answerable with the evidence collected (usually called data). Research design varies by field and by the question being investigated. Many researchers combine qualitative and quantitative forms of analysis to better answer questions which cannot be studied in laboratory settings, particularly in the social sciences and in education.\nIn some fields, quantitative research may begin with a research question (e.g., \"Does listening to vocal music during the learning of a word list have an effect on later memory for these words?\") which is tested through experimentation. Usually, a researcher has a certain theory regarding the topic under investigation. Based on this theory, statements or hypotheses will be proposed (e.g., \"Listening to vocal voice has a negative effect on learning a word list.\"). From these hypotheses, predictions about specific events are derived (e.g., \"People who study a word list while listening to vocal music will remember fewer words on a later memory test than people who study a word list in silence.\"). These predictions can then be tested with a suitable experiment. Depending on the outcomes of the experiment, the theory on which the hypotheses and predictions were based will be supported or not, or may need to be modified and then subjected to further testing.\n\n\n== Terminology ==\nThe term empirical was originally used to refer to certain ancient Greek practitioners of medicine who rejected adherence to the dogmatic doctrines of the day, preferring instead to rely on the observation of phenomena as perceived in experience. Later empiricism referred to a theory of knowledge in philosophy which adheres to the principle that knowledge arises from experience and evidence gathered specifically using the senses. In scientific use, the term empirical refers to the gathering of data using only evidence that is observable by the senses or in some cases using calibrated scientific instruments. What early philosophers described as empiricist and empirical research have in common is the dependence on observable data to formulate and test theories and come to conclusions.\n\n\n== Usage ==\nThe researcher attempts to describe accurately the interaction between the instrument (or the human senses) and the entity being observed. If instrumentation is involved, the researcher is expected to calibrate his/her instrument by applying it to known standard objects and documenting the results before applying it to unknown objects. In other words, it describes the research that has not taken place before and their results.\nIn practice, the accumulation of evidence for or against any particular theory involves planned research designs for the collection of empirical data, and academic rigor plays a large part of judging the merits of research design. Several typologies for such designs have been suggested, one of the most popular of which comes from Campbell and Stanley. They are responsible for popularizing the widely cited distinction among pre-experimental, experimental, and quasi-experimental designs and are staunch advocates of the central role of randomized experiments in educational research.\n\n\n=== Scientific research ===\nAccurate analysis of data using standardized statistical methods in scientific studies is critical to determining the validity of empirical research. Statistical formulas such as regression, uncertainty coefficient, t-test, chi square, and various types of ANOVA (analyses of variance) are fundamental to forming logical, valid conclusions. If empirical data reach significance under the appropriate statistical formula, the research hypothesis is supported. If not, the null hypothesis is supported (or, more accurately, not rejected), meaning no effect of the independent variable(s) was observed on the dependent variable(s).\nThe outcome of empirical research using statistical hypothesis testing is never proof. It can only support a hypothesis, reject it, or do neither. These methods yield only probabilities.\nAmong scientific researchers, empirical evidence (as distinct from empirical research) refers to objective evidence that appears the same regardless of the observer. For example, a thermometer will not display different temperatures for each individual who observes it. Temperature, as measured by an accurate, well calibrated thermometer, is empirical evidence. By contrast, non-empirical evidence is subjective, depending on the observer. Following the previous example, observer A might truthfully report that a room is warm, while observer B might truthfully report that the same room is cool, though both observe the same reading on the thermometer. The use of empirical evidence negates this effect of personal (i.e., subjective) experience or time.\nThe varying perception of empiricism and rationalism shows concern with the limit to which there is dependency on experience of sense as an effort of gaining knowledge. According to rationalism, there are a number of different ways in which sense experience is gained independently for the knowledge and concepts. According to empiricism, sense experience is considered as the main source of every piece of knowledge and the concepts. In reference with a specific piece of knowledge, this paper will focus on differentiating between rationalism and empiricism or rational views and empirical views. \nIn general, rationalists are known for the development of their own views following two different way. First, the key argument can be placed that there are cases in which the content of knowledge or concepts end up outstripping the information. This outstripped information is provided by the sense experience (Hj\u00f8rland, 2010, 2). Second, there is construction of accounts as to how reasoning helps in the provision of addition knowledge about a specific or broader scope. Empiricists are known to be presenting complementary senses related to thought. First there is development of accounts of how there is provision of information by experience that is cited by rationalists. This is insofar for having it in the initial place. At times, empiricists tend to be opting skepticism as an option of rationalism. If experience is not helpful in the provision of knowledge or concept cited by rationalists, then they do not exist (Pearce, 2010, 35). Second, empiricists hold the tendency of attacking the accounts of rationalists while considering reasoning to be an important source of knowledge or concepts. The overall disagreement between empiricists and rationalists show primary concerns in how there is gaining of knowledge with respect to the sources of knowledge and concept. In some of the cases, disagreement at the point of gaining knowledge results in the provision of conflicting responses to other aspects as well. There might be a disagreement in the overall feature of warrant, while limiting the knowledge and thought. Empiricists are known for sharing the view that there is no existence of innate knowledge and rather that is derivation of knowledge out of experience. These experiences are either reasoned using the mind or sensed through the five senses human possess (Bernard, 2011, 5). On the other hand, rationalists are known to be sharing the view that there is existence of innate knowledge and this is different for the objects of innate knowledge being chosen. In order to follow rationalism, there must be adoption of one of the three claims related to the theory that are Deduction or Intuition, Innate Knowledge, and Innate Concept. The more there is removal of concept from mental operations and experience, there can be performance over experience with increased plausibility in being innate. Further ahead, empiricism in context with a specific subject provides a rejection of corresponding version related to innate knowledge and deduction or intuition (Weiskopf, 2008, 16). Insofar as there is acknowledgement of concepts and knowledge within the area of subject, the knowledge has major dependence on experience through human senses.\n\n\n== Empirical cycle ==\n\nA.D. de Groot's empirical cycle:\nObservation: The observation of a phenomenon and inquiry concerning its causes.\nInduction: The formulation of hypotheses - generalized explanations for the phenomenon.\nDeduction: The formulation of experiments that will test the hypotheses (i.e. confirm them if true, refute them if false).\nTesting: The procedures by which the hypotheses are tested and data are collected.\nEvaluation: The interpretation of the data and the formulation of a theory - an abductive argument that presents the results of the experiment as the most reasonable explanation for the phenomenon.\n\n\n== See also ==\nCase study\nFact\n\n\n== References ==\n\n\n== External links ==\n The dictionary definition of empirical research at Wiktionary\nSome Key Concepts for the Design and Review of Empirical Research", "order theory": "Order theory is a branch of mathematics which investigates the intuitive notion of order using binary relations. It provides a formal framework for describing statements such as \"this is less than that\" or \"this precedes that\". This article introduces the field and provides basic definitions. A list of order-theoretic terms can be found in the order theory glossary.\n\n\n== Background and motivation ==\nOrders are everywhere in mathematics and related fields like computer science. The first order often discussed in primary school is the standard order on the natural numbers e.g. \"2 is less than 3\", \"10 is greater than 5\", or \"Does Tom have fewer cookies than Sally?\". This intuitive concept can be extended to orders on other sets of numbers, such as the integers and the reals. The idea of being greater than or less than another number is one of the basic intuitions of number systems (compare with numeral systems) in general (although one usually is also interested in the actual difference of two numbers, which is not given by the order). Other familiar examples of orderings are the alphabetical order of words in a dictionary and the genealogical property of lineal descent within a group of people.\nThe notion of order is very general, extending beyond contexts that have an immediate, intuitive feel of sequence or relative quantity. In other contexts orders may capture notions of containment or specialization. Abstractly, this type of order amounts to the subset relation, e.g., \"Pediatricians are physicians,\" and \"Circles are merely special-case ellipses.\"\nSome orders, like \"less-than\" on the natural numbers and alphabetical order on words, have a special property: each element can be compared to any other element, i.e. it is smaller (earlier) than, larger (later) than, or identical to. However, many other orders do not. Consider for example the subset order on a collection of sets: though the set of birds and the set of dogs are both subsets of the set of animals, neither the birds nor the dogs constitutes a subset of the other. Those orders like the \"subset-of\" relation for which there exist incomparable elements are called partial orders; orders for which every pair of elements is comparable are total orders.\nOrder theory captures the intuition of orders that arises from such examples in a general setting. This is achieved by specifying properties that a relation \u2264 must have to be a mathematical order. This more abstract approach makes much sense, because one can derive numerous theorems in the general setting, without focusing on the details of any particular order. These insights can then be readily transferred to many less abstract applications.\nDriven by the wide practical usage of orders, numerous special kinds of ordered sets have been defined, some of which have grown into mathematical fields of their own. In addition, order theory does not restrict itself to the various classes of ordering relations, but also considers appropriate functions between them. A simple example of an order theoretic property for functions comes from analysis where monotone functions are frequently found.\n\n\n== Basic definitions ==\nThis section introduces ordered sets by building upon the concepts of set theory, arithmetic, and binary relations.\n\n\n=== Partially ordered sets ===\nOrders are special binary relations. Suppose that P is a set and that \u2264 is a relation on P. Then \u2264 is a partial order if it is reflexive, antisymmetric, and transitive, i.e., for all a, b and c in P, we have that:\n\na \u2264 a (reflexivity)\nif a \u2264 b and b \u2264 a then a = b (antisymmetry)\nif a \u2264 b and b \u2264 c then a \u2264 c (transitivity).A set with a partial order on it is called a partially ordered set, poset, or just an ordered set if the intended meaning is clear. By checking these properties, one immediately sees that the well-known orders on natural numbers, integers, rational numbers and reals are all orders in the above sense. However, they have the additional property of being total, i.e., for all a and b in P, we have that:\n\na \u2264 b or b \u2264 a (totality).These orders can also be called linear orders or chains. While many classical orders are linear, the subset order on sets provides an example where this is not the case. Another example is given by the divisibility (or \"is-a-factor-of\") relation \"|\". For two natural numbers n and m, we write n|m if n divides m without remainder. One easily sees that this yields a partial order.\nThe identity relation = on any set is also a partial order in which every two distinct elements are incomparable. It is also the only relation that is both a partial order and an equivalence relation. Many advanced properties of posets are interesting mainly for non-linear orders.\n\n\n=== Visualizing a poset ===\n\nHasse diagrams can visually represent the elements and relations of a partial ordering. These are graph drawings where the vertices are the elements of the poset and the ordering relation is indicated by both the edges and the relative positioning of the vertices. Orders are drawn bottom-up: if an element x is smaller than (precedes) y then there exists a path from x to y that is directed upwards. It is often necessary for the edges connecting elements to cross each other, but elements must never be located within an edge. An instructive exercise is to draw the Hasse diagram for the set of natural numbers that are smaller than or equal to 13, ordered by | (the divides relation).\nEven some infinite sets can be diagrammed by superimposing an ellipsis (...) on a finite sub-order. This works well for the natural numbers, but it fails for the reals, where there is no immediate successor above 0; however, quite often one can obtain an intuition related to diagrams of a similar kind.\n\n\n=== Special elements within an order ===\nIn a partially ordered set there may be some elements that play a special role. The most basic example is given by the least element of a poset. For example, 1 is the least element of the positive integers and the empty set is the least set under the subset order. Formally, an element m is a least element if:\n\nm \u2264 a, for all elements a of the order.The notation 0 is frequently found for the least element, even when no numbers are concerned. However, in orders on sets of numbers, this notation might be inappropriate or ambiguous, since the number 0 is not always least. An example is given by the above divisibility order |, where 1 is the least element since it divides all other numbers. In contrast, 0 is the number that is divided by all other numbers. Hence it is the greatest element of the order.  Other frequent terms for the least and greatest elements is bottom and top or zero and unit.\nLeast and greatest elements may fail to exist, as the example of the real numbers shows. But if they exist, they are always unique. In contrast, consider the divisibility relation | on the set {2,3,4,5,6}. Although this set has neither top nor bottom, the elements 2, 3, and 5 have no elements below them, while 4, 5 and 6 have none above. Such elements are called minimal and maximal, respectively. Formally, an element m is minimal if:\n\na \u2264 m implies a = m, for all elements a of the order.Exchanging \u2264 with \u2265 yields the definition of maximality. As the example shows, there can be many maximal elements and some elements may be both maximal and minimal (e.g. 5 above). However, if there is a least element, then it is the only minimal element of the order. Again, in infinite posets maximal elements do not always exist - the set of all finite subsets of a given infinite set, ordered by subset inclusion, provides one of many counterexamples. An important tool to ensure the existence of maximal elements under certain conditions is Zorn's Lemma.\nSubsets of partially ordered sets inherit the order. We already applied this by considering the subset {2,3,4,5,6} of the natural numbers with the induced divisibility ordering. Now there are also elements of a poset that are special with respect to some subset of the order. This leads to the definition of upper bounds. Given a subset S of some poset P, an upper bound of S is an element b of P that is above all elements of S. Formally, this means that\n\ns \u2264 b, for all s in S.Lower bounds again are defined by inverting the order. For example, -5 is a lower bound of the natural numbers as a subset of the integers. Given a set of sets, an upper bound for these sets under the subset ordering is given by their union. In fact, this upper bound is quite special: it is the smallest set that contains all of the sets. Hence, we have found the least upper bound of a set of sets. This concept is also called supremum or join, and for a set S one writes sup(S) or \n  \n    \n      \n        \u22c1\n        S\n      \n    \n    {\\displaystyle \\bigvee S}\n   for its least upper bound. Conversely, the greatest lower bound is known as infimum or meet and denoted inf(S) or \n  \n    \n      \n        \u22c0\n        S\n      \n    \n    {\\displaystyle \\bigwedge S}\n  . These concepts play an important role in many applications of order theory. For two elements x and y, one also writes \n  \n    \n      \n        x\n        \u2228\n        y\n      \n    \n    {\\displaystyle x\\vee y}\n   and \n  \n    \n      \n        x\n        \u2227\n        y\n      \n    \n    {\\displaystyle x\\wedge y}\n   for sup({x,y}) and inf({x,y}), respectively.\nFor example, 1 is the infimum of the positive integers as a subset of integers.\nFor another example, consider again the relation | on natural numbers. The least upper bound of two numbers is the smallest number that is divided by both of them, i.e. the least common multiple of the numbers. Greatest lower bounds in turn are given by the greatest common divisor.\n\n\n=== Duality ===\nIn the previous definitions, we often noted that a concept can be defined by just inverting the ordering in a former definition. This is the case for \"least\" and \"greatest\", for \"minimal\" and \"maximal\", for \"upper bound\" and \"lower bound\", and so on. This is a general situation in order theory: A given order can be inverted by just exchanging its direction, pictorially flipping the Hasse diagram top-down. This yields the so-called dual, inverse, or opposite order.\nEvery order theoretic definition has its dual: it is the notion one obtains by applying the definition to the inverse order. Since all concepts are symmetric, this operation preserves the theorems of partial orders. For a given mathematical result, one can just invert the order and replace all definitions by their duals and one obtains another valid theorem. This is important and useful, since one obtains two theorems for the price of one. Some more details and examples can be found in the article on duality in order theory.\n\n\n=== Constructing new orders ===\nThere are many ways to construct orders out of given orders. The dual order is one example. Another important construction is the cartesian product of two partially ordered sets, taken together with the product order on pairs of elements. The ordering is defined by (a, x) \u2264 (b, y) if (and only if) a \u2264 b and x \u2264 y. (Notice carefully that there are three distinct meanings for the relation symbol \u2264 in this definition.) The disjoint union of two posets is another typical example of order construction, where the order is just the (disjoint) union of the original orders.\nEvery partial order \u2264 gives rise to a so-called strict order <, by defining a < b if a \u2264 b and not b \u2264 a. This transformation can be inverted by setting a \u2264 b if a < b or a = b. The two concepts are equivalent although in some circumstances one can be more convenient to work with than the other.\n\n\n== Functions between orders ==\nIt is reasonable to consider functions between partially ordered sets having certain additional properties that are related to the ordering relations of the two sets. The most fundamental condition that occurs in this context is monotonicity. A function f from a poset P to a poset Q is monotone, or order-preserving, if a \u2264 b in P implies f(a) \u2264 f(b) in Q (Noting that, strictly, the two relations here are different since they apply to different sets.). The converse of this implication leads to functions that are order-reflecting, i.e. functions f as above for which f(a) \u2264 f(b) implies a \u2264 b. On the other hand, a function may also be order-reversing or antitone, if a \u2264 b implies f(b) \u2264 f(a).\nAn order-embedding is a function f between orders that is both order-preserving and order-reflecting. Examples for these definitions are found easily. For instance, the function that maps a natural number to its successor is clearly monotone with respect to the natural order. Any function from a discrete order, i.e. from a set ordered by the identity order \"=\", is also monotone. Mapping each natural number to the corresponding real number gives an example for an order embedding. The set complement on a powerset is an example of an antitone function.\nAn important question is when two orders are \"essentially equal\", i.e. when they are the same up to renaming of elements. Order isomorphisms are functions that define such a renaming. An order-isomorphism is a monotone bijective function that has a monotone inverse. This is equivalent to being a surjective order-embedding. Hence, the image f(P) of an order-embedding is always isomorphic to P, which justifies the term \"embedding\".\nA more elaborate type of functions is given by so-called Galois connections. Monotone Galois connections can be viewed as a generalization of order-isomorphisms, since they constitute of a pair of two functions in converse directions, which are \"not quite\" inverse to each other, but that still have close relationships.\nAnother special type of self-maps on a poset are closure operators, which are not only monotonic, but also idempotent, i.e. f(x) = f(f(x)), and extensive (or inflationary), i.e. x \u2264 f(x). These have many applications in all kinds of \"closures\" that appear in mathematics.\nBesides being compatible with the mere order relations, functions between posets may also behave well with respect to special elements and constructions. For example, when talking about posets with least element, it may seem reasonable to consider only monotonic functions that preserve this element, i.e. which map least elements to least elements. If binary infima \u2227 exist, then a reasonable property might be to require that f(x \u2227 y) = f(x) \u2227 f(y), for all x and y. All of these properties, and indeed many more, may be compiled under the label of limit-preserving functions.\nFinally, one can invert the view, switching from functions of orders to orders of functions. Indeed, the functions between two posets P and Q can be ordered via the pointwise order. For two functions f and g, we have f \u2264 g if f(x) \u2264 g(x) for all elements x of P. This occurs for example in domain theory, where function spaces play an important role.\n\n\n== Special types of orders ==\nMany of the structures that are studied in order theory employ order relations with further properties. In fact, even some relations that are not partial orders are of special interest. Mainly the concept of a preorder has to be mentioned. A preorder is a relation that is reflexive and transitive, but not necessarily antisymmetric. Each preorder induces an equivalence relation between elements, where a is equivalent to b, if a \u2264 b and b \u2264 a. Preorders can be turned into orders by identifying all elements that are equivalent with respect to this relation.\nSeveral types of orders can be defined from numerical data on the items of the order: a total order results from attaching distinct real numbers to each item and using the numerical comparisons to order the items; instead, if distinct items are allowed to have equal numerical scores, one obtains a strict weak ordering. Requiring two scores to be separated by a fixed threshold before they may be compared leads to the concept of a semiorder, while allowing the threshold to vary on a per-item basis produces an interval order.\nAn additional simple but useful property leads to so-called well-orders, for which all non-empty subsets have a minimal element.  Generalizing well-orders from linear to partial orders, a set is well partially ordered if all its non-empty subsets have a finite number of minimal elements.\nMany other types of orders arise when the existence of infima and suprema of certain sets is guaranteed. Focusing on this aspect, usually referred to as completeness of orders, one obtains:\n\nBounded posets, i.e. posets with a least and greatest element (which are just the supremum and infimum of the empty subset),\nLattices, in which every non-empty finite set has a supremum and infimum,\nComplete lattices, where every set has a supremum and infimum, and\nDirected complete partial orders (dcpos), that guarantee the existence of suprema of all directed subsets and that are studied in domain theory.\nPartial orders with complements, or poc sets, are posets S having a unique bottom element 0\u2208S, along with an order-reversing involution, such that \n  \n    \n      \n        a\n        \u2264\n        \n          a\n          \n            \u2217\n          \n        \n        \u21d2\n        a\n        =\n        0\n      \n    \n    {\\displaystyle a\\leq a^{*}\\Rightarrow a=0}\n  .However, one can go even further: if all finite non-empty infima exist, then \u2227 can be viewed as a total binary operation in the sense of universal algebra. Hence, in a lattice, two operations \u2227 and \u2228 are available, and one can define new properties by giving identities, such as\n\nx \u2227 (y \u2228 z)  =  (x \u2227 y) \u2228 (x \u2227 z), for all x, y, and z.This condition is called distributivity and gives rise to distributive lattices. There are some other important distributivity laws which are discussed in the article on distributivity in order theory. Some additional order structures that are often specified via algebraic operations and defining identities are\n\nHeyting algebras and\nBoolean algebras,which both introduce a new operation ~ called negation. Both structures play a role in mathematical logic and especially Boolean algebras have major applications in computer science.\nFinally, various structures in mathematics combine orders with even more algebraic operations, as in the case of quantales, that allow for the definition of an addition operation.\nMany other important properties of posets exist. For example, a poset is locally finite if every closed interval [a, b] in it is finite. Locally finite posets give rise to incidence algebras which in turn can be used to define the Euler characteristic of finite bounded posets.\n\n\n== Subsets of ordered sets ==\nIn an ordered set, one can define many types of special subsets based on the given order. A simple example are upper sets; i.e. sets that contain all elements that are above them in the order. Formally, the upper closure of a set S in a poset P is given by the set {x in P | there is some y in S with y \u2264 x}. A set that is equal to its upper closure is called an upper set. Lower sets are defined dually.\nMore complicated lower subsets are ideals, which have the additional property that each two of their elements have an upper bound within the ideal. Their duals are given by filters. A related concept is that of a directed subset, which like an ideal contains upper bounds of finite subsets, but does not have to be a lower set. Furthermore, it is often generalized to preordered sets.\nA subset which is - as a sub-poset - linearly ordered, is called a chain. The opposite notion, the antichain, is a subset that contains no two comparable elements; i.e. that is a discrete order.\n\n\n== Related mathematical areas ==\nAlthough most mathematical areas use orders in one or the other way, there are also a few theories that have relationships which go far beyond mere application. Together with their major points of contact with order theory, some of these are to be presented below.\n\n\n=== Universal algebra ===\nAs already mentioned, the methods and formalisms of universal algebra are an important tool for many order theoretic considerations. Beside formalizing orders in terms of algebraic structures that satisfy certain identities, one can also establish other connections to algebra. An example is given by the correspondence between Boolean algebras and Boolean rings. Other issues are concerned with the existence of free constructions, such as free lattices based on a given set of generators. Furthermore, closure operators are important in the study of universal algebra.\n\n\n=== Topology ===\nIn topology, orders play a very prominent role. In fact, the set of open sets provides a classical example of a complete lattice, more precisely a complete Heyting algebra (or \"frame\" or \"locale\"). Filters and nets are notions closely related to order theory and the closure operator of sets can be used to define topology. Beyond these relations, topology can be looked at solely in terms of the open set lattices, which leads to the study of pointless topology. Furthermore, a natural preorder of elements of the underlying set of a topology is given by the so-called specialization order, that is actually a partial order if the topology is T0.\nConversely, in order theory, one often makes use of topological results. There are various ways to define subsets of an order which can be considered as open sets of a topology. Considering topologies on a poset (X, \u2264) that in turn induce \u2264 as their specialization order, the finest such topology is the Alexandrov topology, given by taking all upper sets as opens. Conversely, the coarsest topology that induces the specialization order is the upper topology, having the complements of principal ideals (i.e. sets of the form {y in X | y \u2264 x} for some x) as a subbase. Additionally, a topology with specialization order \u2264 may be order consistent, meaning that their open sets are \"inaccessible by directed suprema\" (with respect to \u2264). The finest order consistent topology is the Scott topology, which is coarser than the Alexandrov topology. A third important topology in this spirit is the Lawson topology. There are close connections between these topologies and the concepts of order theory. For example, a function preserves directed suprema iff it is continuous with respect to the Scott topology (for this reason this order theoretic property is also called Scott-continuity).\n\n\n=== Category theory ===\nThe visualization of orders with Hasse diagrams has a straightforward generalization: instead of displaying lesser elements below greater ones, the direction of the order can also be depicted by giving directions to the edges of a graph. In this way, each order is seen to be equivalent to a directed acyclic graph, where the nodes are the elements of the poset and there is a directed path from a to b if and only if a \u2264 b. Dropping the requirement of being acyclic, one can also obtain all preorders.\nWhen equipped with all transitive edges, these graphs in turn are just special categories, where elements are objects and each set of morphisms between two elements is at most singleton. Functions between orders become functors between categories. Many ideas of order theory are just concepts of category theory in small. For example, an infimum is just a categorical product. More generally, one can capture infima and suprema under the abstract notion of a categorical limit (or colimit, respectively). Another place where categorical ideas occur is the concept of a (monotone) Galois connection, which is just the same as a pair of adjoint functors.\nBut category theory also has its impact on order theory on a larger scale. Classes of posets with appropriate functions as discussed above form interesting categories. Often one can also state constructions of orders, like the product order, in terms of categories. Further insights result when categories of orders are found categorically equivalent to other categories, for example of topological spaces. This line of research leads to various representation theorems, often collected under the label of Stone duality.\n\n\n== History ==\nAs explained before, orders are ubiquitous in mathematics. However, earliest explicit mentionings of partial orders are probably to be found not before the 19th century. In this context the works of George Boole are of great importance. Moreover, works of Charles Sanders Peirce, Richard Dedekind, and Ernst Schr\u00f6der also consider concepts of order theory. Certainly, there are others to be named in this context and surely there exists more detailed material on the history of order theory. \nThe term poset as an abbreviation for partially ordered set was coined by Garrett Birkhoff in the second edition of his influential book Lattice Theory.\n\n\n== See also ==\nCyclic order\nHierarchy\nIncidence algebra\nCausal Sets\n\n\n== Notes ==\n\n\n== References ==\nBirkhoff, Garrett (1940). Lattice Theory. 25 (3rd Revised ed.). American Mathematical Society. ISBN 978-0-8218-1025-5. \nBurris, S. N.; Sankappanavar, H. P. (1981). A Course in Universal Algebra. Springer. ISBN 978-0-387-90578-5. \nDavey, B. A.; Priestley, H. A. (2002). Introduction to Lattices and Order (2nd ed.). Cambridge University Press. ISBN 0-521-78451-4. \nGierz, G.; Hofmann, K. H.; Keimel, K.; Mislove, M.; Scott, D. S. (2003). Continuous Lattices and Domains. Encyclopedia of Mathematics and its Applications. 93. Cambridge University Press. ISBN 978-0-521-80338-0. \n\n\n== External links ==\nOrders at ProvenMath partial order, linear order, well order, initial segment; formal definitions and proofs within the axioms of set theory.\nNagel, Felix (2013). Set Theory and Topology. An Introduction to the Foundations of Analysis", "physics": "Physics (from Ancient Greek: \u03c6\u03c5\u03c3\u03b9\u03ba\u03ae (\u1f10\u03c0\u03b9\u03c3\u03c4\u03ae\u03bc\u03b7), translit. physik\u1e17 (epist\u1e17m\u0113), lit. 'knowledge of nature', from \u03c6\u03cd\u03c3\u03b9\u03c2 ph\u00fdsis \"nature\") is the natural science that studies matter and its motion and behavior through space and time and that studies the related entities of energy and force. Physics is one of the most fundamental scientific disciplines, and its main goal is to understand how the universe behaves.Physics is one of the oldest academic disciplines and, through its inclusion of astronomy, perhaps the oldest. Over the last two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the scientific revolution in the 17th century, these natural sciences emerged as unique research endeavors in their own right. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in academic disciplines such as mathematics and philosophy.\nAdvances in physics often enable advances in new technologies. For example, advances in the understanding of electromagnetism and nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.\n\n\n== History ==\n\n\n=== Ancient astronomy ===\n\nAstronomy is one of the oldest natural sciences. Early civilizations dating back to beyond 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilization, had a predictive knowledge and a basic understanding of the motions of the Sun, Moon, and stars. The stars and planets were often worshipped, believed to represent gods. While the explanations for the observed positions of the stars were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy, as the stars were found to traverse great circles across the sky, which however did not explain the positions of the planets. \nAccording to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy. Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies, while Greek poet Homer wrote of various celestial objects in his Iliad and Odyssey; later Greek astronomers provided names, which are still used today, for most constellations visible from the northern hemisphere.\n\n\n=== Natural philosophy ===\n\nNatural philosophy has its origins in Greece during the Archaic period, (650 BCE \u2013 480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause. They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment; for example, atomism was found to be correct approximately 2000 years after it was proposed by Leucippus and his pupil Democritus.\n\n\n=== Physics in the medieval European and Islamic world ===\n\nThe Western Roman Empire fell in the fifth century, and this resulted in a decline in intellectual pursuits in the western part of Europe. By contrast, the Eastern Roman Empire (also known as the Byzantine Empire) resisted the attacks from the barbarians, and continued to advance various fields of learning, including physics.In the sixth century Isidore of Miletus created an important compilation of Archimedes' works that are copied in the Archimedes Palimpsest. \nIn sixth century Europe John Philoponus, a byzantine scholar, questioned Aristotle's teaching of physics and noting its flaws. He introduced the theory of impetus. Aristotle\u2019s physics was not scrutinized until John Philoponus appeared, and unlike Aristotle who based his physics on verbal argument, Philoponus relied on observation. On Aristotle's physics John Philoponus wrote:\n\u201cBut this is completely erroneous, and our view may be corroborated by actual observation more effectively than by any sort of verbal argument. For if you let fall from the same height two weights of which one is many times as heavy as the other, you will see that the ratio of the times required for the motion does not depend on the ratio of the weights, but that the difference in time is a very small one. And so, if the difference in the weights is not considerable, that is, of one is, let us say, double the other, there will be no difference, or else an imperceptible difference, in time, though the difference in weight is by no means negligible, with one body weighing twice as much as the other\u201dJohn Philoponus' criticism of Aristotelian principles of physics served as an inspiration for Galileo Galilei ten centuries later, during the Scientific Revolution. Galileo cited Philoponus substantially in his works when arguing that Aristotelian physics was flawed. In the 1300s Jean Buridan, a teacher in the faculty of arts at the University of Paris, developed the concept of impetus. It was a step toward the modern ideas of inertia and momentum.Islamic scholarship inherited Aristotelian physics from the Greeks and during the Islamic Golden Age developed it further, especially placing emphasis on observation and a priori reasoning, developing early forms of the scientific method.\nThe most notable innovations were in the field of optics and vision, which came from the works of many scientists like Ibn Sahl, Al-Kindi, Ibn al-Haytham, Al-Farisi and Avicenna. The most notable work was The Book of Optics (also known as Kit\u0101b al-Man\u0101\u1e93ir), written by Ibn al-Haytham, in which he conclusively disproved the ancient Greek idea about vision, but also came up with a new theory. In the book, he presented a study of the phenomenon of the camera obscura (his thousand-year-old version of the pinhole camera) and delved further into the way the eye itself works. Using dissections and the knowledge of previous scholars, he was able to begin to explain how light enters the eye. He asserted that the light ray is focused, but the actual explanation of how light projected to the back of the eye had to wait until 1604. His Treatise on Light explained the camera obscura, hundreds of years before the modern development of photography.\n\nThe seven-volume Book of Optics (Kitab al-Manathir) hugely influenced thinking across disciplines from the theory of visual perception to the nature of perspective in medieval art, in both the East and the West, for more than 600 years. Many later European scholars and fellow polymaths, from Robert Grosseteste and Leonardo da Vinci to Ren\u00e9 Descartes, Johannes Kepler and Isaac Newton, were in his debt. Indeed, the influence of Ibn al-Haytham's Optics ranks alongside that of Newton's work of the same title, published 700 years later.\nThe translation of The Book of Optics had a huge impact on Europe. From it, later European scholars were able to build devices that replicated those Ibn al-Haytham had built, and understand the way light works. From this, such important things as eyeglasses, magnifying glasses, telescopes, and cameras were developed.\n\n\n=== Classical physics ===\n\nPhysics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics.Major developments in this period include the replacement of the geocentric model of the solar system with the heliocentric Copernican model, the laws governing the motion of planetary bodies determined by Johannes Kepler between 1609 and 1619, pioneering work on telescopes and observational astronomy by Galileo Galilei in the 16th and 17th Centuries, and Isaac Newton's discovery and unification of the laws of motion and universal gravitation that would come to bear his name. Newton also developed calculus, the mathematical study of change, which provided new mathematical methods for solving physical problems.The discovery of new laws in thermodynamics, chemistry, and electromagnetics resulted from greater research efforts during the Industrial Revolution as energy needs increased. The laws comprising classical physics remain very widely used for objects on everyday scales travelling at non-relativistic speeds, since they provide a very close approximation in such situations, and theories such as quantum mechanics and the theory of relativity simplify to their classical equivalents at such scales. However, inaccuracies in classical mechanics for very small objects and very high velocities led to the development of modern physics in the 20th century.\n\n\n=== Modern physics ===\n\nModern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories came about due to inaccuracies in classical mechanics in certain situations. Classical mechanics predicted a varying speed of light, which could not be resolved with the constant speed predicted by Maxwell's equations of electromagnetism; this discrepancy was corrected by Einstein's theory of special relativity, which replaced classical mechanics for fast-moving bodies and allowed for a constant speed of light. Black body radiation provided another problem for classical physics, which was corrected when Planck proposed that the excitation of material oscillators is possible only in discrete steps proportional to their frequency; this, along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals, led to the theory of quantum mechanics taking over from classical physics at very small scales.Quantum mechanics would come to be pioneered by Werner Heisenberg, Erwin Schr\u00f6dinger and Paul Dirac. From this early work, and work in related fields, the Standard Model of particle physics was derived. Following the discovery of a particle with properties consistent with the Higgs boson at CERN in 2012, all fundamental particles predicted by the standard model, and no others, appear to exist; however, physics beyond the Standard Model, with theories such as supersymmetry, is an active area of research. Areas of mathematics in general are important to this field, such as the study of probabilities and groups.\n\n\n== Philosophy ==\n\nIn many ways, physics stems from ancient Greek philosophy. From Thales' first attempt to characterise matter, to Democritus' deduction that matter ought to reduce to an invariant state, the Ptolemaic astronomy of a crystalline firmament, and Aristotle's book Physics (an early book on physics, which attempted to analyze and define motion from a philosophical point of view), various Greek philosophers advanced their own theories of nature. Physics was known as natural philosophy until the late 18th century.By the 19th century, physics was realised as a discipline distinct from philosophy and the other sciences. Physics, as with the rest of science, relies on philosophy of science and its \"scientific method\" to advance our knowledge of the physical world. The scientific method employs a priori reasoning as well as a posteriori reasoning and the use of Bayesian inference to measure the validity of a given theory.The development of physics has answered many questions of early philosophers, but has also raised new questions. Study of the philosophical issues surrounding physics, the philosophy of physics, involves issues such as the nature of space and time, determinism, and metaphysical outlooks such as empiricism, naturalism and realism.Many physicists have written about the philosophical implications of their work, for instance Laplace, who championed causal determinism, and Erwin Schr\u00f6dinger, who wrote on quantum mechanics. The mathematical physicist Roger Penrose has been called a Platonist by Stephen Hawking, a view Penrose discusses in his book, The Road to Reality. Hawking refers to himself as an \"unashamed reductionist\" and takes issue with Penrose's views.\n\n\n== Core theories ==\n\nThough physics deals with a wide variety of systems, certain theories are used by all physicists. Each of these theories were experimentally tested numerous times and found to be an adequate approximation of nature. For instance, the theory of classical mechanics accurately describes the motion of objects, provided they are much larger than atoms and moving at much less than the speed of light. These theories continue to be areas of active research today. Chaos theory, a remarkable aspect of classical mechanics was discovered in the 20th century, three centuries after the original formulation of classical mechanics by Isaac Newton (1642\u20131727).\nThese central theories are important tools for research into more specialised topics, and any physicist, regardless of their specialisation, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity.\n\n\n=== Classical physics ===\n\nClassical physics includes the traditional branches and topics that were recognised and well-developed before the beginning of the 20th century\u2014classical mechanics, acoustics, optics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics, aerodynamics, and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received.  Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics.Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest.\n\n\n=== Modern physics ===\n\nClassical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale. For example, atomic and nuclear physics studies matter on the smallest scale at which chemical elements can be identified. The physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter; this branch of physics is also known as high-energy physics because of the extremely high energies necessary to produce many types of particles in particle accelerators. On this scale, ordinary, commonsense notions of space, time, matter, and energy are no longer valid.The two chief theories of modern physics present a different picture of the concepts of space, time, and matter from that presented by classical physics. Classical mechanics approximates nature as continuous, while quantum theory is concerned with the discrete nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena. The theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer; the special theory of relativity is concerned with relative uniform motion in a straight line and the general theory of relativity with accelerated motion and its connection with gravitation. Both quantum theory and the theory of relativity find applications in all areas of modern physics.\n\n\n=== Difference between classical and modern physics ===\n\nWhile physics aims to discover universal laws, its theories lie in explicit domains of applicability. Loosely speaking, the laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics. Albert Einstein contributed the framework of special relativity, which replaced notions of absolute time and space with spacetime and allowed an accurate description of systems whose components have speeds approaching the speed of light. Max Planck, Erwin Schr\u00f6dinger, and others introduced quantum mechanics, a probabilistic notion of particles and interactions that allowed an accurate description of atomic and subatomic scales. Later, quantum field theory unified quantum mechanics and special relativity. General relativity allowed for a dynamical, curved spacetime, with which highly massive systems and the large-scale structure of the universe can be well-described. General relativity has not yet been unified with the other fundamental descriptions; several candidate theories of quantum gravity are being developed.\n\n\n== Relation to other fields ==\n\n\n=== Prerequisites ===\nMathematics provides a compact and exact language used to describe of the order in nature. This was noted and advocated by Pythagoras, Plato, Galileo, and Newton.\nPhysics uses mathematics to organise and formulate experimental results. From those results, precise or estimated solutions, quantitative results from which new predictions can be made and experimentally confirmed or negated. The results from physics experiments are numerical measurements. Technologies based on mathematics, like computation have made computational physics an active area of research.\n\nOntology is a prerequisite for physics, but not for mathematics. It means physics is ultimately concerned with descriptions of the real world, while mathematics is concerned with abstract patterns, even beyond the real world. Thus physics statements are synthetic, while mathematical statements are analytic. Mathematics contains hypotheses, while physics contains theories. Mathematics statements have to be only logically true, while predictions of physics statements must match observed and experimental data.\nThe distinction is clear-cut, but not always obvious. For example, mathematical physics is the application of mathematics in physics. Its methods are mathematical, but its subject is physical. The problems in this field start with a \"mathematical model of a physical situation\" (system) and a \"mathematical description of a physical law\" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for.Physics is a branch of fundamental science, not practical science. Physics is also called \"the fundamental science\" because the subject of study of all branches of natural science like chemistry, astronomy, geology, and biology are constrained by laws of physics, similar to how chemistry is often called the central science because of its role in linking the physical sciences. For example, chemistry studies properties, structures, and reactions of matter (chemistry's focus on the atomic scale distinguishes it from physics). Structures are formed because particles exert electrical forces on each other, properties include physical characteristics of given substances, and reactions are bound by laws of physics, like conservation of energy, mass, and charge.\nPhysics is applied in industries like engineering and medicine.\n\n\n=== Application and influence ===\n\nApplied physics is a general term for physics research which is intended for a particular use. An applied physics curriculum usually contains a few classes in an applied discipline, like geology or electrical engineering. It usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem.\nThe approach is similar to that of applied mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics.\nPhysics is used heavily in engineering. For example, statics, a subfield of mechanics, is used in the building of bridges and other static structures. The understanding and use of acoustics results in sound control and better concert halls; similarly, the use of optics creates better optical devices. An understanding of physics makes for more realistic flight simulators, video games, and movies, and is often critical in forensic investigations.\nWith the standard consensus that the laws of physics are universal and do not change with time, physics can be used to study things that would ordinarily be mired in uncertainty. For example, in the study of the origin of the earth, one can reasonably model earth's mass, temperature, and rate of rotation, as a function of time allowing one to extrapolate forward or backward in time and so predict future or prior events. It also allows for simulations in engineering which drastically speed up the development of a new technology.\nBut there is also considerable interdisciplinarity in the physicist's methods, so many other important fields are influenced by physics (e.g., the fields of econophysics and sociophysics).\n\n\n== Research ==\n\n\n=== Scientific method ===\nPhysicists use the scientific method to test the validity of a physical theory. By using a methodical approach to compare the implications of a theory with the conclusions drawn from its related experiments and observations, physicists are better able to test the validity of a theory in a logical, unbiased, and repeatable way. To that end, experiments are performed and observations are made in order to determine the validity or invalidity of the theory.A scientific law is a concise verbal or mathematical statement of a relation which expresses a fundamental principle of some theory, such as Newton's law of universal gravitation.\n\n\n=== Theory and experiment ===\n\nTheorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results, while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena. Although theory and experiment are developed separately, they are strongly dependent upon each other. Progress in physics frequently comes about when experimentalists make a discovery that existing theories cannot explain, or when new theories generate experimentally testable predictions, which inspire new experiments.Physicists who work at the interplay of theory and experiment are called phenomenologists, who study complex phenomena observed in experiment and work to relate them to a fundamental theory.Theoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way. Beyond the known universe, the field of theoretical physics also deals with hypothetical issues, such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas in hopes of solving particular problems with existing theories. They then explore the consequences of these ideas and work toward making testable predictions.\nExperimental physics expands, and is expanded by, engineering and technology. Experimental physicists involved in basic research design and perform experiments with equipment such as particle accelerators and lasers, whereas those involved in applied research often work in industry developing technologies such as magnetic resonance imaging (MRI) and transistors. Feynman has noted that experimentalists may seek areas which are not well-explored by theorists.\n\n\n=== Scope and aims ===\n\nPhysics covers a wide range of phenomena, from elementary particles (such as quarks, neutrinos, and electrons) to the largest superclusters of galaxies. Included in these phenomena are the most basic objects composing all other things. Therefore, physics is sometimes called the \"fundamental science\". Physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena. Thus, physics aims to both connect the things observable to humans to root causes, and then connect these causes together.\nFor example, the ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one force\u2014electromagnetism. This process of \"unifying\" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (Theory of Everything) for why nature is as it is (see section Current research below for more information).\n\n\n=== Research fields ===\nContemporary research in physics can be broadly divided into nuclear and particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach.Since the 20th century, the individual fields of physics have become increasingly specialised, and today most physicists work in a single field for their entire careers. \"Universalists\" such as Albert Einstein (1879\u20131955) and Lev Landau (1908\u20131968), who worked in multiple fields of physics, are now very rare.The major fields of physics, along with their subfields and the theories and concepts they employ, are shown in the following table.\n\n\n==== Nuclear and particle physics ====\n\nParticle physics is the study of the elementary constituents of matter and energy and the interactions between them. In addition, particle physicists design and develop the high energy accelerators, detectors, and computer programs necessary for this research. The field is also called \"high-energy physics\" because many elementary particles do not occur naturally but are created only during high-energy collisions of other particles.Currently, the interactions of elementary particles and fields are described by the Standard Model. The model accounts for the 12 known particles of matter (quarks and leptons) that interact via the strong, weak, and electromagnetic fundamental forces. Dynamics are described in terms of matter particles exchanging gauge bosons (gluons, W and Z bosons, and photons, respectively). The Standard Model also predicts a particle known as the Higgs boson. In July 2012 CERN, the European laboratory for particle physics, announced the detection of a particle consistent with the Higgs boson, an integral part of a Higgs mechanism.\nNuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology, but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology.\n\n\n==== Atomic, molecular, and optical physics ====\n\nAtomic, molecular, and optical physics (AMO) is the study of matter\u2013matter and light\u2013matter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods used, and the commonality of their relevant energy scales. All three areas include both classical, semi-classical and quantum treatments; they can treat their subject from a microscopic view (in contrast to a macroscopic view).\nAtomic physics studies the electron shells of atoms. Current research focuses on activities in quantum control, cooling and trapping of atoms and ions, low-temperature collision dynamics and the effects of electron correlation on structure and dynamics. Atomic physics is influenced by the nucleus (see, e.g., hyperfine splitting), but intra-nuclear phenomena such as fission and fusion are considered part of nuclear physics.\nMolecular physics focuses on multi-atomic structures and their internal and external interactions with matter and light. Optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm.\n\n\n==== Condensed matter physics ====\n\nCondensed matter physics is the field of physics that deals with the macroscopic physical properties of matter. In particular, it is concerned with the \"condensed\" phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong.The most familiar examples of condensed phases are solids and liquids, which arise from the bonding by way of the electromagnetic force between atoms. More exotic condensed phases include the superfluid and the Bose\u2013Einstein condensate found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials, and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices.Condensed matter physics is the largest field of contemporary physics. Historically, condensed matter physics grew out of solid-state physics, which is now considered one of its main subfields. The term condensed matter physics was apparently coined by Philip Anderson when he renamed his research group\u2014previously solid-state theory\u2014in 1967. In 1978, the Division of Solid State Physics of the American Physical Society was renamed as the Division of Condensed Matter Physics. Condensed matter physics has a large overlap with chemistry, materials science, nanotechnology and engineering.\n\n\n==== Astrophysics ====\n\nAstrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure, stellar evolution, the origin of the Solar System, and related problems of cosmology. Because astrophysics is a broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.The discovery by Karl Jansky in 1931 that radio signals were emitted by celestial bodies initiated the science of radio astronomy. Most recently, the frontiers of astronomy have been expanded by space exploration. Perturbations and interference from the earth's atmosphere make space-based observations necessary for infrared, ultraviolet, gamma-ray, and X-ray astronomy.\nPhysical cosmology is the study of the formation and evolution of the universe on its largest scales. Albert Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe and the Big Bang.\nThe Big Bang was confirmed by the success of Big Bang nucleosynthesis and the discovery of the cosmic microwave background in 1964. The Big Bang model rests on two theoretical pillars: Albert Einstein's general relativity and the cosmological principle. Cosmologists have recently established the \u039bCDM model of the evolution of the universe, which includes cosmic inflation, dark energy, and dark matter.\nNumerous possibilities and discoveries are anticipated to emerge from new data from the Fermi Gamma-ray Space Telescope over the upcoming decade and vastly revise or clarify existing models of the universe. In particular, the potential for a tremendous discovery surrounding dark matter is possible over the next several years. Fermi will search for evidence that dark matter is composed of weakly interacting massive particles, complementing similar experiments with the Large Hadron Collider and other underground detectors.\nIBEX is already yielding new astrophysical discoveries: \"No one knows what is creating the ENA (energetic neutral atoms) ribbon\" along the termination shock of the solar wind, \"but everyone agrees that it means the textbook picture of the heliosphere\u2014in which the Solar System's enveloping pocket filled with the solar wind's charged particles is plowing through the onrushing 'galactic wind' of the interstellar medium in the shape of a comet\u2014is wrong.\"\n\n\n== Current research ==\n\nResearch in physics is continually progressing on a large number of fronts.\nIn condensed matter physics, an important unsolved theoretical problem is that of high-temperature superconductivity. Many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers.In particle physics, the first pieces of experimental evidence for physics beyond the Standard Model have begun to appear. Foremost among these are indications that neutrinos have non-zero mass. These experimental results appear to have solved the long-standing solar neutrino problem, and the physics of massive neutrinos remains an area of active theoretical and experimental research. The Large Hadron Collider has already found the Higgs Boson, but future research aims to prove or disprove the supersymmetry, which extends the Standard Model of particle physics. Research on the nature of the major mysteries of dark matter and dark energy is also currently ongoing.Theoretical attempts to unify quantum mechanics and general relativity into a single theory of quantum gravity, a program ongoing for over half a century, have not yet been decisively resolved. The current leading candidates are M-theory, superstring theory and loop quantum gravity.\nMany astronomical and cosmological phenomena have yet to be satisfactorily explained, including the origin of ultra-high energy cosmic rays, the baryon asymmetry, the acceleration of the universe and the anomalous rotation rates of galaxies.\nAlthough much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity, chaos, or turbulence are still poorly understood. Complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved; examples include the formation of sandpiles, nodes in trickling water, the shape of water droplets, mechanisms of surface tension catastrophes, and self-sorting in shaken heterogeneous collections.These complex phenomena have received growing attention since the 1970s for several reasons, including the availability of modern mathematical methods and computers, which enabled complex systems to be modeled in new ways. Complex physics has become part of increasingly interdisciplinary research, as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems. In the 1932 Annual Review of Fluid Mechanics, Horace Lamb said:\nI am an old man now, and when I die and go to heaven there are two matters on which I hope for enlightenment. One is quantum electrodynamics, and the other is the turbulent motion of fluids. And about the former I am rather optimistic.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Sources ==\n\n\n== Further reading ==\nPeter Woit (January 2017). Fake Physics,\n\n\n== External links ==\nGeneral\n\nEncyclopedia of Physics at Scholarpedia\nde Haas, Paul, Historic Papers in Physics (20th Century) at the Wayback Machine (archived 26 August 2009)\nPhysicsCentral \u2013 Web portal run by the American Physical Society\nPhysics.org \u2013 Web portal run by the Institute of Physics\nThe Skeptic's Guide to Physics\nUsenet Physics FAQ \u2013 A FAQ compiled by sci.physics and other physics newsgroups\nWebsite of the Nobel Prize in physics\nWorld of Physics  An online encyclopedic dictionary of physics\nNature: Physics\nPhysics announced 17 July 2008 by the American Physical Society\nPhysics/Publications at Curlie (based on DMOZ)\nPhysicsworld.com \u2013 News website from Institute of Physics Publishing\nPhysics Central \u2013 includes articles on astronomy, particle physics, and mathematics.\nThe Vega Science Trust \u2013 science videos, including physics\nVideo: Physics \"Lightning\" Tour with Justin Morgan\n52-part video course: The Mechanical Universe...and Beyond Note: also available at 01 \u2013 Introduction at Google Videos\nHyperPhysics website \u2013 HyperPhysics, a physics and astronomy mind-map from Georgia State UniversityOrganizations\n\nAIP.org \u2013 Website of the American Institute of Physics\nAPS.org \u2013 Website of the American Physical Society\nIOP.org \u2013 Website of the Institute of Physics\nPlanetPhysics.org\nRoyal Society \u2013 Although not exclusively a physics institution, it has a strong history of physics\nSPS National \u2013 Website of the Society of Physics StudentsOnline course learning resources\n\nPHYSICS at MIT OCW-Website of MIT OpenCourseWare", "general relativity": "General relativity (GR, also known as the general theory of relativity or GTR) is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. General relativity generalizes special relativity and Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations, a system of partial differential equations.\nSome predictions of general relativity differ significantly from those of classical physics, especially concerning the passage of time, the geometry of space, the motion of bodies in free fall, and the propagation of light. Examples of such differences include gravitational time dilation, gravitational lensing, the gravitational redshift of light, and the gravitational time delay. The predictions of general relativity have been confirmed in all observations and experiments to date. Although general relativity is not the only relativistic theory of gravity, it is the simplest theory that is consistent with experimental data. However, unanswered questions remain, the most fundamental being how general relativity can be reconciled with the laws of quantum physics to produce a complete and self-consistent theory of quantum gravity.\nEinstein's theory has important astrophysical implications. For example, it implies the existence of black holes\u2014regions of space in which space and time are distorted in such a way that nothing, not even light, can escape\u2014as an end-state for massive stars. There is ample evidence that the intense radiation emitted by certain kinds of astronomical objects is due to black holes; for example, microquasars and active galactic nuclei result from the presence of stellar black holes and supermassive black holes, respectively. The bending of light by gravity can lead to the phenomenon of gravitational lensing, in which multiple images of the same distant astronomical object are visible in the sky. General relativity also predicts the existence of gravitational waves, which have since been observed directly by the physics collaboration LIGO. In addition, general relativity is the basis of current cosmological models of a consistently expanding universe.\nWidely acknowledged as a theory of extraordinary beauty, general relativity has often been described as the most beautiful of all existing physical theories.\n\n\n== History ==\n\nSoon after publishing the special theory of relativity in 1905, Einstein started thinking about how to incorporate gravity into his new relativistic framework. In 1907, beginning with a simple thought experiment involving an observer in free fall, he embarked on what would be an eight-year search for a relativistic theory of gravity. After numerous detours and false starts, his work culminated in the presentation to the Prussian Academy of Science in November 1915 of what are now known as the Einstein field equations. These equations specify how the geometry of space and time is influenced by whatever matter and radiation are present, and form the core of Einstein's general theory of relativity.The Einstein field equations are nonlinear and very difficult to solve. Einstein used approximation methods in working out initial predictions of the theory. But as early as 1916, the astrophysicist Karl Schwarzschild found the first non-trivial exact solution to the Einstein field equations, the Schwarzschild metric. This solution laid the groundwork for the description of the final stages of gravitational collapse, and the objects known today as black holes. In the same year, the first steps towards generalizing Schwarzschild's solution to electrically charged objects were taken, which eventually resulted in the Reissner\u2013Nordstr\u00f6m solution, now associated with electrically charged black holes. In 1917, Einstein applied his theory to the universe as a whole, initiating the field of relativistic cosmology. In line with contemporary thinking, he assumed a static universe, adding a new parameter to his original field equations\u2014the cosmological constant\u2014to match that observational presumption. By 1929, however, the work of Hubble and others had shown that our universe is expanding. This is readily described by the expanding cosmological solutions found by Friedmann in 1922, which do not require a cosmological constant. Lema\u00eetre used these solutions to formulate the earliest version of the Big Bang models, in which our universe has evolved from an extremely hot and dense earlier state. Einstein later declared the cosmological constant the biggest blunder of his life.During that period, general relativity remained something of a curiosity among physical theories. It was clearly superior to Newtonian gravity, being consistent with special relativity and accounting for several effects unexplained by the Newtonian theory. Einstein himself had shown in 1915 how his theory explained the anomalous perihelion advance of the planet Mercury without any arbitrary parameters (\"fudge factors\"). Similarly, a 1919 expedition led by Eddington confirmed general relativity's prediction for the deflection of starlight by the Sun during the total solar eclipse of May 29, 1919, making Einstein instantly famous. Yet the theory entered the mainstream of theoretical physics and astrophysics only with the developments between approximately 1960 and 1975, now known as the golden age of general relativity. Physicists began to understand the concept of a black hole, and to identify quasars as one of these objects' astrophysical manifestations. Ever more precise solar system tests confirmed the theory's predictive power, and relativistic cosmology, too, became amenable to direct observational tests.Over the years, general relativity has acquired a reputation as a theory of extraordinary beauty.  Subrahmanyan Chandrasekhar has noted that at multiple levels, general relativity exhibits what Frances Bacon has termed, a \"strangeness in the proportion\" (i.e. elements that excite wonderment and surprise). It juxtaposes fundamental concepts (space and time versus matter and motion) which had previously been considered as entirely independent. Chandrasekhar also noted that Einstein's only guides in his search for an exact theory were the principle of equivalence and his sense that a proper description of gravity should be geometrical at its basis, so that there was an \"element of revelation\" in the manner in which Einstein arrived at his theory. Other elements of beauty associated with the general theory of relativity are its simplicity, symmetry, the manner in which it incorporates invariance and unification, and its perfect logical consistency.\n\n\n== From classical mechanics to general relativity ==\nGeneral relativity can be understood by examining its similarities with and departures from classical physics. The first step is the realization that classical mechanics and Newton's law of gravity admit a geometric description. The combination of this description with the laws of special relativity results in a heuristic derivation of general relativity.\n\n\n=== Geometry of Newtonian gravity ===\n\nAt the base of classical mechanics is the notion that a body's motion can be described as a combination of free (or inertial) motion, and deviations from this free motion. Such deviations are caused by external forces acting on a body in accordance with Newton's second law of motion, which states that the net force acting on a body is equal to that body's (inertial) mass multiplied by its acceleration. The preferred inertial motions are related to the geometry of space and time: in the standard reference frames of classical mechanics, objects in free motion move along straight lines at constant speed. In modern parlance, their paths are geodesics, straight world lines in curved spacetime.Conversely, one might expect that inertial motions, once identified by observing the actual motions of bodies and making allowances for the external forces (such as electromagnetism or friction), can be used to define the geometry of space, as well as a time coordinate. However, there is an ambiguity once gravity comes into play. According to Newton's law of gravity, and independently verified by experiments such as that of E\u00f6tv\u00f6s and its successors (see E\u00f6tv\u00f6s experiment), there is a universality of free fall (also known as the weak equivalence principle, or the universal equality of inertial and passive-gravitational mass): the trajectory of a test body in free fall depends only on its position and initial speed, but not on any of its material properties. A simplified version of this is embodied in Einstein's elevator experiment, illustrated in the figure on the right: for an observer in a small enclosed room, it is impossible to decide, by mapping the trajectory of bodies such as a dropped ball, whether the room is at rest in a gravitational field, or in free space aboard a rocket that is accelerating at a rate equal to that of the gravitational field.Given the universality of free fall, there is no observable distinction between inertial motion and motion under the influence of the gravitational force. This suggests the definition of a new class of inertial motion, namely that of objects in free fall under the influence of gravity. This new class of preferred motions, too, defines a geometry of space and time\u2014in mathematical terms, it is the geodesic motion associated with a specific connection which depends on the gradient of the gravitational potential. Space, in this construction, still has the ordinary Euclidean geometry. However, spacetime as a whole is more complicated. As can be shown using simple thought experiments following the free-fall trajectories of different test particles, the result of transporting spacetime vectors that can denote a particle's velocity (time-like vectors) will vary with the particle's trajectory; mathematically speaking, the Newtonian connection is not integrable. From this, one can deduce that spacetime is curved. The resulting Newton\u2013Cartan theory is a geometric formulation of Newtonian gravity using only covariant concepts, i.e. a description which is valid in any desired coordinate system. In this geometric description, tidal effects\u2014the relative acceleration of bodies in free fall\u2014are related to the derivative of the connection, showing how the modified geometry is caused by the presence of mass.\n\n\n=== Relativistic generalization ===\n\nAs intriguing as geometric Newtonian gravity may be, its basis, classical mechanics, is merely a limiting case of (special) relativistic mechanics. In the language of symmetry: where gravity can be neglected, physics is Lorentz invariant as in special relativity rather than Galilei invariant as in classical mechanics. (The defining symmetry of special relativity is the Poincar\u00e9 group, which includes translations, rotations and boosts.) The differences between the two become significant when dealing with speeds approaching the speed of light, and with high-energy phenomena.With Lorentz symmetry, additional structures come into play. They are defined by the set of light cones (see image). The light-cones define a causal structure: for each event A, there is a set of events that can, in principle, either influence or be influenced by A via signals or interactions that do not need to travel faster than light (such as event B in the image), and a set of events for which such an influence is impossible (such as event C in the image). These sets are observer-independent. In conjunction with the world-lines of freely falling particles, the light-cones can be used to reconstruct the space\u2013time's semi-Riemannian metric, at least up to a positive scalar factor. In mathematical terms, this defines a conformal structure or conformal geometry.\nSpecial relativity is defined in the absence of gravity, so for practical applications, it is a suitable model whenever gravity can be neglected. Bringing gravity into play, and assuming the universality of free fall, an analogous reasoning as in the previous section applies: there are no global inertial frames. Instead there are approximate inertial frames moving alongside freely falling particles. Translated into the language of spacetime: the straight time-like lines that define a gravity-free inertial frame are deformed to lines that are curved relative to each other, suggesting that the inclusion of gravity necessitates a change in spacetime geometry.A priori, it is not clear whether the new local frames in free fall coincide with the reference frames in which the laws of special relativity hold\u2014that theory is based on the propagation of light, and thus on electromagnetism, which could have a different set of preferred frames. But using different assumptions about the special-relativistic frames (such as their being earth-fixed, or in free fall), one can derive different predictions for the gravitational redshift, that is, the way in which the frequency of light shifts as the light propagates through a gravitational field (cf. below). The actual measurements show that free-falling frames are the ones in which light propagates as it does in special relativity. The generalization of this statement, namely that the laws of special relativity hold to good approximation in freely falling (and non-rotating) reference frames, is known as the Einstein equivalence principle, a crucial guiding principle for generalizing special-relativistic physics to include gravity.The same experimental data shows that time as measured by clocks in a gravitational field\u2014proper time, to give the technical term\u2014does not follow the rules of special relativity. In the language of spacetime geometry, it is not measured by the Minkowski metric. As in the Newtonian case, this is suggestive of a more general geometry. At small scales, all reference frames that are in free fall are equivalent, and approximately Minkowskian. Consequently, we are now dealing with a curved generalization of Minkowski space. The metric tensor that defines the geometry\u2014in particular, how lengths and angles are measured\u2014is not the Minkowski metric of special relativity, it is a generalization known as a semi- or pseudo-Riemannian metric. Furthermore, each Riemannian metric is naturally associated with one particular kind of connection, the Levi-Civita connection, and this is, in fact, the connection that satisfies the equivalence principle and makes space locally Minkowskian (that is, in suitable locally inertial coordinates, the metric is Minkowskian, and its first partial derivatives and the connection coefficients vanish).\n\n\n=== Einstein's equations ===\n\nHaving formulated the relativistic, geometric version of the effects of gravity, the question of gravity's source remains. In Newtonian gravity, the source is mass. In special relativity, mass turns out to be part of a more general quantity called the energy\u2013momentum tensor, which includes both energy and momentum densities as well as stress: pressure and shear. Using the equivalence principle, this tensor is readily generalized to curved spacetime. Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume for a small cloud of test particles that are initially at rest, and then fall freely. In special relativity, conservation of energy\u2013momentum corresponds to the statement that the energy\u2013momentum tensor is divergence-free. This formula, too, is readily generalized to curved spacetime by replacing partial derivatives with their curved-manifold counterparts, covariant derivatives studied in differential geometry. With this additional condition\u2014the covariant divergence of the energy\u2013momentum tensor, and hence of whatever is on the other side of the equation, is zero\u2014 the simplest set of equations are what are called Einstein's (field) equations:\n\nOn the left-hand side is the Einstein tensor, a specific divergence-free combination of the Ricci tensor \n  \n    \n      \n        \n          R\n          \n            \u03bc\n            \u03bd\n          \n        \n      \n    \n    {\\displaystyle R_{\\mu \\nu }}\n   and the metric. Where \n  \n    \n      \n        \n          G\n          \n            \u03bc\n            \u03bd\n          \n        \n      \n    \n    {\\displaystyle G_{\\mu \\nu }}\n   is symmetric. In particular,\n\n  \n    \n      \n        R\n        =\n        \n          g\n          \n            \u03bc\n            \u03bd\n          \n        \n        \n          R\n          \n            \u03bc\n            \u03bd\n          \n        \n        \n      \n    \n    {\\displaystyle R=g^{\\mu \\nu }R_{\\mu \\nu }\\,}\n  is the curvature scalar. The Ricci tensor itself is related to the more general Riemann curvature tensor as\n\n  \n    \n      \n        \n          R\n          \n            \u03bc\n            \u03bd\n          \n        \n        =\n        \n          \n            \n              R\n              \n                \u03b1\n              \n            \n          \n          \n            \u03bc\n            \u03b1\n            \u03bd\n          \n        \n        .\n        \n      \n    \n    {\\displaystyle R_{\\mu \\nu }={R^{\\alpha }}_{\\mu \\alpha \\nu }.\\,}\n  On the right-hand side, \n  \n    \n      \n        \n          T\n          \n            \u03bc\n            \u03bd\n          \n        \n      \n    \n    {\\displaystyle T_{\\mu \\nu }}\n   is the energy\u2013momentum tensor. All tensors are written in abstract index notation. Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as \u03ba = 8\u03c0G/c4, with G the gravitational constant and c the speed of light. When there is no matter present, so that the energy\u2013momentum tensor vanishes, the results are the vacuum Einstein equations,\n\n  \n    \n      \n        \n          R\n          \n            \u03bc\n            \u03bd\n          \n        \n        =\n        0.\n        \n      \n    \n    {\\displaystyle R_{\\mu \\nu }=0.\\,}\n  \n\n\n=== Alternatives to general relativity ===\n\nThere are alternatives to general relativity built upon the same premises, which include additional rules and/or constraints, leading to different field equations. Examples are Whitehead's theory, Brans\u2013Dicke theory, teleparallelism, f(R) gravity and Einstein\u2013Cartan theory.\n\n\n== Definition and basic applications ==\n\nThe derivation outlined in the previous section contains all the information needed to define general relativity, describe its key properties, and address a question of crucial importance in physics, namely how the theory can be used for model-building.\n\n\n=== Definition and basic properties ===\nGeneral relativity is a metric theory of gravitation. At its core are Einstein's equations, which describe the relation between the geometry of a four-dimensional, pseudo-Riemannian manifold representing spacetime, and the energy\u2013momentum contained in that spacetime. Phenomena that in classical mechanics are ascribed to the action of the force of gravity (such as free-fall, orbital motion, and spacecraft trajectories), correspond to inertial motion within a curved geometry of spacetime in general relativity; there is no gravitational force deflecting objects from their natural, straight paths. Instead, gravity corresponds to changes in the properties of space and time, which in turn changes the straightest-possible paths that objects will naturally follow. The curvature is, in turn, caused by the energy\u2013momentum of matter. Paraphrasing the relativist John Archibald Wheeler, spacetime tells matter how to move; matter tells spacetime how to curve.While general relativity replaces the scalar gravitational potential of classical physics by a symmetric rank-two tensor, the latter reduces to the former in certain limiting cases. For weak gravitational fields and slow speed relative to the speed of light, the theory's predictions converge on those of Newton's law of universal gravitation.As it is constructed using tensors, general relativity exhibits general covariance: its laws\u2014and further laws formulated within the general relativistic framework\u2014take on the same form in all coordinate systems. Furthermore, the theory does not contain any invariant geometric background structures, i.e. it is background independent. It thus satisfies a more stringent general principle of relativity, namely that the laws of physics are the same for all observers. Locally, as expressed in the equivalence principle, spacetime is Minkowskian, and the laws of physics exhibit local Lorentz invariance.\n\n\n=== Model-building ===\nThe core concept of general-relativistic model-building is that of a solution of Einstein's equations. Given both Einstein's equations and suitable equations for the properties of matter, such a solution consists of a specific semi-Riemannian manifold (usually defined by giving the metric in specific coordinates), and specific matter fields defined on that manifold. Matter and geometry must satisfy Einstein's equations, so in particular, the matter's energy\u2013momentum tensor must be divergence-free. The matter must, of course, also satisfy whatever additional equations were imposed on its properties. In short, such a solution is a model universe that satisfies the laws of general relativity, and possibly additional laws governing whatever matter might be present.Einstein's equations are nonlinear partial differential equations and, as such, difficult to solve exactly. Nevertheless, a number of exact solutions are known, although only a few have direct physical applications. The best-known exact solutions, and also those most interesting from a physics point of view, are the Schwarzschild solution, the Reissner\u2013Nordstr\u00f6m solution and the Kerr metric, each corresponding to a certain type of black hole in an otherwise empty universe, and the Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker and de Sitter universes, each describing an expanding cosmos. Exact solutions of great theoretical interest include the G\u00f6del universe (which opens up the intriguing possibility of time travel in curved spacetimes), the Taub-NUT solution (a model universe that is homogeneous, but anisotropic), and anti-de Sitter space (which has recently come to prominence in the context of what is called the Maldacena conjecture).Given the difficulty of finding exact solutions, Einstein's field equations are also solved frequently by numerical integration on a computer, or by considering small perturbations of exact solutions. In the field of numerical relativity, powerful computers are employed to simulate the geometry of spacetime and to solve Einstein's equations for interesting situations such as two colliding black holes. In principle, such methods may be applied to any system, given sufficient computer resources, and may address fundamental questions such as naked singularities. Approximate solutions may also be found by perturbation theories such as linearized gravity and its generalization, the post-Newtonian expansion, both of which were developed by Einstein. The latter provides a systematic approach to solving for the geometry of a spacetime that contains a distribution of matter that moves slowly compared with the speed of light. The expansion involves a series of terms; the first terms represent Newtonian gravity, whereas the later terms represent ever smaller corrections to Newton's theory due to general relativity. An extension of this expansion is the parametrized post-Newtonian (PPN) formalism, which allows quantitative comparisons between the predictions of general relativity and alternative theories.\n\n\n== Consequences of Einstein's theory ==\nGeneral relativity has a number of physical consequences. Some follow directly from the theory's axioms, whereas others have become clear only in the course of many years of research that followed Einstein's initial publication.\n\n\n=== Gravitational time dilation and frequency shift ===\n\nAssuming that the equivalence principle holds, gravity influences the passage of time. Light sent down into a gravity well is blueshifted, whereas light sent in the opposite direction (i.e., climbing out of the gravity well) is redshifted; collectively, these two effects are known as the gravitational frequency shift. More generally, processes close to a massive body run more slowly when compared with processes taking place farther away; this effect is known as gravitational time dilation.Gravitational redshift has been measured in the laboratory and using astronomical observations. Gravitational time dilation in the Earth's gravitational field has been measured numerous times using atomic clocks, while ongoing validation is provided as a side effect of the operation of the Global Positioning System (GPS). Tests in stronger gravitational fields are provided by the observation of binary pulsars. All results are in agreement with general relativity. However, at the current level of accuracy, these observations cannot distinguish between general relativity and other theories in which the equivalence principle is valid.\n\n\n=== Light deflection and gravitational time delay ===\n\nGeneral relativity predicts that the path of light will follow the curvature of spacetime as it passes near a star. This effect was initially confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.This and related predictions follow from the fact that light follows what is called a light-like or null geodesic\u2014a generalization of the straight lines along which light travels in classical physics. Such geodesics are the generalization of the invariance of lightspeed in special relativity. As one examines suitable model spacetimes (either the exterior Schwarzschild solution or, for more than a single mass, the post-Newtonian expansion), several effects of gravity on light propagation emerge. Although the bending of light can also be derived by extending the universality of free fall to light, the angle of deflection resulting from such calculations is only half the value given by general relativity.Closely related to light deflection is the gravitational time delay (or Shapiro delay), the phenomenon that light signals take longer to move through a gravitational field than they would in the absence of that field. There have been numerous successful tests of this prediction. In the parameterized post-Newtonian formalism (PPN), measurements of both the deflection of light and the gravitational time delay determine a parameter called \u03b3, which encodes the influence of gravity on the geometry of space.\n\n\n=== Gravitational waves ===\n\nPredicted in 1916 by Albert Einstein,  there are gravitational waves: ripples in the metric of spacetime that propagate at the speed of light. These are one of several analogies between weak-field gravity and electromagnetism in that, they are analogous to electromagnetic waves. On February 11, 2016, the Advanced LIGO team announced that they had directly detected gravitational waves from a pair of black holes merging.The simplest type of such a wave can be visualized by its action on a ring of freely floating particles. A sine wave propagating through such a ring towards the reader distorts the ring in a characteristic, rhythmic fashion (animated image to the right). Since Einstein's equations are non-linear, arbitrarily strong gravitational waves do not obey linear superposition, making their description difficult. However, for weak fields, a linear approximation can be made. Such linearized gravitational waves are sufficiently accurate to describe the exceedingly weak waves that are expected to arrive here on Earth from far-off cosmic events, which typically result in relative distances increasing and decreasing by \n  \n    \n      \n        \n          10\n          \n            \u2212\n            21\n          \n        \n      \n    \n    {\\displaystyle 10^{-21}}\n   or less. Data analysis methods routinely make use of the fact that these linearized waves can be Fourier decomposed.Some exact solutions describe gravitational waves without any approximation, e.g., a wave train traveling through empty space or Gowdy universes, varieties of an expanding cosmos filled with gravitational waves. But for gravitational waves produced in astrophysically relevant situations, such as the merger of two black holes, numerical methods are presently the only way to construct appropriate models.\n\n\n=== Orbital effects and the relativity of direction ===\n\nGeneral relativity differs from classical mechanics in a number of predictions concerning orbiting bodies. It predicts an overall rotation (precession) of planetary orbits, as well as orbital decay caused by the emission of gravitational waves and effects related to the relativity of direction.\n\n\n==== Precession of apsides ====\n\nIn general relativity, the apsides of any orbit (the point of the orbiting body's closest approach to the system's center of mass) will precess; the orbit is not an ellipse, but akin to an ellipse that rotates on its focus, resulting in a rose curve-like shape (see image). Einstein first derived this result by using an approximate metric representing the Newtonian limit and treating the orbiting body as a test particle. For him, the fact that his theory gave a straightforward explanation of Mercury's anomalous perihelion shift, discovered earlier by Urbain Le Verrier in 1859, was important evidence that he had at last identified the correct form of the gravitational field equations.The effect can also be derived by using either the exact Schwarzschild metric (describing spacetime around a spherical mass) or the much more general post-Newtonian formalism. It is due to the influence of gravity on the geometry of space and to the contribution of self-energy to a body's gravity (encoded in the nonlinearity of Einstein's equations). Relativistic precession has been observed for all planets that allow for accurate precession measurements (Mercury, Venus, and Earth), as well as in binary pulsar systems, where it is larger by five orders of magnitude.In general relativity the perihelion shift \u03c3, expressed in radians per revolution, is approximately given by:\n\n  \n    \n      \n        \u03c3\n        =\n        \n          \n            \n              24\n              \n                \u03c0\n                \n                  3\n                \n              \n              \n                L\n                \n                  2\n                \n              \n            \n            \n              \n                T\n                \n                  2\n                \n              \n              \n                c\n                \n                  2\n                \n              \n              (\n              1\n              \u2212\n              \n                e\n                \n                  2\n                \n              \n              )\n            \n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle \\sigma ={\\frac {24\\pi ^{3}L^{2}}{T^{2}c^{2}(1-e^{2})}}\\ ,}\n  where:\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   is the semi-major axis\n\n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n   is the orbital period\n\n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n   is the speed of light\n\n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n   is the orbital eccentricity\n\n\n==== Orbital decay ====\n\nAccording to general relativity, a binary system will emit gravitational waves, thereby losing energy. Due to this loss, the distance between the two orbiting bodies decreases, and so does their orbital period. Within the Solar System or for ordinary double stars, the effect is too small to be observable. This is not the case for a close binary pulsar, a system of two orbiting neutron stars, one of which is a pulsar: from the pulsar, observers on Earth receive a regular series of radio pulses that can serve as a highly accurate clock, which allows precise measurements of the orbital period. Because neutron stars are immensely compact, significant amounts of energy are emitted in the form of gravitational radiation.The first observation of a decrease in orbital period due to the emission of gravitational waves was made by Hulse and Taylor, using the binary pulsar PSR1913+16 they had discovered in 1974. This was the first detection of gravitational waves, albeit indirect, for which they were awarded the 1993 Nobel Prize in physics. Since then, several other binary pulsars have been found, in particular the double pulsar PSR J0737-3039, in which both stars are pulsars.\n\n\n==== Geodetic precession and frame-dragging ====\n\nSeveral relativistic effects are directly related to the relativity of direction. One is geodetic precession: the axis direction of a gyroscope in free fall in curved spacetime will change when compared, for instance, with the direction of light received from distant stars\u2014even though such a gyroscope represents the way of keeping a direction as stable as possible (\"parallel transport\"). For the Moon\u2013Earth system, this effect has been measured with the help of lunar laser ranging. More recently, it has been measured for test masses aboard the satellite Gravity Probe B to a precision of better than 0.3%.Near a rotating mass, there are gravitomagnetic or frame-dragging effects. A distant observer will determine that objects close to the mass get \"dragged around\". This is most extreme for rotating black holes where, for any object entering a zone known as the ergosphere, rotation is inevitable. Such effects can again be tested through their influence on the orientation of gyroscopes in free fall. Somewhat controversial tests have been performed using the LAGEOS satellites, confirming the relativistic prediction. Also the Mars Global Surveyor probe around Mars has been used.\n\n\n== Astrophysical applications ==\n\n\n=== Gravitational lensing ===\n\nThe deflection of light by gravity is responsible for a new class of astronomical phenomena. If a massive object is situated between the astronomer and a distant target object with appropriate mass and relative distances, the astronomer will see multiple distorted images of the target. Such effects are known as gravitational lensing. Depending on the configuration, scale, and mass distribution, there can be two or more images, a bright ring known as an Einstein ring, or partial rings called arcs.\nThe earliest example was discovered in 1979; since then, more than a hundred gravitational lenses have been observed. Even if the multiple images are too close to each other to be resolved, the effect can still be measured, e.g., as an overall brightening of the target object; a number of such \"microlensing events\" have been observed.Gravitational lensing has developed into a tool of observational astronomy. It is used to detect the presence and distribution of dark matter, provide a \"natural telescope\" for observing distant galaxies, and to obtain an independent estimate of the Hubble constant. Statistical evaluations of lensing data provide valuable insight into the structural evolution of galaxies.\n\n\n=== Gravitational wave astronomy ===\n\nObservations of binary pulsars provide strong indirect evidence for the existence of gravitational waves (see Orbital decay, above). Detection of these waves is a major goal of current relativity-related research. Several land-based gravitational wave detectors are currently in operation, most notably the interferometric detectors GEO 600, LIGO (two detectors), TAMA 300 and VIRGO. Various pulsar timing arrays are using millisecond pulsars to detect gravitational waves in the 10\u22129 to 10\u22126 Hertz frequency range, which originate from binary supermassive blackholes. A European space-based detector, eLISA / NGO, is currently under development, with a precursor mission (LISA Pathfinder) having launched in December 2015.Observations of gravitational waves promise to complement observations in the electromagnetic spectrum. They are expected to yield information about black holes and other dense objects such as neutron stars and white dwarfs, about certain kinds of supernova implosions, and about processes in the very early universe, including the signature of certain types of hypothetical cosmic string. In February 2016, the Advanced LIGO team announced that they had detected gravitational waves from a black hole merger.\n\n\n=== Black holes and other compact objects ===\n\nWhenever the ratio of an object's mass to its radius becomes sufficiently large, general relativity predicts the formation of a black hole, a region of space from which nothing, not even light, can escape. In the currently accepted models of stellar evolution, neutron stars of around 1.4 solar masses, and stellar black holes with a few to a few dozen solar masses, are thought to be the final state for the evolution of massive stars. Usually a galaxy has one supermassive black hole with a few million to a few billion solar masses in its center, and its presence is thought to have played an important role in the formation of the galaxy and larger cosmic structures.\n\nAstronomically, the most important property of compact objects is that they provide a supremely efficient mechanism for converting gravitational energy into electromagnetic radiation. Accretion, the falling of dust or gaseous matter onto stellar or supermassive black holes, is thought to be responsible for some spectacularly luminous astronomical objects, notably diverse kinds of active galactic nuclei on galactic scales and stellar-size objects such as microquasars. In particular, accretion can lead to relativistic jets, focused beams of highly energetic particles that are being flung into space at almost light speed.\nGeneral relativity plays a central role in modelling all these phenomena, and observations provide strong evidence for the existence of black holes with the properties predicted by the theory.Black holes are also sought-after targets in the search for gravitational waves (cf. Gravitational waves, above). Merging black hole binaries should lead to some of the strongest gravitational wave signals reaching detectors here on Earth, and the phase directly before the merger (\"chirp\") could be used as a \"standard candle\" to deduce the distance to the merger events\u2013and hence serve as a probe of cosmic expansion at large distances. The gravitational waves produced as a stellar black hole plunges into a supermassive one should provide direct information about the supermassive black hole's geometry.\n\n\n=== Cosmology ===\n\nThe current models of cosmology are based on Einstein's field equations, which include the cosmological constant \u039b since it has important influence on the large-scale dynamics of the cosmos,\n\n  \n    \n      \n        \n          R\n          \n            \u03bc\n            \u03bd\n          \n        \n        \u2212\n        \n          \n            \n              1\n            \n            2\n          \n        \n        R\n        \n        \n          g\n          \n            \u03bc\n            \u03bd\n          \n        \n        +\n        \u039b\n         \n        \n          g\n          \n            \u03bc\n            \u03bd\n          \n        \n        =\n        \n          \n            \n              8\n              \u03c0\n              G\n            \n            \n              c\n              \n                4\n              \n            \n          \n        \n        \n        \n          T\n          \n            \u03bc\n            \u03bd\n          \n        \n      \n    \n    {\\displaystyle R_{\\mu \\nu }-{\\textstyle 1 \\over 2}R\\,g_{\\mu \\nu }+\\Lambda \\ g_{\\mu \\nu }={\\frac {8\\pi G}{c^{4}}}\\,T_{\\mu \\nu }}\n  where \n  \n    \n      \n        \n          g\n          \n            \u03bc\n            \u03bd\n          \n        \n      \n    \n    {\\displaystyle g_{\\mu \\nu }}\n   is the spacetime metric. Isotropic and homogeneous solutions of these enhanced equations, the Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker solutions, allow physicists to model a universe that has evolved over the past 14 billion years from a hot, early Big Bang phase. Once a small number of parameters (for example the universe's mean matter density) have been fixed by astronomical observation, further observational data can be used to put the models to the test. Predictions, all successful, include the initial abundance of chemical elements formed in a period of primordial nucleosynthesis, the large-scale structure of the universe, and the existence and properties of a \"thermal echo\" from the early cosmos, the cosmic background radiation.Astronomical observations of the cosmological expansion rate allow the total amount of matter in the universe to be estimated, although the nature of that matter remains mysterious in part. About 90% of all matter appears to be dark matter, which has mass (or, equivalently, gravitational influence), but does not interact electromagnetically and, hence, cannot be observed directly. There is no generally accepted description of this new kind of matter, within the framework of known particle physics or otherwise. Observational evidence from redshift surveys of distant supernovae and measurements of the cosmic background radiation also show that the evolution of our universe is significantly influenced by a cosmological constant resulting in an acceleration of cosmic expansion or, equivalently, by a form of energy with an unusual equation of state, known as dark energy, the nature of which remains unclear.An inflationary phase, an additional phase of strongly accelerated expansion at cosmic times of around 10\u221233 seconds, was hypothesized in 1980 to account for several puzzling observations that were unexplained by classical cosmological models, such as the nearly perfect homogeneity of the cosmic background radiation. Recent measurements of the cosmic background radiation have resulted in the first evidence for this scenario. However, there is a bewildering variety of possible inflationary scenarios, which cannot be restricted by current observations. An even larger question is the physics of the earliest universe, prior to the inflationary phase and close to where the classical models predict the big bang singularity. An authoritative answer would require a complete theory of quantum gravity, which has not yet been developed (cf. the section on quantum gravity, below).\n\n\n=== Time travel ===\nKurt G\u00f6del showed that solutions to Einstein's equations exist that contain closed timelike curves (CTCs), which allow for loops in time.   The solutions require extreme physical conditions unlikely ever to occur in practice, and it remains an open question whether further laws of physics will eliminate them completely.  Since then, other\u2014similarly impractical\u2014GR solutions containing CTCs have been found, such as the Tipler cylinder and traversable wormholes.\n\n\n== Advanced concepts ==\n\n\n=== Causal structure and global geometry ===\n\nIn general relativity, no material body can catch up with or overtake a light pulse. No influence from an event A can reach any other location X before light sent out at A to X. In consequence, an exploration of all light worldlines (null geodesics) yields key information about the spacetime's causal structure. This structure can be displayed using Penrose\u2013Carter diagrams in which infinitely large regions of space and infinite time intervals are shrunk (\"compactified\") so as to fit onto a finite map, while light still travels along diagonals as in standard spacetime diagrams.Aware of the importance of causal structure, Roger Penrose and others developed what is known as global geometry. In global geometry, the object of study is not one particular solution (or family of solutions) to Einstein's equations. Rather, relations that hold true for all geodesics, such as the Raychaudhuri equation, and additional non-specific assumptions about the nature of matter (usually in the form of energy conditions) are used to derive general results.\n\n\n=== Horizons ===\n\nUsing global geometry, some spacetimes can be shown to contain boundaries called horizons, which demarcate one region from the rest of spacetime. The best-known examples are black holes: if mass is compressed into a sufficiently compact region of space (as specified in the hoop conjecture, the relevant length scale is the Schwarzschild radius), no light from inside can escape to the outside. Since no object can overtake a light pulse, all interior matter is imprisoned as well. Passage from the exterior to the interior is still possible, showing that the boundary, the black hole's horizon, is not a physical barrier.\n\nEarly studies of black holes relied on explicit solutions of Einstein's equations, notably the spherically symmetric Schwarzschild solution (used to describe a static black hole) and the axisymmetric Kerr solution (used to describe a rotating, stationary black hole, and introducing interesting features such as the ergosphere). Using global geometry, later studies have revealed more general properties of black holes. In the long run, they are rather simple objects characterized by eleven parameters specifying energy, linear momentum, angular momentum, location at a specified time and electric charge. This is stated by the black hole uniqueness theorems: \"black holes have no hair\", that is, no distinguishing marks like the hairstyles of humans. Irrespective of the complexity of a gravitating object collapsing to form a black hole, the object that results (having emitted gravitational waves) is very simple.Even more remarkably, there is a general set of laws known as black hole mechanics, which is analogous to the laws of thermodynamics. For instance, by the second law of black hole mechanics, the area of the event horizon of a general black hole will never decrease with time, analogous to the entropy of a thermodynamic system. This limits the energy that can be extracted by classical means from a rotating black hole (e.g. by the Penrose process). There is strong evidence that the laws of black hole mechanics are, in fact, a subset of the laws of thermodynamics, and that the black hole area is proportional to its entropy. This leads to a modification of the original laws of black hole mechanics: for instance, as the second law of black hole mechanics becomes part of the second law of thermodynamics, it is possible for black hole area to decrease\u2014as long as other processes ensure that, overall, entropy increases. As thermodynamical objects with non-zero temperature, black holes should emit thermal radiation. Semi-classical calculations indicate that indeed they do, with the surface gravity playing the role of temperature in Planck's law. This radiation is known as Hawking radiation (cf. the quantum theory section, below).There are other types of horizons. In an expanding universe, an observer may find that some regions of the past cannot be observed (\"particle horizon\"), and some regions of the future cannot be influenced (event horizon). Even in flat Minkowski space, when described by an accelerated observer (Rindler space), there will be horizons associated with a semi-classical radiation known as Unruh radiation.\n\n\n=== Singularities ===\n\nAnother general feature of general relativity is the appearance of spacetime boundaries known as singularities. Spacetime can be explored by following up on timelike and lightlike geodesics\u2014all possible ways that light and particles in free fall can travel. But some solutions of Einstein's equations have \"ragged edges\"\u2014regions known as spacetime singularities, where the paths of light and falling particles come to an abrupt end, and geometry becomes ill-defined. In the more interesting cases, these are \"curvature singularities\", where geometrical quantities characterizing spacetime curvature, such as the Ricci scalar, take on infinite values. Well-known examples of spacetimes with future singularities\u2014where worldlines end\u2014are the Schwarzschild solution, which describes a singularity inside an eternal static black hole, or the Kerr solution with its ring-shaped singularity inside an eternal rotating black hole. The Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker solutions and other spacetimes describing universes have past singularities on which worldlines begin, namely Big Bang singularities, and some have future singularities (Big Crunch) as well.Given that these examples are all highly symmetric\u2014and thus simplified\u2014it is tempting to conclude that the occurrence of singularities is an artifact of idealization. The famous singularity theorems, proved using the methods of global geometry, say otherwise: singularities are a generic feature of general relativity, and unavoidable once the collapse of an object with realistic matter properties has proceeded beyond a certain stage and also at the beginning of a wide class of expanding universes. However, the theorems say little about the properties of singularities, and much of current research is devoted to characterizing these entities' generic structure (hypothesized e.g. by the BKL conjecture). The cosmic censorship hypothesis states that all realistic future singularities (no perfect symmetries, matter with realistic properties) are safely hidden away behind a horizon, and thus invisible to all distant observers. While no formal proof yet exists, numerical simulations offer supporting evidence of its validity.\n\n\n=== Evolution equations ===\n\nEach solution of Einstein's equation encompasses the whole history of a universe \u2014 it is not just some snapshot of how things are, but a whole, possibly matter-filled, spacetime. It describes the state of matter and geometry everywhere and at every moment in that particular universe. Due to its general covariance, Einstein's theory is not sufficient by itself to determine the time evolution of the metric tensor. It must be combined with a coordinate condition, which is analogous to gauge fixing in other field theories.To understand Einstein's equations as partial differential equations, it is helpful to formulate them in a way that describes the evolution of the universe over time. This is done in \"3+1\" formulations, where spacetime is split into three space dimensions and one time dimension. The best-known example is the ADM formalism. These decompositions show that the spacetime evolution equations of general relativity are well-behaved: solutions always exist, and are uniquely defined, once suitable initial conditions have been specified. Such formulations of Einstein's field equations are the basis of numerical relativity.\n\n\n=== Global and quasi-local quantities ===\n\nThe notion of evolution equations is intimately tied in with another aspect of general relativistic physics. In Einstein's theory, it turns out to be impossible to find a general definition for a seemingly simple property such as a system's total mass (or energy). The main reason is that the gravitational field\u2014like any physical field\u2014must be ascribed a certain energy, but that it proves to be fundamentally impossible to localize that energy.Nevertheless, there are possibilities to define a system's total mass, either using a hypothetical \"infinitely distant observer\" (ADM mass) or suitable symmetries (Komar mass). If one excludes from the system's total mass the energy being carried away to infinity by gravitational waves, the result is the Bondi mass at null infinity. Just as in classical physics, it can be shown that these masses are positive. Corresponding global definitions exist for momentum and angular momentum. There have also been a number of attempts to define quasi-local quantities, such as the mass of an isolated system formulated using only quantities defined within a finite region of space containing that system. The hope is to obtain a quantity useful for general statements about isolated systems, such as a more precise formulation of the hoop conjecture.\n\n\n== Relationship with quantum theory ==\nIf general relativity were considered to be one of the two pillars of modern physics, then quantum theory, the basis of understanding matter from elementary particles to solid state physics, would be the other. However, how to reconcile quantum theory with general relativity is still an open question.\n\n\n=== Quantum field theory in curved spacetime ===\n\nOrdinary quantum field theories, which form the basis of modern elementary particle physics, are defined in flat Minkowski space, which is an excellent approximation when it comes to describing the behavior of microscopic particles in weak gravitational fields like those found on Earth. In order to describe situations in which gravity is strong enough to influence (quantum) matter, yet not strong enough to require quantization itself, physicists have formulated quantum field theories in curved spacetime. These theories rely on general relativity to describe a curved background spacetime, and define a generalized quantum field theory to describe the behavior of quantum matter within that spacetime. Using this formalism, it can be shown that black holes emit a blackbody spectrum of particles known as Hawking radiation leading to the possibility that they evaporate over time. As briefly mentioned above, this radiation plays an important role for the thermodynamics of black holes.\n\n\n=== Quantum gravity ===\n\nThe demand for consistency between a quantum description of matter and a geometric description of spacetime, as well as the appearance of singularities (where curvature length scales become microscopic), indicate the need for a full theory of quantum gravity: for an adequate description of the interior of black holes, and of the very early universe, a theory is required in which gravity and the associated geometry of spacetime are described in the language of quantum physics. Despite major efforts, no complete and consistent theory of quantum gravity is currently known, even though a number of promising candidates exist.\n\nAttempts to generalize ordinary quantum field theories, used in elementary particle physics to describe fundamental interactions, so as to include gravity have led to serious problems. Some have argued that at low energies, this approach proves successful, in that it results in an acceptable effective (quantum) field theory of gravity. At very high energies, however, the perturbative results are badly divergent and lead to models devoid of predictive power (\"perturbative non-renormalizability\").\n\nOne attempt to overcome these limitations is string theory, a quantum theory not of point particles, but of minute one-dimensional extended objects. The theory promises to be a unified description of all particles and interactions, including gravity; the price to pay is unusual features such as six extra dimensions of space in addition to the usual three. In what is called the second superstring revolution, it was conjectured that both string theory and a unification of general relativity and supersymmetry known as supergravity form part of a hypothesized eleven-dimensional model known as M-theory, which would constitute a uniquely defined and consistent theory of quantum gravity.Another approach starts with the canonical quantization procedures of quantum theory. Using the initial-value-formulation of general relativity (cf. evolution equations above), the result is the Wheeler\u2013deWitt equation (an analogue of the Schr\u00f6dinger equation) which, regrettably, turns out to be ill-defined without a proper ultraviolet (lattice) cutoff. However, with the introduction of what are now known as Ashtekar variables, this leads to a promising model known as loop quantum gravity. Space is represented by a web-like structure called a spin network, evolving over time in discrete steps.Depending on which features of general relativity and quantum theory are accepted unchanged, and on what level changes are introduced, there are numerous other attempts to arrive at a viable theory of quantum gravity, some examples being the lattice theory of gravity based on the Feynman Path Integral approach and Regge Calculus, dynamical triangulations, causal sets, twistor models or the path integral based models of quantum cosmology.All candidate theories still have major formal and conceptual problems to overcome. They also face the common problem that, as yet, there is no way to put quantum gravity predictions to experimental tests (and thus to decide between the candidates where their predictions vary), although there is hope for this to change as future data from cosmological observations and particle physics experiments becomes available.\n\n\n== Current status ==\n\nGeneral relativity has emerged as a highly successful model of gravitation and cosmology, which has so far passed many unambiguous observational and experimental tests. However, there are strong indications the theory is incomplete. The problem of quantum gravity and the question of the reality of spacetime singularities remain open. Observational data that is taken as evidence for dark energy and dark matter could indicate the need for new physics. Even taken as is, general relativity is rich with possibilities for further exploration. Mathematical relativists seek to understand the nature of singularities and the fundamental properties of Einstein's equations, while numerical relativists run increasingly powerful computer simulations (such as those describing merging black holes). In February 2016, it was announced that the existence of gravitational waves was directly detected by the Advanced LIGO team on September 14, 2015. A century after its introduction, general relativity remains a highly active area of research.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n=== Popular books ===\nGeroch, R. (1981), General Relativity from A to B, Chicago: University of Chicago Press, ISBN 0-226-28864-1 \nLieber, Lillian (2008), The Einstein Theory of Relativity: A Trip to the Fourth Dimension, Philadelphia: Paul Dry Books, Inc., ISBN 978-1-58988-044-3 \nWald, Robert M. (1992), Space, Time, and Gravity: the Theory of the Big Bang and Black Holes, Chicago: University of Chicago Press, ISBN 0-226-87029-4 \nWheeler, John; Ford, Kenneth (1998), Geons, Black Holes, & Quantum Foam: a life in physics, New York: W. W. Norton, ISBN 0-393-31991-1 \n\n\n=== Beginning undergraduate textbooks ===\nCallahan, James J. (2000), The Geometry of Spacetime: an Introduction to Special and General Relativity, New York: Springer, ISBN 0-387-98641-3 \nTaylor, Edwin F.; Wheeler, John Archibald (2000), Exploring Black Holes: Introduction to General Relativity, Addison Wesley, ISBN 0-201-38423-X CS1 maint: Multiple names: authors list (link) \n\n\n=== Advanced undergraduate textbooks ===\nB. F. Schutz (2009), A First Course in General Relativity (Second Edition), Cambridge University Press, ISBN 978-0-521-88705-2 \nCheng, Ta-Pei (2005), Relativity, Gravitation and Cosmology: a Basic Introduction, Oxford and New York: Oxford University Press, ISBN 0-19-852957-0 \nGron, O.; Hervik, S. (2007), Einstein's General theory of Relativity, Springer, ISBN 978-0-387-69199-2 \nHartle, James B. (2003), Gravity: an Introduction to Einstein's General Relativity, San Francisco: Addison-Wesley, ISBN 0-8053-8662-9 \nHughston, L. P. & Tod, K. P. (1991), Introduction to General Relativity, Cambridge: Cambridge University Press, ISBN 0-521-33943-X CS1 maint: Multiple names: authors list (link) \nd'Inverno, Ray (1992), Introducing Einstein's Relativity, Oxford: Oxford University Press, ISBN 0-19-859686-3 \nLudyk, G\u00fcnter (2013). Einstein in Matrix Form (1st ed.). Berlin: Springer. ISBN 978-3-642-35797-8. \n\n\n=== Graduate-level textbooks ===\nCarroll, Sean M. (2004), Spacetime and Geometry: An Introduction to General Relativity, San Francisco: Addison-Wesley, ISBN 0-8053-8732-3 \nGr\u00f8n, \u00d8yvind; Hervik, Sigbj\u00f8rn (2007), Einstein's General Theory of Relativity, New York: Springer, ISBN 978-0-387-69199-2 \nLandau, Lev D.; Lifshitz, Evgeny F. (1980), The Classical Theory of Fields (4th ed.), London: Butterworth-Heinemann, ISBN 0-7506-2768-9 \nMisner, Charles W.; Thorne, Kip. S.; Wheeler, John A. (1973), Gravitation, W. H. Freeman, ISBN 0-7167-0344-0 \nStephani, Hans (1990), General Relativity: An Introduction to the Theory of the Gravitational Field, Cambridge: Cambridge University Press, ISBN 0-521-37941-5 \nWald, Robert M. (1984), General Relativity, University of Chicago Press, ISBN 0-226-87033-2 \n\n\n== External links ==\nEinstein Online \u2013 Articles on a variety of aspects of relativistic physics for a general audience; hosted by the Max Planck Institute for Gravitational Physics\nNCSA Spacetime Wrinkles \u2013 produced by the numerical relativity group at the NCSA, with an elementary introduction to general relativity\nEinstein's General Theory of Relativity on YouTube (lecture by Leonard Susskind recorded September 22, 2008 at Stanford University).\nSeries of lectures on General Relativity given in 2006 at the Institut Henri Poincar\u00e9 (introductory/advanced).\nGeneral Relativity Tutorials by John Baez.\nBrown, Kevin. \"Reflections on relativity\". Mathpages.com. Retrieved May 29, 2005. \nCarroll, Sean M. \"Lecture Notes on General Relativity\". arXiv:gr-qc/9712019\u202f. \nMoor, Rafi. \"Understanding General Relativity\". Retrieved July 11, 2006. \nWaner, Stefan. \"Introduction to Differential Geometry and General Relativity\" (PDF). Retrieved 2015-04-05.", "algorithms": "In mathematics and computer science, an algorithm ( ( listen)) is an unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing and automated reasoning tasks.\nAs an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.The concept of algorithm has existed for centuries and the use of the concept can be ascribed to Greek mathematicians, e.g. the sieve of Eratosthenes and Euclid's algorithm; the term algorithm itself derives from the 9th Century mathematician Mu\u1e25ammad ibn M\u016bs\u0101 al'Khw\u0101rizm\u012b, latinized 'Algoritmi'. A partial formalization of what would become the modern notion of algorithm began with attempts to solve the Entscheidungsproblem (the \"decision problem\") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define \"effective calculability\" or \"effective method\"; those formalizations included the G\u00f6del\u2013Herbrand\u2013Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936\u20137 and 1939. \n\n\n== Etymology ==\nThe word 'algorithm' has its roots in latinizing the name of Muhammad ibn Musa al-Khwarizmi in a first step to algorismus. Al-Khw\u0101rizm\u012b (Persian: \u062e\u0648\u0627\u0631\u0632\u0645\u06cc\u200e, c. 780\u2013850) was a Persian  mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarezm', a region that was part of Greater Iran and is now in Uzbekistan.About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu\u2013Arabic numeral system, which was translated into Latin during the 12th century under the title Algoritmi de numero Indorum. This title means \"Algoritmi on the numbers of the Indians\", where \"Algoritmi\" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, algorismus, English 'algorism', the corruption of his name, simply meant the \"decimal number system\". In the 15th century, under the influence of the Greek word \u1f00\u03c1\u03b9\u03b8\u03bc\u03cc\u03c2 'number' (cf. 'arithmetic'), the Latin word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that \"algorithm\" took on the meaning that it has in modern English.\nAnother early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins thus:\n\nHaec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.\n\nwhich translates as:\n\nAlgorism is the art by which at present we use those Indian figures, which number two times five.\n\nThe poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.\n\n\n== Informal definition ==\n\nAn informal definition could be \"a set of rules that precisely defines a sequence of operations.\" which would include all computer programs, including programs that do not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.A prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\nBoolos, Jeffrey & 1974, 1999 offer an informal meaning of the word in the following quotation:\n\nNo human being can write fast enough, or long enough, or small enough\u2020 ( \u2020\"smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on electrons\") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.\nAn \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an arbitrary \"input\" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can be an algebraic equation such as y = m + n \u2013 two arbitrary \"input variables\" m and n that produce an output y. But various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):\n\nPrecise instructions (in language understood by \"the computer\") for a fast, efficient, \"good\" process that specifies the \"moves\" of \"the computer\" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = ... and \"effectively\" produce, in a \"reasonable\" time, output-integer y at a specified place and in a specified format.The concept of algorithm is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related with our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.\n\n\n== Formalization ==\nAlgorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):\n\n Minsky: \"But we will also maintain, with Turing . . . that any procedure which could \"naturally\" be called effective, can in fact be realized by a (simple) machine. Although this may seem extreme, the arguments . . . in its favor are hard to refute\".\n Gurevich: \"...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine ... according to Savage [1987], an algorithm is a computational process defined by a Turing machine\".\nTypically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\nFor some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).\nBecause an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\", an idea that is described more formally by flow of control.\nSo far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, \"mechanical\" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. There is an example below of such an assignment.\nFor some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.\n\n\n=== Expressing algorithms ===\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define or document algorithms.\nThere is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite-state machine, state transition table and control table), as flowcharts and drakon-charts (see more at state diagram), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see more at Turing machine).\nRepresentations of algorithms can be classed into three accepted levels of Turing machine description:\n1 High-level description\n\"...prose to describe an algorithm, ignoring the implementation details. At this level we do not need to mention how the machine manages its tape or head.\"\n2 Implementation description\n\"...prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level we do not give details of states or transition function.\"\n3 Formal description\nMost detailed, \"lowest level\", gives the Turing machine's \"state table\".For an example of the simple algorithm \"Add m+n\" described in all three levels, see Algorithm#Examples.\n\n\n== Design ==\n\nAlgorithm design refers to a method or mathematical process for problem solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.\nOne of the most important aspects of algorithm design is creating an algorithm that has an efficient runtime, also known as its Big O.\nTypical steps in development of algorithms:\n\nProblem definition\nDevelopment of a model\nSpecification of algorithm\nDesigning an algorithm\nChecking the correctness of algorithm\nAnalysis of algorithm\nImplementation of algorithm\nProgram testing\nDocumentation preparation\n\n\n== Implementation ==\n\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\n\n\n== Computer algorithms ==\n\nIn computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended \"target\" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.\n\"Elegant\" (compact) programs, \"good\" (fast) programs : The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\n\nKnuth: \". . .we want good algorithms in some loosely defined aesthetic sense. One criterion . . . is the length of time taken to perform the algorithm . . .. Other criteria are adaptability of the algorithm to computers, its simplicity and elegance, etc\"Chaitin: \" . . . a program is 'elegant,' by which I mean that it's the smallest possible program for producing the output that it does\"Chaitin prefaces his definition with: \"I'll show you can't prove that a program is 'elegant'\"\u2014such a proof would solve the Halting problem (ibid).\nAlgorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that \"It is . . . important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms\".Unfortunately there may be a tradeoff between goodness (speed) and elegance (compactness)\u2014an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.\nComputers (and computors), models of computation: A computer (or human \"computor\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.Minsky describes a more congenial variation of Lambek's \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF\u2013THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L \u2190 0), SUCCESSOR (e.g. L \u2190 L+1), and DECREMENT (e.g. L \u2190 L \u2212 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.Simulation of an algorithm: computer (computor) language: Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.This means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky's \"decrement\").\nStructured programming, canonical structures: Per the Church\u2013Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types\u2014conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three B\u00f6hm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.Canonical flowchart symbols: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program of one). Like program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The B\u00f6hm\u2013Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures, are shown in the diagram.\n\n\n== Examples ==\n\n\n=== Algorithm example ===\n\nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description English prose, as:\nHigh-level description:\n\nIf there are no numbers in the set then there is no highest number.\nAssume the first number in the set is the largest number in the set.\nFor each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.\nWhen there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.(Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n\n\n=== Euclid's algorithm ===\n\nEuclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII (\"Elementary Number Theory\") of his Elements. Euclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l \u2212 q\u00d7s, q being the quotient, or remainder r is the \"modulus\", the integer-fractional part left over after the division.For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be \u201cproper\u201d; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields zero).\nEuclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest. While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.\n\n\n==== Computer language for Euclid's algorithm ====\nOnly a few instruction types are required to execute Euclid's algorithm\u2014some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\n\nA location is symbolized by upper case letter(s), e.g. S, A, etc.\nThe varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location's name. For example, location L at the start might contain the number l = 3009.\n\n\n==== An inelegant program for Euclid's algorithm ====\n\nThe following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2\u20134:\nINPUT:\n\n1 [Into two locations L and S put the numbers l and s that represent the two lengths]:\n  INPUT L, S\n2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:\n  R \u2190 L\n\nE0: [Ensure r \u2265 s.]\n\n3 [Ensure the smaller of the two numbers is in S and the larger in R]:\n  IF R > S THEN\n    the contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:\n    GOTO step 6\n  ELSE\n    swap the contents of R and S.\n4   L \u2190 R (this first step is redundant, but is useful for later discussion).\n5   R \u2190 S\n6   S \u2190 L\n\nE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.\n\n7 IF S > R THEN\n    done measuring so\n    GOTO 10\n  ELSE\n    measure again,\n8   R \u2190 R \u2212 S\n9   [Remainder-loop]:\n    GOTO 7.\n\nE2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\n\n10 IF R = 0 THEN\n     done so\n     GOTO step 15\n   ELSE\n     CONTINUE TO step 11,\n\nE3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.\n\n11  L \u2190 R\n12  R \u2190 S\n13  S \u2190 L\n14  [Repeat the measuring process]:\n    GOTO 7\n\nOUTPUT:\n\n15 [Done. S contains the greatest common divisor]:\n   PRINT S\n\nDONE:\n\n16 HALT, END, STOP.\n\n\n==== An elegant program for Euclid's algorithm ====\nThe following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by \"Inelegant\"; worse, \"Inelegant\" requires more types of instructions. The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by \u2190.\n\nThe following version can be used with Object Oriented languages:\n\nHow \"Elegant\" works: In place of an outer \"Euclid loop\", \"Elegant\" shifts back and forth between two \"co-loops\", an A > B loop that computes A \u2190 A \u2212 B, and a B \u2264 A loop that computes B \u2190 B \u2212 A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend \u2212 Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the \"sense\" of the subtraction reverses.\n\n\n=== Testing the Euclid algorithms ===\nDoes an algorithm do what its author wants it to do? A few test cases usually suffice to confirm core functionality. One source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.\nBut exceptional cases must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).\nProof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid's algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.\n\n\n=== Measuring and improving the Euclid algorithms ===\nElegance (compactness) versus goodness (speed): With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does two conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\nCan the algorithms be improved?: Once the programmer judges a program \"fit\" and \"effective\"\u2014that is, it computes the function intended by its author\u2014then the question becomes, can it be improved?\nThe compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.\nThe speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B and R, S would require a detailed analysis.\n\n\n== Algorithmic analysis ==\n\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.\n\n\n=== Formal versus empirical ===\n\nThe analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\nEmpirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\n\n\n=== Execution efficiency ===\n\nTo illustrate the potential improvements possible even in well established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\n\n== Classification ==\nThere are various ways to classify algorithms, each with its own merits.\n\n\n=== By implementation ===\nOne way to classify algorithms is by implementation means.\n\nRecursion\nA recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\nLogical\nAn algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control. The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic programming paradigm. In pure logic programming languages the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant semantics: a change in the axioms has a well-defined change in the algorithm.\nSerial, parallel or distributed\nAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines connected with a computer network. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no parallel algorithms, and are called inherently serial problems.\nDeterministic or non-deterministic\nDeterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.\nExact or approximate\nWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. Approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem. The Knapsack problem is a problem where there is a set of given items. The goal of the problem is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that we can carry is no more than some fixed number X. So, we must consider weights of items as well as their value.\nQuantum algorithm\nThey run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.\n\n\n=== By design paradigm ===\nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories include many different types of algorithms. Some common paradigms are:\n\nBrute-force or exhaustive search\nThis is the naive method of trying every possible solution to see which is best.\nDivide and conquer\nA divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease and conquer algorithm, that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of decrease and conquer algorithm is the binary search algorithm.\nSearch and enumeration\nMany problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.\nRandomized algorithm\nSuch algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.\nLas Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.Reduction of complexity\nThis technique involves solving a difficult problem by transforming it into a better known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\nBack tracking\nIn this approach multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.\n\n\n=== Optimization problems ===\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\nLinear programming\nWhen searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\nDynamic programming\nWhen a problem shows optimal substructures \u2013 meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems \u2013 and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd\u2013Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.\nThe greedy method\nA greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\nThe heuristic method\nIn optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.\n\n\n=== By field of study ===\n\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.\n\n\n=== By complexity ===\n\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\n\nConstant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.\nLinear time: if the time is proportional to the input size. E.g. the traverse of a list.\nLogarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.\nPolynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.\nExponential time: if the time is an exponential function of the input size. E.g. Brute-force search.Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.\n\n\n== Continuous algorithms ==\nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\n\nAn algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations\u2014such algorithms are studied in numerical analysis; or\nAn algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.\n\n\n== Legal issues ==\n\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However, practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).\nResearcher, Andrew Tutt, argues that algorithms should be overseen by a specialist regulatory agency, similar to FDA. His academic work emphasizes that the rise of increasingly complex algorithms calls for the need to think about the effects of algorithms today. Due to the nature and complexity of algorithms, it will prove to be difficult to hold algorithms accountable under criminal law. Tutt recognizes that while some algorithms will be beneficial to help meet technological demand, others should not be used or sold if they fail to meet safety requirements. Thus, for Tutt, algorithms will require \"closer forms of federal uniformity, expert judgment, political independence, and pre-market review to prevent the introduction of unacceptably dangerous algorithms into the market\". The issue of algorithmic accountability (the responsibility of algorithm designers to provide evidence of potential or realised harms) is of particular relevance in the field of dynamic and non-linearly programmed systems, e.g. artificial neural networks, deep learning, and genetic algorithms (see Explainable AI).\n\n\n== History: Development of the notion of \"algorithm\" ==\n\n\n=== Ancient Near East ===\nAlgorithms were used in ancient Greece. Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC). Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.\n\n\n=== Discrete and distinguishable symbols ===\nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks, or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16\u201341). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post\u2013Turing machine computations.\n\n\n=== Manipulation of symbols as \"place holders\" for numbers: algebra ===\nThe work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al-Khwarizmi (from whose name the terms \"algorism\" and \"algorithm\" are derived), and Western European mathematicians culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):\n\nA good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.\n\n\n=== Mechanical contrivances with discrete states ===\nThe clock: Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"\u2014the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer \u2013 Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator \u2013 and is sometimes called \"history's first programmer\" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.\nLogical machines 1870\u2014Stanley Jevons' \"logical abacus\" and \"logical machine\": The technical problem was to reduce Boolean equations when presented in a form similar to what are now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically . . . More recently however I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc] . . .\". With this machine he could analyze a \"syllogism or any other simple logical argument\".This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".Jacquard loom, Hollerith punch cards, telegraphy and telephony\u2014the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.\nTelephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".Davis (2000) observes the particular importance of the electromechanical relay (with its two \"binary states\" open and closed):\n\nIt was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned.\"\n\n\n=== Mathematics during the 19th century up to the mid-20th century ===\nSymbols and rules: In rapid succession the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888\u20131889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's The principles of arithmetic, presented by a new method (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".But Heijenoort gives Frege (1879) this kudos: Frege's is \"perhaps the most important single work ever written in logic. ... in which we see a \" 'formula language', that is a lingua characterica, a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910\u20131913).\nThe paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular the Burali-Forti paradox (1897), the Russell paradox (1902\u201303), and the Richard Paradox. The resultant considerations led to Kurt G\u00f6del's paper (1931)\u2014he specifically cites the paradox of the liar\u2014that completely reduces rules of recursion to numbers.\nEffective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J. B. Rosser's \u03bb-calculus a finely honed definition of \"general recursion\" from the work of G\u00f6del acting on suggestions of Jacques Herbrand (cf. G\u00f6del's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"\u2014in effect almost identical to Post's \"formulation\", J. Barkley Rosser's definition of \"effective method\" in terms of \"a machine\". S. C. Kleene's proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene's renaming his Thesis \"Church's Thesis\" and proposing \"Turing's Thesis\".\n\n\n=== Emil Post (1936) and Alan Turing (1936\u201337, 1939) ===\nHere is a remarkable coincidence of two men not knowing each other but describing a process of men-as-computers working on computations\u2014and they yield virtually identical definitions.\nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\n\n\"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.His symbol space would be\n\n\"a two way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.\"One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes....\"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]\". See more at Post\u2013Turing machine\nAlan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter; and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'\". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.\nTuring\u2014his model of computation is now called a Turing machine\u2014begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\n\"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child's arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite...\"The behaviour of the computer at any moment is determined by the symbols which he is observing, and his \"state of mind\" at that moment. We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...\"Let us imagine that the operations performed by the computer to be split up into 'simple operations' which are so elementary that it is not easy to imagine them further divided.\"Turing's reduction yields the following:\n\n\"The simple operations must therefore include:\n\"(a) Changes of the symbol on one of the observed squares\n\"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must therefore be taken to be one of the following:\n\n\"(A) A possible change (a) of symbol together with a possible change of state of mind.\n\"(B) A possible change (b) of observed squares, together with a possible change of state of mind\"\"We may now construct a machine to do the work of this computer.\"A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:\n\n\"A function is said to be \"effectively calculable\" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition . . . [he discusses the history of the definition pretty much as presented above with respect to G\u00f6del, Herbrand, Kleene, Church, Turing and Post] . . . We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author's definition of a computable function, and to an identification of computability \u2020 with effective calculability . . . .\n\"\u2020 We shall use the expression \"computable function\" to mean a function calculable by a machine, and we let \"effectively calculable\" refer to the intuitive idea without particular identification with any one of these definitions\".\n\n\n=== J. B. Rosser (1939) and S. C. Kleene (1943) ===\nJ. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):\n\n\"'Effective method' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn't matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one.\" (Rosser 1939:225\u20136)Rosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of \u03bb-definability, in particular Church's use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and G\u00f6del and their use of recursion in particular G\u00f6del's use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936\u201337) in their mechanism-models of computation.\nStephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church\u2013Turing thesis. But he did this in the following context (boldface in original):\n\n\"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, \"yes\" or \"no,\" to the question, \"is the predicate value true?\"\" (Kleene 1943:273)\n\n\n=== History after 1950 ===\nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church\u2013Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== Bibliography ==\n\n\n== Further reading ==\n\n\n== External links ==\nHazewinkel, Michiel, ed. (2001) [1994], \"Algorithm\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nAlgorithms at Curlie (based on DMOZ)\nWeisstein, Eric W. \"Algorithm\". MathWorld. \nDictionary of Algorithms and Data Structures\u2014National Institute of Standards and Technology\nAlgorithms and Data Structures by Dr Nikolai BezroukovAlgorithm repositoriesOpenGenus Cosmos - Largest crowd-sourced Algorithm Repository\u2014OpenGenus Foundation\nThe Stony Brook Algorithm Repository\u2014State University of New York at Stony Brook\nNetlib Repository\u2014University of Tennessee and Oak Ridge National Laboratory\nCollected Algorithms of the ACM\u2014Association for Computing Machinery\nThe Stanford GraphBase\u2014Stanford University\nCombinatorica\u2014University of Iowa and State University of New York at Stony Brook\nLibrary of Efficient Datastructures and Algorithms (LEDA)\u2014previously from Max-Planck-Institut f\u00fcr InformatikLecture notesAlgorithms Course Materials. Jeff Erickson. University of Illinois.", "group theory": "In mathematics and abstract algebra, group theory studies the algebraic structures known as groups.  \nThe concept of a group is central to abstract algebra: other well-known algebraic structures, such as rings, fields, and vector spaces, can all be seen as groups endowed with additional operations and axioms. Groups recur throughout mathematics, and the methods of group theory have influenced many parts of algebra. Linear algebraic groups and Lie groups are two branches of group theory that have experienced advances and have become subject areas in their own right.\nVarious physical systems, such as crystals and the hydrogen atom, may be modelled by symmetry groups. Thus group theory and the closely related representation theory have many important applications in physics, chemistry, and materials science. Group theory is also central to public key cryptography.\nOne of the most important mathematical achievements of the 20th century was the collaborative effort, taking up more than 10,000 journal pages and mostly published between 1960 and 1980, that culminated in a complete classification of finite simple groups.\n\n\n== Main classes of groups ==\n\nThe range of groups being considered has gradually expanded from finite permutation groups and special examples of matrix groups to abstract groups that may be specified through a presentation by generators and relations.\n\n\n=== Permutation groups ===\nThe first class of groups to undergo a systematic study was permutation groups. Given any set X and a collection G of bijections of X into itself (known as permutations) that is closed under compositions and inverses, G is a group acting on X. If X consists of n elements and G consists of all permutations, G is the symmetric group Sn; in general, any permutation group G is a subgroup of the symmetric group of X. An early construction due to Cayley exhibited any group as a permutation group, acting on itself (X = G) by means of the left regular representation.\nIn many cases, the structure of a permutation group can be studied using the properties of its action on the corresponding set. For example, in this way one proves that for n \u2265 5, the alternating group An is simple, i.e. does not admit any proper normal subgroups. This fact plays a key role in the impossibility of solving a general algebraic equation of degree n \u2265 5 in radicals.\n\n\n=== Matrix groups ===\nThe next important class of groups is given by matrix groups, or linear groups. Here G is a set consisting of invertible matrices of given order n over a field K that is closed under the products and inverses. Such a group acts on the n-dimensional vector space Kn by linear transformations. This action makes matrix groups conceptually similar to permutation groups, and the geometry of the action may be usefully exploited to establish properties of the group G.\n\n\n=== Transformation groups ===\nPermutation groups and matrix groups are special cases of transformation groups: groups that act on a certain space X preserving its inherent structure. In the case of permutation groups, X is a set; for matrix groups, X is a vector space. The concept of a transformation group is closely related with the concept of a symmetry group: transformation groups frequently consist of all transformations that preserve a certain structure.\nThe theory of transformation groups forms a bridge connecting group theory with differential geometry. A long line of research, originating with Lie and Klein, considers group actions on manifolds by homeomorphisms or diffeomorphisms. The groups themselves may be discrete or continuous.\n\n\n=== Abstract groups ===\nMost groups considered in the first stage of the development of group theory were \"concrete\", having been realized through numbers, permutations, or matrices. It was not until the late nineteenth century that the idea of an abstract group as a set with operations satisfying a certain system of axioms began to take hold. A typical way of specifying an abstract group is through a presentation by generators and relations,\n\n  \n    \n      \n        G\n        =\n        \u27e8\n        S\n        \n          |\n        \n        R\n        \u27e9\n        .\n      \n    \n    {\\displaystyle G=\\langle S|R\\rangle .}\n  A significant source of abstract groups is given by the construction of a factor group, or quotient group, G/H, of a group G by a normal subgroup H. Class groups of algebraic number fields were among the earliest examples of factor groups, of much interest in number theory. If a group G is a permutation group on a set X, the factor group G/H is no longer acting on X; but the idea of an abstract group permits one not to worry about this discrepancy.\nThe change of perspective from concrete to abstract groups makes it natural to consider properties of groups that are independent of a particular realization, or in modern language, invariant under isomorphism, as well as the classes of group with a given such property: finite groups, periodic groups, simple groups, solvable groups, and so on. Rather than exploring properties of an individual group, one seeks to establish results that apply to a whole class of groups. The new paradigm was of paramount importance for the development of mathematics: it foreshadowed the creation of abstract algebra in the works of Hilbert, Emil Artin, Emmy Noether, and mathematicians of their school.\n\n\n=== Topological and algebraic groups ===\nAn important elaboration of the concept of a group occurs if G is endowed with additional structure, notably, of a topological space, differentiable manifold, or algebraic variety. If the group operations m (multiplication) and i (inversion),\n\n  \n    \n      \n        m\n        :\n        G\n        \u00d7\n        G\n        \u2192\n        G\n        ,\n        (\n        g\n        ,\n        h\n        )\n        \u21a6\n        g\n        h\n        ,\n        \n        i\n        :\n        G\n        \u2192\n        G\n        ,\n        g\n        \u21a6\n        \n          g\n          \n            \u2212\n            1\n          \n        \n        ,\n      \n    \n    {\\displaystyle m:G\\times G\\to G,(g,h)\\mapsto gh,\\quad i:G\\to G,g\\mapsto g^{-1},}\n  are compatible with this structure, i.e. are continuous, smooth or regular (in the sense of algebraic geometry) maps, then G becomes a topological group, a Lie group, or an algebraic group.The presence of extra structure relates these types of groups with other mathematical disciplines and means that more tools are available in their study. Topological groups form a natural domain for abstract harmonic analysis, whereas Lie groups (frequently realized as transformation groups) are the mainstays of differential geometry and unitary representation theory. Certain classification questions that cannot be solved in general can be approached and resolved for special subclasses of groups. Thus, compact connected Lie groups have been completely classified. There is a fruitful relation between infinite abstract groups and topological groups: whenever a group \u0393 can be realized as a lattice in a topological group G, the geometry and analysis pertaining to G yield important results about \u0393. A comparatively recent trend in the theory of finite groups exploits their connections with compact topological groups (profinite groups): for example, a single p-adic analytic group G has a family of quotients which are finite p-groups of various orders, and properties of G translate into the properties of its finite quotients.\n\n\n== Branches of group theory ==\n\n\n=== Finite group theory ===\n\nDuring the twentieth century, mathematicians investigated some aspects of the theory of finite groups in great depth, especially the local theory of finite groups and the theory of solvable and nilpotent groups. As a consequence, the complete classification of finite simple groups was achieved, meaning that all those simple groups from which all finite groups can be built are now known.\nDuring the second half of the twentieth century, mathematicians such as Chevalley and Steinberg also increased our understanding of finite analogs of classical groups, and other related groups. One such family of groups is the family of general linear groups over finite fields.  \nFinite groups often occur when considering symmetry of mathematical or\nphysical objects, when those objects admit just a finite number of structure-preserving transformations. The theory of Lie groups,\nwhich may be viewed as dealing with \"continuous symmetry\", is strongly influenced by the associated Weyl groups. These are finite groups generated by reflections which act on a finite-dimensional Euclidean space. The properties of finite groups can thus play a role in subjects such as theoretical physics and chemistry.\n\n\n=== Representation of groups ===\n\nSaying that a group G acts on a set X means that every element of G defines a bijective map on the set X in a way compatible with the group structure. When X has more structure, it is useful to restrict this notion further: a representation of G on a vector space V is a group homomorphism:\n\n  \n    \n      \n        \u03c1\n        :\n        G\n        \u2192\n        GL\n        \u2061\n        (\n        V\n        )\n        ,\n      \n    \n    {\\displaystyle \\rho :G\\to \\operatorname {GL} (V),}\n  where GL(V) consists of the invertible linear transformations of V. In other words, to every group element g is assigned an automorphism \u03c1(g) such that \u03c1(g) \u2218 \u03c1(h) = \u03c1(gh) for any h in G.\nThis definition can be understood in two directions, both of which give rise to whole new domains of mathematics. On the one hand, it may yield new information about the group G: often, the group operation in G is abstractly given, but via \u03c1, it corresponds to the multiplication of matrices, which is very explicit. On the other hand, given a well-understood group acting on a complicated object, this simplifies the study of the object in question. For example, if G is finite, it is known that V above decomposes into irreducible parts. These parts in turn are much more easily manageable than the whole V (via Schur's lemma).\nGiven a group G, representation theory then asks what representations of G exist. There are several settings, and the employed methods and obtained results are rather different in every case: representation theory of finite groups and representations of Lie groups are two main subdomains of the theory. The totality of representations is governed by the group's characters. For example, Fourier polynomials can be interpreted as the characters of U(1), the group of complex numbers of absolute value 1, acting on the L2-space of periodic functions.\n\n\n=== Lie theory ===\n\nA Lie group is a group that is also a differentiable manifold, with the property that the group operations are compatible with the smooth structure. Lie groups are named after Sophus Lie, who laid the foundations of the theory of continuous transformation groups. The term groupes de Lie first appeared in French in 1893 in the thesis of Lie\u2019s student Arthur Tresse, page 3.Lie groups represent the best-developed theory of continuous symmetry of mathematical objects and structures, which makes them indispensable tools for many parts of contemporary mathematics, as well as for modern theoretical physics. They provide a natural framework for analysing the continuous symmetries of differential equations (differential Galois theory), in much the same way as permutation groups are used in Galois theory for analysing the discrete symmetries of algebraic equations. An extension of Galois theory to the case of continuous symmetry groups was one of Lie's principal motivations.\n\n\n=== Combinatorial and geometric group theory ===\n\nGroups can be described in different ways. Finite groups can be described by writing down the group table consisting of all possible multiplications g \u2022 h. A more compact way of defining a group is by generators and relations, also called the presentation of a group. Given any set F of generators \n  \n    \n      \n        {\n        \n          g\n          \n            i\n          \n        \n        \n          }\n          \n            i\n            \u2208\n            I\n          \n        \n      \n    \n    {\\displaystyle \\{g_{i}\\}_{i\\in I}}\n  , the free group generated by F surjects onto the group G. The kernel of this map is called the subgroup of relations, generated by some subset D. The presentation is usually denoted by \n  \n    \n      \n        \u27e8\n        F\n        \u2223\n        D\n        \u27e9\n        .\n      \n    \n    {\\displaystyle \\langle F\\mid D\\rangle .}\n   For example, the group presentation \n  \n    \n      \n        \u27e8\n        a\n        ,\n        b\n        \u2223\n        a\n        b\n        \n          a\n          \n            \u2212\n            1\n          \n        \n        \n          b\n          \n            \u2212\n            1\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle \\langle a,b\\mid aba^{-1}b^{-1}\\rangle }\n   describes a group which is isomorphic to \n  \n    \n      \n        \n          Z\n        \n        \u00d7\n        \n          Z\n        \n        .\n      \n    \n    {\\displaystyle \\mathbb {Z} \\times \\mathbb {Z} .}\n   A string consisting of generator symbols and their inverses is called a word.\nCombinatorial group theory studies groups from the perspective of generators and relations. It is particularly useful where finiteness assumptions are satisfied, for example finitely generated groups, or finitely presented groups (i.e. in addition the relations are finite). The area makes use of the connection of graphs via their fundamental groups. For example, one can show that every subgroup of a free group is free.\nThere are several natural questions arising from giving a group by its presentation. The word problem asks whether two words are effectively the same group element. By relating the problem to Turing machines, one can show that there is in general no algorithm solving this task. Another, generally harder, algorithmically insoluble problem is the group isomorphism problem, which asks whether two groups given by different presentations are actually isomorphic. For example, the group with presentation \n  \n    \n      \n        \u27e8\n        x\n        ,\n        y\n        \u2223\n        x\n        y\n        x\n        y\n        x\n        =\n        e\n        \u27e9\n        ,\n      \n    \n    {\\displaystyle \\langle x,y\\mid xyxyx=e\\rangle ,}\n   is isomorphic to the additive group Z of integers, although this may not be immediately apparent.\n\nGeometric group theory attacks these problems from a geometric viewpoint, either by viewing groups as geometric objects, or by finding suitable geometric objects a group acts on. The first idea is made precise by means of the Cayley graph, whose vertices correspond to group elements and edges correspond to right multiplication in the group. Given two elements, one constructs the word metric given by the length of the minimal path between the elements. A theorem of Milnor and Svarc then says that given a group G acting in a reasonable manner on a metric space X, for example a compact manifold, then G is quasi-isometric (i.e. looks similar from a distance) to the space X.\n\n\n== Connection of groups and symmetry ==\n\nGiven a structured object X of any sort, a symmetry is a mapping of the object onto itself which preserves the structure. This occurs in many cases, for example\n\nIf X is a set with no additional structure, a symmetry is a bijective map from the set to itself, giving rise to permutation groups.\nIf the object X is a set of points in the plane with its metric structure or any other metric space, a symmetry is a bijection of the set to itself which preserves the distance between each pair of points (an isometry). The corresponding group is called isometry group of X.\nIf instead angles are preserved, one speaks of conformal maps. Conformal maps give rise to Kleinian groups, for example.\nSymmetries are not restricted to geometrical objects, but include algebraic objects as well.  For instance, the equation \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n        \u2212\n        3\n        =\n        0\n      \n    \n    {\\displaystyle x^{2}-3=0}\n   has the two solutions \n  \n    \n      \n        \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {3}}}\n   and \n  \n    \n      \n        \u2212\n        \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle -{\\sqrt {3}}}\n  . In this case, the group that exchanges the two roots is the Galois group belonging to the equation.  Every polynomial equation in one variable has a Galois group, that is a certain permutation group on its roots.The axioms of a group formalize the essential aspects of symmetry. Symmetries form a group: they are closed because if you take a symmetry of an object, and then apply another symmetry, the result will still be a symmetry. The identity keeping the object fixed is always a symmetry of an object. Existence of inverses is guaranteed by undoing the symmetry and the associativity comes from the fact that symmetries are functions on a space, and composition of functions are associative.\nFrucht's theorem says that every group is the symmetry group of some graph. So every abstract group is actually the symmetries of some explicit object.\nThe saying of \"preserving the structure\" of an object can be made precise by working in a category. Maps preserving the structure are then the morphisms, and the symmetry group is the automorphism group of the object in question.\n\n\n== Applications of group theory ==\nApplications of group theory abound. Almost all structures in abstract algebra are special cases of groups. Rings, for example, can be viewed as abelian groups (corresponding to addition) together with a second operation (corresponding to multiplication). Therefore, group theoretic arguments underlie large parts of the theory of those entities.\n\n\n=== Galois theory ===\n\nGalois theory uses groups to describe the symmetries of the roots of a polynomial (or more precisely the automorphisms of the algebras generated by these roots). The fundamental theorem of Galois theory provides a link between algebraic field extensions and group theory. It gives an effective criterion for the solvability of polynomial equations in terms of the solvability of the corresponding Galois group. For example, S5, the symmetric group in 5 elements, is not solvable which implies that the general quintic equation cannot be solved by radicals in the way equations of lower degree can. The theory, being one of the historical roots of group theory, is still fruitfully applied to yield new results in areas such as class field theory.\n\n\n=== Algebraic topology ===\n\nAlgebraic topology is another domain which prominently associates groups to the objects the theory is interested in. There, groups are used to describe certain invariants of topological spaces. They are called \"invariants\" because they are defined in such a way that they do not change if the space is subjected to some deformation. For example, the fundamental group \"counts\" how many paths in the space are essentially different. The Poincar\u00e9 conjecture, proved in 2002/2003 by Grigori Perelman, is a prominent application of this idea. The influence is not unidirectional, though. For example, algebraic topology makes use of Eilenberg\u2013MacLane spaces which are spaces with prescribed homotopy groups. Similarly algebraic K-theory relies in a way on classifying spaces of groups. Finally, the name of the torsion subgroup of an infinite group shows the legacy of topology in group theory.\n\n\n=== Algebraic geometry and cryptography ===\n\nAlgebraic geometry and cryptography likewise uses group theory in many ways. Abelian varieties have been introduced above. The presence of the group operation yields additional information which makes these varieties particularly accessible. They also often serve as a test for new conjectures. The one-dimensional case, namely elliptic curves is studied in particular detail. They are both theoretically and practically intriguing. Very large groups of prime order constructed in elliptic curve cryptography serve for public-key cryptography. Cryptographical methods of this kind benefit from the flexibility of the geometric objects, hence their group structures, together with the complicated structure of these groups, which make the discrete logarithm very hard to calculate. One of the earliest encryption protocols, Caesar's cipher, may also be interpreted as a (very easy) group operation. In another direction, toric varieties are algebraic varieties acted on by a torus. Toroidal embeddings have recently led to advances in algebraic geometry, in particular resolution of singularities.\n\n\n=== Algebraic number theory ===\n\nAlgebraic number theory is a special case of group theory, thereby following the rules of the latter. For example, Euler's product formula\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \u2211\n                  \n                    n\n                    \u2265\n                    1\n                  \n                \n                \n                  \n                    1\n                    \n                      n\n                      \n                        s\n                      \n                    \n                  \n                \n              \n              \n                \n                =\n                \n                  \u220f\n                  \n                    p\n                    \n                       prime\n                    \n                  \n                \n                \n                  \n                    1\n                    \n                      1\n                      \u2212\n                      \n                        p\n                        \n                          \u2212\n                          s\n                        \n                      \n                    \n                  \n                \n              \n            \n          \n        \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\sum _{n\\geq 1}{\\frac {1}{n^{s}}}&=\\prod _{p{\\text{ prime}}}{\\frac {1}{1-p^{-s}}}\\\\\\end{aligned}}\\!}\n  captures the fact that any integer decomposes in a unique way into primes. The failure of this statement for more general rings gives rise to class groups and regular primes, which feature in Kummer's treatment of Fermat's Last Theorem.\n\n\n=== Harmonic analysis ===\n\nAnalysis on Lie groups and certain other groups is called harmonic analysis. Haar measures, that is, integrals invariant under the translation in a Lie group, are used for pattern recognition and other image processing techniques.\n\n\n=== Combinatorics ===\nIn combinatorics, the notion of permutation group and the concept of group action are often used to simplify the counting of a set of objects; see in particular Burnside's lemma.\n\n\n=== Music ===\nThe presence of the 12-periodicity in the circle of fifths yields applications of elementary group theory in musical set theory.\n\n\n=== Physics ===\nIn physics, groups are important because they describe the symmetries which the laws of physics seem to obey. According to Noether's theorem, every continuous symmetry of a physical system corresponds to a conservation law of the system. Physicists are very interested in group representations, especially of Lie groups, since these representations often point the way to the \"possible\" physical theories. Examples of the use of groups in physics include the Standard Model, gauge theory, the Lorentz group, and the Poincar\u00e9 group.\n\n\n=== Chemistry and materials science ===\nIn chemistry and materials science, groups are used to classify crystal structures, regular polyhedra, and the symmetries of molecules.  The assigned point groups can then be used to determine physical properties (such as chemical polarity and chirality), spectroscopic properties (particularly useful for Raman spectroscopy, infrared spectroscopy, circular dichroism spectroscopy, magnetic circular dichroism spectroscopy, UV/Vis spectroscopy, and fluorescence spectroscopy), and to construct molecular orbitals.\nMolecular symmetry is responsible for many physical and spectroscopic properties of compounds and provides relevant information about how chemical reactions occur. In order to assign a point group for any given molecule, it is necessary to find the set of symmetry operations present on it. The symmetry operation is an action, such as a rotation around an axis or a reflection through a mirror plane. In other words, it is an operation that moves the molecule such that it is indistinguishable from the original configuration. In group theory, the rotation axes and mirror planes are called \"symmetry elements\". These elements can be a point, line or plane with respect to which the symmetry operation is carried out. The symmetry operations of a molecule determine the specific point group for this molecule.\n\nIn chemistry, there are five important symmetry operations. The identity operation (E) consists of leaving the molecule as it is. This is equivalent to any number of full rotations around any axis. This is a symmetry of all molecules, whereas the symmetry group of a chiral molecule consists of only the identity operation. Rotation around an axis (Cn) consists of rotating the molecule around a specific axis by a specific angle. For example, if a water molecule rotates 180\u00b0 around the axis that passes through the oxygen atom and between the hydrogen atoms, it is in the same configuration as it started. In this case, n = 2, since applying it twice produces the identity operation. Other symmetry operations are: reflection, inversion and improper rotation (rotation followed by reflection).\n\n\n=== Statistical mechanics ===\nGroup theory can be used to resolve the incompleteness of the statistical interpretations of mechanics developed by Willard Gibbs, relating to the summing of an infinite number of probabilities to yield a meaningful solution.\n\n\n== History ==\n\nGroup theory has three main historical sources: number theory, the theory of algebraic equations, and geometry. The number-theoretic strand was begun by Leonhard Euler, and developed by Gauss's work on modular arithmetic and additive and multiplicative groups related to quadratic fields. Early results about permutation groups were obtained by Lagrange, Ruffini, and Abel in their quest for general solutions of polynomial equations of high degree. \u00c9variste Galois coined the term \"group\" and established a connection, now known as Galois theory, between the nascent theory of groups and field theory. In geometry, groups first became important in projective geometry and, later, non-Euclidean geometry. Felix Klein's Erlangen program proclaimed group theory to be the organizing principle of geometry.\nGalois, in the 1830s, was the first to employ groups to determine the solvability of polynomial equations. Arthur Cayley and Augustin Louis Cauchy pushed these investigations further by creating the theory of permutation groups. The second historical source for groups stems from geometrical situations. In an attempt to come to grips with possible geometries (such as euclidean, hyperbolic or projective geometry) using group theory, Felix Klein initiated the Erlangen programme. Sophus Lie, in 1884, started using groups (now called Lie groups) attached to analytic problems. Thirdly, groups were, at first implicitly and later explicitly, used in algebraic number theory.\nThe different scope of these early sources resulted in different notions of groups. The theory of groups was unified starting around 1880. Since then, the impact of group theory has been ever growing, giving rise to the birth of abstract algebra in the early 20th century, representation theory, and many more influential spin-off domains. The classification of finite simple groups is a vast body of work from the mid 20th century, classifying all the finite simple groups.\n\n\n== See also ==\nList of group theory topics\nExamples of groups\n\n\n== Notes ==\n\n\n== References ==\nBorel, Armand (1991), Linear algebraic groups, Graduate Texts in Mathematics, 126 (2nd ed.), Berlin, New York: Springer-Verlag, doi:10.1007/978-1-4612-0941-6, ISBN 978-0-387-97370-8, MR 1102012 \nCarter, Nathan C. (2009), Visual group theory, Classroom Resource Materials Series, Mathematical Association of America, ISBN 978-0-88385-757-1, MR 2504193 \nCannon, John J. (1969), \"Computers in group theory: A survey\", Communications of the ACM, 12: 3\u201312, doi:10.1145/362835.362837, MR 0290613 \nFrucht, R. (1939), \"Herstellung von Graphen mit vorgegebener abstrakter Gruppe\", Compositio Mathematica, 6: 239\u201350, ISSN 0010-437X, archived from the original on 2008-12-01 \nGolubitsky, Martin; Stewart, Ian (2006), \"Nonlinear dynamics of networks: the groupoid formalism\", Bull. Amer. Math. Soc. (N.S.), 43 (03): 305\u2013364, doi:10.1090/S0273-0979-06-01108-6, MR 2223010  Shows the advantage of generalising from group to groupoid.\nJudson, Thomas W. (1997), Abstract Algebra:  Theory and Applications   An introductory undergraduate text in the spirit of texts by Gallian or Herstein, covering groups, rings, integral domains, fields and Galois theory.  Free downloadable PDF with open-source GFDL license.\nKleiner, Israel (1986), \"The evolution of group theory: a brief survey\", Mathematics Magazine, 59 (4): 195\u2013215, doi:10.2307/2690312, ISSN 0025-570X, JSTOR 2690312, MR 0863090 \nLa Harpe, Pierre de (2000), Topics in geometric group theory, University of Chicago Press, ISBN 978-0-226-31721-2 \nLivio, M. (2005), The Equation That Couldn't Be Solved: How Mathematical Genius Discovered the Language of Symmetry, Simon & Schuster, ISBN 0-7432-5820-7  Conveys the practical value of group theory by explaining how it points to symmetries in physics and other sciences.\nMumford, David (1970), Abelian varieties, Oxford University Press, ISBN 978-0-19-560528-0, OCLC 138290 \nRonan M., 2006. Symmetry and the Monster. Oxford University Press. ISBN 0-19-280722-6. For lay readers. Describes the quest to find the basic building blocks for finite groups.\nRotman, Joseph (1994), An introduction to the theory of groups, New York: Springer-Verlag, ISBN 0-387-94285-8   A standard contemporary reference.\nSchupp, Paul E.; Lyndon, Roger C. (2001), Combinatorial group theory, Berlin, New York: Springer-Verlag, ISBN 978-3-540-41158-1 \nScott, W. R. (1987) [1964], Group Theory, New York: Dover, ISBN 0-486-65377-3  Inexpensive and fairly readable, but somewhat dated in emphasis, style, and notation.\nShatz, Stephen S. (1972), Profinite groups, arithmetic, and geometry, Princeton University Press, ISBN 978-0-691-08017-8, MR 0347778 \nWeibel, Charles A. (1994). An introduction to homological algebra. Cambridge Studies in Advanced Mathematics. 38. Cambridge University Press. ISBN 978-0-521-55987-4. MR 1269324. OCLC 36131259. \n\n\n== External links ==\n\nHistory of the abstract group concept\nHigher dimensional group theory This presents a view of group theory as level one of a theory which extends in all dimensions, and has applications in homotopy theory and to higher dimensional nonabelian methods for local-to-global problems.\nPlus teacher and student package: Group Theory This package brings together all the articles on group theory from Plus, the online mathematics magazine produced by the Millennium Mathematics Project at the University of Cambridge, exploring applications and recent breakthroughs, and giving explicit definitions and examples of groups.", "set theory": "Set theory is a branch of mathematical logic that studies sets, which informally are collections of objects. Although any type of object can be collected into a set, set theory is applied most often to objects that are relevant to mathematics. The language of set theory can be used in the definitions of nearly all mathematical objects.\nThe modern study of set theory was initiated by Georg Cantor and Richard Dedekind in the 1870s. After the discovery of paradoxes in naive set theory, such as Russell's paradox, numerous axiom systems were proposed in the early twentieth century, of which the Zermelo\u2013Fraenkel axioms, with or without the axiom of choice, are the best-known.\nSet theory is commonly employed as a foundational system for mathematics, particularly in the form of Zermelo\u2013Fraenkel set theory with the axiom of choice. Beyond its foundational role, set theory is a branch of mathematics in its own right, with an active research community. Contemporary research into set theory includes a diverse collection of topics, ranging from the structure of the real number line to the study of the consistency of large cardinals.\n\n\n== History ==\n\nMathematical topics typically emerge and evolve through interactions among many researchers. Set theory, however, was founded by a single paper in 1874 by Georg Cantor: \"On a Property of the Collection of All Real Algebraic Numbers\".Since the 5th century BC, beginning with Greek mathematician Zeno of Elea in the West and early Indian mathematicians in the East, mathematicians had struggled with the concept of infinity. Especially notable is the work of Bernard Bolzano in the first half of the 19th century. Modern understanding of infinity began in 1870\u20131874 and was motivated by Cantor's work in real analysis. An 1872 meeting between Cantor and Richard Dedekind influenced Cantor's thinking and culminated in Cantor's 1874 paper.\nCantor's work initially polarized the mathematicians of his day. While Karl Weierstrass and Dedekind supported Cantor, Leopold Kronecker, now seen as a founder of mathematical constructivism, did not. Cantorian set theory eventually became widespread, due to the utility of Cantorian concepts, such as one-to-one correspondence among sets, his proof that there are more real numbers than integers, and the \"infinity of infinities\" (\"Cantor's paradise\") resulting from the power set operation. This utility of set theory led to the article \"Mengenlehre\" contributed in 1898 by Arthur Schoenflies to Klein's encyclopedia.\nThe next wave of excitement in set theory came around 1900, when it was discovered that some interpretations of Cantorian set theory gave rise to several contradictions, called antinomies or paradoxes. Bertrand Russell and Ernst Zermelo independently found the simplest and best known paradox, now called Russell's paradox: consider \"the set of all sets that are not members of themselves\", which leads to a contradiction since it must be a member of itself and not a member of itself. In 1899 Cantor had himself posed the question \"What is the cardinal number of the set of all sets?\", and obtained a related paradox. Russell used his paradox as a theme in his 1903 review of continental mathematics in his The Principles of Mathematics.\nIn 1906 English readers gained the book Theory of Sets of Points by husband and wife William Henry Young and Grace Chisholm Young, published by Cambridge University Press.\nThe momentum of set theory was such that debate on the paradoxes did not lead to its abandonment. The work of Zermelo in 1908 and the work of Abraham Fraenkel and Thoralf Skolem in 1922 resulted in the set of axioms ZFC, which became the most commonly used set of axioms for set theory. The work of analysts such as Henri Lebesgue demonstrated the great mathematical utility of set theory, which has since become woven into the fabric of modern mathematics. Set theory is commonly used as a foundational system, although in some areas\u2014such as algebraic geometry and algebraic topology\u2014category theory is thought to be a preferred foundation.\n\n\n== Basic concepts and notation ==\n\nSet theory begins with a fundamental binary relation between an object o and a set A. If o is a member (or element) of A, the notation o \u2208 A is used. Since sets are objects, the membership relation can relate sets as well.\nA derived binary relation between two sets is the subset relation, also called set inclusion. If all the members of set A are also members of set B, then A is a subset of B, denoted A \u2286 B. For example, {1, 2}  is a subset of {1, 2, 3} , and so is {2}  but {1, 4}  is not. As insinuated from this definition, a set is a subset of itself. For cases where this possibility is unsuitable or would make sense to be rejected, the term proper subset is defined. A is called a proper subset of B if and only if A is a subset of B, but A is not equal to B. Note also that 1, 2, and 3 are members (elements) of the set {1, 2, 3}  but are not subsets of it; and in turn, the subsets, such as {1}, are not members of the set {1, 2, 3}.\nJust as arithmetic features binary operations on numbers, set theory features binary operations on sets. The:\n\nUnion of the sets A and B, denoted A \u222a B, is the set of all objects that are a member of A, or B, or both. The union of {1, 2, 3}  and {2, 3, 4}  is the set {1, 2, 3, 4} .\nIntersection of the sets A and B, denoted A \u2229 B, is the set of all objects that are members of both A and B. The intersection of {1, 2, 3}  and {2, 3, 4}  is the set {2, 3} .\nSet difference of U and A, denoted U \\ A, is the set of all members of U that are not members of A. The set difference {1, 2, 3} \\ {2, 3, 4}  is {1} , while, conversely, the set difference {2, 3, 4} \\ {1, 2, 3}  is {4} . When A is a subset of U, the set difference U \\ A is also called the complement of A in U. In this case, if the choice of U is clear from the context, the notation Ac is sometimes used instead of U \\ A, particularly if U is a universal set as in the study of Venn diagrams.\nSymmetric difference of sets A and B, denoted A \u25b3 B or A \u2296 B, is the set of all objects that are a member of exactly one of A and B (elements which are in one of the sets, but not in both). For instance, for the sets {1, 2, 3}  and {2, 3, 4} , the symmetric difference set is {1, 4} . It is the set difference of the union and the intersection, (A \u222a B) \\ (A \u2229 B) or (A \\ B) \u222a (B \\ A).\nCartesian product of A and B, denoted A \u00d7 B, is the set whose members are all possible ordered pairs (a, b) where a is a member of A and b is a member of B. The cartesian product of {1, 2} and {red, white} is {(1, red), (1, white), (2, red), (2, white)}.\nPower set of a set A is the set whose members are all of the possible subsets of A. For example, the power set of {1, 2}  is { {}, {1}, {2}, {1, 2} } .Some basic sets of central importance are the empty set (the unique set containing no elements; occasionally called the null set though this name is ambiguous), the set of natural numbers, and the set of real numbers.\n\n\n== Some ontology ==\n\nA set is pure if all of its members are sets, all members of its members are sets, and so on. For example, the set {{}} containing only the empty set is a nonempty pure set. In modern set theory, it is common to restrict attention to the von Neumann universe of pure sets, and many systems of axiomatic set theory are designed to axiomatize the pure sets only. There are many technical advantages to this restriction, and little generality is lost, because essentially all mathematical concepts can be modeled by pure sets. Sets in the von Neumann universe are organized into a cumulative hierarchy, based on how deeply their members, members of members, etc. are nested. Each set in this hierarchy is assigned (by transfinite recursion) an ordinal number \u03b1, known as its rank. The rank of a pure set X is defined to be the least upper bound of all successors of ranks of members of X. For example, the empty set is assigned rank 0, while the set  {{}}  containing only the empty set is assigned rank 1. For each ordinal \u03b1, the set V\u03b1 is defined to consist of all pure sets with rank less than \u03b1. The entire von Neumann universe is denoted V.\n\n\n== Axiomatic set theory ==\nElementary set theory can be studied informally and intuitively, and so can be taught in primary schools using Venn diagrams. The intuitive approach tacitly assumes that a set may be formed from the class of all objects satisfying any particular defining condition. This assumption gives rise to paradoxes, the simplest and best known of which are Russell's paradox and the Burali-Forti paradox. Axiomatic set theory was originally devised to rid set theory of such paradoxes.The most widely studied systems of axiomatic set theory imply that all sets form a cumulative hierarchy. Such systems come in two flavors, those whose ontology consists of:\n\nSets alone. This includes the most common axiomatic set theory, Zermelo\u2013Fraenkel set theory (ZFC), which includes the axiom of choice. Fragments of ZFC include:\nZermelo set theory, which replaces the axiom schema of replacement with that of separation;\nGeneral set theory, a small fragment of Zermelo set theory sufficient for the Peano axioms and finite sets;\nKripke\u2013Platek set theory, which omits the axioms of infinity, powerset, and choice, and weakens the axiom schemata of separation and replacement.\nSets and proper classes. These include Von Neumann\u2013Bernays\u2013G\u00f6del set theory, which has the same strength as ZFC for theorems about sets alone, and Morse\u2013Kelley set theory and Tarski\u2013Grothendieck set theory, both of which are stronger than ZFC.The above systems can be modified to allow urelements, objects that can be members of sets but that are not themselves sets and do not have any members.\nThe systems of New Foundations NFU (allowing urelements) and NF (lacking them) are not based on a cumulative hierarchy. NF and NFU include a \"set of everything, \" relative to which every set has a complement. In these systems urelements matter, because NF, but not NFU, produces sets for which the axiom of choice does not hold.\nSystems of constructive set theory, such as CST, CZF, and IZF, embed their set axioms in intuitionistic instead of classical logic. Yet other systems accept classical logic but feature a nonstandard membership relation. These include rough set theory and fuzzy set theory, in which the value of an atomic formula embodying the membership relation is not simply True or False. The Boolean-valued models of ZFC are a related subject.\nAn enrichment of ZFC called internal set theory was proposed by Edward Nelson in 1977.\n\n\n== Applications ==\nMany mathematical concepts can be defined precisely using only set theoretic concepts. For example, mathematical structures as diverse as graphs, manifolds, rings, and vector spaces can all be defined as sets satisfying various (axiomatic) properties. Equivalence and order relations are ubiquitous in mathematics, and the theory of mathematical relations can be described in set theory.\nSet theory is also a promising foundational system for much of mathematics. Since the publication of the first volume of Principia Mathematica, it has been claimed that most or even all mathematical theorems can be derived using an aptly designed set of axioms for set theory, augmented with many definitions, using first or second order logic. For example, properties of the natural and real numbers can be derived within set theory, as each number system can be identified with a set of equivalence classes under a suitable equivalence relation whose field is some infinite set.\nSet theory as a foundation for mathematical analysis, topology, abstract algebra, and discrete mathematics is likewise uncontroversial; mathematicians accept that (in principle) theorems in these areas can be derived from the relevant definitions and the axioms of set theory. Few full derivations of complex mathematical theorems from set theory have been formally verified, however, because such formal derivations are often much longer than the natural language proofs mathematicians commonly present. One verification project, Metamath, includes human-written, computer\u2010verified derivations of more than 12,000 theorems starting from ZFC set theory, first order logic and propositional logic.\n\n\n== Areas of study ==\nSet theory is a major area of research in mathematics, with many interrelated subfields.\n\n\n=== Combinatorial set theory ===\n\nCombinatorial set theory concerns extensions of finite combinatorics to infinite sets. This includes the study of cardinal arithmetic and the study of extensions of Ramsey's theorem such as the Erd\u0151s\u2013Rado theorem.\n\n\n=== Descriptive set theory ===\n\nDescriptive set theory is the study of subsets of the real line and, more generally, subsets of Polish spaces. It begins with the study of pointclasses in the Borel hierarchy and extends to the study of more complex hierarchies such as the projective hierarchy and the Wadge hierarchy. Many properties of Borel sets can be established in ZFC, but proving these properties hold for more complicated sets requires additional axioms related to determinacy and large cardinals.\nThe field of effective descriptive set theory is between set theory and recursion theory. It includes the study of lightface pointclasses, and is closely related to hyperarithmetical theory. In many cases, results of classical descriptive set theory have effective versions; in some cases, new results are obtained by proving the effective version first and then extending (\"relativizing\") it to make it more broadly applicable.\nA recent area of research concerns Borel equivalence relations and more complicated definable equivalence relations. This has important applications to the study of invariants in many fields of mathematics.\n\n\n=== Fuzzy set theory ===\n\nIn set theory as Cantor defined and Zermelo and Fraenkel axiomatized, an object is either a member of a set or not. In fuzzy set theory this condition was relaxed by Lotfi A. Zadeh so an object has a degree of membership in a set, a number between 0 and 1. For example, the degree of membership of a person in the set of \"tall people\" is more flexible than a simple yes or no answer and can be a real number such as 0.75.\n\n\n=== Inner model theory ===\n\nAn inner model of Zermelo\u2013Fraenkel set theory (ZF) is a transitive class that includes all the ordinals and satisfies all the axioms of ZF. The canonical example is the constructible universe L developed by G\u00f6del.\nOne reason that the study of inner models is of interest is that it can be used to prove consistency results. For example, it can be shown that regardless of whether a model V of ZF satisfies the continuum hypothesis or the axiom of choice, the inner model L constructed inside the original model will satisfy both the generalized continuum hypothesis and the axiom of choice. Thus the assumption that ZF is consistent (has at least one model) implies that ZF together with these two principles is consistent.\nThe study of inner models is common in the study of determinacy and large cardinals, especially when considering axioms such as the axiom of determinacy that contradict the axiom of choice. Even if a fixed model of set theory satisfies the axiom of choice, it is possible for an inner model to fail to satisfy the axiom of choice. For example, the existence of sufficiently large cardinals implies that there is an inner model satisfying the axiom of determinacy (and thus not satisfying the axiom of choice).\n\n\n=== Large cardinals ===\n\nA large cardinal is a cardinal number with an extra property. Many such properties are studied, including inaccessible cardinals, measurable cardinals, and many more. These properties typically imply the cardinal number must be very large, with the existence of a cardinal with the specified property unprovable in Zermelo-Fraenkel set theory.\n\n\n=== Determinacy ===\n\nDeterminacy refers to the fact that, under appropriate assumptions, certain two-player games of perfect information are determined from the start in the sense that one player must have a winning strategy. The existence of these strategies has important consequences in descriptive set theory, as the assumption that a broader class of games is determined often implies that a broader class of sets will have a topological property. The axiom of determinacy (AD) is an important object of study; although incompatible with the axiom of choice, AD implies that all subsets of the real line are well behaved (in particular, measurable and with the perfect set property). AD can be used to prove that the Wadge degrees have an elegant structure.\n\n\n=== Forcing ===\n\nPaul Cohen invented the method of forcing while searching for a model of ZFC in which the continuum hypothesis fails, or a model of ZF in which the axiom of choice fails. Forcing adjoins to some given model of set theory additional sets in order to create a larger model with properties determined (i.e. \"forced\") by the construction and the original model. For example, Cohen's construction adjoins additional subsets of the natural numbers without changing any of the cardinal numbers of the original model. Forcing is also one of two methods for proving relative consistency by finitistic methods, the other method being Boolean-valued models.\n\n\n=== Cardinal invariants ===\n\nA cardinal invariant is a property of the real line measured by a cardinal number. For example, a well-studied invariant is the smallest cardinality of a collection of meagre sets of reals whose union is the entire real line. These are invariants in the sense that any two isomorphic models of set theory must give the same cardinal for each invariant. Many cardinal invariants have been studied, and the relationships between them are often complex and related to axioms of set theory.\n\n\n=== Set-theoretic topology ===\n\nSet-theoretic topology studies questions of general topology that are set-theoretic in nature or that require advanced methods of set theory for their solution. Many of these theorems are independent of ZFC, requiring stronger axioms for their proof. A famous problem is the normal Moore space question, a question in general topology that was the subject of intense research. The answer to the normal Moore space question was eventually proved to be independent of ZFC.\n\n\n== Objections to set theory as a foundation for mathematics ==\nFrom set theory's inception, some mathematicians have objected to it as a foundation for mathematics. The most common objection to set theory, one Kronecker voiced in set theory's earliest years, starts from the constructivist view that mathematics is loosely related to computation. If this view is granted, then the treatment of infinite sets, both in naive and in axiomatic set theory, introduces into mathematics methods and objects that are not computable even in principle. The feasibility of constructivism as a substitute foundation for mathematics was greatly increased by Errett Bishop's influential book Foundations of Constructive Analysis.A different objection put forth by Henri Poincar\u00e9 is that defining sets using the axiom schemas of specification and replacement, as well as the axiom of power set, introduces impredicativity, a type of circularity, into the definitions of mathematical objects. The scope of predicatively founded mathematics, while less than that of the commonly accepted Zermelo-Fraenkel theory, is much greater than that of constructive mathematics, to the point that Solomon Feferman has said that \"all of scientifically applicable analysis can be developed [using predicative methods]\".Ludwig Wittgenstein condemned set theory. He wrote that \"set theory is wrong\", since it builds on the \"nonsense\" of fictitious symbolism, has \"pernicious idioms\", and that it is nonsensical to talk about \"all numbers\". Wittgenstein's views about the foundations of mathematics were later criticised by Georg Kreisel and Paul Bernays, and investigated by Crispin Wright, among others.\nCategory theorists have proposed topos theory as an alternative to traditional axiomatic set theory. Topos theory can interpret various alternatives to that theory, such as constructivism, finite set theory, and computable set theory. Topoi also give a natural setting for forcing and discussions of the independence of choice from ZF, as well as providing the framework for pointless topology and Stone spaces.An active area of research is the univalent foundations and related to it homotopy type theory. Here, sets may be defined as certain kinds of types, with universal properties of sets arising from higher inductive types. Principles such as the axiom of choice and the law of the excluded middle appear in a spectrum of different forms, some of which can be proven, others which correspond to the classical notions; this allows for a detailed discussion of the effect of these axioms on mathematics.\n\n\n== See also ==\n\nGlossary of set theory\nCategory theory\nList of set theory topics\nRelational model \u2013 borrows from set theory\n\n\n== Notes ==\n\n\n== Further reading ==\nDevlin, Keith, 1993. The Joy of Sets (2nd ed.). Springer Verlag, ISBN 0-387-94094-4\nFerreir\u00f3s, Jose, 2007 (1999). Labyrinth of Thought: A history of set theory and its role in modern mathematics. Basel, Birkh\u00e4user. ISBN 978-3-7643-8349-7\nJohnson, Philip, 1972. A History of Set Theory. Prindle, Weber & Schmidt ISBN 0-87150-154-6\nKunen, Kenneth, 1980. Set Theory: An Introduction to Independence Proofs. North-Holland, ISBN 0-444-85401-0.\nPotter, Michael, 2004. Set Theory and Its Philosophy: A Critical Introduction. Oxford University Press.\nTiles, Mary, 2004 (1989). The Philosophy of Set Theory: An Historical Introduction to Cantor's Paradise. Dover Publications. ISBN 978-0-486-43520-6\nRaymond M. Smullyan, Melvin Fitting, 2010, Set Theory And The Continuum Problem. Dover Publications ISBN 978-0-486-47484-7. Complete foundation in modern set theory with an emphasis on mathematical logic.\n\n\n== External links ==\nForeman, Matthew, Akihiro Kanamori, eds. Handbook of Set Theory. 3 vols., 2010. Each chapter surveys some aspect of contemporary research in set theory. Does not cover established elementary set theory, on which see Devlin (1993).\nHazewinkel, Michiel, ed. (2001) [1994], \"Axiomatic set theory\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nHazewinkel, Michiel, ed. (2001) [1994], \"Set theory\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nJech, Thomas (2002). \"Set Theory\", Stanford Encyclopedia of Philosophy.\nSchoenflies, Arthur (1898). Mengenlehre in Klein's encyclopedia.\nOnline books, and library resources in your library and in other libraries about set theory", "quantum mechanics": "Quantum mechanics (QM; also known as quantum physics, quantum theory, the wave mechanical model, or matrix mechanics), including quantum field theory, is a fundamental theory in physics which describes nature at the smallest scales of energy levels of atoms and subatomic particles.Classical physics (the physics existing before quantum mechanics) is a set of fundamental theories which describes nature at ordinary  (macroscopic) scale. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale. \nQuantum mechanics differs from classical physics in that energy, momentum, angular momentum and other quantities of a system are restricted to discrete values (quantization); objects have characteristics of both particles and waves (wave-particle duality); and there are limits to the precision with which quantities can be measured (uncertainty principle).Quantum mechanics gradually arose from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and from the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Erwin Schr\u00f6dinger, Werner Heisenberg, Max Born and others.  The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle.\nImportant applications of quantum theory include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy.  Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.\n\n\n== History ==\n\nScientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations. In 1803, Thomas Young, an English polymath, performed the famous double-slit experiment that he later described in a paper titled On the nature of light and colours. This experiment played a major role in the general acceptance of the wave theory of light.\nIn 1838, Michael Faraday discovered cathode rays. These studies were followed by the 1859 statement of the black-body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system can be discrete, and the 1900 quantum hypothesis of Max Planck. Planck's hypothesis that energy is radiated and absorbed in discrete \"quanta\" (or energy packets) precisely matched the observed patterns of black-body radiation.\nIn 1896, Wilhelm Wien empirically determined a distribution law of black-body radiation, known as Wien's law in his honor. Ludwig Boltzmann independently arrived at this result by considerations of Maxwell's equations. However, it was valid only at high frequencies and underestimated the radiance at low frequencies. Later, Planck corrected this model using Boltzmann's statistical interpretation of thermodynamics and proposed what is now called Planck's law, which led to the development of quantum mechanics.\nFollowing Max Planck's solution in 1900 to the black-body radiation problem (reported 1859), Albert Einstein offered a quantum-based theory to explain the photoelectric effect (1905, reported 1887).  Around 1900-1910, the atomic theory and the corpuscular theory of light first came to be widely accepted as scientific fact; these latter theories can be viewed as quantum theories of matter and electromagnetic radiation, respectively.\nAmong the first to study quantum phenomena in nature were Arthur Compton, C. V. Raman, and Pieter Zeeman, each of whom has a quantum effect named after him. Robert Andrews Millikan studied the photoelectric effect experimentally, and Albert Einstein developed a theory for it. At the same time, Ernest Rutherford experimentally discovered the nuclear model of the atom, for which Niels Bohr developed his theory of the atomic structure, which was later confirmed by the experiments of Henry Moseley. In 1913, Peter Debye extended Niels Bohr's theory of atomic structure, introducing elliptical orbits, a concept also introduced by Arnold Sommerfeld. This phase is known as old quantum theory.\nAccording to Planck, each energy element (E) is proportional to its frequency (\u03bd):\n\n  \n    \n      \n        E\n        =\n        h\n        \u03bd\n         \n      \n    \n    {\\displaystyle E=h\\nu \\ }\n  ,\nwhere h is Planck's constant.\nPlanck cautiously insisted that this was simply an aspect of the processes of absorption and emission of radiation and had nothing to do with the physical reality of the radiation itself. In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery. However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. He won the 1921 Nobel Prize in Physics for this work.\nEinstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete quantum of energy that was dependent on its frequency.\n\nThe foundations of quantum mechanics were established during the first half of the 20th century by Max Planck, Niels Bohr, Werner Heisenberg, Louis de Broglie, Arthur Compton, Albert Einstein, Erwin Schr\u00f6dinger, Max Born, John von Neumann, Paul Dirac, Enrico Fermi, Wolfgang Pauli, Max von Laue, Freeman Dyson, David Hilbert, Wilhelm Wien, Satyendra Nath Bose, Arnold Sommerfeld, and others. The Copenhagen interpretation of Niels Bohr became widely accepted.\nIn the mid-1920s, developments in quantum mechanics led to its becoming the standard formulation for atomic physics. In the summer of 1925, Bohr and Heisenberg published results that closed the old quantum theory. Out of deference to their particle-like behavior in certain processes and measurements, light quanta came to be called photons (1926). In 1926 Erwin Schr\u00f6dinger suggested a partial differential equation for the wave functions of particles like electrons. And when effectively restricted to a finite region, this equation allowed only certain modes, corresponding to discrete quantum states\u2014whose properties turned out to be exactly the same as implied by matrix mechanics. From Einstein's simple postulation was born a flurry of debating, theorizing, and testing. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.It was found that subatomic particles and electromagnetic waves are neither simply particle nor wave but have certain properties of each. This originated the concept of wave\u2013particle duality.By 1930, quantum mechanics had been further unified and formalized by the work of David Hilbert, Paul Dirac and John von Neumann with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines including quantum chemistry, quantum electronics, quantum optics, and quantum information science. Its speculative modern developments include string theory and quantum gravity theories. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies.While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors,  and superfluids.The word quantum derives from the Latin, meaning \"how great\" or \"how much\". In quantum mechanics, it refers to a discrete unit assigned to certain physical quantities such as the energy of an atom at rest (see Figure 1). The discovery that particles are discrete packets of energy with wave-like properties led to the branch of physics dealing with atomic and subatomic systems which is today called quantum mechanics. It underlies the mathematical framework of many fields of physics and chemistry, including condensed matter physics, solid-state physics, atomic physics, molecular physics, computational physics, computational chemistry, quantum chemistry, particle physics, nuclear chemistry, and nuclear physics. Some fundamental aspects of the theory are still actively studied.Quantum mechanics is essential to understanding the behavior of systems at atomic length scales and smaller. If the physical nature of an atom were solely described by classical mechanics, electrons would not orbit the nucleus, since orbiting electrons emit radiation (due to circular motion) and would eventually collide with the nucleus due to this loss of energy. This framework was unable to explain the stability of atoms. Instead, electrons remain in an uncertain, non-deterministic, smeared, probabilistic wave\u2013particle orbital about the nucleus, defying the traditional assumptions of classical mechanics and electromagnetism.Quantum mechanics was initially developed to provide a better explanation and description of the atom, especially the differences in the spectra of light emitted by different isotopes of the same chemical element, as well as subatomic particles. In short, the quantum-mechanical atomic model has succeeded spectacularly in the realm where classical mechanics and electromagnetism falter.\nBroadly speaking, quantum mechanics incorporates four classes of phenomena for which classical physics cannot account:\n\nquantization of certain physical properties\nquantum entanglement\nprinciple of uncertainty\nwave\u2013particle duality\n\n\n== Mathematical formulations ==\n\nIn the mathematically rigorous formulation of quantum mechanics developed by Paul Dirac, David Hilbert, John von Neumann, and Hermann Weyl, the possible states of a quantum mechanical system are symbolized as unit vectors (called state vectors). Formally, these reside in a complex separable Hilbert space\u2014variously called the state space or the associated Hilbert space of the system\u2014that is well defined up to a complex number of norm 1 (the phase factor). In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system\u2014for example, the state space for position and momentum states is the space of square-integrable functions, while the state space for the spin of a single proton is just the product of two complex planes. Each observable is represented by a maximally Hermitian (precisely: by a self-adjoint) linear operator acting on the state space. Each eigenstate of an observable corresponds to an eigenvector of the operator, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. If the operator's spectrum is discrete, the observable can attain only those discrete eigenvalues.\nIn the formalism of quantum mechanics, the state of a system at a given time is described by a complex wave function, also referred to as state vector in a complex vector space. This abstract mathematical object allows for the calculation of probabilities of outcomes of concrete experiments. For example, it allows one to compute the probability of finding an electron in a particular region around the nucleus at a particular time. Contrary to classical mechanics, one can never make simultaneous predictions of conjugate variables, such as position and momentum, to arbitrary precision. For instance, electrons may be considered (to a certain probability) to be located somewhere within a given region of space, but with their exact positions unknown. Contours of constant probability density, often referred to as \"clouds\", may be drawn around the nucleus of an atom to conceptualize where the electron might be located with the most probability. Heisenberg's uncertainty principle quantifies the inability to precisely locate the particle given its conjugate momentum.According to one interpretation, as the result of a measurement the wave function containing the probability information for a system collapses from a given initial state to a particular eigenstate. The possible results of a measurement are the eigenvalues of the operator representing the observable\u2014which explains the choice of Hermitian operators, for which all the eigenvalues are real. The probability distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator. Heisenberg's uncertainty principle is represented by the statement that the operators corresponding to certain observables do not commute.\nThe probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr\u2013Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a \"measurement\" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of \"wave function collapse\" (see, for example, the relative state interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled, so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.Generally, quantum mechanics does not assign definite values. Instead, it makes a prediction using a probability distribution; that is, it describes the probability of obtaining the possible outcomes from measuring an observable. Often these results are skewed by many causes, such as dense probability clouds. Probability clouds are approximate (but better than the Bohr model) whereby electron location is given by a probability function, the wave function eigenvalue, such that the probability is the squared modulus of the complex amplitude, or quantum state nuclear attraction. Naturally, these probabilities will depend on the quantum state at the \"instant\" of the measurement. Hence, uncertainty is involved in the value. There are, however, certain states that are associated with a definite value of a particular observable. These are known as eigenstates of the observable (\"eigen\" can be translated from German as meaning \"inherent\" or \"characteristic\").In the everyday world, it is natural and intuitive to think of everything (every observable) as being in an eigenstate. Everything appears to have a definite position, a definite momentum, a definite energy, and a definite time of occurrence. However, quantum mechanics does not pinpoint the exact values of a particle's position and momentum (since they are conjugate pairs) or its energy and time (since they too are conjugate pairs).  Rather, it provides only a range of probabilities in which that particle might be given its momentum and momentum probability. Therefore, it is helpful to use different words to describe states having uncertain values and states having definite values (eigenstates).\nUsually, a system will not be in an eigenstate of the observable (particle) we are interested in. However, if one measures the observable, the wave function will instantaneously be an eigenstate (or \"generalized\" eigenstate) of that observable. This process is known as wave function collapse, a controversial and much-debated process that involves expanding the system under study to include the measurement device. If one knows the corresponding wave function at the instant before the measurement, one will be able to compute the probability of the wave function collapsing into each of the possible eigenstates.\nFor example, the free particle in the previous example will usually have a wave function that is a wave packet centered around some mean position x0 (neither an eigenstate of position nor of momentum). When one measures the position of the particle, it is impossible to predict with certainty the result. It is probable, but not certain, that it will be near x0, where the amplitude of the wave function is large. After the measurement is performed, having obtained some result x, the wave function collapses into a position eigenstate centered at x.The time evolution of a quantum state is described by the Schr\u00f6dinger equation, in which the Hamiltonian (the operator corresponding to the total energy of the system) generates the time evolution. The time evolution of wave functions is deterministic in the sense that - given a wave function at an initial time - it makes a definite prediction of what the wave function will be at any later time.During a measurement, on the other hand, the change of the initial wave function into another, later wave function is not deterministic, it is unpredictable (i.e., random). A time-evolution simulation can be seen here.Wave functions change as time progresses. The Schr\u00f6dinger equation describes how wave functions change in time, playing a role similar to Newton's second law in classical mechanics. The Schr\u00f6dinger equation, applied to the aforementioned example of the free particle, predicts that the center of a wave packet will move through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more uncertain with time. This also has the effect of turning a position eigenstate (which can be thought of as an infinitely sharp wave packet) into a broadened wave packet that no longer represents a (definite, certain) position eigenstate.\n\nSome wave functions produce probability distributions that are constant, or independent of time\u2014such as when in a stationary state of constant energy, time vanishes in the absolute square of the wave function. Many systems that are treated dynamically in classical mechanics are described by such \"static\" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics it is described by a static, spherically symmetric wave function surrounding the nucleus (Fig. 1) (note, however, that only the lowest angular momentum states, labeled s, are spherically symmetric).The Schr\u00f6dinger equation acts on the entire probability amplitude, not merely its absolute value. Whereas the absolute value of the probability amplitude encodes information about probabilities, its phase encodes information about the interference between quantum states. This gives rise to the \"wave-like\" behavior of quantum states. As it turns out, analytic solutions of the Schr\u00f6dinger equation are available for only a very small number of relatively simple model Hamiltonians, of which the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom are the most important representatives. Even the helium atom\u2014which contains just one more electron than does the hydrogen atom\u2014has defied all attempts at a fully analytic treatment.\nThere exist several techniques for generating approximate solutions, however. In the important method known as perturbation theory, one uses the analytic result for a simple quantum mechanical model to generate a result for a more complicated model that is related to the simpler model by (for one example) the addition of a weak potential energy. Another method is the \"semi-classical equation of motion\" approach, which applies to systems for which quantum mechanics produces only weak (small) deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of quantum chaos.\n\n\n== Mathematically equivalent formulations of quantum mechanics ==\nThere are numerous mathematically equivalent formulations of quantum mechanics. One of the oldest and most commonly used formulations is the \"transformation theory\" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics - matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schr\u00f6dinger).Especially since Werner Heisenberg was awarded the Nobel Prize in Physics in 1932 for the creation of quantum mechanics, the role of Max Born in the development of QM was overlooked until the 1954 Nobel award. The role is noted in a 2005 biography of Born, which recounts his role in the matrix formulation of quantum mechanics, and the use of probability amplitudes. Heisenberg himself acknowledges having learned matrices from Born, as published in a 1940 festschrift honoring Max Planck.  In the matrix formulation, the instantaneous state of a quantum system encodes the probabilities of its measurable properties, or \"observables\". Examples of observables include energy, position, momentum, and angular momentum. Observables can be either continuous (e.g., the position of a particle) or discrete (e.g., the energy of an electron bound to a hydrogen atom). An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.\n\n\n== Interactions with other scientific theories ==\nThe rules of quantum mechanics are fundamental. They assert that the state space of a system is a Hilbert space (crucially, that the space has an inner product) and that observables of that system are Hermitian operators acting on vectors in that space\u2014although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system. An important guide for making these choices is the correspondence principle, which states that the predictions of quantum mechanics reduce to those of classical mechanics when a system moves to higher energies or, equivalently, larger quantum numbers, i.e. whereas a single particle exhibits a degree of randomness, in systems incorporating millions of particles averaging takes over and, at the high energy limit, the statistical probability of random behaviour approaches zero. In other words, classical mechanics is simply a quantum mechanics of large systems. This \"high energy\" limit is known as the classical or correspondence limit. One can even start from an established classical model of a particular system, then attempt to guess the underlying quantum model that would give rise to the classical model in the correspondence limit.\n\nWhen quantum mechanics was originally formulated, it was applied to models whose\ncorrespondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.\nEarly attempts to merge quantum mechanics with special relativity involved the replacement of the Schr\u00f6dinger equation with a covariant equation such as the Klein\u2013Gordon equation or the Dirac equation. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of quantum field theory, which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory, quantum electrodynamics, provides a fully quantum description of the electromagnetic interaction. The full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been employed since the inception of quantum mechanics, is to treat charged particles as quantum mechanical objects being acted on by a classical electromagnetic field. For example, the elementary quantum model of the hydrogen atom describes the electric field of the hydrogen atom using a classical \n  \n    \n      \n        \n          \u2212\n          \n            e\n            \n              2\n            \n          \n          \n            /\n          \n          (\n          4\n          \u03c0\n          \n            \u03f5\n            \n              \n                \n                \n                  0\n                \n              \n            \n          \n          r\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle -e^{2}/(4\\pi \\epsilon _{_{0}}r)}\n   Coulomb potential. This \"semi-classical\" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of photons by charged particles.\nQuantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg. These three men shared the Nobel Prize in Physics in 1979 for this work.It has proven difficult to construct quantum models of gravity, the remaining fundamental force. Semi-classical approximations are workable, and have led to predictions such as Hawking radiation. However, the formulation of a complete theory of quantum gravity is hindered by apparent incompatibilities between general relativity (the most accurate theory of gravity currently known) and some of the fundamental assumptions of quantum theory. The resolution of these incompatibilities is an area of active research, and theories such as string theory are among the possible candidates for a future theory of quantum gravity.\nClassical mechanics has also been extended into the complex domain, with complex classical mechanics exhibiting behaviors similar to quantum mechanics.\n\n\n=== Quantum mechanics and classical physics ===\nPredictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy. According to the correspondence principle between classical and quantum mechanics, all objects obey the laws of quantum mechanics, and classical mechanics is just an approximation for large systems of objects (or a statistical quantum mechanics of a large collection of particles). The laws of classical mechanics thus follow from the laws of quantum mechanics as a statistical average at the limit of large systems or large quantum numbers. However, chaotic systems do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.\nQuantum coherence is an essential difference between classical and quantum theories as illustrated by the Einstein\u2013Podolsky\u2013Rosen (EPR) paradox \u2014 an attack on a certain philosophical interpretation of quantum mechanics by an appeal to local realism. Quantum interference involves adding together probability amplitudes, whereas classical \"waves\" infer that there is an adding together of intensities. For microscopic bodies, the extension of the system is much smaller than the coherence length, which gives rise to long-range entanglement and other nonlocal phenomena characteristic of quantum systems. Quantum coherence is not typically evident at macroscopic scales, though an exception to this rule may occur at extremely low temperatures (i.e. approaching absolute zero) at which quantum behavior may manifest itself macroscopically. This is in accordance with the following observations:\n\nMany macroscopic properties of a classical system are a direct consequence of the quantum behavior of its parts. For example, the stability of bulk matter (consisting of atoms and molecules which would quickly collapse under electric forces alone), the rigidity of solids, and the mechanical, thermal, chemical, optical and magnetic properties of matter are all results of the interaction of electric charges under the rules of quantum mechanics.\nWhile the seemingly \"exotic\" behavior of matter posited by quantum mechanics and relativity theory become more apparent when dealing with particles of extremely small size or velocities approaching the speed of light, the laws of classical, often considered \"Newtonian\", physics remain accurate in predicting the behavior of the vast majority of \"large\" objects (on the order of the size of large molecules or bigger) at velocities much smaller than the velocity of light.\n\n\n=== Copenhagen interpretation of quantum versus classical kinematics ===\nA big difference between classical and quantum mechanics is that they use very different kinematic descriptions.In Niels Bohr's mature view, quantum mechanical phenomena are required to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics. The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, in terms of both position and momentum, of an initial condition or \"state\" (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition. In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous \"state\" in the classical sense of that word. Thus there are two kinds of processes in quantum mechanics: stationary and transitional. For a stationary process, the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined. Given its initial condition, prediction of its final condition is possible, causally but only probabilistically, because the Schr\u00f6dinger equation is deterministic for wave function evolution, but the wave function describes the system only probabilistically.For many experiments, it is possible to think of the initial and final conditions of the system as being a particle. In some cases it appears that there are potentially several spatially distinct pathways or trajectories by which a particle might pass from initial to final condition. It is an important feature of the quantum kinematic description that it does not permit a unique definite statement of which of those pathways is actually followed. Only the initial and final conditions are definite, and, as stated in the foregoing paragraph, they are defined only as precisely as allowed by the configuration space description or its equivalent. In every case for which a quantum kinematic description is needed, there is always a compelling reason for this restriction of kinematic precision. An example of such a reason is that for a particle to be experimentally found in a definite position, it must be held motionless; for it to be experimentally found to have a definite momentum, it must have free motion; these two are logically incompatible.Classical kinematics does not primarily demand experimental description of its phenomena. It allows completely precise description of an instantaneous state by a value in phase space, the Cartesian product of configuration and momentum spaces. This description simply assumes or imagines a state as a physically existing entity without concern about its experimental measurability. Such a description of an initial condition, together with Newton's laws of motion, allows a precise deterministic and causal prediction of a final condition, with a definite trajectory of passage. Hamiltonian dynamics can be used for this. Classical kinematics also allows the description of a process analogous to the initial and final condition description used by quantum mechanics. Lagrangian mechanics applies to this. For processes that need account to be taken of actions of a small number of Planck constants, classical kinematics is not adequate; quantum mechanics is needed.\n\n\n=== General relativity and quantum mechanics ===\nEven with the defining postulates of both Einstein's theory of general relativity and quantum theory being indisputably supported by rigorous and repeated empirical evidence, and while they do not directly contradict each other theoretically (at least with regard to their primary claims), they have proven extremely difficult to incorporate into one consistent, cohesive model.Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant \"Theory of Everything\" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th and 21st century physics. Many prominent physicists, including Stephen Hawking, have labored for many years in the attempt to discover a theory underlying everything. This TOE would combine not only the different models of subatomic physics, but also derive the four fundamental forces of nature - the strong force, electromagnetism, the weak force, and gravity - from a single force or phenomenon. While Stephen Hawking was initially a believer in the Theory of Everything, after considering G\u00f6del's Incompleteness Theorem, he has concluded that one is not obtainable, and has stated so publicly in his lecture \"G\u00f6del and the End of Physics\" (2002).\n\n\n=== Attempts at a unified field theory ===\n\nThe quest to unify the fundamental forces through quantum mechanics is still ongoing. Quantum electrodynamics (or \"quantum electromagnetism\"), which is currently (in the perturbative regime at least) the most accurately tested physical theory in competition with general relativity, has been successfully merged with the weak nuclear force into the electroweak force and work is currently being done to merge the electroweak and strong force into the electrostrong force. Current predictions state that at around 1014 GeV the three aforementioned forces are fused into a single unified field. Beyond this \"grand unification\", it is speculated that it may be possible to merge gravity with the other three gauge symmetries, expected to occur at roughly 1019 GeV. However \u2014 and while special relativity is parsimoniously incorporated into quantum electrodynamics \u2014 the expanded general relativity, currently the best theory describing the gravitation force, has not been fully incorporated into quantum theory. One of those searching for a coherent TOE is Edward Witten, a theoretical physicist who formulated the M-theory, which is an attempt at describing the supersymmetrical based string theory. M-theory posits that our apparent 4-dimensional spacetime is, in reality, actually an 11-dimensional spacetime containing 10 spatial dimensions and 1 time dimension, although 7 of the spatial dimensions are - at lower energies - completely \"compactified\" (or infinitely curved) and not readily amenable to measurement or probing.\nAnother popular theory is Loop quantum gravity (LQG), a theory first proposed by Carlo Rovelli that describes the quantum properties of gravity. It is also a theory of quantum space and quantum time, because in general relativity the geometry of spacetime is a manifestation of gravity. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. The main output of the theory is a physical picture of space where space is granular. The granularity is a direct consequence of the quantization. It has the same nature of the granularity of the photons in the quantum theory of electromagnetism or the discrete levels of the energy of the atoms. But here it is space itself which is discrete.\nMore precisely, space can be viewed as an extremely fine fabric or network \"woven\" of finite loops. These networks of loops are called spin networks. The evolution of a spin network over time is called a spin foam. The predicted size of this structure is the Planck length, which is approximately 1.616\u00d710\u221235 m. According to theory, there is no meaning to length shorter than this (cf. Planck scale energy). Therefore, LQG predicts that not just matter, but also space itself, has an atomic structure.\n\n\n== Philosophical implications ==\n\nSince its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. Even fundamental issues, such as Max Born's basic rules concerning probability amplitudes and probability distributions, took decades to be appreciated by society and many leading scientists. Richard Feynman once said, \"I think I can safely say that nobody understands quantum mechanics.\"  According to Steven Weinberg, \"There is now in my opinion no entirely satisfactory interpretation of quantum mechanics.\"The Copenhagen interpretation \u2014 due largely to Niels Bohr and Werner Heisenberg \u2014 remains most widely accepted amongst physicists, some 75 years after its enunciation. According to this interpretation, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but instead must be considered a final renunciation of the classical idea of \"causality.\" It is also believed therein that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the conjugate nature of evidence obtained under different experimental situations.\nAlbert Einstein, himself one of the founders of quantum theory, did not accept some of the more philosophical or metaphysical interpretations of quantum mechanics, such as rejection of determinism and of causality. He is famously quoted as saying, in response to this aspect, \"God does not play with dice\". He rejected the concept that the state of a physical system depends on the experimental arrangement for its measurement. He held that a state of nature occurs in its own right, regardless of whether or how it might be observed. In that view, he is supported by the currently accepted definition of a quantum state, which remains invariant under arbitrary choice of configuration space for its representation, that is to say, manner of observation. He also held that underlying quantum mechanics there should be a theory that thoroughly and directly expresses the rule against action at a distance; in other words, he insisted on the principle of locality. He considered, but rejected on theoretical grounds, a particular proposal for hidden variables to obviate the indeterminism or acausality of quantum mechanical measurement. He considered that quantum mechanics was a currently valid but not a permanently definitive theory for quantum phenomena. He thought its future replacement would require profound conceptual advances, and would not come quickly or easily. The Bohr-Einstein debates provide a vibrant critique of the Copenhagen Interpretation from an epistemological point of view. In arguing for his views, he produced a series of objections, the most famous of which has become known as the Einstein\u2013Podolsky\u2013Rosen paradox.\nJohn Bell showed that this EPR paradox led to experimentally testable differences between quantum mechanics and theories that rely on added hidden variables. Experiments have been performed confirming the accuracy of quantum mechanics, thereby demonstrating that quantum mechanics cannot be improved upon by addition of hidden variables. Alain Aspect's initial experiments in 1982, and many subsequent experiments since, have definitively verified quantum entanglement. By the early 1980s, experiments had shown that such inequalities were indeed violated in practice\u2014so that there were in fact correlations of the kind suggested by quantum mechanics. At first these just seemed like isolated esoteric effects, but by the mid-1990s, they were being codified in the field of quantum information theory, and led to constructions with names like quantum cryptography and quantum teleportation.Entanglement, as demonstrated in Bell-type experiments, does not, however, violate causality, since no transfer of information happens. Quantum entanglement forms the basis of quantum cryptography, which is proposed for use in high-security commercial applications in banking and government.\nThe Everett many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes. This is not accomplished by introducing some \"new axiom\" to quantum mechanics, but on the contrary, by removing the axiom of the collapse of the wave packet. All of the possible consistent states of the measured system and the measuring apparatus (including the observer) are present in a real physical - not just formally mathematical, as in other interpretations - quantum superposition. Such a superposition of consistent state combinations of different systems is called an entangled state. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we can only observe the universe (i.e., the consistent state contribution to the aforementioned superposition) that we, as observers, inhabit. Everett's interpretation is perfectly consistent with John Bell's experiments and makes them intuitively understandable. However, according to the theory of quantum decoherence, these \"parallel universes\" will never be accessible to us. The inaccessibility can be understood as follows: once a measurement is done, the measured system becomes entangled with both the physicist who measured it and a huge number of other particles, some of which are photons flying away at the speed of light towards the other end of the universe. In order to prove that the wave function did not collapse, one would have to bring all these particles back and measure them again, together with the system that was originally measured. Not only is this completely impractical, but even if one could theoretically do this, it would have to destroy any evidence that the original measurement took place (including the physicist's memory). In light of these Bell tests, Cramer (1986) formulated his transactional interpretation which is unique in providing a physical explanation for the Born rule..Relational quantum mechanics appeared in the late 1990s as the modern derivative of the Copenhagen Interpretation.\n\n\n== Applications ==\nQuantum mechanics has had enormous success in explaining many of the features of our universe. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Quantum mechanics has strongly influenced string theories, candidates for a Theory of Everything (see reductionism).\nQuantum mechanics is also critically important for understanding how individual atoms are joined by covalent bond to form molecules. The application of quantum mechanics to chemistry is known as quantum chemistry.  Quantum mechanics can also provide quantitative insight into ionic and covalent bonding processes by explicitly showing which molecules are energetically favorable to which others and the magnitudes of the energies involved. Furthermore, most of the calculations performed in modern computational chemistry rely on quantum mechanics.\nIn many aspects modern technology operates at a scale where quantum effects are significant.\n\n\n=== Electronics ===\nMany modern electronic devices are designed using quantum mechanics. Examples include the laser, the transistor (and thus the microchip), the electron microscope, and magnetic resonance imaging (MRI). The study of semiconductors led to the invention of the diode and the transistor, which are indispensable parts of modern electronics systems, computer and telecommunication devices. Another application is for making laser diode and light emitting diode which are a high-efficiency source of light.\n\nMany electronic devices operate under effect of quantum tunneling. It even exists in the simple light switch. The switch would not work if electrons could not quantum tunnel through the layer of oxidation on the metal contact surfaces. Flash memory chips found in USB drives use quantum tunneling to erase their memory cells. Some negative differential resistance devices also utilize quantum tunneling effect, such as resonant tunneling diode. Unlike classical diodes, its current is carried by resonant tunneling through two or more potential barriers (see right figure). Its negative resistance behavior can only be understood with quantum mechanics: As the confined state moves close to Fermi level, tunnel current increases. As it moves away, current decreases. Quantum  mechanics is necessary to understanding and designing such electronic devices.\n\n\n=== Cryptography ===\nResearchers are currently seeking robust methods of directly manipulating quantum states. Efforts are being made to more fully develop quantum cryptography, which will theoretically allow guaranteed secure transmission of information.\nAn inherent advantage yielded by quantum cryptography when compared to classical cryptography is the detection of passive eavesdropping. This is a natural result of the behavior of quantum bits; due to the observer effect, if a bit in a superposition state were to be observed, the superposition state would collapse into an eigenstate. Because the intended recipient was expecting to receive the bit in a superposition state, the intended recipient would know there was an attack, because the bit's state would no longer be in a superposition.\n\n\n=== Quantum computing ===\nAnother goal is the development of quantum computers, which are expected to perform certain computational tasks exponentially faster than classical computers. Instead of using classical bits, quantum computers use qubits, which can be in superpositions of states. Quantum programmers are able to  manipulate the superposition of qubits in order to solve problems that classical computing cannot do effectively, such as searching unsorted databases or integer factorization. IBM claims that the advent of quantum computing may progress the fields of medicine, logistics, financial services, artificial intelligence and cloud security.Another active research topic is quantum teleportation, which deals with techniques to transmit quantum information over arbitrary distances.\n\n\n=== Macroscale quantum effects ===\nWhile quantum mechanics primarily applies to the smaller atomic regimes of matter and energy, some systems exhibit quantum mechanical effects on a large scale. Superfluidity, the frictionless flow of a liquid at temperatures near absolute zero, is one well-known example. So is the closely related phenomenon of superconductivity, the frictionless flow of an electron gas in a conducting material (an electric current) at sufficiently low temperatures. The fractional quantum Hall effect is a topological ordered state which corresponds to patterns of long-range quantum entanglement. States with different topological orders (or different patterns of long range entanglements) cannot change into each other without a phase transition.\n\n\n=== Quantum theory ===\nQuantum theory also provides accurate descriptions for many previously unexplained phenomena, such as black-body radiation and the stability of the orbitals of electrons in atoms. It has also given insight into the workings of many different biological systems, including smell receptors and protein structures. Recent work on photosynthesis has provided evidence that quantum correlations play an essential role in this fundamental process of plants and many other organisms. Even so, classical physics can often provide good approximations to results otherwise obtained by quantum physics, typically in circumstances with large numbers of particles or large quantum numbers. Since classical formulas are much simpler and easier to compute than quantum formulas, classical approximations are used and preferred when the system is large enough to render the effects of quantum mechanics insignificant.\n\n\n== Examples ==\n\n\n=== Free particle ===\nFor example, consider a free particle. In quantum mechanics, a free matter is described by a wave function.  The particle properties of the matter become apparent when we measure its position and velocity.  The wave properties of the matter become apparent when we measure its wave properties like interference.  The wave\u2013particle duality feature is incorporated in the relations of coordinates and operators in the formulation of quantum mechanics.  Since the matter is free (not subject to any interactions), its quantum state can be represented as a wave of arbitrary shape and extending over space as a wave function. The position and momentum of the particle are observables. The Uncertainty Principle states that both the position and the momentum cannot simultaneously be measured with complete precision. However, one can measure the position (alone) of a moving free particle, creating an eigenstate of position with a wave function that is very large (a Dirac delta) at a particular position x, and zero everywhere else. If one performs a position measurement on such a wave function, the resultant x will be obtained with 100% probability (i.e., with full certainty, or complete precision). This is called an eigenstate of position\u2014or, stated in mathematical terms, a generalized position eigenstate (eigendistribution). If the particle is in an eigenstate of position, then its momentum is completely unknown. On the other hand, if the particle is in an eigenstate of momentum, then its position is completely unknown.\nIn an eigenstate of momentum having a plane wave form, it can be shown that the wavelength is equal to h/p, where h is Planck's constant and p is the momentum of the eigenstate.\n\n\n=== Particle in a box ===\n\nThe particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhere inside a certain region, and therefore infinite potential energy everywhere outside that region. For the one-dimensional case in the \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   direction, the time-independent Schr\u00f6dinger equation may be written\n\n  \n    \n      \n        \u2212\n        \n          \n            \n              \u210f\n              \n                2\n              \n            \n            \n              2\n              m\n            \n          \n        \n        \n          \n            \n              \n                d\n                \n                  2\n                \n              \n              \u03c8\n            \n            \n              d\n              \n                x\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        E\n        \u03c8\n        .\n      \n    \n    {\\displaystyle -{\\frac {\\hbar ^{2}}{2m}}{\\frac {d^{2}\\psi }{dx^{2}}}=E\\psi .}\n  With the differential operator defined by\n\n  \n    \n      \n        \n          \n            \n              \n                p\n                ^\n              \n            \n          \n          \n            x\n          \n        \n        =\n        \u2212\n        i\n        \u210f\n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {p}}_{x}=-i\\hbar {\\frac {d}{dx}}}\n  the previous equation is evocative of the classic kinetic energy analogue,\n\n  \n    \n      \n        \n          \n            1\n            \n              2\n              m\n            \n          \n        \n        \n          \n            \n              \n                p\n                ^\n              \n            \n          \n          \n            x\n          \n          \n            2\n          \n        \n        =\n        E\n        ,\n      \n    \n    {\\displaystyle {\\frac {1}{2m}}{\\hat {p}}_{x}^{2}=E,}\n  with state \n  \n    \n      \n        \u03c8\n      \n    \n    {\\displaystyle \\psi }\n   in this case having energy \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   coincident with the kinetic energy of the particle.\nThe general solutions of the Schr\u00f6dinger equation for the particle in a box are\n\n  \n    \n      \n        \u03c8\n        (\n        x\n        )\n        =\n        A\n        \n          e\n          \n            i\n            k\n            x\n          \n        \n        +\n        B\n        \n          e\n          \n            \u2212\n            i\n            k\n            x\n          \n        \n        \n        \n        E\n        =\n        \n          \n            \n              \n                \u210f\n                \n                  2\n                \n              \n              \n                k\n                \n                  2\n                \n              \n            \n            \n              2\n              m\n            \n          \n        \n      \n    \n    {\\displaystyle \\psi (x)=Ae^{ikx}+Be^{-ikx}\\qquad \\qquad E={\\frac {\\hbar ^{2}k^{2}}{2m}}}\n  or, from Euler's formula,\n\n  \n    \n      \n        \u03c8\n        (\n        x\n        )\n        =\n        C\n        sin\n        \u2061\n        k\n        x\n        +\n        D\n        cos\n        \u2061\n        k\n        x\n        .\n        \n      \n    \n    {\\displaystyle \\psi (x)=C\\sin kx+D\\cos kx.\\!}\n  The infinite potential walls of the box determine the values of C, D, and k at x = 0 and x = L where \u03c8 must be zero. Thus, at x = 0,\n\n  \n    \n      \n        \u03c8\n        (\n        0\n        )\n        =\n        0\n        =\n        C\n        sin\n        \u2061\n        0\n        +\n        D\n        cos\n        \u2061\n        0\n        =\n        D\n        \n      \n    \n    {\\displaystyle \\psi (0)=0=C\\sin 0+D\\cos 0=D\\!}\n  and D = 0. At x = L,\n\n  \n    \n      \n        \u03c8\n        (\n        L\n        )\n        =\n        0\n        =\n        C\n        sin\n        \u2061\n        k\n        L\n        .\n        \n      \n    \n    {\\displaystyle \\psi (L)=0=C\\sin kL.\\!}\n  in which C cannot be zero as this would conflict with the Born interpretation. Therefore, since sin(kL) = 0, kL must be an integer multiple of \u03c0,\n\n  \n    \n      \n        k\n        =\n        \n          \n            \n              n\n              \u03c0\n            \n            L\n          \n        \n        \n        \n        n\n        =\n        1\n        ,\n        2\n        ,\n        3\n        ,\n        \u2026\n        .\n      \n    \n    {\\displaystyle k={\\frac {n\\pi }{L}}\\qquad \\qquad n=1,2,3,\\ldots .}\n  The quantization of energy levels follows from this constraint on k, since\n\n  \n    \n      \n        E\n        =\n        \n          \n            \n              \n                \u210f\n                \n                  2\n                \n              \n              \n                \u03c0\n                \n                  2\n                \n              \n              \n                n\n                \n                  2\n                \n              \n            \n            \n              2\n              m\n              \n                L\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                n\n                \n                  2\n                \n              \n              \n                h\n                \n                  2\n                \n              \n            \n            \n              8\n              m\n              \n                L\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle E={\\frac {\\hbar ^{2}\\pi ^{2}n^{2}}{2mL^{2}}}={\\frac {n^{2}h^{2}}{8mL^{2}}}.}\n  \n\nThe ground state energy of the particles is E1  for n=1.\nEnergy of particle in the nth state is En =n2E1, n=2,3,4,.....\n\nParticle in a box with boundary condition V(x)=0 -a/2<x<+a/2\nIn this condition the general solution will be same, there will a little change to the final result, since the boundary conditions are changed\n\n  \n    \n      \n        \u03c8\n        (\n        x\n        )\n        =\n        C\n        sin\n        \u2061\n        k\n        x\n        +\n        D\n        cos\n        \u2061\n        k\n        x\n        .\n        \n      \n    \n    {\\displaystyle \\psi (x)=C\\sin kx+D\\cos kx.\\!}\n  \n\nAt x=0, the wave function is not actually zero at all value of n.\nClearly, from the wave function variation graph we have,\nAt n=1,3,4,...... the wave function follows a cosine curve with x=0 as origin\nAt n=2,4,6,...... the wave function follows a sine curve with x=0 as origin\n\nFrom this observation we can conclude that the wave function is alternatively sine and cosine.\nSo in this case the resultant wave equation is\n\n\u03c8n(x) = Acos(knx)   n=1,3,5,.............\n= Bsin(knx)    n=2,4,6,.............\n\n\n=== Finite potential well ===\n\nA finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth.\nThe finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well.\n\n\n=== Rectangular potential barrier ===\n\nThis is a model for the quantum tunneling effect which plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy. Quantum tunneling is central to physical phenomena involved in superlattices.\n\n\n=== Harmonic oscillator ===\n\nAs in the classical case, the potential for the quantum harmonic oscillator is given by\n\n  \n    \n      \n        V\n        (\n        x\n        )\n        =\n        \n          \n            1\n            2\n          \n        \n        m\n        \n          \u03c9\n          \n            2\n          \n        \n        \n          x\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle V(x)={\\frac {1}{2}}m\\omega ^{2}x^{2}.}\n  This problem can either be treated by directly solving the Schr\u00f6dinger equation, which is not trivial, or by using the more elegant \"ladder method\" first proposed by Paul Dirac. The eigenstates are given by\n\n  \n    \n      \n        \n          \u03c8\n          \n            n\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            \n              1\n              \n                \n                  2\n                  \n                    n\n                  \n                \n                \n                n\n                !\n              \n            \n          \n        \n        \u22c5\n        \n          \n            (\n            \n              \n                \n                  m\n                  \u03c9\n                \n                \n                  \u03c0\n                  \u210f\n                \n              \n            \n            )\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n        \u22c5\n        \n          e\n          \n            \u2212\n            \n              \n                \n                  m\n                  \u03c9\n                  \n                    x\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \u210f\n                \n              \n            \n          \n        \n        \u22c5\n        \n          H\n          \n            n\n          \n        \n        \n          (\n          \n            \n              \n                \n                  \n                    m\n                    \u03c9\n                  \n                  \u210f\n                \n              \n            \n            x\n          \n          )\n        \n        ,\n        \n      \n    \n    {\\displaystyle \\psi _{n}(x)={\\sqrt {\\frac {1}{2^{n}\\,n!}}}\\cdot \\left({\\frac {m\\omega }{\\pi \\hbar }}\\right)^{1/4}\\cdot e^{-{\\frac {m\\omega x^{2}}{2\\hbar }}}\\cdot H_{n}\\left({\\sqrt {\\frac {m\\omega }{\\hbar }}}x\\right),\\qquad }\n  \n  \n    \n      \n        n\n        =\n        0\n        ,\n        1\n        ,\n        2\n        ,\n        \u2026\n        .\n      \n    \n    {\\displaystyle n=0,1,2,\\ldots .}\n  where Hn are the Hermite polynomials\n\n  \n    \n      \n        \n          H\n          \n            n\n          \n        \n        (\n        x\n        )\n        =\n        (\n        \u2212\n        1\n        \n          )\n          \n            n\n          \n        \n        \n          e\n          \n            \n              x\n              \n                2\n              \n            \n          \n        \n        \n          \n            \n              d\n              \n                n\n              \n            \n            \n              d\n              \n                x\n                \n                  n\n                \n              \n            \n          \n        \n        \n          (\n          \n            e\n            \n              \u2212\n              \n                x\n                \n                  2\n                \n              \n            \n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle H_{n}(x)=(-1)^{n}e^{x^{2}}{\\frac {d^{n}}{dx^{n}}}\\left(e^{-x^{2}}\\right),}\n  and the corresponding energy levels are\n\n  \n    \n      \n        \n          E\n          \n            n\n          \n        \n        =\n        \u210f\n        \u03c9\n        \n          (\n          \n            n\n            +\n            \n              \n                1\n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle E_{n}=\\hbar \\omega \\left(n+{1 \\over 2}\\right).}\n  This is another example illustrating the quantification of energy for bound states.\n\n\n=== Step potential ===\n\nThe potential in this case is given by:\n\n  \n    \n      \n        V\n        (\n        x\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  0\n                  ,\n                \n                \n                  x\n                  <\n                  0\n                  ,\n                \n              \n              \n                \n                  \n                    V\n                    \n                      0\n                    \n                  \n                  ,\n                \n                \n                  x\n                  \u2265\n                  0.\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle V(x)={\\begin{cases}0,&x<0,\\\\V_{0},&x\\geq 0.\\end{cases}}}\n  The solutions are superpositions of left- and right-moving waves:\n\n  \n    \n      \n        \n          \u03c8\n          \n            1\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              \n                k\n                \n                  1\n                \n              \n            \n          \n        \n        \n          (\n          \n            \n              A\n              \n                \u2192\n              \n            \n            \n              e\n              \n                i\n                \n                  k\n                  \n                    1\n                  \n                \n                x\n              \n            \n            +\n            \n              A\n              \n                \u2190\n              \n            \n            \n              e\n              \n                \u2212\n                i\n                \n                  k\n                  \n                    1\n                  \n                \n                x\n              \n            \n          \n          )\n        \n        \n        x\n        <\n        0\n      \n    \n    {\\displaystyle \\psi _{1}(x)={\\frac {1}{\\sqrt {k_{1}}}}\\left(A_{\\rightarrow }e^{ik_{1}x}+A_{\\leftarrow }e^{-ik_{1}x}\\right)\\qquad x<0}\n  and\n\n  \n    \n      \n        \n          \u03c8\n          \n            2\n          \n        \n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              \n                k\n                \n                  2\n                \n              \n            \n          \n        \n        \n          (\n          \n            \n              B\n              \n                \u2192\n              \n            \n            \n              e\n              \n                i\n                \n                  k\n                  \n                    2\n                  \n                \n                x\n              \n            \n            +\n            \n              B\n              \n                \u2190\n              \n            \n            \n              e\n              \n                \u2212\n                i\n                \n                  k\n                  \n                    2\n                  \n                \n                x\n              \n            \n          \n          )\n        \n        \n        x\n        >\n        0\n      \n    \n    {\\displaystyle \\psi _{2}(x)={\\frac {1}{\\sqrt {k_{2}}}}\\left(B_{\\rightarrow }e^{ik_{2}x}+B_{\\leftarrow }e^{-ik_{2}x}\\right)\\qquad x>0}\n  ,with coefficients A and B determined from the boundary conditions and by imposing a continuous derivative on the solution, and where the wave vectors are related to the energy via\n\n  \n    \n      \n        \n          k\n          \n            1\n          \n        \n        =\n        \n          \n            2\n            m\n            E\n            \n              /\n            \n            \n              \u210f\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle k_{1}={\\sqrt {2mE/\\hbar ^{2}}}}\n  and\n\n  \n    \n      \n        \n          k\n          \n            2\n          \n        \n        =\n        \n          \n            2\n            m\n            (\n            E\n            \u2212\n            \n              V\n              \n                0\n              \n            \n            )\n            \n              /\n            \n            \n              \u210f\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle k_{2}={\\sqrt {2m(E-V_{0})/\\hbar ^{2}}}}\n  .Each term of the solution can be interpreted as an incident, reflected, or transmitted component of the wave, allowing the calculation of transmission and reflection coefficients. Notably, in contrast to classical mechanics, incident particles with energies greater than the potential step are partially reflected.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nBernstein, Jeremy (2009). Quantum Leaps. Cambridge, Massachusetts: Belknap Press of Harvard University Press. ISBN 978-0-674-03541-6. \nBohm, David (1989). Quantum Theory. Dover Publications. ISBN 0-486-65969-0. \nEisberg, Robert; Resnick, Robert (1985). Quantum Physics of Atoms, Molecules, Solids, Nuclei, and Particles (2nd ed.). Wiley. ISBN 0-471-87373-X. CS1 maint: Multiple names: authors list (link) \nLiboff, Richard L. (2002). Introductory Quantum Mechanics. Addison-Wesley. ISBN 0-8053-8714-5. \nMerzbacher, Eugen (1998). Quantum Mechanics. Wiley, John & Sons, Inc. ISBN 0-471-88702-1. \nSakurai, J. J. (1994). Modern Quantum Mechanics. Addison Wesley. ISBN 0-201-53929-2. \nShankar, R. (1994). Principles of Quantum Mechanics. Springer. ISBN 0-306-44790-8. \nStone, A. Douglas (2013). Einstein and the Quantum. Princeton University Press. ISBN 978-0-691-13968-5. \nMartinus J. G. Veltman (2003),  Facts and Mysteries in Elementary Particle Physics.\nZucav, Gary (1979, 2001). The Dancing Wu Li Masters: An overview of the new physics (Perennial Classics Edition) HarperCollins.On Wikibooks\n\nThis Quantum World\n\n\n== External links ==\n\n3D animations, applications and research for basic quantum effects (animations also available in commons.wikimedia.org (Universit\u00e9 paris Sud))\nQuantum Cook Book by R. Shankar, Open Yale PHYS 201 material (4pp)\nThe Modern Revolution in Physics - an online textbook.\nJ. O'Connor and E. F. Robertson: A history of quantum mechanics.\nIntroduction to Quantum Theory at Quantiki.\nQuantum Physics Made Relatively Simple: three video lectures by Hans Bethe\nH is for h-bar.\nQuantum Mechanics Books Collection: Collection of free booksCourse materialA collection of lectures on Quantum Mechanics\nQuantum Physics Database - Fundamentals and Historical Background of Quantum Theory.\nDoron Cohen: Lecture notes in Quantum Mechanics (comprehensive, with advanced topics).\nMIT OpenCourseWare: Chemistry.\nMIT OpenCourseWare: Physics. See 8.04\nStanford Continuing Education PHY 25: Quantum Mechanics by Leonard Susskind, see course description Fall 2007\n5\u00bd Examples in Quantum Mechanics\nImperial College Quantum Mechanics Course.\nSpark Notes - Quantum Physics.\nQuantum Physics Online : interactive introduction to quantum mechanics (RS applets).\nExperiments to the foundations of quantum physics with single photons.\nAQME : Advancing Quantum Mechanics for Engineers \u2014 by T.Barzso, D.Vasileska and G.Klimeck online learning resource with simulation tools on nanohub\nQuantum Mechanics by Martin Plenio\nQuantum Mechanics by Richard Fitzpatrick\nOnline course on Quantum TransportFAQsMany-worlds or relative-state interpretation.\nMeasurement in Quantum mechanics.MediaPHYS 201: Fundamentals of Physics II by Ramamurti Shankar, Open Yale Course\nLectures on Quantum Mechanics by Leonard Susskind\nEverything you wanted to know about the quantum world \u2014 archive of articles from New Scientist.\nQuantum Physics Research from Science Daily\nOverbye, Dennis (December 27, 2005). \"Quantum Trickery: Testing Einstein's Strangest Theory\". The New York Times. Retrieved April 12, 2010. \nAudio: Astronomy Cast Quantum Mechanics \u2014 June 2009. Fraser Cain interviews Pamela L. Gay.\n\"The Physics of Reality\", BBC Radio 4 discussion with Roger Penrose, Fay Dowker & Tony Sudbery (In Our Time, May 2, 2002).PhilosophyIsmael, Jenann. \"Quantum Mechanics\".  In Zalta, Edward N. Stanford Encyclopedia of Philosophy. \nKrips, Henry. \"Measurement in Quantum Theory\".  In Zalta, Edward N. Stanford Encyclopedia of Philosophy.", "arithmetic": "Arithmetic (from the Greek \u1f00\u03c1\u03b9\u03b8\u03bc\u03cc\u03c2 arithmos, \"number\") is a branch of mathematics that consists of the study of numbers, especially the properties of the traditional operations on them\u2014addition, subtraction, multiplication and division. Arithmetic is an elementary part of number theory, and number theory is considered to be one of the top-level divisions of modern mathematics, along with algebra, geometry, and analysis. The terms arithmetic and higher arithmetic were used until the beginning of the 20th century as synonyms for number theory and are sometimes still used to refer to a wider part of number theory.\n\n\n== History ==\nThe prehistory of arithmetic is limited to a small number of artifacts which may indicate the conception of addition and subtraction, the best-known being the Ishango bone from central Africa, dating from somewhere between 20,000 and 18,000 BC, although its interpretation is disputed.The earliest written records indicate the Egyptians and Babylonians used all the elementary arithmetic operations as early as 2000 BC. These artifacts do not always reveal the specific process used for solving problems, but the characteristics of the particular numeral system strongly influence the complexity of the methods. The hieroglyphic system for Egyptian numerals, like the later Roman numerals, descended from tally marks used for counting. In both cases, this origin resulted in values that used a decimal base but did not include positional notation. Complex calculations with Roman numerals required the assistance of a counting board or the Roman abacus to obtain the results.\nEarly number systems that included positional notation were not decimal, including the sexagesimal (base 60) system for Babylonian numerals and the vigesimal (base 20) system that defined Maya numerals. Because of this place-value concept, the ability to reuse the same digits for different values contributed to simpler and more efficient methods of calculation.\nThe continuous historical development of modern arithmetic starts with the Hellenistic civilization of ancient Greece, although it originated much later than the Babylonian and Egyptian examples. Prior to the works of Euclid around 300 BC, Greek studies in mathematics overlapped with philosophical and mystical beliefs. For example, Nicomachus summarized the viewpoint of the earlier Pythagorean approach to numbers, and their relationships to each other, in his Introduction to Arithmetic.\nGreek numerals were used by Archimedes, Diophantus and others in a positional notation not very different from ours. The ancient Greeks lacked a symbol for zero until the Hellenistic period, and they used three separate sets of symbols as digits: one set for the units place, one for the tens place, and one for the hundreds. For the thousands place they would reuse the symbols for the units place, and so on. Their addition algorithm was identical to ours, and their multiplication algorithm was only very slightly different. Their long division algorithm was the same, and the digit-by-digit square root algorithm, popularly used as recently as the 20th century, was known to Archimedes, who may have invented it. He preferred it to Hero's method of successive approximation because, once computed, a digit doesn't change, and the square roots of perfect squares, such as 7485696, terminate immediately as 2736. For numbers with a fractional part, such as 546.934, they used negative powers of 60 instead of negative powers of 10 for the fractional part 0.934.The ancient Chinese had advanced arithmetic studies dating from the Shang Dynasty and continuing through the Tang Dynasty, from basic numbers to advanced algebra. The ancient Chinese used a positional notation similar to that of the Greeks. Since they also lacked a symbol for zero, they had one set of symbols for the unit's place, and a second set for the ten's place. For the hundred's place they then reused the symbols for the unit's place, and so on. Their symbols were based on the ancient counting rods. It is a complicated question to determine exactly when the Chinese started calculating with positional representation, but it was definitely before 400 BC. The ancient Chinese were the first to meaningfully discover, understand, and apply negative numbers as explained in the Nine Chapters on the Mathematical Art (Jiuzhang Suanshu), which was written by Liu Hui.\nThe gradual development of Hindu\u2013Arabic numerals independently devised the place-value concept and positional notation, which combined the simpler methods for computations with a decimal base and the use of a digit representing 0. This allowed the system to consistently represent both large and small integers. This approach eventually replaced all other systems. In the early 6th century AD, the Indian mathematician Aryabhata incorporated an existing version of this system in his work, and experimented with different notations. In the 7th century, Brahmagupta established the use of 0 as a separate number and determined the results for multiplication, division, addition and subtraction of zero and all other numbers, except for the result of division by 0. His contemporary, the Syriac bishop Severus Sebokht (650 AD) said, \"Indians possess a method of calculation that no word can praise enough. Their rational system of mathematics, or of their method of calculation. I mean the system using nine symbols.\" The Arabs also learned this new method and called it hesab.\n\nAlthough the Codex Vigilanus described an early form of Arabic numerals (omitting 0) by 976 AD, Leonardo of Pisa (Fibonacci) was primarily responsible for spreading their use throughout Europe after the publication of his book Liber Abaci in 1202. He wrote, \"The method of the Indians (Latin Modus Indoram) surpasses any known method to compute. It's a marvelous method. They do their computations using nine figures and symbol zero\".In the Middle Ages, arithmetic was one of the seven liberal arts taught in universities.\nThe flourishing of algebra in the medieval Islamic world and in Renaissance Europe was an outgrowth of the enormous simplification of computation through decimal notation.\nVarious types of tools have been invented and widely used to assist in numeric calculations. Before Renaissance, they were various types of abaci. More recent examples include slide rules, nomograms and mechanical calculators, such as Pascal's calculator. At present, they have been supplanted by electronic calculators and computers.\n\n\n== Arithmetic operations ==\n\nThe basic arithmetic operations are addition, subtraction, multiplication and division, although this subject also includes more advanced operations, such as manipulations of percentages, square roots, exponentiation, and logarithmic functions. Arithmetic is performed according to an order of operations. Any set of objects upon which all four arithmetic operations (except division by 0) can be performed, and where these four operations obey the usual laws, is called a field.\n\n\n=== Addition (+) ===\n\nAddition is the basic operation of arithmetic. In its simplest form, addition combines two numbers, the addends or terms, into a single number, the sum of the numbers (Such as 2 + 2 = 4 or 3 + 5 = 8).\nAdding more than two numbers can be viewed as repeated addition; this procedure is known as summation and includes ways to add infinitely many numbers in an infinite series; repeated addition of the number 1 is the most basic form of counting.\nAddition is commutative and associative so the order the terms are added in does not matter. The identity element of addition (the additive identity) is 0, that is, adding 0 to any number yields that same number. Also, the inverse element of addition (the additive inverse) is the opposite of any number, that is, adding the opposite of any number to the number itself yields the additive identity, 0. For example, the opposite of 7 is \u22127, so 7 + (\u22127) = 0.\nAddition can be given geometrically as in the following example:\n\nIf we have two sticks of lengths 2 and 5, then if we place the sticks one after the other, the length of the stick thus formed is 2 + 5 = 7.\n\n\n=== Subtraction (\u2212) ===\n\nSubtraction is the inverse of addition. Subtraction finds the difference between two numbers, the minuend  minus the subtrahend. If the minuend is larger than the subtrahend, the difference is positive; if the minuend is smaller than the subtrahend, the difference is negative; if they are equal, the difference is 0.\nSubtraction is neither commutative nor associative. For that reason, it is often helpful to look at subtraction as addition of the minuend and the opposite of the subtrahend, that is a \u2212 b = a + (\u2212b). When written as a sum, all the properties of addition hold.\nThere are several methods for calculating results, some of which are particularly advantageous to machine calculation. For example, digital computers employ the method of two's complement. Of great importance is the counting up method by which change is made. Suppose an amount P is given to pay the required amount Q, with P greater than Q. Rather than performing the subtraction P \u2212 Q and counting out that amount in change, money is counted out starting at Q and continuing until reaching P. Although the amount counted out must equal the result of the subtraction P \u2212 Q, the subtraction was never really done and the value of P \u2212 Q might still be unknown to the change-maker.\n\n\n=== Multiplication (\u00d7 or \u00b7 or *) ===\n\nMultiplication is the second basic operation of arithmetic. Multiplication also combines two numbers into a single number, the product. The two original numbers are called the multiplier and the multiplicand, sometimes both simply called factors.\nMultiplication may be viewed as a scaling operation. If the numbers are imagined as lying in a line, multiplication by a number, say x, greater than 1 is the same as stretching everything away from 0 uniformly, in such a way that the number 1 itself is stretched to where x was. Similarly, multiplying by a number less than 1 can be imagined as squeezing towards 0. (Again, in such a way that 1 goes to the multiplicand.)\nMultiplication is commutative and associative; further it is distributive over addition and subtraction. The multiplicative identity is 1, that is, multiplying any number by 1 yields that same number. Also, the multiplicative inverse is the reciprocal of any number (except 0; 0 is the only number without a multiplicative inverse), that is, multiplying the reciprocal of any number by the number itself yields the multiplicative identity.\nThe product of a and b is written as a \u00d7 b or a\u00b7b. When a or b are expressions not written simply with digits, it is also written by simple juxtaposition: ab. In computer programming languages and software packages in which one can only use characters normally found on a keyboard, it is often written with an asterisk: a * b.\n\n\n=== Division (\u00f7 or /) ===\n\nDivision is essentially the inverse of multiplication. Division finds the quotient of two numbers, the dividend divided by the divisor. Any dividend divided by 0 is undefined. For distinct positive numbers, if the dividend is larger than the divisor, the quotient is greater than 1, otherwise it is less than 1 (a similar rule applies for negative numbers). The quotient multiplied by the divisor always yields the dividend.\nDivision is neither commutative nor associative. As it is helpful to look at subtraction as addition, it is helpful to look at division as multiplication of the dividend times the reciprocal of the divisor, that is a \u00f7 b = a \u00d7 1/b.  When written as a product, it obeys all the properties of multiplication.\n\n\n== Decimal arithmetic ==\nDecimal representation refers exclusively, in common use, to the written numeral system employing arabic numerals as the digits for a radix 10 (\"decimal\") positional notation; however, any numeral system based on powers of 10, e.g., Greek, Cyrillic, Roman, or Chinese numerals may conceptually be described as \"decimal notation\" or \"decimal representation\".\nModern methods for four fundamental operations (addition, subtraction, multiplication and division) were first devised by Brahmagupta of India. This was known during medieval Europe as \"Modus Indoram\" or Method of the Indians. Positional notation (also known as \"place-value notation\") refers to the representation or encoding of numbers using the same symbol for the different orders of magnitude (e.g., the \"ones place\", \"tens place\", \"hundreds place\") and, with a radix point, using those same symbols to represent fractions (e.g., the \"tenths place\", \"hundredths place\"). For example, 507.36 denotes 5 hundreds (102), plus 0 tens (101), plus 7 units (100), plus 3 tenths (10\u22121) plus 6 hundredths (10\u22122).\nThe concept of 0 as a number comparable to the other basic digits is essential to this notation, as is the concept of 0's use as a placeholder, and as is the definition of multiplication and addition with 0. The use of 0 as a placeholder and, therefore, the use of a positional notation is first attested to in the Jain text from India entitled the Lokavibh\u00e2ga, dated 458 AD and it was only in the early 13th century that these concepts, transmitted via the scholarship of the Arabic world, were introduced into Europe by Fibonacci using the Hindu\u2013Arabic numeral system.\nAlgorism comprises all of the rules for performing arithmetic computations using this type of written numeral. For example, addition produces the sum of two arbitrary numbers. The result is calculated by the repeated addition of single digits from each number that occupies the same position, proceeding from right to left. An addition table with ten rows and ten columns displays all possible values for each sum. If an individual sum exceeds the value 9, the result is represented with two digits. The rightmost digit is the value for the current position, and the result for the subsequent addition of the digits to the left increases by the value of the second (leftmost) digit, which is always one. This adjustment is termed a carry of the value 1.\nThe process for multiplying two arbitrary numbers is similar to the process for addition. A multiplication table with ten rows and ten columns lists the results for each pair of digits. If an individual product of a pair of digits exceeds 9, the carry adjustment increases the result of any subsequent multiplication from digits to the left by a value equal to the second (leftmost) digit, which is any value from 1 to 8 (9 \u00d7 9 = 81). Additional steps define the final result.\nSimilar techniques exist for subtraction and division.\nThe creation of a correct process for multiplication relies on the relationship between values of adjacent digits. The value for any single digit in a numeral depends on its position. Also, each position to the left represents a value ten times larger than the position to the right. In mathematical terms, the exponent for the radix (base) of 10 increases by 1 (to the left) or decreases by 1 (to the right). Therefore, the value for any arbitrary digit is multiplied by a value of the form 10n with integer n. The list of values corresponding to all possible positions for a single digit is written as {..., 102, 10, 1, 10\u22121, 10\u22122, ...}.\nRepeated multiplication of any value in this list by 10 produces another value in the list. In mathematical terminology, this characteristic is defined as closure, and the previous list is described as closed under multiplication. It is the basis for correctly finding the results of multiplication using the previous technique. This outcome is one example of the uses of number theory.\n\n\n== Compound unit arithmetic ==\nCompound unit arithmetic is the application of arithmetic operations to mixed radix quantities such as feet and inches, gallons and pints, pounds shillings and pence, and so on. Prior to the use of decimal-based systems of money and units of measure, the use of compound unit arithmetic formed a significant part of commerce and industry.\n\n\n=== Basic arithmetic operations ===\nThe techniques used for compound unit arithmetic were developed over many centuries and are well-documented in many textbooks in many different languages. In addition to the basic arithmetic functions encountered in decimal arithmetic, compound unit arithmetic employs three more functions:\n\nReduction where a compound quantity is reduced to a single quantity, for example conversion of a distance expressed in yards, feet and inches to one expressed in inches.\nExpansion, the inverse function to reduction, is the conversion of a quantity that is expressed as a single unit of measure to a compound unit, such as expanding 24 oz to 1 lb, 8 oz.\nNormalization is the conversion of a set of compound units to a standard form \u2013 for example rewriting \"1 ft 13 in\" as \"2 ft 1 in\".Knowledge of the relationship between the various units of measure, their multiples and their submultiples forms an essential part of compound unit arithmetic.\n\n\n=== Principles of compound unit arithmetic ===\nThere are two basic approaches to compound unit arithmetic:\n\nReduction\u2013expansion method where all the compound unit variables are reduced to single unit variables, the calculation performed and the result expanded back to compound units. This approach is suited for automated calculations. A typical example is the handling of time by Microsoft Excel where all time intervals are processed internally as days and decimal fractions of a day.\nOn-going normalization method in which each unit is treated separately and the problem is continuously normalized as the solution develops. This approach, which is widely described in classical texts, is best suited for manual calculations. An example of the ongoing normalization method as applied to addition is shown below.The addition operation is carried out from right to left; in this case, pence are processed first, then shillings followed by pounds. The numbers below the \"answer line\" are intermediate results.\nThe total in the pence column is 25. Since there are 12 pennies in a shilling, 25 is divided by 12 to give 2 with a remainder of 1. The value \"1\" is then written to the answer row and the value \"2\" carried forward to the shillings column. This operation is repeated using the values in the shillings column, with the additional step of adding the value that was carried forward from the pennies column. The intermediate total is divided by 20 as there are 20 shillings in a pound. The pound column is then processed, but as pounds are the largest unit that is being considered, no values are carried forward from the pounds column.\nFor the sake of simplicity, the example chosen did not have farthings.\n\n\n=== Operations in practice ===\n\nDuring the 19th and 20th centuries various aids were developed to aid the manipulation of compound units, particularly in commercial applications. The most common aids were mechanical tills which were adapted in countries such as the United Kingdom to accommodate pounds, shillings, pennies and farthings and \"Ready Reckoners\" \u2013 books aimed at traders that catalogued the results of various routine calculations such as the percentages or multiples of various sums of money. One typical booklet that ran to 150 pages tabulated multiples \"from one to ten thousand at the various prices from one farthing to one pound\".\nThe cumbersome nature of compound unit arithmetic has been recognized for many years \u2013 in 1586, the Flemish mathematician Simon Stevin published a small pamphlet called De Thiende (\"the tenth\") in which he declared the universal introduction of decimal coinage, measures, and weights to be merely a question of time. In the modern era, many conversion programs, such as that included in the Microsoft Windows 7 operating system calculator, display compound units in a reduced decimal format rather than using an expanded format (i.e. \"2.5 ft\" is displayed rather than \"2 ft 6 in\").\n\n\n== Number theory ==\n\nUntil the 19th century, number theory was a synonym of \"arithmetic\". The addressed problems were directly related to the basic operations and concerned primality, divisibility, and the solution of equations in integers, such as Fermat's last theorem. It appeared that most of these problems, although very elementary to state, are very difficult and may not be solved without very deep mathematics involving concepts and methods from many other branches of mathematics. This led to new branches of number theory such as analytic number theory, algebraic number theory, Diophantine geometry and arithmetic algebraic geometry. Wiles' proof of Fermat's Last Theorem is a typical example of the necessity of sophisticated methods, which go far beyond the classical methods of arithmetic, for solving problems that can be stated in elementary arithmetic.\n\n\n== Arithmetic in education ==\nPrimary education in mathematics often places a strong focus on algorithms for the arithmetic of natural numbers, integers, fractions, and decimals (using the decimal place-value system). This study is sometimes known as algorism.\nThe difficulty and unmotivated appearance of these algorithms has long led educators to question this curriculum, advocating the early teaching of more central and intuitive mathematical ideas. One notable movement in this direction was the New Math of the 1960s and 1970s, which attempted to teach arithmetic in the spirit of axiomatic development from set theory, an echo of the prevailing trend in higher mathematics.Also, arithmetic was used by Islamic Scholars in order to teach application of the rulings related to Zakat and Irth. This was done in a book entitled The Best of Arithmetic by Abd-al-Fattah-al-Dumyati.The book begins with the foundations of mathematics and proceeds to its application in the later chapters.\n\n\n== See also ==\nLists of mathematics topics\nOutline of arithmetic\nSlide rule\n\n\n=== Related topics ===\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nMathWorld article about arithmetic\nThe New Student's Reference Work/Arithmetic (historical)\nThe Great Calculation According to the Indians, of Maximus Planudes \u2013 an early Western work on arithmetic at Convergence\n Weyde, P. H. Vander (1879). \"Arithmetic\". The American Cyclop\u00e6dia.", "model theory": "In mathematics, model theory is the study of classes of mathematical structures (e.g. groups, fields, graphs, universes of set theory) from the perspective of mathematical logic. The objects of study are models of theories in a formal language. A set of sentences in a formal language is one of the components that form a theory. A model of a theory is a structure (e.g. an interpretation) that satisfies the sentences of that theory.\nModel theory recognises and is intimately concerned with a duality: it examines semantical elements (meaning and truth) by means of syntactical elements (formulas and proofs) of a corresponding language.\n\nuniversal algebra + logic = model theory.\nModel theory developed rapidly during the 1990s, and a more modern definition is provided by Wilfrid Hodges (1997):\n\nmodel theory = algebraic geometry \u2212 fields,Other nearby areas of mathematics include combinatorics, number theory, arithmetic dynamics, analytic functions, and non-standard analysis.\nIn a similar way to proof theory, model theory is situated in an area of interdisciplinarity among mathematics, philosophy, and computer science. The most prominent professional organization in the field of model theory is the Association for Symbolic Logic.\n\n\n== Branches of model theory ==\nThis article focuses on finitary first order model theory of infinite structures. Finite model theory, which concentrates on finite structures, diverges significantly from the study of infinite structures in both the problems studied and the techniques used. Model theory in higher-order logics or infinitary logics is hampered by the fact that completeness and compactness do not in general hold for these logics. However, a great deal of study has also been done in such logics.\nInformally, model theory can be divided into classical model theory, model theory applied to groups and fields, and geometric model theory. A missing subdivision is computable model theory, but this can arguably be viewed as an independent subfield of logic.\nExamples of early theorems from classical model theory include G\u00f6del's completeness theorem, the upward and downward L\u00f6wenheim\u2013Skolem theorems, Vaught's two-cardinal theorem, Scott's isomorphism theorem, the omitting types theorem, and the Ryll-Nardzewski theorem. Examples of early results from model theory applied to fields are Tarski's elimination of quantifiers for real closed fields, Ax's theorem on pseudo-finite fields, and Robinson's development of non-standard analysis. An important step in the evolution of classical model theory occurred with the birth of stability theory (through Morley's theorem on uncountably categorical theories and Shelah's classification program), which developed a calculus of independence and rank based on syntactical conditions satisfied by theories.\nDuring the last several decades applied model theory has repeatedly merged with the more pure stability theory. The result of this synthesis is called geometric model theory in this article (which is taken to include o-minimality, for example, as well as classical geometric stability theory). An example of a theorem from geometric model theory is Hrushovski's proof of the Mordell\u2013Lang conjecture for function fields. The ambition of geometric model theory is to provide a geography of mathematics by embarking on a detailed study of definable sets in various mathematical structures, aided by the substantial tools developed in the study of pure model theory.\n\n\n== Universal algebra ==\n\nFundamental concepts in universal algebra are signatures \u03c3 and \u03c3-algebras. Since these concepts are formally defined in the article on structures, the present article is an informal introduction which consists of examples of the way these terms are used.\n\nThe standard signature of rings is \u03c3ring = {\u00d7,+,\u2212,0,1}, where \u00d7 and + are binary, \u2212 is unary, and 0 and 1 are nullary.\nThe standard signature of semirings is \u03c3smr = {\u00d7,+,0,1}, where the arities are as above.\nThe standard signature of groups (with multiplicative notation) is \u03c3grp = {\u00d7,\u22121,1}, where \u00d7 is binary, \u22121 is unary and 1 is nullary.\nThe standard signature of monoids is \u03c3mnd = {\u00d7,1}.\nA ring is a \u03c3ring-structure which satisfies the identities u + (v + w) = (u + v) + w, u + v = v + u, u + 0 = u, u + (\u2212u) = 0, u \u00d7 (v \u00d7 w) = (u \u00d7 v) \u00d7 w, u \u00d7 1 = u, 1 \u00d7 u = u, u \u00d7 (v + w) = (u \u00d7 v) + (u \u00d7 w) and (v + w) \u00d7 u = (v \u00d7 u) + (w \u00d7 u).\nA group is a \u03c3grp-structure which satisfies the identities u \u00d7 (v \u00d7 w) = (u \u00d7 v) \u00d7 w, u \u00d7 1 = u, 1 \u00d7 u = u, u \u00d7 u\u22121 = 1 and u\u22121 \u00d7 u = 1.\nA monoid is a \u03c3mnd-structure which satisfies the identities u \u00d7 (v \u00d7 w) = (u \u00d7 v) \u00d7 w,  u \u00d7 1 = u and 1 \u00d7 u = u.\nA semigroup is a {\u00d7}-structure which satisfies the identity u \u00d7 (v \u00d7 w) = (u \u00d7 v) \u00d7 w.\nA magma is just a {\u00d7}-structure.This is a very efficient way to define most classes of algebraic structures, because there is also the concept of \u03c3-homomorphism, which correctly specializes to the usual notions of homomorphism for groups, semigroups, magmas and rings. For this to work, the signature must be chosen well.\nTerms such as the \u03c3ring-term t(u,v,w) given by (u + (v \u00d7 w)) + (\u22121) are used to define identities t = t', but also to construct free algebras. An equational class is a class of structures which, like the examples above and many others, is defined as the class of all \u03c3-structures which satisfy a certain set of identities. Birkhoff's theorem states:\n\nA class of \u03c3-structures is an equational class if and only if it is not empty and closed under subalgebras, homomorphic images, and direct products.An important non-trivial tool in universal algebra are ultraproducts \n  \n    \n      \n        \n          \u03a0\n          \n            i\n            \u2208\n            I\n          \n        \n        \n          A\n          \n            i\n          \n        \n        \n          /\n        \n        U\n      \n    \n    {\\displaystyle \\Pi _{i\\in I}A_{i}/U}\n  , where I is an infinite set indexing a system of \u03c3-structures Ai, and U is an ultrafilter on I.\nWhile model theory is generally considered a part of mathematical logic, universal algebra, which grew out of Alfred North Whitehead's (1898) work on abstract algebra, is part of algebra. This is reflected by their respective MSC classifications. Nevertheless, model theory can be seen as an extension of universal algebra.\n\n\n== Finite model theory ==\n\nFinite model theory is the area of model theory which has the closest ties to universal algebra. Like some parts of universal algebra, and in contrast with the other areas of model theory, it is mainly concerned with finite algebras, or more generally, with finite \u03c3-structures for signatures \u03c3 which may contain relation symbols as in the following example:\n\nThe standard signature for graphs is \u03c3grph={E}, where E is a binary relation symbol.\nA graph is a \u03c3grph-structure satisfying the sentence \n  \n    \n      \n        \u2200\n        u\n        \u2200\n        v\n        (\n        u\n        E\n        v\n        \u2192\n        v\n        E\n        u\n        )\n      \n    \n    {\\displaystyle \\forall u\\forall v(uEv\\rightarrow vEu)}\n  .A \u03c3-homomorphism is a map that commutes with the operations and preserves the relations in \u03c3. This definition gives rise to the usual notion of graph homomorphism, which has the interesting property that a bijective homomorphism need not be invertible. Structures are also a part of universal algebra; after all, some algebraic structures such as ordered groups have a binary relation <. What distinguishes finite model theory from universal algebra is its use of more general logical sentences (as in the example above) in place of identities. (In a model-theoretic context an identity t=t' is written as a sentence \n  \n    \n      \n        \u2200\n        \n          u\n          \n            1\n          \n        \n        \n          u\n          \n            2\n          \n        \n        \u2026\n        \n          u\n          \n            n\n          \n        \n        (\n        t\n        =\n        \n          t\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle \\forall u_{1}u_{2}\\dots u_{n}(t=t')}\n  .)\nThe logics employed in finite model theory are often substantially more expressive than first-order logic, the standard logic for model theory of infinite structures.\n\n\n== First-order logic ==\n\nWhereas universal algebra provides the semantics for a signature, logic provides the syntax. With terms, identities and quasi-identities, even universal algebra has some limited syntactic tools; first-order logic is the result of making quantification explicit and adding negation into the picture.\nA first-order formula is built out of atomic formulas such as R(f(x,y),z) or y = x + 1 by means of the Boolean connectives \n  \n    \n      \n        \u00ac\n        ,\n        \u2227\n        ,\n        \u2228\n        ,\n        \u2192\n      \n    \n    {\\displaystyle \\neg ,\\land ,\\lor ,\\rightarrow }\n   and prefixing of quantifiers \n  \n    \n      \n        \u2200\n        v\n      \n    \n    {\\displaystyle \\forall v}\n   or \n  \n    \n      \n        \u2203\n        v\n      \n    \n    {\\displaystyle \\exists v}\n  . A sentence is a formula in which each occurrence of a variable is in the scope of a corresponding quantifier. Examples for formulas are \u03c6 (or \u03c6(x) to mark the fact that at most x is an unbound variable in \u03c6) and \u03c8 defined as follows:\n\n  \n    \n      \n        \n          \u03c6\n          \n          =\n          \n          \u2200\n          u\n          \u2200\n          v\n          (\n          \u2203\n          w\n          (\n          x\n          \u00d7\n          w\n          =\n          u\n          \u00d7\n          v\n          )\n          \u2192\n          (\n          \u2203\n          w\n          (\n          x\n          \u00d7\n          w\n          =\n          u\n          )\n          \u2228\n          \u2203\n          w\n          (\n          x\n          \u00d7\n          w\n          =\n          v\n          )\n          )\n          )\n          \u2227\n          x\n          \u2260\n          0\n          \u2227\n          x\n          \u2260\n          1\n          ,\n        \n      \n    \n    {\\displaystyle {\\varphi \\;=\\;\\forall u\\forall v(\\exists w(x\\times w=u\\times v)\\rightarrow (\\exists w(x\\times w=u)\\lor \\exists w(x\\times w=v)))\\land x\\neq 0\\land x\\neq 1,}}\n  \n\n  \n    \n      \n        \u03c8\n        \n        =\n        \n        \u2200\n        u\n        \u2200\n        v\n        (\n        (\n        u\n        \u00d7\n        v\n        =\n        x\n        )\n        \u2192\n        (\n        u\n        =\n        x\n        )\n        \u2228\n        (\n        v\n        =\n        x\n        )\n        )\n        \u2227\n        x\n        \u2260\n        0\n        \u2227\n        x\n        \u2260\n        1.\n      \n    \n    {\\displaystyle \\psi \\;=\\;\\forall u\\forall v((u\\times v=x)\\rightarrow (u=x)\\lor (v=x))\\land x\\neq 0\\land x\\neq 1.}\n  (Note that the equality symbol has a double meaning here.) It is intuitively clear how to translate such formulas into mathematical meaning. In the \u03c3smr-structure \n  \n    \n      \n        \n          \n            N\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {N}}}\n   of the natural numbers, for example, an element n satisfies the formula \u03c6 if and only if n is a prime number. The formula \u03c8 similarly defines irreducibility. Tarski gave a rigorous definition, sometimes called \"Tarski's definition of truth\", for the satisfaction relation \n  \n    \n      \n        \u22a8\n      \n    \n    {\\displaystyle \\models }\n  , so that one easily proves:\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        \u22a8\n        \u03c6\n        (\n        n\n        )\n        \n        \u27fa\n        \n        n\n      \n    \n    {\\displaystyle {\\mathcal {N}}\\models \\varphi (n)\\iff n}\n   is a prime number.\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        \u22a8\n        \u03c8\n        (\n        n\n        )\n        \n        \u27fa\n        \n        n\n      \n    \n    {\\displaystyle {\\mathcal {N}}\\models \\psi (n)\\iff n}\n   is irreducible.A set T of sentences is called a (first-order) theory. A theory is satisfiable if it has a model \n  \n    \n      \n        \n          \n            M\n          \n        \n        \u22a8\n        T\n      \n    \n    {\\displaystyle {\\mathcal {M}}\\models T}\n  , i.e. a structure (of the appropriate signature) which satisfies all the sentences in the set T. Consistency of a theory is usually defined in a syntactical way, but in first-order logic by the completeness theorem there is no need to distinguish between satisfiability and consistency. Therefore, model theorists often use \"consistent\" as a synonym for \"satisfiable\".\nA theory is called categorical if it determines a structure up to isomorphism, but it turns out that this definition is not useful, due to serious restrictions in the expressivity of first-order logic. The L\u00f6wenheim\u2013Skolem theorem implies that for every theory T having a countable signature which has an infinite model for some infinite cardinal number, then it has a model of size \u03ba for any infinite cardinal number \u03ba. Since two models of different sizes cannot possibly be isomorphic, only finitary structures can be described by a categorical theory.\nLack of expressivity (when compared to higher logics such as second-order logic) has its advantages, though. For model theorists, the L\u00f6wenheim\u2013Skolem theorem is an important practical tool rather than the source of Skolem's paradox. In a certain sense made precise by Lindstr\u00f6m's theorem, first-order logic is the most expressive logic for which both the L\u00f6wenheim\u2013Skolem theorem and the compactness theorem hold.\nAs a corollary (i.e., its contrapositive), the compactness theorem says that every unsatisfiable first-order theory has a finite unsatisfiable subset. This theorem is of central importance in infinite model theory, where the words \"by compactness\" are commonplace. One way to prove it is by means of ultraproducts. An alternative proof uses the completeness theorem, which is otherwise reduced to a marginal role in most of modern model theory.\n\n\n== Axiomatizability, elimination of quantifiers, and model-completeness ==\nThe first step, often trivial, for applying the methods of model theory to a class of mathematical objects such as groups, or trees in the sense of graph theory, is to choose a signature \u03c3 and represent the objects as \u03c3-structures. The next step is to show that the class is an elementary class, i.e. axiomatizable in first-order logic (i.e. there is a theory T such that a \u03c3-structure is in the class if and only if it satisfies T). E.g. this step fails for the trees, since connectedness cannot be expressed in first-order logic. Axiomatizability ensures that model theory can speak about the right objects. Quantifier elimination can be seen as a condition which ensures that model theory does not say too much about the objects.\nA theory T has quantifier elimination if every first-order formula \u03c6(x1,...,xn) over its signature is equivalent modulo T to a first-order formula \u03c8(x1,...,xn) without quantifiers, i.e. \n  \n    \n      \n        \u2200\n        \n          x\n          \n            1\n          \n        \n        \u2026\n        \u2200\n        \n          x\n          \n            n\n          \n        \n        (\n        \u03d5\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        \u2194\n        \u03c8\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle \\forall x_{1}\\dots \\forall x_{n}(\\phi (x_{1},\\dots ,x_{n})\\leftrightarrow \\psi (x_{1},\\dots ,x_{n}))}\n   holds in all models of T. For example, the theory of algebraically closed fields in the signature \u03c3ring=(\u00d7,+,\u2212,0,1) has quantifier elimination because every formula is equivalent to a Boolean combination of equations between polynomials.\nA substructure of a \u03c3-structure is a subset of its domain, closed under all functions in its signature \u03c3, which is regarded as a \u03c3-structure by restricting all functions and relations in \u03c3 to the subset. An embedding of a \u03c3-structure \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n   into another \u03c3-structure \n  \n    \n      \n        \n          \n            B\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {B}}}\n   is a map f: A \u2192 B between the domains which can be written as an isomorphism of \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n   with a substructure of \n  \n    \n      \n        \n          \n            B\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {B}}}\n  . Every embedding is an injective homomorphism, but the converse holds only if the signature contains no relation symbols.\nIf a theory does not have quantifier elimination, one can add additional symbols to its signature so that it does. Early model theory spent much effort on proving axiomatizability and quantifier elimination results for specific theories, especially in algebra. But often instead of quantifier elimination a weaker property suffices:\nA theory T is called model-complete if every substructure of a model of T which is itself a model of T is an elementary substructure. There is a useful criterion for testing whether a substructure is an elementary substructure, called the Tarski\u2013Vaught test. It follows from this criterion that a theory T is model-complete if and only if every first-order formula \u03c6(x1,...,xn) over its signature is equivalent modulo T to an existential first-order formula, i.e. a formula of the following form:\n\n  \n    \n      \n        \u2203\n        \n          v\n          \n            1\n          \n        \n        \u2026\n        \u2203\n        \n          v\n          \n            m\n          \n        \n        \u03c8\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        ,\n        \n          v\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          v\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle \\exists v_{1}\\dots \\exists v_{m}\\psi (x_{1},\\dots ,x_{n},v_{1},\\dots ,v_{m})}\n  ,where \u03c8 is quantifier free. A theory that is not model-complete may or may not have a model completion, which is a related model-complete theory that is not, in general, an extension of the original theory. A more general notion is that of model companions.\n\n\n== Categoricity ==\nAs observed in the section on first-order logic, first-order theories cannot be categorical, i.e. they cannot describe a unique model up to isomorphism, unless that model is finite. But two famous model-theoretic theorems deal with the weaker notion of \u03ba-categoricity for a cardinal \u03ba. A theory T is called \u03ba-categorical if any two models of T that are of cardinality \u03ba are isomorphic. It turns out that the question of \u03ba-categoricity depends critically on whether \u03ba is bigger than the cardinality of the language (i.e. \n  \n    \n      \n        \n          \u2135\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\aleph _{0}}\n   + |\u03c3|, where |\u03c3| is the cardinality of the signature). For finite or countable signatures this means that there is a fundamental difference between \n  \n    \n      \n        \n          \u2135\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\aleph _{0}}\n  -cardinality and \u03ba-cardinality for uncountable \u03ba.\nA few characterizations of \n  \n    \n      \n        \n          \u2135\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\aleph _{0}}\n  -categoricity include:\n\nFor a complete first-order theory T in a finite or countable signature the following conditions are equivalent:\nT is \n  \n    \n      \n        \n          \u2135\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\aleph _{0}}\n  -categorical.\nFor every natural number n, the Stone space Sn(T) is finite.\nFor every natural number n, the number of formulas \u03c6(x1, ..., xn) in n free variables, up to equivalence modulo T, is finite.This result, due independently to Engeler, Ryll-Nardzewski and Svenonius, is sometimes referred to as the Ryll-Nardzewski theorem.\nFurther, \n  \n    \n      \n        \n          \u2135\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\aleph _{0}}\n  -categorical theories and their countable models have strong ties with oligomorphic groups. They are often constructed as Fra\u00efss\u00e9 limits.\nMichael Morley's highly non-trivial result that (for countable languages) there is only one notion of uncountable categoricity was the starting point for modern model theory, and in particular classification theory and stability theory:\n\nMorley's categoricity theorem\nIf a first-order theory T in a finite or countable signature is \u03ba-categorical for some uncountable cardinal \u03ba, then T is \u03ba-categorical for all uncountable cardinals \u03ba.Uncountably categorical (i.e. \u03ba-categorical for all uncountable cardinals \u03ba) theories are from many points of view the most well-behaved theories. A theory that is both \n  \n    \n      \n        \n          \u2135\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\aleph _{0}}\n  -categorical and uncountably categorical is called totally categorical.\n\n\n== Set theory ==\nSet theory (which is expressed in a countable language), if it is consistent, has a countable model; this is known as Skolem's paradox, since there are sentences in set theory which postulate the existence of uncountable sets and yet these sentences are true in our countable model. Particularly the proof of the independence of the continuum hypothesis requires considering sets in models which appear to be uncountable when viewed from within the model, but are countable to someone outside the model.\nThe model-theoretic viewpoint has been useful in set theory; for example in Kurt G\u00f6del's work on the constructible universe, which, along with the method of forcing developed by Paul Cohen can be shown to prove the (again philosophically interesting) independence of the axiom of choice and the continuum hypothesis from the other axioms of set theory.\nIn the other direction, model theory itself can be formalized within ZFC set theory.  The development of the fundamentals of model theory (such as the compactness theorem) rely on the axiom of choice, or more exactly the Boolean prime ideal theorem.  Other results in model theory depend on set-theoretic axioms beyond the standard ZFC framework.  For example, if the Continuum Hypothesis holds then every countable model has an ultrapower which is saturated (in its own cardinality).  Similarly, if the Generalized Continuum Hypothesis holds then every model has a saturated elementary extension.  Neither of these results are provable in ZFC alone.  Finally, some questions arising from model theory (such as compactness for infinitary logics) have been shown to be equivalent to large cardinal axioms.\n\n\n== Other basic notions ==\n\n\n=== Reducts and expansions ===\n\nA field or a vector space can be regarded as a (commutative) group by simply ignoring some of its structure. The corresponding notion in model theory is that of a reduct of a structure to a subset of the original signature. The opposite relation is called an expansion - e.g. the (additive) group of the rational numbers, regarded as a structure in the signature {+,0} can be expanded to a field with the signature {\u00d7,+,1,0} or to an ordered group with the signature {+,0,<}.\nSimilarly, if \u03c3' is a signature that extends another signature \u03c3, then a complete \u03c3'-theory can be restricted to \u03c3 by intersecting the set of its sentences with the set of \u03c3-formulas. Conversely, a complete \u03c3-theory can be regarded as a \u03c3'-theory, and one can extend it (in more than one way) to a complete \u03c3'-theory. The terms reduct and expansion are sometimes applied to this relation as well.\n\n\n=== Interpretability ===\n\nGiven a mathematical structure, there are very often associated structures which can be constructed as a quotient of part of the original structure via an equivalence relation. An important example is a quotient group of a group.\nOne might say that to understand the full structure one must understand these quotients. When the equivalence relation is definable, we can give the previous sentence a precise meaning. We say that these structures are interpretable.\nA key fact is that one can translate sentences from the language of the interpreted structures to the language of the original structure. Thus one can show that if a structure M interprets another whose theory is undecidable, then M itself is undecidable.\n\n\n=== Using the compactness and completeness theorems ===\nG\u00f6del's completeness theorem (not to be confused with his incompleteness theorems) says that a theory has a model if and only if it is consistent, i.e. no contradiction is proved by the theory.  This is the heart of model theory as it lets us answer questions about theories by looking at models and vice versa.  One should not confuse the completeness theorem with the notion of a complete theory.  A complete theory is a theory that contains every sentence or its negation.  Importantly, one can find a complete consistent theory extending any consistent theory. However, as shown by G\u00f6del's incompleteness theorems only in relatively simple cases will it be possible to have a complete consistent theory that is also recursive, i.e. that can be described by a recursively enumerable set of axioms. In particular, the theory of natural numbers has no recursive complete and consistent theory. Non-recursive theories are of little practical use, since it is undecidable if a proposed axiom is indeed an axiom, making proof-checking a supertask.\nThe compactness theorem states that a set of sentences S is satisfiable if every finite subset of S is satisfiable. In the context of proof theory the analogous statement is trivial, since every proof can have only a finite number of antecedents used in the proof.  In the context of model theory, however, this proof is somewhat more difficult.  There are two well known proofs, one by G\u00f6del (which goes via proofs) and one by Malcev (which is more direct and allows us to restrict the cardinality of the resulting model).\nModel theory is usually concerned with first-order logic, and many important results (such as the completeness and compactness theorems) fail in second-order logic or other alternatives. In first-order logic all infinite cardinals look the same to a language which is countable. This is expressed in the L\u00f6wenheim\u2013Skolem theorems, which state that any countable theory with an infinite model \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathfrak {A}}}\n   has models of all infinite cardinalities (at least that of the language) which agree with \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathfrak {A}}}\n   on all sentences, i.e. they are 'elementarily equivalent'.\n\n\n=== Types ===\n\nFix an \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  -structure \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  , and a natural number \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  . The set of definable subsets of \n  \n    \n      \n        \n          M\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle M^{n}}\n   over some parameters \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is a Boolean algebra. By Stone's representation theorem for Boolean algebras there is a natural dual notion to this. One can consider this to be the topological space consisting of maximal consistent sets of formulae over \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  . We call this the space of (complete) \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -types over \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , and write \n  \n    \n      \n        \n          S\n          \n            n\n          \n        \n        (\n        A\n        )\n      \n    \n    {\\displaystyle S_{n}(A)}\n  .\nNow consider an element \n  \n    \n      \n        m\n        \u2208\n        \n          M\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle m\\in M^{n}}\n  . Then the set of all formulae \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   with parameters in \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   in free variables \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots ,x_{n}}\n   so that \n  \n    \n      \n        M\n        \u22a8\n        \u03d5\n        (\n        m\n        )\n      \n    \n    {\\displaystyle M\\models \\phi (m)}\n   is consistent and maximal such. It is called the type of \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   over \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  .\nOne can show that for any \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -type \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  , there exists some elementary extension \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   of \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n   and some \n  \n    \n      \n        a\n        \u2208\n        \n          N\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle a\\in N^{n}}\n   so that \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   is the type of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   over \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  .\nMany important properties in model theory can be expressed with types. Further many proofs go via constructing models with elements that contain elements with certain types and then using these elements.\nIllustrative Example: Suppose \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n   is an algebraically closed field. The theory has quantifier elimination . This allows us to show that a type is determined exactly by the polynomial equations it contains. Thus the space of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -types over a subfield \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is bijective with the set of prime ideals of the polynomial ring \n  \n    \n      \n        A\n        [\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        ]\n      \n    \n    {\\displaystyle A[x_{1},\\ldots ,x_{n}]}\n  . This is the same set as the spectrum of \n  \n    \n      \n        A\n        [\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            n\n          \n        \n        ]\n      \n    \n    {\\displaystyle A[x_{1},\\ldots ,x_{n}]}\n  . Note however that the topology considered on the type space is the constructible topology: a set of types is basic open iff it is of the form \n  \n    \n      \n        {\n        p\n        :\n        f\n        (\n        x\n        )\n        =\n        0\n        \u2208\n        p\n        }\n      \n    \n    {\\displaystyle \\{p:f(x)=0\\in p\\}}\n   or of the form \n  \n    \n      \n        {\n        p\n        :\n        f\n        (\n        x\n        )\n        \u2260\n        0\n        \u2208\n        p\n        }\n      \n    \n    {\\displaystyle \\{p:f(x)\\neq 0\\in p\\}}\n  . This is finer than the Zariski topology.\n\n\n== History ==\nModel theory as a subject has existed since approximately the middle of the 20th century. However some earlier research, especially in mathematical logic, is often regarded as being of a model-theoretical nature in retrospect. The first significant result in what is now model theory was a special case of the downward L\u00f6wenheim\u2013Skolem theorem, published by Leopold L\u00f6wenheim in 1915. The compactness theorem was implicit in work by Thoralf Skolem, but it was first published in 1930, as a lemma in Kurt G\u00f6del's proof of his completeness theorem. The L\u00f6wenheim\u2013Skolem theorem and the compactness theorem received their respective general forms in 1936 and 1941 from Anatoly Maltsev.\nThe development of model theory can be traced to Alfred Tarski, a member of the Lw\u00f3w\u2013Warsaw school during the interbellum. Tarski's work included logical consequence, deductive systems, the algebra of logic, the theory of definability, and the semantic definition of truth, among other topics. His semantic methods culminated in the model theory he and a number of his Berkeley students developed in the 1950s and 60s. These modern concepts of model theory influenced Hilbert's program and modern mathematics.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Canonical textbooks ===\nChang, Chen Chung; Keisler, H. Jerome (1990) [1973]. Model Theory. Studies in Logic and the Foundations of Mathematics (3rd ed.). Elsevier. ISBN 978-0-444-88054-3. \nHodges, Wilfrid (1997). A shorter model theory. Cambridge: Cambridge University Press. ISBN 978-0-521-58713-6. \nKopperman, R. (1972). Model Theory and Its Applications. Boston: Allyn and Bacon. \nMarker, David (2002). Model Theory: An Introduction. Graduate Texts in Mathematics 217. Springer. ISBN 0-387-98760-6. \n\n\n=== Other textbooks ===\nBell, John L.; Slomson, Alan B. (2006) [1969]. Models and Ultraproducts: An Introduction (reprint of 1974 ed.). Dover Publications. ISBN 0-486-44979-3. \nEbbinghaus, Heinz-Dieter; Flum, J\u00f6rg; Thomas, Wolfgang (1994). Mathematical Logic. Springer. ISBN 0-387-94258-0. \nHinman, Peter G. (2005). Fundamentals of Mathematical Logic. A K Peters. ISBN 1-56881-262-0. \nHodges, Wilfrid (1993). Model theory. Cambridge University Press. ISBN 0-521-30442-3. \nManzano, Mar\u00eda (1999). Model theory. Oxford University Press. ISBN 0-19-853851-0. \nPoizat, Bruno (2000). A Course in Model Theory. Springer. ISBN 0-387-98655-3. \nRautenberg, Wolfgang (2010). A Concise Introduction to Mathematical Logic (3rd ed.). New York: Springer Science+Business Media. doi:10.1007/978-1-4419-1221-3. ISBN 978-1-4419-1220-6. \nRothmaler, Philipp (2000). Introduction to Model Theory (new ed.). Taylor & Francis. ISBN 90-5699-313-5. \nZiegler, Martin; Tent, Katrin (2012). A Course in Model Theory. Cambridge University Press. ISBN 9780521763240. \n\n\n=== Free online texts ===\nChatzidakis, Zo\u00e9 (2001). Introduction to Model Theory (PDF). pp. 26 pages. \nPillay, Anand (2002). Lecture Notes \u2013 Model Theory (PDF). pp. 61 pages. \nHazewinkel, Michiel, ed. (2001) [1994], \"Model theory\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nHodges, Wilfrid, Model theory. The Stanford Encyclopedia Of Philosophy, E. Zalta (ed.).\nHodges, Wilfrid, First-order Model theory. The Stanford Encyclopedia Of Philosophy, E. Zalta (ed.).\nSimmons, Harold (2004), An introduction to Good old fashioned model theory. Notes of an introductory course for postgraduates (with exercises).\nJ. Barwise and S. Feferman (editors), Model-Theoretic Logics, Perspectives in Mathematical Logic, Volume 8, New York: Springer-Verlag, 1985.", "probability": "Probability is the measure of the likelihood that an event will occur. See glossary of probability and statistics. Probability is quantified as a number between 0 and 1, where, loosely speaking, 0 indicates impossibility and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur.     A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (\"heads\" and \"tails\") are both equally probable; the probability of \"heads\" equals the probability of \"tails\"; and since no other outcomes are possible, the probability  of either \"heads\" or \"tails\" is 1/2 (which could also be written as 0.5 or 50%).\nThese concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.\n\n\n== Interpretations ==\n\nWhen dealing with experiments that are random and well-defined in a purely theoretical setting (like tossing a fair coin), probabilities can be numerically described by the number of desired outcomes divided by the total number of all outcomes. For example, tossing a fair coin twice will yield \"head-head\", \"head-tail\", \"tail-head\", and \"tail-tail\" outcomes. The probability of getting an outcome of \"head-head\" is 1 out of 4 outcomes or 1/4 or 0.25 (or 25%). When it comes to practical application however, there are two major competing categories of probability interpretations, whose adherents possess different views about the fundamental nature of probability:\n\nObjectivists assign numbers to describe some objective or physical state of affairs. The most popular version of objective probability is frequentist probability, which claims that the probability of a random event denotes the relative frequency of occurrence of an experiment's outcome, when repeating the experiment. This interpretation considers probability to be the relative frequency \"in the long run\" of outcomes. A modification of this is propensity probability, which interprets probability as the tendency of some experiment to yield a certain outcome, even if it is performed only once.\nSubjectivists assign numbers per subjective probability, i.e., as a degree of belief. The degree of belief has been interpreted as, \"the price at which you would buy or sell a bet that pays 1 unit of utility if E, 0 if not E.\"  The most popular version of subjective probability is Bayesian probability, which includes expert knowledge as well as experimental data to produce probabilities.  The expert knowledge is represented by some (subjective) prior probability distribution.  These data are incorporated in a likelihood function. The product of the prior and the likelihood, normalized, results in a posterior probability distribution that incorporates all the information known to date. By Aumann's agreement theorem, Bayesian agents whose prior beliefs are similar will end up with similar posterior beliefs. However, sufficiently different priors can lead to different conclusions regardless of how much information the agents share.\n\n\n== Etymology ==\n\nThe word probability derives from the Latin probabilitas, which can also mean \"probity\", a measure of the authority of a witness in a legal case in Europe, and often correlated with the witness's nobility. In a sense, this differs much from the modern meaning of probability, which, in contrast, is a measure of the weight of empirical evidence, and is arrived at from inductive reasoning and statistical inference.\n\n\n== History ==\n\nThe scientific study of probability is a modern development of mathematics. Gambling shows that there has been an interest in quantifying the ideas of probability for millennia, but exact mathematical descriptions arose much later. There are reasons for the slow development of the mathematics of probability. Whereas games of chance provided the impetus for the mathematical study of probability, fundamental issues are still obscured by the superstitions of gamblers.\n\nAccording to Richard Jeffrey, \"Before the middle of the seventeenth century, the term 'probable' (Latin probabilis) meant approvable, and was applied in that sense, unequivocally, to opinion and to action. A probable action or opinion was one such as sensible people would undertake or hold, in the circumstances.\" However, in legal contexts especially, 'probable' could also apply to propositions for which there was good evidence.\n\nThe sixteenth century Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes (which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes).\nAside from the elementary work by Cardano, the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal (1654). Christiaan Huygens (1657) gave the earliest known scientific treatment of the subject. Jakob Bernoulli's Ars Conjectandi (posthumous, 1713) and Abraham de Moivre's Doctrine of Chances (1718) treated the subject as a branch of mathematics. See Ian Hacking's The Emergence of Probability and James Franklin's The Science of Conjecture for histories of the early development of the very concept of mathematical probability.\nThe theory of errors may be traced back to Roger Cotes's Opera Miscellanea (posthumous, 1722), but a memoir prepared by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of errors of observation. The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.\nThe first two laws of error that were proposed both originated with Pierre-Simon Laplace. The first law was published in 1774 and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error, disregarding sign. The second law of error was proposed in 1778 by Laplace and stated that the frequency of the error is an exponential function of the square of the error. The second law of error is called the normal distribution or the Gauss law. \"It is difficult historically to attribute that law to Gauss, who in spite of his well-known precocity had probably not made this discovery before he was two years old.\"Daniel Bernoulli (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.\n\nAdrien-Marie Legendre (1805) developed the method of least squares, and introduced it in his Nouvelles m\u00e9thodes pour la d\u00e9termination des orbites des com\u00e8tes (New Methods for Determining the Orbits of Comets). In ignorance of Legendre's contribution, an Irish-American writer, Robert Adrain, editor of \"The Analyst\" (1808), first deduced the law of facility of error,\n\n  \n    \n      \n        \u03d5\n        (\n        x\n        )\n        =\n        c\n        \n          e\n          \n            \u2212\n            \n              h\n              \n                2\n              \n            \n            \n              x\n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\phi (x)=ce^{-h^{2}x^{2}},}\n  where \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   is a constant depending on precision of observation, and \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n   is a scale factor ensuring that the area under the curve equals 1. He gave two proofs, the second being essentially the same as John Herschel's (1850). Gauss gave the first proof that seems to have been known in Europe (the third after Adrain's) in 1809. Further proofs were given by Laplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W. F. Donkin (1844, 1856), and Morgan Crofton (1870). Other contributors were Ellis (1844), De Morgan (1864), Glaisher (1872), and Giovanni Schiaparelli (1875). Peters's (1856) formula for r, the probable error of a single observation, is well known. \nIn the nineteenth century authors on the general theory included Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion, and Karl Pearson. Augustus De Morgan and George Boole improved the exposition of the theory.\nAndrey Markov introduced the notion of Markov chains (1906), which played an important role in stochastic processes theory and its applications. The modern theory of probability based on the measure theory was developed by Andrey Kolmogorov (1931).On the geometric side (see integral geometry) contributors to The Educational Times were influential (Miller, Crofton, McColl, Wolstenholme, Watson, and Artemas Martin).\n\n\n== Theory ==\n\nLike other theories, the theory of probability is a representation of its concepts in formal terms\u2014that is, in terms that can be considered separately from their meaning. These formal terms are manipulated by the rules of mathematics and logic, and any results are interpreted or translated back into the problem domain.\nThere have been at least two successful attempts to formalize probability, namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's formulation (see probability space), sets are interpreted as events and probability itself as a measure on a class of sets. In Cox's theorem, probability is taken as a primitive (that is, not further analyzed) and the emphasis is on constructing a consistent assignment of probability values to propositions. In both cases, the laws of probability are the same, except for technical details.\nThere are other methods for quantifying uncertainty, such as the Dempster\u2013Shafer theory or possibility theory, but those are essentially different and not compatible with the laws of probability as usually understood.\n\n\n== Applications ==\nProbability theory is applied in everyday life in risk assessment and modeling. The insurance industry and markets use actuarial science to determine pricing and make trading decisions. Governments apply probabilistic methods in environmental regulation, entitlement analysis (Reliability theory of aging and longevity), and financial regulation.\nA good example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices, which have ripple effects in the economy as a whole. An assessment by a commodity trader that a war is more likely can send that commodity's prices up or down, and signals other traders of that opinion. Accordingly, the probabilities are neither assessed independently nor necessarily very rationally. The theory of behavioral finance emerged to describe the effect of such groupthink on pricing, on policy, and on peace and conflict.In addition to financial assessment, probability can be used to analyze trends in biology (e.g. disease spread) as well as ecology (e.g. biological Punnett squares). As with finance, risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring and can assist with implementing protocols to avoid encountering such circumstances. Probability is used to design games of chance so that casinos can make a guaranteed profit, yet provide payouts to players that are frequent enough to encourage continued play.The discovery of rigorous methods to assess and combine probability assessments has changed society. It is important for most citizens to understand how probability assessments are made, and how they contribute to decisions.Another significant application of probability theory in everyday life is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. Failure probability may influence a manufacturer's decisions on a product's warranty.The cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory.\n\n\n== Mathematical treatment ==\n\nConsider an experiment that can produce a number of results. The collection of all possible results is called the sample space of the experiment. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling a dice can produce six possible results. One collection of possible results gives an odd number on the dice. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called \"events\". In this case, {1,3,5} is the event that the dice falls on some odd number. If the results that actually occur fall in a given event, the event is said to have occurred.\nA probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events with no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.The probability of an event A is written as \n  \n    \n      \n        P\n        (\n        A\n        )\n      \n    \n    {\\displaystyle P(A)}\n  , \n  \n    \n      \n        p\n        (\n        A\n        )\n      \n    \n    {\\displaystyle p(A)}\n  , or \n  \n    \n      \n        \n          Pr\n        \n        (\n        A\n        )\n      \n    \n    {\\displaystyle {\\text{Pr}}(A)}\n  . This mathematical definition of probability can extend to infinite sample spaces, and even uncountable sample spaces, using the concept of a measure.\nThe opposite or complement of an event A is the event [not A] (that is, the event of A not occurring), often denoted as \n  \n    \n      \n        \n          \n            A\n            \u00af\n          \n        \n        ,\n        \n          A\n          \n            \u2201\n          \n        \n        ,\n        \u00ac\n        A\n      \n    \n    {\\displaystyle {\\overline {A}},A^{\\complement },\\neg A}\n  , or \n  \n    \n      \n        \n          \u223c\n        \n        A\n      \n    \n    {\\displaystyle {\\sim }A}\n  ; its probability is given by P(not A) = 1 \u2212 P(A). As an example, the chance of not rolling a six on a six-sided die is 1 \u2013 (chance of rolling a six) \n  \n    \n      \n        =\n        1\n        \u2212\n        \n          \n            \n              1\n              6\n            \n          \n        \n        =\n        \n          \n            \n              5\n              6\n            \n          \n        \n      \n    \n    {\\displaystyle =1-{\\tfrac {1}{6}}={\\tfrac {5}{6}}}\n  . See Complementary event for a more complete treatment.\nIf two events A and B occur on a single performance of an experiment, this is called the intersection or joint probability of A and B, denoted as \n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)}\n  .\n\n\n=== Independent events ===\nIf two events, A and B are independent then the joint probability is\n\n  \n    \n      \n        P\n        (\n        A\n        \n          \n             and \n          \n        \n        B\n        )\n        =\n        P\n        (\n        A\n        \u2229\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        )\n        ,\n        \n      \n    \n    {\\displaystyle P(A{\\mbox{ and }}B)=P(A\\cap B)=P(A)P(B),\\,}\n  for example, if two coins are flipped the chance of both being heads is \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        \u00d7\n        \n          \n            \n              1\n              2\n            \n          \n        \n        =\n        \n          \n            \n              1\n              4\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}\\times {\\tfrac {1}{2}}={\\tfrac {1}{4}}}\n  .\n\n\n=== Mutually exclusive events ===\nIf either event A or event B but never both occurs on a single performance of an experiment, then they are called mutually exclusive events.\nIf two events are mutually exclusive then the probability of both occurring is denoted as \n  \n    \n      \n        P\n        (\n        A\n        \u2229\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cap B)}\n  .\n\n  \n    \n      \n        P\n        (\n        A\n        \n          \n             and \n          \n        \n        B\n        )\n        =\n        P\n        (\n        A\n        \u2229\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(A{\\mbox{ and }}B)=P(A\\cap B)=0}\n  If two events are mutually exclusive then the probability of either occurring is denoted as \n  \n    \n      \n        P\n        (\n        A\n        \u222a\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\cup B)}\n  .\n\n  \n    \n      \n        P\n        (\n        A\n        \n          \n             or \n          \n        \n        B\n        )\n        =\n        P\n        (\n        A\n        \u222a\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n        \u2212\n        P\n        (\n        A\n        \u2229\n        B\n        )\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n        \u2212\n        0\n        =\n        P\n        (\n        A\n        )\n        +\n        P\n        (\n        B\n        )\n      \n    \n    {\\displaystyle P(A{\\mbox{ or }}B)=P(A\\cup B)=P(A)+P(B)-P(A\\cap B)=P(A)+P(B)-0=P(A)+P(B)}\n  For example, the chance of rolling a 1 or 2 on a six-sided die is \n  \n    \n      \n        P\n        (\n        1\n        \n          \n             or \n          \n        \n        2\n        )\n        =\n        P\n        (\n        1\n        )\n        +\n        P\n        (\n        2\n        )\n        =\n        \n          \n            \n              1\n              6\n            \n          \n        \n        +\n        \n          \n            \n              1\n              6\n            \n          \n        \n        =\n        \n          \n            \n              1\n              3\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle P(1{\\mbox{ or }}2)=P(1)+P(2)={\\tfrac {1}{6}}+{\\tfrac {1}{6}}={\\tfrac {1}{3}}.}\n  \n\n\n=== Not mutually exclusive events ===\nIf the events are not mutually exclusive then\n\n  \n    \n      \n        P\n        \n          (\n          \n            A\n            \n              \n                 or \n              \n            \n            B\n          \n          )\n        \n        =\n        P\n        (\n        A\n        \u222a\n        B\n        )\n        =\n        P\n        \n          (\n          A\n          )\n        \n        +\n        P\n        \n          (\n          B\n          )\n        \n        \u2212\n        P\n        \n          (\n          \n            A\n            \n              \n                 and \n              \n            \n            B\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle P\\left(A{\\hbox{ or }}B\\right)=P(A\\cup B)=P\\left(A\\right)+P\\left(B\\right)-P\\left(A{\\mbox{ and }}B\\right).}\n  For example, when drawing a single card at random from a regular deck of cards, the chance of getting a heart or a face card (J,Q,K) (or one that is both) is \n  \n    \n      \n        \n          \n            \n              13\n              52\n            \n          \n        \n        +\n        \n          \n            \n              12\n              52\n            \n          \n        \n        \u2212\n        \n          \n            \n              3\n              52\n            \n          \n        \n        =\n        \n          \n            \n              11\n              26\n            \n          \n        \n      \n    \n    {\\displaystyle {\\tfrac {13}{52}}+{\\tfrac {12}{52}}-{\\tfrac {3}{52}}={\\tfrac {11}{26}}}\n  , because of the 52 cards of a deck 13 are hearts, 12 are face cards, and 3 are both: here the possibilities included in the \"3 that are both\" are included in each of the \"13 hearts\" and the \"12 face cards\" but should only be counted once.\n\n\n=== Conditional probability ===\nConditional probability is the probability of some event A, given the occurrence of some other event B.\nConditional probability is written \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n  , and is read \"the probability of A, given B\". It is defined by\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              \u2229\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n        .\n        \n      \n    \n    {\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}.\\,}\n  If \n  \n    \n      \n        P\n        (\n        B\n        )\n        =\n        0\n      \n    \n    {\\displaystyle P(B)=0}\n   then \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        )\n      \n    \n    {\\displaystyle P(A\\mid B)}\n   is formally undefined by this expression. However, it is possible to define a conditional probability for some zero-probability events using a \u03c3-algebra of such events (such as those arising from a continuous random variable).For example, in a bag of 2 red balls and 2 blue balls (4 balls in total), the probability of taking a red ball is \n  \n    \n      \n        1\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle 1/2}\n  ; however, when taking a second ball, the probability of it being either a red ball or a blue ball depends on the ball previously taken, such as, if a red ball was taken, the probability of picking a red ball again would be \n  \n    \n      \n        1\n        \n          /\n        \n        3\n      \n    \n    {\\displaystyle 1/3}\n   since only 1 red and 2 blue balls would have been remaining.\n\n\n=== Inverse probability ===\nIn probability theory and applications, Bayes' rule relates the odds of event \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle A_{1}}\n   to event \n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle A_{2}}\n  , before (prior to) and after (posterior to) conditioning on another event \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  . The odds on \n  \n    \n      \n        \n          A\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle A_{1}}\n   to event \n  \n    \n      \n        \n          A\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle A_{2}}\n   is simply the ratio of the probabilities of the two events. When arbitrarily many events \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood, \n  \n    \n      \n        P\n        (\n        A\n        \n          |\n        \n        B\n        )\n        \u221d\n        P\n        (\n        A\n        )\n        P\n        (\n        B\n        \n          |\n        \n        A\n        )\n      \n    \n    {\\displaystyle P(A|B)\\propto P(A)P(B|A)}\n   where the proportionality symbol means that the left hand side is proportional to  (i.e., equals a constant times) the right hand side as \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   varies, for fixed or given \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005). See Inverse probability and Bayes' rule.\n\n\n=== Summary of probabilities ===\n\n\n== Relation to randomness and probability in quantum mechanics ==\n\nIn a deterministic universe, based on Newtonian concepts, there would be no probability if all conditions were known (Laplace's demon), (but there are situations in which sensitivity to initial conditions exceeds our ability to measure them, i.e. know them).  In the case of a roulette wheel, if the force of the hand and the period of that force are known, the number on which the ball will stop would be a certainty (though as a practical matter, this would likely be true only of a roulette wheel that had not been exactly levelled \u2013 as Thomas A. Bass' Newtonian Casino revealed).  This also assumes knowledge of inertia and friction of the wheel, weight, smoothness and roundness of the ball, variations in hand speed during the turning and so forth. A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel. Physicists face the same situation in kinetic theory of gases, where the system, while deterministic in principle, is so complex (with the number of molecules typically the order of magnitude of the Avogadro constant 6.02\u00d71023) that only a statistical description of its properties is feasible.\nProbability theory is required to describe quantum phenomena. A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at sub-atomic scales and are governed by the laws of quantum mechanics. The objective wave function evolves deterministically but, according to the Copenhagen interpretation, it deals with probabilities of observing, the outcome being explained by a wave function collapse when an observation is made. However, the loss of determinism for the sake of instrumentalism did not meet with universal approval. Albert Einstein famously remarked in a letter to Max Born: \"I am convinced that God does not play dice\". Like Einstein, Erwin Schr\u00f6dinger, who discovered the wave function, believed quantum mechanics is a statistical approximation of an underlying deterministic reality. In some modern interpretations of the statistical mechanics of measurement, quantum decoherence is invoked to account for the appearance of subjectively probabilistic experimental outcomes.\n\n\n== See also ==\n\nChance (disambiguation)\nClass membership probabilities\nEquiprobability\nHeuristics in judgment and decision-making\nProbability theory\nStatistics\nEstimators\nEstimation Theory\nProbability density functionIn LawBalance of probabilities\n\n\n== Notes ==\n\n\n== Bibliography ==\nKallenberg, O. (2005) Probabilistic Symmetries and Invariance Principles. Springer -Verlag, New York. 510 pp. ISBN 0-387-25115-4\nKallenberg, O. (2002) Foundations of Modern Probability, 2nd ed. Springer Series in Statistics. 650 pp. ISBN 0-387-95313-2\nOlofsson, Peter (2005) Probability, Statistics, and Stochastic Processes, Wiley-Interscience. 504 pp ISBN 0-471-67969-0.\n\n\n== External links ==\nVirtual Laboratories in Probability and Statistics (Univ. of Ala.-Huntsville)\nProbability  on In Our Time at the BBC\nProbability and Statistics EBook\nEdwin Thompson Jaynes. Probability Theory: The Logic of Science. Preprint: Washington University, (1996). \u2014 HTML index with links to PostScript files and PDF (first three chapters)\nPeople from the History of Probability and Statistics (Univ. of Southampton)\nProbability and Statistics on the Earliest Uses Pages (Univ. of Southampton)\nEarliest Uses of Symbols in Probability and Statistics on Earliest Uses of Various Mathematical Symbols\nA tutorial on probability and Bayes' theorem devised for first-year Oxford University students\n[1] pdf file of An Anthology of Chance Operations (1963) at UbuWeb\nIntroduction to Probability - eBook, by Charles Grinstead, Laurie Snell Source (GNU Free Documentation License)\n(in English) (in Italian) Bruno de Finetti, Probabilit\u00e0 e induzione, Bologna, CLUEB, 1993. ISBN 88-8091-176-7 (digital version)\nRichard P. Feynman's Lecture on probability.", "mathematical logic": "Mathematical logic is a subfield of mathematics exploring the applications of formal logic to mathematics.  It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science.  The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.\nMathematical logic is often divided into the fields of set theory, model theory, recursion theory, and proof theory.  These areas share basic results on logic, particularly first-order logic, and definability. In computer science (particularly in the ACM Classification) mathematical logic encompasses additional topics not detailed in this article; see Logic in computer science for those.\nSince its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt G\u00f6del, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.\n\n\n== Subfields and scope ==\nThe Handbook of Mathematical Logic (Barwise 1989) makes a rough division of contemporary mathematical logic into four areas:\n\nset theory\nmodel theory\nrecursion theory, and\nproof theory and constructive mathematics (considered as parts of a single area).Each area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp.  G\u00f6del's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to L\u00f6b's theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.\nThe mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.\n\n\n== History ==\nMathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics (Ferreir\u00f3s 2001, p. 443).  \"Mathematical logic, also called  'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the last [nineteenth] century with the aid of an artificial notation and a rigorously deductive method.\"  Before this emergence, logic was studied with rhetoric, with calculationes, through the syllogism, and with philosophy.  The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.\n\n\n=== Early history ===\n\nTheories of logic were developed in many cultures in history, including China, India, Greece and the Islamic world. In 18th-century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including Leibniz and Lambert, but their labors remained isolated and little known.\n\n\n=== 19th century ===\nIn the middle of the nineteenth century, George Boole and then Augustus De Morgan presented systematic mathematical treatments of logic.  Their work, building on work by algebraists such as George Peacock, extended the traditional Aristotelian doctrine of logic into a sufficient framework for the study of foundations of mathematics (Katz 1998, p. 686).\nCharles Sanders Peirce built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.\nGottlob Frege presented an independent development of logic with quantifiers in his Begriffsschrift, published in 1879, a work generally considered as marking a turning point in the history of logic. Frege's work remained obscure, however, until Bertrand Russell began to promote it near the turn of the century.  The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.\nFrom 1890 to 1905, Ernst Schr\u00f6der published Vorlesungen \u00fcber die Algebra der Logik in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.\n\n\n==== Foundational theories ====\nConcerns that mathematics had not been built on a proper foundation led to the development of axiomatic systems for fundamental areas of mathematics such as arithmetic, analysis, and geometry.\nIn logic, the term arithmetic refers to the theory of the natural numbers. Giuseppe Peano (1889) published a set of axioms for arithmetic that came to bear his name (Peano axioms), using a variation of the logical system of Boole and Schr\u00f6der but adding quantifiers. Peano was unaware of Frege's work at the time. Around the same time Richard Dedekind showed that the natural numbers are uniquely characterized by their induction properties. Dedekind (1888) proposed a different characterization, which lacked the formal logical character of Peano's axioms. Dedekind's work, however, proved theorems inaccessible in Peano's system, including the uniqueness of the set of natural numbers (up to isomorphism) and the  recursive definitions of addition and multiplication from the successor function and mathematical induction.\nIn the mid-19th century, flaws in Euclid's axioms for geometry became known (Katz 1998, p. 774).  In addition to the independence of the parallel postulate, established by Nikolai Lobachevsky in 1826 (Lobachevsky 1840), mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms. Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect. Hilbert (1899) developed a complete set of axioms for geometry, building on previous work by Pasch (1882).  The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the real line.  This would prove to be a major area of research in the first half of the 20th century.\nThe 19th century saw great advances in the theory of real analysis, including theories of convergence of functions and Fourier series. Mathematicians such as Karl Weierstrass began to construct functions that stretched intuition, such as nowhere-differentiable continuous functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate.  Weierstrass began to advocate the arithmetization of analysis, which sought to axiomatize analysis using properties of the natural numbers. The modern (\u03b5, \u03b4)-definition of limit and continuous functions was already developed by Bolzano in 1817 (Felscher 2000), but remained relatively unknown.\nCauchy in 1821 defined continuity in terms of infinitesimals (see Cours d'Analyse, page 34).  In 1858, Dedekind proposed a definition of the real numbers in terms of Dedekind cuts of rational numbers (Dedekind 1872), a definition still employed in contemporary texts.\nGeorg Cantor developed the fundamental concepts of infinite set theory. His early results developed the theory of cardinality and proved that the reals and the natural numbers have different cardinalities (Cantor 1874). Over the next twenty years, Cantor developed a theory of transfinite numbers in a series of publications. In 1891, he published a new proof of the uncountability of the real numbers that introduced the diagonal argument, and used this method to prove Cantor's theorem that no set can have the same cardinality as its powerset. Cantor believed that every set could be well-ordered, but was unable to produce a proof for this result, leaving it as an open problem in 1895 (Katz 1998, p. 807).\n\n\n=== 20th century ===\nIn the early decades of the 20th century, the main areas of study were set theory and formal logic. The discovery of paradoxes in informal set theory caused some to wonder whether mathematics itself is inconsistent, and to look for proofs of consistency.\nIn 1900, Hilbert posed a famous list of 23 problems for the next century. The first two of these were to resolve the continuum hypothesis and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the integers has a solution. Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert's Entscheidungsproblem, posed in 1928. This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.\n\n\n==== Set theory and paradoxes ====\nErnst Zermelo (1904) gave a proof that every set could be well-ordered, a result Georg Cantor had been unable to obtain. To achieve the proof, Zermelo introduced the axiom of choice, which drew heated debate and research among mathematicians and the pioneers of set theory. The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof (Zermelo 1908a). This paper led to the general acceptance of the axiom of choice in the mathematics community.\nSkepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory. Cesare Burali-Forti (1897) was the first to state a paradox: the Burali-Forti paradox shows that the collection of all ordinal numbers cannot form a set. Very soon thereafter, Bertrand Russell discovered Russell's paradox in 1901, and Jules Richard (1905) discovered Richard's paradox.\nZermelo (1908b) provided the first set of axioms for set theory. These axioms, together with the additional axiom of replacement proposed by Abraham Fraenkel, are now called Zermelo\u2013Fraenkel set theory (ZF). Zermelo's axioms incorporated the principle of limitation of size to avoid Russell's paradox.\nIn 1910, the first volume of Principia Mathematica by Russell and Alfred North Whitehead was published. This seminal work developed the theory of functions and cardinality in a completely formal framework of type theory, which Russell and Whitehead developed in an effort to avoid the paradoxes. Principia Mathematica is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics (Ferreir\u00f3s 2001, p. 445).\nFraenkel (1922) proved that the axiom of choice cannot be proved from the axioms of Zermelo's set theory with urelements. Later work by Paul Cohen (1966) showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF. Cohen's proof developed the method of forcing, which is now an important tool for establishing independence results in set theory.\n\n\n==== Symbolic logic ====\nLeopold L\u00f6wenheim (1915) and Thoralf Skolem (1920) obtained the L\u00f6wenheim\u2013Skolem theorem, which says that first-order logic cannot control the cardinalities of infinite structures. Skolem realized that this theorem would apply to first-order formalizations of set theory, and that it implies any such formalization has a countable model. This counterintuitive fact became known as Skolem's paradox.\nIn his doctoral thesis, Kurt G\u00f6del (1929) proved the completeness theorem, which establishes a correspondence between syntax and semantics in first-order logic. G\u00f6del used the completeness theorem to prove the compactness theorem, demonstrating the finitary nature of first-order logical consequence. These results helped establish first-order logic as the dominant logic used by mathematicians.\nIn 1931, G\u00f6del published On Formally Undecidable Propositions of Principia Mathematica and Related Systems, which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories. This result, known as G\u00f6del's incompleteness theorem, establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert's program. It showed the impossibility of providing a consistency proof of arithmetic within any formal theory of arithmetic.  Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.G\u00f6del's theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system. This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider. Gentzen (1936) proved the consistency of arithmetic using a finitistic system together with a principle of transfinite induction. Gentzen's result introduced the ideas of cut elimination and proof-theoretic ordinals, which became key tools in proof theory.  G\u00f6del (1958) gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intuitionistic arithmetic in higher types.\n\n\n==== Beginnings of the other branches ====\nAlfred Tarski developed the basics of model theory.\nBeginning in 1935, a group of prominent mathematicians collaborated under the pseudonym Nicolas Bourbaki to publish a series of encyclopedic mathematics texts. These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations. Terminology coined by these texts, such as the words bijection, injection, and surjection, and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.\nThe study of computability came to be known as recursion theory, because early formalizations by G\u00f6del and Kleene relied on recursive definitions of functions. When these definitions were shown equivalent to Turing's formalization involving Turing machines, it became clear that a new concept \u2013 the computable function \u2013 had been discovered, and that this definition was robust enough to admit numerous independent characterizations. In his work on the incompleteness theorems in 1931, G\u00f6del lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.\nNumerous results in recursion theory were obtained in the 1940s by Stephen Cole Kleene and Emil Leon Post. Kleene (1943) introduced the concepts of relative computability, foreshadowed by Turing (1939), and the arithmetical hierarchy. Kleene later generalized recursion theory to higher-order functionals. Kleene and Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory.\n\n\n== Formal logical systems ==\nAt its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language.  The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties.  Stronger classical logics such as second-order logic or infinitary logic are also studied, along with nonclassical logics such as intuitionistic logic.\n\n\n=== First-order logic ===\n\nFirst-order logic is a particular formal system of logic. Its syntax involves only finite expressions as well-formed formulas, while its semantics are characterized by the limitation of all quantifiers to a fixed domain of discourse.\nEarly results from formal logic established limitations of first-order logic. The L\u00f6wenheim\u2013Skolem theorem (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality. This shows that it is impossible for a set of first-order axioms to characterize the natural numbers, the real numbers, or any other infinite structure up to isomorphism. As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.\nG\u00f6del's completeness theorem (G\u00f6del 1929) established the equivalence between semantic and syntactic definitions of logical consequence in first-order logic. It shows that if a particular sentence is true in every model that satisfies a particular set of axioms, then there must be a finite deduction of the sentence from the axioms. The compactness theorem first appeared as a lemma in G\u00f6del's proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely. It says that a set of sentences has a model if and only if every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset. The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of model theory, and they are a key reason for the prominence of first-order logic in mathematics.\nG\u00f6del's incompleteness theorems (G\u00f6del 1931) establish additional limits on first-order axiomatizations. The first incompleteness theorem states that for any consistent, effectively given (defined below) logical system that is capable of interpreting arithmetic, there exists a statement that is true (in the sense that it holds for the natural numbers) but not provable within that logical system (and which indeed may fail in some non-standard models of arithmetic which may be consistent with the logical system). For example, in every logical system capable of expressing the Peano axioms, the G\u00f6del sentence holds for the natural numbers but cannot be proved.\nHere a logical system is said to be effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom, and one which can express the Peano axioms is called \"sufficiently strong.\" When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not elementarily equivalent, a stronger limitation than the one established by the L\u00f6wenheim\u2013Skolem theorem. The second incompleteness theorem states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert's program cannot be completed.\n\n\n=== Other classical logics ===\nMany logics besides first-order logic are studied.  These include infinitary logics, which allow for formulas to provide an infinite amount of information, and higher-order logics, which include a portion of set theory directly in their semantics.\nThe most well studied infinitary logic is \n  \n    \n      \n        \n          L\n          \n            \n              \u03c9\n              \n                1\n              \n            \n            ,\n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle L_{\\omega _{1},\\omega }}\n  . In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them. Thus, for example, it is possible to say that an object is a whole number using a formula of \n  \n    \n      \n        \n          L\n          \n            \n              \u03c9\n              \n                1\n              \n            \n            ,\n            \u03c9\n          \n        \n      \n    \n    {\\displaystyle L_{\\omega _{1},\\omega }}\n   such as\n\n  \n    \n      \n        (\n        x\n        =\n        0\n        )\n        \u2228\n        (\n        x\n        =\n        1\n        )\n        \u2228\n        (\n        x\n        =\n        2\n        )\n        \u2228\n        \u22ef\n        .\n      \n    \n    {\\displaystyle (x=0)\\lor (x=1)\\lor (x=2)\\lor \\cdots .}\n  Higher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type. The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type.  The logics studied before the development of first-order logic, for example Frege's logic, had similar set-theoretic aspects. Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.\nAnother type of logics are fixed-point logics that allow inductive definitions, like one writes for primitive recursive functions.\nOne can formally define an extension of first-order logic \u2014 a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g. it does not encompass intuitionistic, modal or fuzzy logic. Lindstr\u00f6m's theorem implies that the only extension of first-order logic satisfying both the compactness theorem and the Downward L\u00f6wenheim\u2013Skolem theorem is first-order logic.\n\n\n=== Nonclassical and modal logic ===\nModal logics include additional modal operators, such as an operator which states that a particular formula is not only true, but necessarily true. Although modal logic is not often used to axiomatize mathematics, it has been used to study the properties of first-order provability (Solovay 1976) and set-theoretic forcing (Hamkins and L\u00f6we 2007).\nIntuitionistic logic was developed by Heyting to study Brouwer's program of intuitionism, in which Brouwer himself avoided formalization. Intuitionistic logic specifically does not include the law of the excluded middle, which states that each sentence is either true or its negation is true.  Kleene's work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs. For example, any provably total function in intuitionistic arithmetic is computable; this is not true in classical theories of arithmetic such as Peano arithmetic.\n\n\n=== Algebraic logic ===\nAlgebraic logic uses the methods of abstract algebra to study the semantics of formal logics. A fundamental example is the use of Boolean algebras to represent truth values in classical propositional logic, and the use of Heyting algebras to represent truth values in intuitionistic propositional logic. Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as cylindric algebras.\n\n\n== Set theory ==\n\nSet theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo (1908b), was extended slightly to become Zermelo\u2013Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.\nOther formalizations of set theory have been proposed, including von Neumann\u2013Bernays\u2013G\u00f6del set theory (NBG), Morse\u2013Kelley set theory (MK), and New Foundations (NF).  Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of Kripke\u2013Platek set theory is closely related to generalized recursion theory.\nTwo famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo (1904), was proved independent of ZF by Fraenkel (1922), but has come to be widely accepted by mathematicians.  It states that given a collection of nonempty sets there is a single set C that contains exactly one element from each set in the collection. The set C is said to \"choose\" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. Stefan Banach and Alfred Tarski (1924) showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size. This theorem, known as the Banach\u2013Tarski paradox, is one of many counterintuitive results of the axiom of choice.\nThe continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. G\u00f6del showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo\u2013Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold. In 1963, Paul Cohen showed that the continuum hypothesis cannot be proven from the axioms of Zermelo\u2013Fraenkel set theory (Cohen 1966). This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear (Woodin 2001).\nContemporary research in set theory includes the study of large cardinals and determinacy.  Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC.  Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line.  Determinacy refers to the possible existence of winning strategies for certain two-player games (the games are said to be determined). The existence of these strategies implies structural properties of the real line and other Polish spaces.\n\n\n== Model theory ==\n\nModel theory studies the models of various formal theories.  Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory. Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.\nThe set of all models of a particular theory is called an elementary class; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.\nThe method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated.  Tarski (1948) established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable. (He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic.) A modern subfield developing from this is concerned with o-minimal structures.\nMorley's categoricity theorem, proved by Michael D. Morley (1965), states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.\nA trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis.  Many special cases of this conjecture have been established.\n\n\n== Recursion theory ==\n\nRecursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability.  Recursion theory also includes the study of generalized computability and definability.  Recursion theory grew from the work of R\u00f3zsa P\u00e9ter, Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.Classical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, \u03bb calculus, and other systems.  More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.\nGeneralized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and \u03b1-recursion theory.\nContemporary research in recursion theory includes the study of applications such as algorithmic randomness, computable model theory, and reverse mathematics, as well as new results in pure recursion theory.\n\n\n=== Algorithmically unsolvable problems ===\nAn important subfield of recursion theory studies algorithmic unsolvability; a decision problem or function problem is algorithmically unsolvable if there is no possible computable algorithm that returns the correct answer for all legal inputs to the problem. The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the Entscheidungsproblem is algorithmically unsolvable. Turing proved this by establishing the unsolvability of the halting problem, a result with far-ranging implications in both recursion theory and computer science.\nThere are many known examples of undecidable problems from ordinary mathematics. The word problem for groups was proved algorithmically unsolvable by Pyotr Novikov in 1955 and independently by W. Boone in 1959.  The busy beaver problem, developed by Tibor Rad\u00f3 in 1962, is another well-known example.\nHilbert's tenth problem asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers. Partial progress was made by Julia Robinson, Martin Davis and Hilary Putnam. The algorithmic unsolvability of the problem was proved by Yuri Matiyasevich in 1970 (Davis 1973).\n\n\n== Proof theory and constructive mathematics ==\n\nProof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques.  Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.\nThe study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems.  An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods (Weyl 1918).\nBecause proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability.   The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest.  Results such as the G\u00f6del\u2013Gentzen negative translation show that it is possible to embed (or translate) classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.\nRecent developments in proof theory include the study of proof mining by Ulrich Kohlenbach and the study of proof-theoretic ordinals by Michael Rathjen.\n\n\n== Applications ==\n\"Mathematical logic has been successfully applied not only to mathematics and its foundations (G. Frege, B. Russell, D. Hilbert, P. Bernays, H. Scholz, R. Carnap, S. Lesniewski, T. Skolem), but also to physics (R. Carnap, A. Dittrich, B. Russell, C. E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology (J. H. Woodger, A. Tarski), to psychology (F. B. Fitch, C. G. Hempel), to law and morals (K. Menger, U. Klug, P. Oppenheim), to economics (J. Neumann, O. Morgenstern), to practical questions (E. C. Berkeley, E. Stamm), and even to metaphysics (J. [Jan] Salamucha, H. Scholz, J. M. Bochenski).  Its applications to the history of logic have proven extremely fruitful (J. Lukasiewicz, H. Scholz, B. Mates, A. Becker, E. Moody, J. Salamucha, K. Duerr, Z. Jordan, P. Boehner, J. M. Bochenski, S. [Stanislaw] T. Schayer, D. Ingalls).\"  \"Applications have also been made to theology (F. Drewnowski, J. Salamucha, I. Thomas).\"\n\n\n== Connections with computer science ==\n\nThe study of computability theory in computer science is closely related to the study of computability in mathematical logic.  There is a difference of emphasis, however.  Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.\nThe theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry\u2013Howard isomorphism between proofs and programs relates to proof theory, especially intuitionistic logic. Formal calculi such as the lambda calculus and combinatory logic are now studied as idealized programming languages.\nComputer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as automated theorem proving and logic programming.\nDescriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.\n\n\n== Foundations of mathematics ==\n\nIn the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass' nowhere-differentiable continuous function were discovered.\nCantor's study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated \"God made the integers; all else is the work of man,\" endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. David Hilbert argued in favor of the study of the infinite, saying \"No one shall expel us from the Paradise that Cantor has created.\"\nMathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs.  In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, non-Euclidean geometry can be proved consistent by defining point to mean a point on a fixed sphere and line to mean a great circle on the sphere. The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.\nWith the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term finitary to refer to the methods he would allow but not precisely defining them. This project, known as Hilbert's program, was seriously affected by G\u00f6del's incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.\nA second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of constructive. At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. \nIn the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a philosophy of mathematics. This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to intuit the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true.  Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.\n\n\n== See also ==\n\nKnowledge representation and reasoning\nList of computability and complexity topics\nList of first-order theories\nList of logic symbols\nList of mathematical logic topics\nList of set theory topics\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Undergraduate texts ===\nWalicki, Micha\u0142 (2011), Introduction to Mathematical Logic, Singapore: World Scientific Publishing, ISBN 978-981-4343-87-9 .Boolos, George; Burgess, John; Jeffrey, Richard (2002), Computability and Logic (4th ed.), Cambridge: Cambridge University Press, ISBN 978-0-521-00758-0 .\nCrossley, J.N.; Ash, C.J.; Brickhill, C.J.; Stillwell, J.C.; Williams, N.H. (1972), What is mathematical logic?, London-Oxford-New York: Oxford University Press, ISBN 0-19-888087-1, Zbl 0251.02001 .\nEnderton, Herbert (2001), A mathematical introduction to logic (2nd ed.), Boston, MA: Academic Press, ISBN 978-0-12-238452-3 .\nFisher, Alec (1982), Formal Number Theory and Computability: A Workbook (1st ed.), USA: Oxford University Press, ISBN 0-19-853188-5 . Suitable as a first course for independent study.\nHamilton, A.G. (1988), Logic for Mathematicians (2nd ed.), Cambridge: Cambridge University Press, ISBN 978-0-521-36865-0 .\nEbbinghaus, H.-D.; Flum, J.; Thomas, W. (1994), Mathematical Logic (2nd ed.), New York: Springer, ISBN 0-387-94258-0 .\nKatz, Robert (1964), Axiomatic Analysis, Boston, MA: D. C. Heath and Company .\nMendelson, Elliott (1997), Introduction to Mathematical Logic (4th ed.), London: Chapman & Hall, ISBN 978-0-412-80830-2 .\nRautenberg, Wolfgang (2010), A Concise Introduction to Mathematical Logic (3rd ed.), New York: Springer Science+Business Media, doi:10.1007/978-1-4419-1221-3, ISBN 978-1-4419-1220-6 .\nSchwichtenberg, Helmut (2003\u20132004), Mathematical Logic (PDF), Munich, Germany: Mathematisches Institut der Universit\u00e4t M\u00fcnchen, retrieved 2016-02-24 .\nShawn Hedman, A first course in logic: an introduction to model theory, proof theory, computability, and complexity, Oxford University Press, 2004, ISBN 0-19-852981-3. Covers logics in close relation with computability theory and complexity theory\nvan Dalen, Dirk (2013), Logic and Structure, Berlin: Springer-Verlag, ISBN 978-1-4471-4557-8 .\n\n\n=== Graduate texts ===\nAndrews, Peter B. (2002), An Introduction to Mathematical Logic and Type Theory: To Truth Through Proof (2nd ed.), Boston: Kluwer Academic Publishers, ISBN 978-1-4020-0763-7 .\nBarwise, Jon, ed. (1989). Handbook of Mathematical Logic. Studies in Logic and the Foundations of Mathematics. North Holland. ISBN 978-0-444-86388-1. .\nHodges, Wilfrid (1997), A shorter model theory, Cambridge: Cambridge University Press, ISBN 978-0-521-58713-6 .\nJech, Thomas (2003), Set Theory: Millennium Edition, Springer Monographs in Mathematics, Berlin, New York: Springer-Verlag, ISBN 978-3-540-44085-7 .\nKleene, Stephen Cole.(1952), Introduction to Metamathematics. New York: Van Nostrand. (Ishi Press: 2009 reprint).\nKleene, Stephen Cole. (1967),  Mathematical Logic. John Wiley. Dover reprint, 2002. ISBN 0-486-42533-9.\nShoenfield, Joseph R. (2001) [1967], Mathematical Logic (2nd ed.), A K Peters, ISBN 978-1-56881-135-2 .\nTroelstra, Anne Sjerp; Schwichtenberg, Helmut (2000), Basic Proof Theory, Cambridge Tracts in Theoretical Computer Science (2nd ed.), Cambridge: Cambridge University Press, ISBN 978-0-521-77911-1 .\n\n\n=== Research papers, monographs, texts, and surveys ===\nAugusto, Luis M. (2017). Logical consequences. Theory and applications: An introduction. London: College Publications. ISBN 978-1-84890-236-7. \nCohen, P. J. (1966), Set Theory and the Continuum Hypothesis, Menlo Park, CA: W. A. Benjamin .\nCohen, Paul Joseph (2008) [1966]. Set theory and the continuum hypothesis. Mineola, New York: Dover Publications. ISBN 978-0-486-46921-8. .\nJ.D. Sneed, The Logical Structure of Mathematical Physics. Reidel, Dordrecht, 1971 (revised edition 1979).\nDavis, Martin (1973), \"Hilbert's tenth problem is unsolvable\", The American Mathematical Monthly, 80 (3): 233\u2013269, doi:10.2307/2318447, JSTOR 2318447 , reprinted as an appendix in Martin Davis, Computability and Unsolvability, Dover reprint 1982.\nFelscher, Walter (2000), \"Bolzano, Cauchy, Epsilon, Delta\", The American Mathematical Monthly, 107 (9): 844\u2013862, doi:10.2307/2695743, JSTOR 2695743 .\nFerreir\u00f3s, Jos\u00e9 (2001), \"The Road to Modern Logic-An Interpretation\", Bulletin of Symbolic Logic, 7 (4): 441\u2013484, doi:10.2307/2687794, JSTOR 2687794 .\nHamkins, Joel David; Benedikt L\u00f6we (2007), \"The modal logic of forcing\", Transactions of the American Mathematical Society, 360: 1793\u20131818, arXiv:math/0509616\u202f, doi:10.1090/s0002-9947-07-04297-3 \nKatz, Victor J. (1998), A History of Mathematics, Addison\u2013Wesley, ISBN 0-321-01618-1 .\nMorley, Michael (1965), \"Categoricity in Power\", Transactions of the American Mathematical Society, 114 (2): 514\u2013538, doi:10.2307/1994188, JSTOR 1994188 .\nSoare, Robert I. (1996), \"Computability and recursion\", Bulletin of Symbolic Logic, 2 (3): 284\u2013321, CiteSeerX 10.1.1.35.5803\u202f, doi:10.2307/420992, JSTOR 420992 .\nSolovay, Robert M. (1976), \"Provability Interpretations of Modal Logic\", Israel Journal of Mathematics, 25 (3\u20134): 287\u2013304, doi:10.1007/BF02757006 .\nWoodin, W. Hugh (2001), \"The Continuum Hypothesis, Part I\", Notices of the American Mathematical Society, 48 (6) . PDF\n\n\n=== Classical papers, texts, and collections ===\nBurali-Forti, Cesare (1897), A question on transfinite numbers , reprinted in van Heijenoort 1976, pp. 104\u2013111.\nDedekind, Richard (1872), Stetigkeit und irrationale Zahlen . English translation of title: \"Consistency and irrational numbers\".\nDedekind, Richard (1888), Was sind und was sollen die Zahlen?  Two English translations:\n1963 (1901). Essays on the Theory of Numbers. Beman, W. W., ed. and trans. Dover.\n1996. In From Kant to Hilbert: A Source Book in the Foundations of Mathematics, 2 vols, Ewald, William B., ed., Oxford University Press: 787\u2013832.\nFraenkel, Abraham A. (1922), \"Der Begriff 'definit' und die Unabh\u00e4ngigkeit des Auswahlsaxioms\", Sitzungsberichte der Preussischen Akademie der Wissenschaften, Physikalisch-mathematische Klasse, pp. 253\u2013257  (German), reprinted in English translation as \"The notion of 'definite' and the independence of the axiom of choice\", van Heijenoort 1976, pp. 284\u2013289.Frege, Gottlob (1879), Begriffsschrift, eine der arithmetischen nachgebildete Formelsprache des reinen Denkens. Halle a. S.: Louis Nebert. Translation: Concept Script, a formal language of pure thought modelled upon that of arithmetic, by S. Bauer-Mengelberg in Jean Van Heijenoort, ed., 1967. From Frege to G\u00f6del: A Source Book in Mathematical Logic, 1879\u20131931. Harvard University Press.\nFrege, Gottlob (1884), Die Grundlagen der Arithmetik: eine logisch-mathematische Untersuchung \u00fcber den Begriff der Zahl. Breslau: W. Koebner. Translation: J. L. Austin, 1974. The Foundations of Arithmetic: A logico-mathematical enquiry into the concept of number, 2nd ed. Blackwell.\nGentzen, Gerhard (1936), \"Die Widerspruchsfreiheit der reinen Zahlentheorie\", Mathematische Annalen, 112: 132\u2013213, doi:10.1007/BF01565428 , reprinted in English translation in Gentzen's Collected works, M. E. Szabo, ed., North-Holland, Amsterdam, 1969.\nG\u00f6del, Kurt (1929), \u00dcber die Vollst\u00e4ndigkeit des Logikkalk\u00fcls, doctoral dissertation, University Of Vienna . English translation of title: \"Completeness of the logical calculus\".\nG\u00f6del, Kurt (1930), \"Die Vollst\u00e4ndigkeit der Axiome des logischen Funktionen-kalk\u00fcls\", Monatshefte f\u00fcr Mathematik und Physik, 37: 349\u2013360, doi:10.1007/BF01696781 . English translation of title: \"The completeness of the axioms of the calculus of logical functions\".\nG\u00f6del, Kurt (1931), \"\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter Systeme I\", Monatshefte f\u00fcr Mathematik und Physik, 38 (1): 173\u2013198, doi:10.1007/BF01700692 , see On Formally Undecidable Propositions of Principia Mathematica and Related Systems for details on English translations.\nG\u00f6del, Kurt (1958), \"\u00dcber eine bisher noch nicht ben\u00fctzte Erweiterung des finiten Standpunktes\", Dialectica. International Journal of Philosophy, 12 (3\u20134): 280\u2013287, doi:10.1111/j.1746-8361.1958.tb01464.x , reprinted in English translation in G\u00f6del's Collected Works, vol II, Solomon Feferman et al., eds. Oxford University Press, 1990.\nvan Heijenoort, Jean, ed. (1976) [1967], From Frege to G\u00f6del: A Source Book in Mathematical Logic, 1879\u20131931 (3rd ed.), Cambridge, Mass: Harvard University Press, ISBN 0-674-32449-8, (pbk.) \nHilbert, David (1899), Grundlagen der Geometrie, Leipzig: Teubner , English 1902 edition (The Foundations of Geometry) republished 1980, Open Court, Chicago.\nDavid, Hilbert (1929), \"Probleme der Grundlegung der Mathematik\", Mathematische Annalen, 102: 1\u20139, doi:10.1007/BF01782335 . Lecture given at the International Congress of Mathematicians, 3 September 1928. Published in English translation as \"The Grounding of Elementary Number Theory\", in Mancosu 1998, pp. 266\u2013273.\nHilbert, David; Bernays, Paul (1934). Grundlagen der Mathematik. I. Die Grundlehren der mathematischen Wissenschaften. 40. Berlin, New York: Springer-Verlag. ISBN 978-3-540-04134-4. JFM 60.0017.02. MR 0237246. \nKleene, Stephen Cole (1943), \"Recursive Predicates and Quantifiers\", American Mathematical Society Transactions, Transactions of the American Mathematical Society, Vol. 53, No. 1, 54 (1): 41\u201373, doi:10.2307/1990131, JSTOR 1990131 .\nLobachevsky, Nikolai (1840), Geometrishe Untersuchungen zur Theorie der Parellellinien  (German). Reprinted in English translation as \"Geometric Investigations on the Theory of Parallel Lines\" in Non-Euclidean Geometry, Robert Bonola (ed.), Dover, 1955. ISBN 0-486-60027-0\nL\u00f6wenheim, Leopold (1915), \"\u00dcber M\u00f6glichkeiten im Relativkalk\u00fcl\", Mathematische Annalen, 76 (4): 447\u2013470, doi:10.1007/BF01458217, ISSN 0025-5831  (German). Translated as \"On possibilities in the calculus of relatives\" in Jean van Heijenoort, 1967. A Source Book in Mathematical Logic, 1879\u20131931. Harvard Univ. Press: 228\u2013251.\nMancosu, Paolo, ed. (1998), From Brouwer to Hilbert. The Debate on the Foundations of Mathematics in   the 1920s, Oxford: Oxford University Press .\nPasch, Moritz (1882), Vorlesungen \u00fcber neuere Geometrie .\nPeano, Giuseppe (1889), Arithmetices principia, nova methodo exposita  (Latin), excerpt reprinted in English translation as \"The principles of arithmetic, presented by a new method\", van Heijenoort 1976, pp. 83 97.\nRichard, Jules (1905), \"Les principes des math\u00e9matiques et le probl\u00e8me des ensembles\", Revue g\u00e9n\u00e9rale des sciences pures et appliqu\u00e9es, 16: 541  (French), reprinted in English translation as \"The principles of mathematics and the problems of sets\", van Heijenoort 1976, pp. 142\u2013144.\nSkolem, Thoralf (1920), \"Logisch-kombinatorische Untersuchungen \u00fcber die Erf\u00fcllbarkeit oder Beweisbarkeit mathematischer S\u00e4tze nebst einem Theoreme \u00fcber dichte Mengen\", Videnskapsselskapet Skrifter, I. Matematisk-naturvidenskabelig Klasse, 6: 1\u201336 .\nTarski, Alfred (1948), A decision method for elementary algebra and geometry, Santa Monica, California: RAND Corporation \nTuring, Alan M. (1939), \"Systems of Logic Based on Ordinals\", Proceedings of the London Mathematical Society, 45 (2): 161\u2013228, doi:10.1112/plms/s2-45.1.161 \nZermelo, Ernst (1904), \"Beweis, da\u00df jede Menge wohlgeordnet werden kann\", Mathematische Annalen, 59 (4): 514\u2013516, doi:10.1007/BF01445300  (German), reprinted in English translation as \"Proof that every set can be well-ordered\", van Heijenoort 1976, pp. 139\u2013141.\nZermelo, Ernst (1908a), \"Neuer Beweis f\u00fcr die M\u00f6glichkeit einer Wohlordnung\", Mathematische Annalen, 65: 107\u2013128, doi:10.1007/BF01450054, ISSN 0025-5831  (German), reprinted in English translation as \"A new proof of the possibility of a well-ordering\", van Heijenoort 1976, pp. 183\u2013198.\nZermelo, Ernst (1908b), \"Untersuchungen \u00fcber die Grundlagen der Mengenlehre\", Mathematische Annalen, 65 (2): 261\u2013281, doi:10.1007/BF01449999 .\n\n\n== External links ==\nHazewinkel, Michiel, ed. (2001) [1994], \"Mathematical logic\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nPolyvalued logic and Quantity Relation Logic\nforall x: an introduction to formal logic, a free textbook by P. D. Magnus.\nA Problem Course in Mathematical Logic, a free textbook by Stefan Bilaniuk.\nDetlovs, Vilnis, and Podnieks, Karlis (University of Latvia), Introduction to Mathematical Logic. (hyper-textbook).\nIn the Stanford Encyclopedia of Philosophy:\nClassical Logic by Stewart Shapiro.\nFirst-order Model Theory by Wilfrid Hodges.\nIn the London Philosophy Study Guide:\nMathematical Logic\nSet Theory & Further Logic\nPhilosophy of Mathematics", "statistics": "Statistics is a branch of mathematics dealing with the collection, organization, analysis, interpretation and presentation of data. In applying statistics to, for example, a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.\nSee glossary of probability and statistics.\nWhen census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\nA standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\"). Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\nStatistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. In more recent years statistics has relied more on statistical software to produce tests such as descriptive analysis.\n\n\n== Scope ==\nSome definitions are:\n\nMerriam-Webster dictionary defines statistics as \"a branch of mathematics dealing with the collection, analysis, interpretation, and presentation of masses of numerical data.\"\nStatistician Sir Arthur Lyon Bowley defines statistics as \"Numerical statements of facts in any department of inquiry placed in relation to each other.\"Statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data, or as a branch of mathematics. Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.\n\n\n=== Mathematical statistics ===\n\nMathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.\n\n\n== Overview ==\nIn applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as \"all persons living in a country\" or \"every atom composing a crystal\".\nIdeally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).\nWhen a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis).  Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.\n\n\n== Data collection ==\n\n\n=== Sampling ===\nWhen full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models. The idea of making inferences based on sampled data began around the mid-1600s in connection with estimating populations and developing precursors of life insurance.To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\nSampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method.\nThe difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction\u2014inductively inferring from samples to the parameters of a larger or total population.\n\n\n=== Experimental and observational studies ===\nA common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective.\nAn experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated.\nWhile the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data\u2014like natural experiments and observational studies\u2014for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\n\n\n==== Experiments ====\nThe basic steps of a statistical experiment are:\n\nPlanning the research, including finding the number of replicates of the study, using the following information:  preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.\nDesign of experiments, using blocking to reduce the influence of confounding variables, and  randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.\nPerforming the experiment following the experimental protocol and analyzing the data following the experimental protocol.\nFurther examining the data set in secondary analyses, to suggest new hypotheses for future study.\nDocumenting and presenting the results of the study.Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.\n\n\n==== Observational study ====\nAn example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group. A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.\n\n\n== Types of data ==\n\nVarious attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\nBecause variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\nOther categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998), van den Berg (1991).The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer\" (Hand, 2004, p. 82).\n\n\n== Terminology and theory of inferential statistics ==\n\n\n=== Statistics, estimators and pivotal quantities ===\nConsider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.\nA statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters.\nConsider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\nA random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.\nBetween two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\nOther desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\nThis still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\n\n\n=== Null hypothesis and alternative hypothesis ===\nInterpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\nWhat statisticians call an alternative hypothesis is simply a hypothesis that contradicts the null hypothesis.\n\n\n=== Error ===\nWorking from a null hypothesis, two basic forms of error are recognized:\n\nType I errors where the null hypothesis is falsely rejected giving a \"false positive\".\nType II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\".Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\nA statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\nMean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\n\nMany statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\nMeasurement processes that generate statistical data are also subject to error.  Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\n\n=== Interval estimation ===\n\nMost studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable.  Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\nIn principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\n\n\n=== Significance ===\n\nStatistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\n\nThe standard approach is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\nReferring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\nWhile in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.\nSome problems are usually associated with this framework (See criticism of hypothesis testing):\n\nA difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.\nFallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is the probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.\nRejecting the null hypothesis does not automatically prove the alternative hypothesis.\nAs everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.\n\n\n=== Examples ===\nSome well-known statistical tests and procedures are:\n\n\n== Misuse ==\n\nMisuse of statistics can produce subtle, but serious errors in description and interpretation\u2014subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\nEven when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data\u2014which measures the extent to which a trend could be caused by random variation in the sample\u2014may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\nThere is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter. A  mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).Ways to avoid misuse of statistics include using proper diagrams and avoiding bias. Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias. Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs. Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented. To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.  According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:\nWho says so? (Does he/she have an axe to grind?)\nHow does he/she know? (Does he/she have the resources to know the facts?)\nWhat\u2019s missing? (Does he/she give us a complete picture?)\nDid someone change the subject? (Does he/she offer us the right answer to the wrong problem?)\nDoes it make sense? (Is his/her conclusion logical and consistent with what  we already know?)\n\n\n=== Misinterpretation: correlation ===\nThe concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)\n\n\n== History of statistical science ==\n\nSome scholars pinpoint the origin of statistics to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt. Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\nIts mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel. The method of least squares was first described by  Adrien-Marie Legendre in 1805.\n\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics\u2014height, weight, eyelash length among others. Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things. Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.Ronald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".The second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance, which was the first to use the statistical term, variance, his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments, where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information. In his 1930 book The Genetical Theory of Natural Selection he applied statistics to various biological concepts such as Fisher's principle). Nevertheless, A. W. F. Edwards has remarked that it is \"probably the most celebrated argument in evolutionary biology\". (about the sex ratio), the Fisherian runaway, a concept in sexual selection about a positive feedback runaway affect found in evolution.\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.\n\n\n== Applications ==\n\n\n=== Applied statistics, theoretical statistics and mathematical statistics ===\nApplied statistics comprises descriptive statistics and the application of inferential statistics. Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\n\n\n=== Machine learning and data mining ===\nThere are two applications for machine learning and data mining: data management and data analysis.  Statistics tools are necessary for the data analysis.\n\n\n=== Statistics in society ===\nStatistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.\n\n\n=== Statistical computing ===\n\nThe rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\nIncreased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica,  SAS, SPSS, and  R.\n\n\n=== Statistics applied to mathematics or the arts ===\nTraditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was \"required learning\" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically. Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.\n\nIn number theory, scatter plots of data generated by a distribution function may be transformed with familiar tools used in statistics to reveal underlying patterns, which may then lead to hypotheses.\nMethods of statistics including predictive methods in forecasting are combined with chaos theory and fractal geometry to create video works that are considered to have great beauty.\nThe process art of Jackson Pollock relied on artistic experiments whereby underlying distributions in nature were artistically revealed. With the advent of computers, statistical methods were applied to formalize such distribution-driven natural processes to make and analyze moving video art.\nMethods of statistics may be used predicatively in performance art, as in a card trick based on a Markov process that only works some of the time, the occasion of which can be predicted using statistical methodology.\nStatistics can be used to predicatively create art, as in the statistical or stochastic music invented by Iannis Xenakis, where the music is performance-specific. Though this type of artistry does not always come out as expected, it does behave in ways that are predictable and tunable using statistics.\n\n\n== Specialized disciplines ==\n\nStatistical techniques are used in a wide range of types of scientific and social research, including: biostatistics,  computational biology,  computational sociology,  network biology,  social science,  sociology and  social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\n\nIn addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\n\nStatistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.\n\n\n== See also ==\n\nFoundations and major areas of statistics\n\n\n== References ==\n\n\n== Further reading ==\nBarbara Illowsky; Susan Dean (2014). Introductory Statistics. OpenStax CNX. ISBN 9781938168208. \nDavid W. Stockburger, Introductory Statistics: Concepts, Models, and Applications, 3rd Web Ed.  Missouri State University.\nStephen Jones, 2010. Statistics in Psychology: Explanations without Equations. Palgrave Macmillan. ISBN 9781137282392.\nCohen, J. (1990). \"Things I have learned (so far)\". American Psychologist, 45, 1304\u20131312.\nGigerenzer, G. (2004). \"Mindless statistics\". Journal of Socio-Economics, 33, 587\u2013606. doi:10.1016/j.socec.2004.09.033\nIoannidis, J. P. A. (2005). \"Why most published research findings are false\". PLoS Medicine, 2, 696\u2013701. doi:10.1371/journal.pmed.0040168\n\n\n== External links ==\n\n(Electronic Version): StatSoft, Inc. (2013). Electronic Statistics Textbook. Tulsa, OK: StatSoft.\nOnline Statistics Education: An Interactive Multimedia Course of Study. Developed by Rice University (Lead Developer), University of Houston Clear Lake, Tufts University, and National Science Foundation.\nUCLA Statistical Computing Resources\nPhilosophy of Statistics from the Stanford Encyclopedia of Philosophy", "optimization": "In mathematics, computer science and operations research, mathematical optimization or mathematical programming, alternatively spelled optimisation, is the selection of a best element (with regard to some criterion) from some set of available alternatives.In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.\n\n\n== Optimization problemsEdit ==\n\nAn optimization problem can be represented in the following way:\n\nGiven: a function \n  \n    \n      \n        f\n        :\n        A\n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle f\\colon A\\to \\mathbb {R} }\n   from some set \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   to the real numbers\nSought: an element \n  \n    \n      \n        \n          \n            x\n          \n          \n            0\n          \n        \n        \u2208\n        A\n      \n    \n    {\\displaystyle \\mathbf {x} _{0}\\in A}\n   such that \n  \n    \n      \n        f\n        \n          (\n          \n            \n              x\n            \n            \n              0\n            \n          \n          )\n        \n        \u2264\n        f\n        \n          (\n          \n            x\n          \n          )\n        \n      \n    \n    {\\displaystyle f\\left(\\mathbf {x} _{0}\\right)\\leq f\\left(\\mathbf {x} \\right)}\n   for all \n  \n    \n      \n        \n          x\n        \n        \u2208\n        A\n      \n    \n    {\\displaystyle \\mathbf {x} \\in A}\n   (\"minimization\") or such that \n  \n    \n      \n        f\n        \n          (\n          \n            \n              x\n            \n            \n              0\n            \n          \n          )\n        \n        \u2265\n        f\n        \n          (\n          \n            x\n          \n          )\n        \n      \n    \n    {\\displaystyle f\\left(\\mathbf {x} _{0}\\right)\\geq f\\left(\\mathbf {x} \\right)}\n   for all \n  \n    \n      \n        \n          x\n        \n        \u2208\n        A\n      \n    \n    {\\displaystyle \\mathbf {x} \\in A}\n   (\"maximization\").Such a formulation is called an optimization problem or a mathematical programming problem (a term not directly related to computer programming, but still in use for example in linear programming \u2013 see History below). Many real-world and theoretical problems may be modeled in this general framework. Problems formulated using this technique in the fields of physics and computer vision may refer to the technique as energy minimization, speaking of the value of the function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   as representing the energy of the system being modeled.\nTypically, \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is some subset of the Euclidean space \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n}}\n  , often specified by a set of constraints, equalities or inequalities that the members of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   have to satisfy.  The domain \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   of \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is called the search space or the choice set,\nwhile the elements of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   are called candidate solutions or feasible solutions.\nThe function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is called, variously, an objective function, a loss function or cost function (minimization),  a utility function or fitness function (maximization), or, in certain fields, an energy function or energy functional. A feasible solution that minimizes (or maximizes, if that is the goal) the objective function is called an optimal solution.\nIn mathematics, conventional optimization problems are usually stated in terms of minimization. \nA local minimum \n\n  \n    \n      \n        \n          \n            x\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} ^{\\ast }}\n  \nis defined as an element for which there exists some \n  \n    \n      \n        \u03b4\n        >\n        0\n      \n    \n    {\\displaystyle \\delta >0}\n   such that \n\nfor all \n  \n    \n      \n        \n          x\n        \n        \u2208\n        A\n      \n    \n    {\\displaystyle \\mathbf {x} \\in A}\n   where \n  \n    \n      \n        \n          \u2016\n          \n            \n              x\n            \n            \u2212\n            \n              \n                x\n              \n              \n                \u2217\n              \n            \n          \n          \u2016\n        \n        \u2264\n        \u03b4\n        ,\n        \n      \n    \n    {\\displaystyle \\left\\Vert \\mathbf {x} -\\mathbf {x} ^{\\ast }\\right\\Vert \\leq \\delta ,\\,}\n   the expression \n  \n    \n      \n        f\n        \n          (\n          \n            \n              x\n            \n            \n              \u2217\n            \n          \n          )\n        \n        \u2264\n        f\n        \n          (\n          \n            x\n          \n          )\n        \n      \n    \n    {\\displaystyle f\\left(\\mathbf {x} ^{\\ast }\\right)\\leq f\\left(\\mathbf {x} \\right)}\n   holds;that is to say, on some region around \n\n  \n    \n      \n        \n          \n            x\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} ^{\\ast }}\n  \nall of the function values are greater than or equal to the value at that element.  \nLocal maxima are defined similarly.\nWhile a local minimum is at least as good as any nearby elements, a global minimum is at least as good as every feasible element. \nGenerally, unless both the objective function and the feasible region are convex in a minimization problem, there may be several local minima. \nIn a convex problem, if there is a local minimum that is interior (not on the edge of the set of feasible elements), it is also the global minimum, but a nonconvex problem may have more than one local minimum not all of which need be global minima.\nA large number of algorithms proposed for solving nonconvex problems \u2013 including the majority of commercially available solvers \u2013 are  not capable of making a distinction between locally optimal solutions and globally optimal solutions, and will treat the former as actual solutions to the original problem. Global optimization is the branch of applied mathematics and numerical analysis that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time to the actual optimal solution of a nonconvex problem.\n\n\n== NotationEdit ==\nOptimization problems are often expressed with special notation. Here are some examples:\n\n\n=== Minimum and maximum value of a functionEdit ===\nConsider the following notation:\n\n  \n    \n      \n        \n          min\n          \n            x\n            \u2208\n            \n              R\n            \n          \n        \n        \n        (\n        \n          x\n          \n            2\n          \n        \n        +\n        1\n        )\n      \n    \n    {\\displaystyle \\min _{x\\in \\mathbb {R} }\\;(x^{2}+1)}\n  This denotes the minimum value of the objective function \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n        +\n        1\n      \n    \n    {\\displaystyle x^{2}+1}\n  , when choosing x from the set of real numbers \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\mathbb {R} }\n  . The minimum value in this case is \n  \n    \n      \n        1\n      \n    \n    {\\displaystyle 1}\n  , occurring at \n  \n    \n      \n        x\n        =\n        0\n      \n    \n    {\\displaystyle x=0}\n  .\nSimilarly, the notation\n\n  \n    \n      \n        \n          max\n          \n            x\n            \u2208\n            \n              R\n            \n          \n        \n        \n        2\n        x\n      \n    \n    {\\displaystyle \\max _{x\\in \\mathbb {R} }\\;2x}\n  asks for the maximum value of the objective function 2x, where x may be any real number. In this case, there is no such maximum as the objective function is unbounded, so the answer is \"infinity\" or \"undefined\".\n\n\n=== Optimal input argumentsEdit ===\n\nConsider the following notation:\n\n  \n    \n      \n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              i\n              n\n            \n            \n              x\n              \u2208\n              (\n              \u2212\n              \u221e\n              ,\n              \u2212\n              1\n              ]\n            \n          \n        \n        \n        \n          x\n          \n            2\n          \n        \n        +\n        1\n        ,\n      \n    \n    {\\displaystyle {\\underset {x\\in (-\\infty ,-1]}{\\operatorname {arg\\,min} }}\\;x^{2}+1,}\n  or equivalently\n\n  \n    \n      \n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              i\n              n\n            \n            x\n          \n        \n        \n        \n          x\n          \n            2\n          \n        \n        +\n        1\n        ,\n        \n        \n          subject to:\n        \n        \n        x\n        \u2208\n        (\n        \u2212\n        \u221e\n        ,\n        \u2212\n        1\n        ]\n        .\n      \n    \n    {\\displaystyle {\\underset {x}{\\operatorname {arg\\,min} }}\\;x^{2}+1,\\;{\\text{subject to:}}\\;x\\in (-\\infty ,-1].}\n  This represents the value (or values) of the argument x in the interval \n  \n    \n      \n        (\n        \u2212\n        \u221e\n        ,\n        \u2212\n        1\n        ]\n      \n    \n    {\\displaystyle (-\\infty ,-1]}\n   that minimizes (or minimize) the objective function x2 + 1 (the actual minimum value of that function is not what the problem asks for). In this case, the answer is x = -1, since x = 0 is infeasible, i.e. does not belong to the feasible set.\nSimilarly, \n\n  \n    \n      \n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              a\n              x\n            \n            \n              x\n              \u2208\n              [\n              \u2212\n              5\n              ,\n              5\n              ]\n              ,\n              \n              y\n              \u2208\n              \n                R\n              \n            \n          \n        \n        \n        x\n        cos\n        \u2061\n        (\n        y\n        )\n        ,\n      \n    \n    {\\displaystyle {\\underset {x\\in [-5,5],\\;y\\in \\mathbb {R} }{\\operatorname {arg\\,max} }}\\;x\\cos(y),}\n  or equivalently\n\n  \n    \n      \n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              a\n              x\n            \n            \n              x\n              ,\n              \n              y\n            \n          \n        \n        \n        x\n        cos\n        \u2061\n        (\n        y\n        )\n        ,\n        \n        \n          subject to:\n        \n        \n        x\n        \u2208\n        [\n        \u2212\n        5\n        ,\n        5\n        ]\n        ,\n        \n        y\n        \u2208\n        \n          R\n        \n        ,\n      \n    \n    {\\displaystyle {\\underset {x,\\;y}{\\operatorname {arg\\,max} }}\\;x\\cos(y),\\;{\\text{subject to:}}\\;x\\in [-5,5],\\;y\\in \\mathbb {R} ,}\n  represents the \n  \n    \n      \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle (x,y)}\n   pair (or pairs) that maximizes (or maximize) the value of the objective function \n  \n    \n      \n        x\n        cos\n        \u2061\n        (\n        y\n        )\n      \n    \n    {\\displaystyle x\\cos(y)}\n  , with the added constraint that x lie in the interval \n  \n    \n      \n        [\n        \u2212\n        5\n        ,\n        5\n        ]\n      \n    \n    {\\displaystyle [-5,5]}\n   (again, the actual maximum value of the expression does not matter). In this case, the solutions are the pairs of the form (5, 2k\u03c0) and (\u22125,(2k+1)\u03c0), where k ranges over all integers.\narg min and arg max are sometimes also written argmin and argmax, and stand for argument of the minimum and argument of the maximum.\n\n\n== HistoryEdit ==\nFermat and Lagrange found calculus-based formulae for identifying optima, while Newton and Gauss proposed iterative methods for moving towards an optimum.\nThe term \"linear programming\" for certain optimization cases was due to George B. Dantzig, although much of the theory had been introduced by Leonid Kantorovich in 1939. (Programming in this context does not refer to computer programming, but from the use of program by the United States military to refer to proposed training and logistics schedules, which were the problems Dantzig studied at that time.) Dantzig published the Simplex algorithm in 1947, and John von Neumann developed the theory of duality in the same year.\nOther major researchers in mathematical optimization include the following:\n\n\n== Major subfieldsEdit ==\nConvex programming studies the case when the objective function is convex (minimization) or concave (maximization) and the constraint set is convex. This can be viewed as a particular case of nonlinear programming or as generalization of linear or convex quadratic programming.\nLinear programming (LP), a type of convex programming, studies the case in which the objective function f is linear and the constraints are specified using only linear equalities and inequalities. Such a constraint set is called a polyhedron or a polytope if it is bounded.\nSecond order cone programming (SOCP) is a convex program, and includes certain types of quadratic programs.\nSemidefinite programming (SDP) is a subfield of convex optimization where the underlying variables are semidefinite matrices. It is a generalization of linear and convex quadratic programming.\nConic programming is a general form of convex programming.  LP, SOCP and SDP can all be viewed as conic programs with the appropriate type of cone.\nGeometric programming is a technique whereby objective and inequality constraints expressed as posynomials and equality constraints as monomials can be transformed into a convex program.\nInteger programming studies linear programs in which some or all variables are constrained to take on integer values.  This is not convex, and in general much more difficult than regular linear programming.\nQuadratic programming allows the objective function to have quadratic terms, while the feasible set must be specified with linear equalities and inequalities.  For specific forms of the quadratic term, this is a type of convex programming.\nFractional programming studies optimization of ratios of two nonlinear functions. The special class of concave fractional programs can be transformed to a convex optimization problem.\nNonlinear programming studies the general case in which the objective function or the constraints or both contain nonlinear parts.  This may or may not be a convex program. In general, whether the program is convex affects the difficulty of solving it.\nStochastic programming studies the case in which some of the constraints or parameters depend on random variables.\nRobust programming is, like stochastic programming, an attempt to capture uncertainty in the data underlying the optimization problem. Robust optimization targets to find solutions that are valid under all possible realizations of the uncertainties.\nCombinatorial optimization is concerned with problems where the set of feasible solutions is discrete or can be reduced to a discrete one.\nStochastic optimization is used with random (noisy) function measurements or random inputs in the search process.\nInfinite-dimensional optimization studies the case when the set of feasible solutions is a subset of an infinite-dimensional space, such as a space of functions.\nHeuristics and metaheuristics make few or no assumptions about the problem being optimized. Usually, heuristics do not guarantee that any optimal solution need be found. On the other hand, heuristics are used to find approximate solutions for many complicated optimization problems.\nConstraint satisfaction studies the case in which the objective function f is constant (this is used in artificial intelligence, particularly in automated reasoning).\nConstraint programming is a programming paradigm wherein relations between variables are stated in the form of constraints.\nDisjunctive programming is used where at least one constraint must be satisfied but not all. It is of particular use in scheduling.\nSpace mapping is a concept for modeling and optimization of an engineering system to high-fidelity (fine) model accuracy exploiting a suitable physically meaningful coarse or surrogate model.In a number of subfields, the techniques are designed primarily for optimization in dynamic contexts (that is, decision making over time):\n\nCalculus of variations seeks to optimize an action integral over some space to an extremum by varying a function of the coordinates.\nOptimal control theory is a generalization of the calculus of variations which introduces control policies.\nDynamic programming studies the case in which the optimization strategy is based on splitting the problem into smaller subproblems. The equation that describes the relationship between these subproblems is called the Bellman equation.\nMathematical programming with equilibrium constraints is where the constraints include variational inequalities or  complementarities.\n\n\n=== Multi-objective optimizationEdit ===\n\nAdding more than one objective to an optimization problem adds complexity. For example, to optimize a structural design, one would desire a design that is both light and rigid. When two objectives conflict, a trade-off must be created. There may be one lightest design, one stiffest design, and an infinite number of designs that are some compromise of weight and rigidity. The set of trade-off designs that cannot be improved upon according to one criterion without hurting another criterion is known as the Pareto set. The curve created plotting weight against stiffness of the best designs is known as the Pareto frontier.\nA design is judged to be \"Pareto optimal\" (equivalently, \"Pareto efficient\" or in the Pareto set) if it is not dominated by any other design: If it is worse than another design in some respects and no better in any respect, then it is dominated and is not Pareto optimal.\nThe choice among \"Pareto optimal\" solutions to determine the \"favorite solution\" is delegated to the decision maker. In other words, defining the problem as multi-objective optimization signals that some information is missing: desirable objectives are given but combinations of them are not rated relative to each other. In some cases, the missing information can be derived by interactive sessions with the decision maker.\nMulti-objective optimization problems have been generalized further into vector optimization problems where the (partial) ordering is no longer given by the Pareto ordering.\n\n\n=== Multi-modal optimizationEdit ===\nOptimization problems are often multi-modal; that is, they possess multiple good solutions. They could all be globally good (same cost function value) or there could be a mix of globally good and locally good solutions. Obtaining all (or at least some of) the multiple solutions is the goal of a multi-modal optimizer.\nClassical optimization techniques due to their iterative approach do not perform satisfactorily when they are used to obtain multiple solutions, since it is not guaranteed that different solutions will be obtained even with different starting points in multiple runs of the algorithm. Evolutionary algorithms, however, are a very popular approach to obtain multiple solutions in a multi-modal optimization task.\n\n\n== Classification of critical points and extremaEdit ==\n\n\n=== Feasibility problemEdit ===\nThe satisfiability problem, also called the feasibility problem, is just the problem of finding any feasible solution at all without regard to objective value. This can be regarded as the special case of mathematical optimization where the objective value is the same for every solution, and thus any solution is optimal.\nMany optimization algorithms need to start from a feasible point. One way to obtain such a point is to relax the feasibility conditions using a slack variable; with enough slack, any starting point is feasible. Then, minimize that slack variable until slack is null or negative.\n\n\n=== ExistenceEdit ===\nThe extreme value theorem of Karl Weierstrass states that a continuous real-valued function on a compact set attains its maximum and minimum value. More generally, a lower semi-continuous function on a compact set attains its minimum; an upper semi-continuous function on a compact set attains its maximum.\n\n\n=== Necessary conditions for optimalityEdit ===\nOne of Fermat's theorems states that optima of unconstrained problems are found at stationary points, where the first derivative or the gradient of the objective function is zero (see first derivative test). More generally, they may be found at critical points, where the first derivative or gradient of the objective function is zero or is undefined, or on the boundary of the choice set. An equation (or set of equations) stating that the first derivative(s) equal(s) zero at an interior optimum is called a 'first-order condition' or a set of first-order conditions.\nOptima of equality-constrained problems can be found by the Lagrange multiplier method. The optima of problems with equality and/or inequality constraints can be found using the 'Karush\u2013Kuhn\u2013Tucker conditions'.\n\n\n=== Sufficient conditions for optimalityEdit ===\nWhile the first derivative test identifies points that might be extrema, this test does not distinguish a point that is a minimum from one that is a maximum or one that is neither. When the objective function is twice differentiable, these cases can be distinguished by checking the second derivative or the matrix of second derivatives (called the Hessian matrix) in unconstrained problems, or the matrix of second derivatives of the objective function and the constraints called the bordered Hessian in constrained problems. The conditions that distinguish maxima, or minima, from other stationary points are called 'second-order conditions' (see 'Second derivative test'). If a candidate solution satisfies the first-order conditions, then satisfaction of the second-order conditions as well is sufficient to establish at least local optimality.\n\n\n=== Sensitivity and continuity of optimaEdit ===\nThe envelope theorem describes how the value of an optimal solution changes when an underlying parameter changes. The process of computing this change is called comparative statics.\nThe maximum theorem of Claude Berge (1963) describes the continuity of an optimal solution as a function of underlying parameters.\n\n\n=== Calculus of optimizationEdit ===\n\nFor unconstrained problems with twice-differentiable functions, some critical points can be found by finding the points where the gradient of the objective function is zero (that is, the stationary points). More generally, a zero subgradient certifies that a local minimum has been found for minimization problems with convex functions and other locally Lipschitz functions.\nFurther, critical points can be classified  using the definiteness of the Hessian matrix: If the Hessian is positive definite at a critical point, then the point is a local minimum; if the Hessian matrix is negative definite, then the point is a local maximum; finally, if indefinite, then the point is some kind of saddle point.\nConstrained problems can often be transformed into unconstrained problems with the help of Lagrange multipliers. Lagrangian relaxation can also provide approximate solutions to difficult constrained problems.\nWhen the objective function is convex, then any local minimum will also be a global minimum. There exist efficient numerical techniques for minimizing convex functions, such as interior-point methods.\n\n\n== Computational optimization techniquesEdit ==\nTo solve problems, researchers may use algorithms that terminate in a finite number of steps, or iterative methods that converge to a solution (on some specified class of problems), or heuristics that may provide approximate solutions to some problems (although their iterates need not converge).\n\n\n=== Optimization algorithmsEdit ===\n\nSimplex algorithm of George Dantzig, designed for linear programming.\nExtensions of the simplex algorithm, designed for quadratic programming and for linear-fractional programming.\nVariants of the simplex algorithm that are especially suited for network optimization.\nCombinatorial algorithms\nQuantum optimization algorithms\n\n\n=== Iterative methodsEdit ===\n\nThe iterative methods used to solve problems of nonlinear programming differ according to whether they evaluate Hessians, gradients, or only function values. While evaluating Hessians (H) and gradients (G) improves the rate of convergence, for functions for which these quantities exist and vary sufficiently smoothly, such evaluations increase the computational complexity (or computational cost) of each iteration. In some cases, the computational complexity may be excessively high.\nOne major criterion for optimizers is just the number of required function evaluations as this often is already a large computational effort, usually much more effort than within the optimizer itself, which mainly has to operate over the N variables.\nThe derivatives provide detailed information for such optimizers, but are even harder to calculate, e.g. approximating the gradient takes at least N+1 function evaluations. For approximations of the 2nd derivatives (collected in the Hessian matrix) the number of function evaluations is in the order of N\u00b2. Newton's method requires the 2nd order derivates, so for each iteration the number of function calls is in the order of N\u00b2, but for a simpler pure gradient optimizer it is only N. However, gradient optimizers need usually more iterations than Newton's algorithm. Which one is best with respect to the number of function calls depends on the problem itself.\n\nMethods that evaluate Hessians (or approximate Hessians, using finite differences):\nNewton's method\nSequential quadratic programming: A Newton-based method for small-medium scale constrained problems. Some versions can handle large-dimensional problems.\nInterior point methods: This is a large class of methods for constrained optimization. Some interior-point  methods use only (sub)gradient information, and others of which require the evaluation of Hessians.\nMethods that evaluate gradients, or approximate gradients in some way (or even subgradients):\nCoordinate descent methods: Algorithms which update a single coordinate in each iteration\nConjugate gradient methods: Iterative methods for large problems. (In theory, these methods terminate in a finite number of steps with quadratic objective functions, but this finite termination is not observed in practice on finite\u2013precision computers.)\nGradient descent (alternatively, \"steepest descent\" or \"steepest ascent\"): A (slow) method of historical and theoretical interest, which has had renewed interest for finding approximate solutions of enormous problems.\nSubgradient methods - An iterative method for large locally Lipschitz functions using generalized gradients. Following Boris T. Polyak, subgradient\u2013projection methods are similar to conjugate\u2013gradient methods.\nBundle method of descent: An iterative method for small\u2013medium-sized problems with locally Lipschitz functions, particularly for convex minimization problems.  (Similar to conjugate gradient methods)\nEllipsoid method: An iterative method for small problems with quasiconvex objective functions and of great theoretical interest, particularly in establishing the polynomial time complexity of some combinatorial optimization problems. It has similarities with Quasi-Newton methods.\nConditional gradient method (Frank\u2013Wolfe) for approximate minimization of specially structured problems with linear constraints, especially with traffic networks. For general unconstrained problems, this method reduces to the gradient method, which is regarded as obsolete (for almost all problems).\nQuasi-Newton methods: Iterative methods for medium-large problems (e.g. N<1000).\nSimultaneous perturbation stochastic approximation (SPSA) method for stochastic optimization; uses random (efficient) gradient approximation.\nMethods that evaluate only function values: If a problem is continuously differentiable, then gradients can be approximated using finite differences, in which case a gradient-based method can be used.\nInterpolation methods\nPattern search methods, which have better convergence properties than the Nelder\u2013Mead heuristic (with simplices), which is listed below.\n\n\n=== Global convergenceEdit ===\nMore generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. The first and still popular method for ensuring convergence relies on line searches, which optimize a function along one dimension. A second and increasingly popular method for ensuring convergence uses trust regions. Both line searches and trust regions are used in modern methods of non-differentiable optimization. Usually a global optimizer is much slower than advanced local optimizers (such as BFGS), so often an efficient global optimizer can be constructed by starting the local optimizer from different starting points.\n\n\n=== HeuristicsEdit ===\n\nBesides (finitely terminating) algorithms and (convergent) iterative methods, there are heuristics. A heuristic is any algorithm which is not guaranteed (mathematically) to find the solution, but which is nevertheless useful in certain practical situations. List of some well-known heuristics:\n\n\n== ApplicationsEdit ==\n\n\n=== MechanicsEdit ===\nProblems in rigid body dynamics (in particular articulated rigid body dynamics) often require mathematical programming techniques, since you can view rigid body dynamics as attempting to solve an ordinary differential equation on a constraint manifold; the constraints are various nonlinear geometric constraints such as \"these two points must always coincide\", \"this surface must not penetrate any other\", or \"this point must always lie somewhere on this curve\". Also, the problem of computing contact forces can be done by solving a linear complementarity problem, which can also be viewed as a QP (quadratic programming) problem.\nMany design problems can also be expressed as optimization programs. This application is called design optimization. One subset is the engineering optimization, and another recent and growing subset of this field is multidisciplinary design optimization, which, while useful in many problems, has in particular been applied to aerospace engineering problems.\nThis approach may be applied in cosmology and astrophysics,.\n\n\n=== Economics and financeEdit ===\nEconomics is closely enough linked to optimization of agents that an influential definition relatedly describes economics qua science as the \"study of human behavior as a relationship between ends and scarce means\" with alternative uses.  Modern optimization theory includes traditional optimization theory but also overlaps with game theory and the study of economic equilibria. The Journal of Economic Literature codes classify mathematical programming, optimization techniques, and related topics under JEL:C61-C63.\nIn microeconomics, the utility maximization problem and its dual problem, the expenditure minimization problem, are economic optimization problems. Insofar as they behave consistently, consumers are assumed to maximize their utility, while firms are usually assumed to maximize their profit. Also, agents are often modeled as being risk-averse, thereby preferring to avoid risk. Asset prices are also modeled using optimization theory, though the underlying mathematics relies on optimizing stochastic processes rather than on static optimization. International trade theory also uses optimization to explain trade patterns between nations. The optimization of portfolios is an example of multi-objective optimization in economics.\nSince the 1970s, economists have modeled dynamic decisions over time using control theory. For example, dynamic search models are used to study labor-market behavior. A crucial distinction is between deterministic and stochastic models.  Macroeconomists build dynamic stochastic general equilibrium (DSGE) models that describe the dynamics of the whole economy as the result of the interdependent optimizing decisions of workers, consumers, investors, and governments.\n\n\n=== Electrical engineeringEdit ===\nSome common applications of optimization techniques in electrical engineering include active filter design, stray field reduction in superconducting magnetic energy storage systems, space mapping design of microwave structures, handset antennas, electromagnetics-based design. Electromagnetically validated design optimization of microwave components and antennas has made extensive use of an appropriate physics-based or empirical surrogate model and space mapping methodologies since the discovery of space mapping in 1993. \n\n\n=== Civil engineeringEdit ===\nOptimization has been widely used in civil engineering. The most common civil engineering problems that are solved by optimization are cut and fill of roads, life-cycle analysis of structures and infrastructures, resource leveling and schedule optimization.\n\n\n=== Operations researchEdit ===\nAnother field that uses optimization techniques extensively is operations research. Operations research also uses stochastic modeling and simulation to support improved decision-making. Increasingly, operations research uses stochastic programming to model dynamic decisions that adapt to events; such problems can be solved with large-scale optimization and stochastic optimization methods.\n\n\n=== Control engineeringEdit ===\nMathematical optimization is used in much modern controller design. High-level controllers such as model predictive control (MPC) or real-time optimization (RTO) employ mathematical optimization. These algorithms run online and repeatedly determine values for decision variables, such as choke openings in a process plant, by iteratively solving a mathematical optimization problem including constraints and a model of the system to be controlled.\n\n\n=== GeophysicsEdit ===\nOptimization techniques are regularly used in geophysical parameter estimation problems. Given a set of geophysical measurements, e.g. seismic recordings, it is common to solve for the physical properties and geometrical shapes of the underlying rocks and fluids.\n\n\n=== Molecular modelingEdit ===\n\nNonlinear optimization methods are widely used in conformational analysis.\n\n\n== SolversEdit ==\n\n\n== See alsoEdit ==\n\n\n== NotesEdit ==\n\n\n== Further readingEdit ==\n\n\n=== ComprehensiveEdit ===\n\n\n==== Undergraduate levelEdit ====\nBradley, S.; Hax, A.; Magnanti, T. (1977). Applied mathematical programming. Addison Wesley. \nParkinson, A.; Balling, R.; Hedengren, J. (2013). Optimization Methods for Engineering Design. Provo, UT: Brigham Young University. \nRardin, Ronald L. (1997). Optimization in operations research. Prentice Hall. p. 919. ISBN 0-02-398415-5. copyright: 1998 \nSnyman, J.A.; Wilke, D.N. (2018). Practical Mathematical Optimization - Basic Optimization Theory and Gradient-Based Algorithms. Springer Optimization and Its Applications Vol. 133 (2 ed.). Springer International Publishing. pp. xxvi+372. ISBN 978-3-319-77585-2. \nStrang, Gilbert (1986). Introduction to applied mathematics. Wellesley, MA: Wellesley-Cambridge Press (Strang's publishing company). pp. xii+758. ISBN 0-9614088-0-4. MR 0870634. \n\n\n==== Graduate levelEdit ====\nMagnanti, Thomas L. (1989). \"Twenty years of mathematical programming\".  In Cornet, Bernard; Tulkens, Henry. Contributions to Operations Research and Economics: The twentieth anniversary of CORE (Papers from the symposium held in Louvain-la-Neuve, January 1987). Cambridge, MA: MIT Press. pp. 163\u2013227. ISBN 0-262-03149-3. MR 1104662. \nMinoux, M. (1986). Mathematical programming: Theory and algorithms. Egon Balas foreword) (Translated  by Steven Vajda from the (1983 Paris: Dunod) French ed.). Chichester: A Wiley-Interscience Publication. John Wiley & Sons, Ltd. pp. xxviii+489. ISBN 0-471-90170-9. MR 2571910. (2008 Second ed., in French: Programmation math\u00e9matique: Th\u00e9orie et algorithmes. Editions Tec & Doc, Paris,  2008. xxx+711 pp. ISBN 978-2-7430-1000-3. MR868279. \nNemhauser, G. L.; Rinnooy Kan, A. H. G.; Todd, M. J., eds. (1989). Optimization. Handbooks in Operations Research and Management Science. 1. Amsterdam: North-Holland Publishing Co. pp. xiv+709. ISBN 0-444-87284-1. MR 1105099. \nJ. E. Dennis, Jr. and Robert B. Schnabel, A view of unconstrained optimization (pp. 1\u201372);\nDonald Goldfarb and Michael J. Todd, Linear programming (pp. 73\u2013170);\nPhilip E. Gill, Walter Murray, Michael A. Saunders, and Margaret H. Wright, Constrained nonlinear programming (pp. 171\u2013210);\nRavindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin, Network flows (pp. 211\u2013369);\nW. R. Pulleyblank, Polyhedral combinatorics (pp. 371\u2013446);\nGeorge L. Nemhauser and Laurence A. Wolsey, Integer programming (pp. 447\u2013527);\nClaude Lemar\u00e9chal, Nondifferentiable optimization (pp. 529\u2013572);\nRoger J-B Wets, Stochastic programming (pp. 573\u2013629);\nA. H. G. Rinnooy Kan and G. T. Timmer, Global optimization (pp. 631\u2013662);\nP. L. Yu, Multiple criteria decision making: five basic concepts (pp. 663\u2013699).\nShapiro, Jeremy F. (1979). Mathematical programming: Structures and algorithms. New York: Wiley-Interscience [John Wiley & Sons]. pp. xvi+388. ISBN 0-471-77886-9. MR 0544669. \nSpall, J. C. (2003), Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control, Wiley, Hoboken, NJ.\nUniversity, Edwin K. P. Chong, Colorado State University, Stanislaw H. \u017bak, Purdue (2013). An introduction to optimization (Fourth edition. ed.). Hoboken, New Jersey: John Wiley & Sons. ISBN 9781118279014. \n\n\n=== Continuous optimizationEdit ===\nRoger Fletcher (2000). Practical methods of optimization. Wiley. ISBN 978-0-471-49463-8. \nMordecai Avriel (2003). Nonlinear Programming: Analysis and Methods. Dover Publishing. ISBN 0-486-43227-0. \nP. E. Gill, W. Murray and M. H. Wright (1982). Practical Optimization. Emerald Publishing. ISBN 978-0-12-283952-8. \nXin-She Yang (2010). Engineering Optimization: An Introduction with Metaheuristic Applications. Wiley. ISBN 978-0-470-58246-6. \nBonnans, J. Fr\u00e9d\u00e9ric; Gilbert, J. Charles; Lemar\u00e9chal, Claude; Sagastiz\u00e1bal, Claudia A. (2006). Numerical optimization: Theoretical and practical aspects. Universitext (Second revised ed. of  translation of 1997  French ed.). Berlin: Springer-Verlag. pp. xiv+490. doi:10.1007/978-3-540-35447-5. ISBN 3-540-35445-X. MR 2265882. \nBonnans, J. Fr\u00e9d\u00e9ric; Shapiro, Alexander (2000). Perturbation analysis of optimization problems. Springer Series in Operations Research. New York: Springer-Verlag. pp. xviii+601. ISBN 0-387-98705-3. MR 1756264. \nBoyd, Stephen P.; Vandenberghe, Lieven (2004). Convex Optimization (pdf). Cambridge University Press. ISBN 978-0-521-83378-3. Retrieved October 15, 2011. \nJorge Nocedal and Stephen J. Wright (2006). Numerical Optimization. Springer. ISBN 0-387-30303-0. \nRuszczy\u0144ski, Andrzej (2006). Nonlinear Optimization. Princeton, NJ: Princeton University Press. pp. xii+454. ISBN 978-0-691-11915-1. MR 2199043. \nRobert J. Vanderbei (2013). Linear Programming: Foundations and Extensions, 4th Edition. Springer. ISBN 978-1-4614-7629-0. \n\n\n=== Combinatorial optimizationEdit ===\nR. K. Ahuja, Thomas L. Magnanti, and James B. Orlin (1993). Network Flows: Theory, Algorithms, and Applications. Prentice-Hall, Inc. ISBN 0-13-617549-X.\nWilliam J. Cook, William H. Cunningham, William R. Pulleyblank, Alexander Schrijver; Combinatorial Optimization; John Wiley & Sons; 1 edition (November 12, 1997); ISBN 0-471-55894-X.\nGondran, Michel; Minoux, Michel (1984). Graphs and algorithms. Wiley-Interscience Series in Discrete Mathematics (Translated by Steven Vajda from the second (Collection de la Direction des \u00c9tudes et Recherches d'\u00c9lectricit\u00e9 de France [Collection of the Department of Studies and Research of \u00c9lectricit\u00e9 de France], v. 37. Paris: \u00c9ditions Eyrolles 1985. xxviii+545 pp. MR868083) French ed.). Chichester: John Wiley & Sons, Ltd. pp. xix+650. ISBN 978-2-7430-1035-5. MR 2552933. (Fourth ed. Collection EDF R&D. Paris: Editions Tec & Doc 2009. xxxii+784 pp. MR745802. \nEugene Lawler (2001). Combinatorial Optimization: Networks and Matroids. Dover. ISBN 0-486-41453-1. \nLawler, E. L.; Lenstra, J. K.; Rinnooy Kan, A. H. G.; Shmoys, D. B. (1985), The traveling salesman problem: A guided tour of combinatorial optimization, John Wiley & Sons, ISBN 0-471-90413-9 .\nJon Lee; A First Course in Combinatorial Optimization; Cambridge University Press; 2004; ISBN 0-521-01012-8.\nChristos H. Papadimitriou and Kenneth Steiglitz Combinatorial Optimization : Algorithms and Complexity; Dover Pubns; (paperback, Unabridged edition, July 1998) ISBN 0-486-40258-4.\n\n\n=== Relaxation (extension method)Edit ===\nMethods to obtain suitable (in some sense) natural extensions of optimization problems that otherwise lack of existence or stability of solutions to obtain problems with guaranteed existence of solutions and their stability in some sense (typically under various perturbation of data) are in general called relaxation. Solutions of such extended (=relaxed) problems in some sense characterizes (at least certain features) of the original problems, e.g. as far as their optimizing sequences concerns. Relaxed problems may also possesses their own natural linear structure that may yield specific optimality conditions different from optimality conditions for the original problems.  \n\nH. O. Fattorini: Infinite Dimensional Optimization and Control Theory. Cambridge Univ. Press, 1999.\nP. Pedregal: Parametrized Measures and Variational Principles. Birkh\u00e4user, Basel, 1997\nT. Roubicek: \"Relaxation in Optimization Theory and Variational Calculus\". W. de Gruyter, Berlin, 1997. ISBN 3-11-014542-1.\nJ. Warga: Optimal control of differential and functional equations. Academic Press, 1972.\n\n\n== JournalsEdit ==\nComputational Optimization and Applications\nJournal of Computational Optimization in Economics and Finance\nJournal of Economic Dynamics and Control\nSIAM Journal on Optimization (SIOPT) and  Editorial Policy\nSIAM Journal on Control and Optimization (SICON) and Editorial Policy\n\n\n== External linksEdit ==\nCOIN-OR\u2014Computational Infrastructure for Operations Research\nDecision Tree for Optimization Software Links to optimization source codes\nGlobal optimization\nMathematical Programming Glossary\nMathematical Programming Society\nNEOS Guide currently being replaced by the NEOS Wiki\nOptimization Online A repository for optimization e-prints\nOptimization Related Links\nConvex Optimization I EE364a: Course from Stanford University\nConvex Optimization \u2013 Boyd and Vandenberghe Book on Convex Optimization\nBook and Course on Optimization Methods for Engineering Design\nMathematical Optimization in Operations Research from the Institute for Operations Research and the Management Sciences (INFORMS)", "electromagnetism": "Electromagnetism is a branch of physics involving the study of the electromagnetic force, a type of physical interaction that occurs between electrically charged particles. The electromagnetic force usually exhibits electromagnetic fields such as electric fields, magnetic fields and light, and is one of the four fundamental interactions (commonly called forces) in nature. The other three fundamental interactions are the strong interaction, the weak interaction and gravitation.\nThe word electromagnetism is a compound form of two Greek terms, \u1f24\u03bb\u03b5\u03ba\u03c4\u03c1\u03bf\u03bd \u0113lektron, \"amber\", and \u03bc\u03b1\u03b3\u03bd\u1fc6\u03c4\u03b9\u03c2 \u03bb\u03af\u03b8\u03bf\u03c2 magn\u0113tis lithos, which means \"\u039cagnesian stone\", a type of iron ore.  Electromagnetic phenomena are defined in terms of the electromagnetic force, sometimes called the Lorentz force, which includes both electricity and magnetism as different manifestations of the same phenomenon.\nThe electromagnetic force plays a major role in determining the internal properties of most objects encountered in daily life.  Ordinary matter takes its form as a result of intermolecular forces between individual atoms and molecules in matter, and is a manifestation of the electromagnetic force. Electrons are bound by the electromagnetic force to atomic nuclei, and their orbital shapes and their influence on nearby atoms with their electrons is described by quantum mechanics. The electromagnetic force governs all chemical processes, which arise from interactions between the electrons of neighboring atoms.\nThere are numerous mathematical descriptions of the electromagnetic field.  In classical electrodynamics, electric fields are described as electric potential and electric current. In Faraday's law, magnetic fields are associated with electromagnetic induction and magnetism, and Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents.\nThe theoretical implications of electromagnetism, particularly the establishment of the speed of light based on properties of the \"medium\" of propagation (permeability and permittivity), led to the development of special relativity by Albert Einstein in 1905.\nAlthough electromagnetism is considered one of the four fundamental forces, at high energy the weak force and electromagnetic force are unified as a single electroweak force. In the history of the universe, during the quark epoch the unified force broke into the two separate forces as the universe cooled.\n\n\n== History of the theory ==\n\nOriginally, electricity and magnetism were considered to be two separate forces. This view changed, however, with the publication of James Clerk Maxwell's 1873 A Treatise on Electricity and Magnetism in which the interactions of positive and negative charges were shown to be mediated by one force. There are four main effects resulting from these interactions, all of which have been clearly demonstrated by experiments:\n\nElectric charges attract or repel one another with a force inversely proportional to the square of the distance between them: unlike charges attract, like ones repel.\nMagnetic poles (or states of polarization at individual points) attract or repel one another in a manner similar to positive and negative charges and always exist as pairs: every north pole is yoked to a south pole.\nAn electric current inside a wire creates a corresponding circumferential magnetic field outside the wire.  Its direction (clockwise or counter-clockwise) depends on the direction of the current in the wire.\nA current is induced in a loop of wire when it is moved toward or away from a magnetic field, or a magnet is moved towards or away from it; the direction of current depends on that of the movement.\nWhile preparing for an evening lecture on 21 April 1820, Hans Christian \u00d8rsted made a surprising observation. As he was setting up his materials, he noticed a compass needle deflected away from magnetic north when the electric current from the battery he was using was switched on and off. This deflection convinced him that magnetic fields radiate from all sides of a wire carrying an electric current, just as light and heat do, and that it confirmed a direct relationship between electricity and magnetism.\n\nAt the time of discovery, \u00d8rsted did not suggest any satisfactory explanation of the phenomenon, nor did he try to represent the phenomenon in a mathematical framework. However, three months later he began more intensive investigations. Soon thereafter he published his findings, proving that an electric current produces a magnetic field as it flows through a wire. The CGS unit of magnetic induction (oersted) is named in honor of his contributions to the field of electromagnetism.\n\nHis findings resulted in intensive research throughout the scientific community in electrodynamics. They influenced French physicist Andr\u00e9-Marie Amp\u00e8re's developments of a single mathematical form to represent the magnetic forces between current-carrying conductors. \u00d8rsted's discovery also represented a major step toward a unified concept of energy.\nThis unification, which was observed by Michael Faraday, extended by James Clerk Maxwell, and partially reformulated by Oliver Heaviside and Heinrich Hertz, is one of the key accomplishments of 19th century mathematical physics. It has had far-reaching consequences, one of which was the understanding of the nature of light. Unlike what was proposed by the electromagnetic theory of that time, light and other electromagnetic waves are at present seen as taking the form of quantized, self-propagating oscillatory electromagnetic field disturbances called photons. Different frequencies of oscillation give rise to the different forms of electromagnetic radiation, from radio waves at the lowest frequencies, to visible light at intermediate frequencies, to gamma rays at the highest frequencies.\n\u00d8rsted was not the only person to examine the relationship between electricity and magnetism. In 1802, Gian Domenico Romagnosi, an Italian legal scholar, deflected a magnetic needle using a Voltaic pile. The factual setup of the experiment is not completely clear, so if current flew across the needle or not. An account of the discovery was published in 1802 in an Italian newspaper, but it was largely overlooked by the contemporary scientific community, because Romagnosi seemingly did not belong to this community.\n\nAn earlier (1735), and often neglected, connection between electricity and magnetism was reported by a Dr. Cookson. The account stated, \"A tradesman at Wakefield in Yorkshire, having put up a great number of knives and forks in a large box ...  and having placed the box in the corner of a large room, there happened a sudden storm of thunder, lightning, &c.  ... The owner emptying the box on a counter where some nails lay, the persons who took up the knives, that lay on the nails, observed that the knives took up the nails. On this the whole number was tried, and found\nto do the same, and that, to such a degree as to take up large nails, packing\nneedles, and other iron things of considerable weight ...\"   E. T. Whittaker suggested in 1910 that \nthis particular event was responsible for lightning to be \"credited with the power of magnetizing steel; and it was doubtless this which led Franklin in 1751 to attempt to magnetize a sewing-needle by means of the discharge of Leyden jars.\" \n\n\n== Fundamental forces ==\n\nThe electromagnetic force is one of the four known fundamental forces. The other fundamental forces are:\n\nthe weak nuclear force, which binds to all known particles in the Standard Model, and causes certain forms of radioactive decay. (In particle physics though, the electroweak interaction is the unified description of two of the four known fundamental interactions of nature: electromagnetism and the weak interaction);\nthe strong nuclear force, which binds quarks to form nucleons, and binds nucleons to form nuclei\nthe gravitational force.All other forces (e.g., friction, contact forces) are derived from these four fundamental forces (including momentum which is carried by the movement of particles).The electromagnetic force is responsible for practically all phenomena one encounters in daily life above the nuclear scale, with the exception of gravity. Roughly speaking, all the forces involved in interactions between atoms can be explained by the electromagnetic force acting between the electrically charged atomic nuclei and electrons of the atoms. Electromagnetic forces also explain how these particles carry momentum by their movement. This includes the forces we experience in \"pushing\" or \"pulling\" ordinary material objects, which result from the intermolecular forces that act between the individual molecules in our bodies and those in the objects. The electromagnetic force is also involved in all forms of chemical phenomena.\nA necessary part of understanding the intra-atomic and intermolecular forces is the effective force generated by the momentum of the electrons' movement, such that as electrons move between interacting atoms they carry momentum with them. As a collection of electrons becomes more confined, their minimum momentum necessarily increases due to the Pauli exclusion principle. The behaviour of matter at the molecular scale including its density is determined by the balance between the electromagnetic force and the force generated by the exchange of momentum carried by the electrons themselves.\n\n\n== Classical electrodynamics ==\n\nIn 1600, William Gilbert proposed, in his De Magnete, that electricity and magnetism, while both capable of causing attraction and repulsion of objects, were distinct effects. Mariners had noticed that lightning strikes had the ability to disturb a compass needle. The link between lightning and electricity was not confirmed until Benjamin Franklin's proposed experiments in 1752.  One of the first to discover and publish a link between man-made electric current and magnetism was Romagnosi, who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle. However, the effect did not become widely known until 1820, when \u00d8rsted performed a similar experiment. \u00d8rsted's work influenced Amp\u00e8re to produce a theory of electromagnetism that set the subject on a mathematical foundation.\nA theory of electromagnetism, known as classical electromagnetism, was developed by various physicists during the period between 1820 and 1873 when it culminated in the publication of a treatise by James Clerk Maxwell, which unified the preceding developments into a single theory and discovered the electromagnetic nature of light. In classical electromagnetism, the behavior of the electromagnetic field is described by a set of equations known as Maxwell's equations, and the electromagnetic force is given by the Lorentz force law.One of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics, but it is compatible with special relativity. According to Maxwell's equations, the speed of light in a vacuum is a universal constant that is dependent only on the electrical permittivity and magnetic permeability of free space. This violates Galilean invariance, a long-standing cornerstone of classical mechanics. One way to reconcile the two theories (electromagnetism and classical mechanics) is to assume the existence of a luminiferous aether through which the light propagates. However, subsequent experimental efforts failed to detect the presence of the aether. After important contributions of Hendrik Lorentz and Henri Poincar\u00e9, in 1905, Albert Einstein solved the problem with the introduction of special relativity, which replaced classical kinematics with a new theory of kinematics compatible with classical electromagnetism. (For more information, see History of special relativity.)\nIn addition, relativity theory implies that in moving frames of reference, a magnetic field transforms to a field with a nonzero electric component and conversely, a moving electric field transforms to a nonzero magnetic component, thus firmly showing that the phenomena are two sides of the same coin. Hence the term \"electromagnetism\". (For more information, see Classical electromagnetism and special relativity and Covariant formulation of classical electromagnetism.)\n\n\n== Chaotic and emergent phenomena ==\n\nThe mathematical models used in classical electromagnetism, quantum electrodynamics (QED) and the standard model all view the electromagnetic force as a linear set of equations. In these theories electromagnetism is a U(1) gauge theory, whose topological properties do not allow any complex nonlinear interaction of a field with and on itself. For example, in the QED vacuum the field fluctuates randomly as a consequence of the uncertainty principle but these fluctuations cancel each other out with no overall observable effect. However, there are many observed nonlinear physical electromagnetic phenomena such as Aharonov\u2013Bohm (AB) and Altshuler\u2013Aronov\u2013 Spivak (AAS) effects, Berry, Aharonov\u2013 Anandan, Pancharatnam and Chiao\u2013Wu phase rotation effects, Josephson effect, Quantum Hall effect, the de Haas\u2013van Alphen effect, the Sagnac effect and many other physically observable phenomena which would indicate that the electromagnetic potential field has real physical meaning rather than being a mathematical artifact and therefore an all encompassing theory would not confine electromagnetism as a local force as is currently done, but as a SU(2) gauge theory or higher geometry. Higher symmetries allow for nonlinear, aperiodic behaviour which manifest as a variety of complex non-equilibrium phenomena that do not arise in the linearised U(1) theory, such as multiple stable states, symmetry breaking, chaos and emergence.  In higher symmetry groups, the electromagnetic field is not a calm, randomly fluctuating, passive substance, but can at times be viewed as a turbulent virtual plasma that can have complex vortices, entangled states and a rich nonlinear structure.\nWhat are called Maxwell's equation's today are in fact a simplified version of the original equations reformulated by Heaviside, FitzGerald, Lodge and Hertz. The original equations used Hamilton's more expressive quaternion notation, a kind of Clifford algebra, which fully subsumes the standard Maxwell vectorial equations largely used today. In the late 1880s there was a debate over the relative merits of vector analysis and quaternions. According to Heaviside the electromagnetic potential field was purely metaphysical, an arbitrary mathematical fiction, that needed to be \"murdered\".  It was concluded that there was no need for the greater physical insights provided by the quaternions if the theory was purely local in nature. Local vector analysis has become the dominant way of using Maxwell's equations ever since. However, this strictly vectorial approach has led to a restrictive topological understanding in some areas of electromagnetism, for example, a full understanding of the energy transfer dynamics in Tesla's oscillator-shuttle-circuit can be achieved only in quaternionic algebra or higher SU(2) symmetries.   It has often been argued that quaternions are not compatible with special relativity, but multiple papers have shown ways of incorporating relativity\n\nA good example of nonlinear electromagnetics is in high energy dense plasmas, where vortical phenomena occur which seemingly violate the second law of thermodynamics by increasing the energy gradient within the electromagnetic field and violate Maxwell's laws by creating ion currents which capture and concentrate their own and surrounding magnetic fields. In particular Lorentz force law, which elaborates Maxwell's equations is violated by these force free vortices. These apparent violations are due to the fact that the traditional conservation laws in classical and quantum electrodynamics (QED) only display linear U(1) symmetry (in particular, by the extended Noether theorem, conservation laws such as the laws of thermodynamics need not always apply to dissipative systems, which are expressed in gauges of higher symmetry). The second law of thermodynamics states that in a closed linear system entropy flow can only be positive (or exactly zero at the end of a cycle). However, negative entropy (i.e. increased order, structure or self-organisation) can spontaneously appear in an open nonlinear thermodynamic system that is far from equilibrium, so long as this emergent order accelerates the overall flow of entropy in the total system.\n\nGiven the complex and adaptive behaviour that arises from nonlinear systems considerable attention in recent years has gone into studying a new class of phase transitions which occur at absolute zero temperature. These are quantum phase transitions which are driven by electromagnetic field fluctuations as a consequence of zero-point energy A good example of a spontaneous phase transition that are attributed to zero-point fluctuations can be found in superconductors. Superconductivity is one of the best known empirically quantified macroscopic electromagnetic phenomena whose basis is recognised to be quantum mechanical in origin. The behaviour of the electric and magnetic fields under superconductivity is governed by the London equations. However, it has been questioned in a series of journal articles whether the quantum mechanically canonised London equations can be given a purely classical derivation. Bostick for instance, has claimed to show that the London equations do indeed have a classical origin that applies to superconductors and to some collisionless plasmas as well. In particular it has been asserted that the Beltrami vortices in the plasma focus display the same paired flux-tube morphology as Type II superconductors. Others have also pointed out this connection, Fr\u00f6hlich has shown that the hydrodynamic equations of compressible fluids, together with the London equations, lead to a macroscopic parameter (\n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   = electric charge density / mass density), without involving either quantum phase factors or Planck's constant. In essence, it has been asserted that Beltrami plasma vortex structures are able to at least simulate the morphology of Type I and Type II superconductors. This occurs because the \"organised\" dissipative energy of the vortex configuration comprising the ions and electrons far exceeds the \"disorganised\" dissipative random thermal energy. The transition from disorganised fluctuations to organised helical structures is a phase transition involving a change in the condensate's energy  (i.e. the ground state or zero-point energy) but without any associated rise in temperature. This is an example of zero-point energy having multiple stable states (see Quantum phase transition, Quantum critical point, Topological degeneracy, Topological order) and where the overall system structure is independent of a reductionist or deterministic view, that \"classical\" macroscopic order can also causally affect quantum phenomena.\n\n\n== Quantities and units ==\n\nElectromagnetic units are part of a system of electrical units based primarily upon the magnetic properties of electric currents, the fundamental SI unit being the ampere. The units are:\n\nIn the electromagnetic cgs system, electric current is a fundamental quantity defined via Amp\u00e8re's law and takes the permeability as a dimensionless quantity (relative permeability) whose value in a vacuum is unity. As a consequence, the square of the speed of light appears explicitly in some of the equations interrelating quantities in this system.\n\nFormulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on what system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit \"sub-systems\", including Gaussian, \"ESU\", \"EMU\", and Heaviside\u2013Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase \"CGS units\" is often used to refer specifically to CGS-Gaussian units.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n=== Web sources ===\n\n\n=== Textbooks ===\n\n\n=== General references ===\n\n\n== External links ==\n\"magnetic field strength converter\". Retrieved 2007-06-04. \nElectromagnetic Force - from Eric Weisstein's World of Physics\nThe Deflection of a Magnetic Compass Needle by a Current in a Wire (video) on YouTube", "astronomy": "Astronomy (from Greek: \u1f00\u03c3\u03c4\u03c1\u03bf\u03bd\u03bf\u03bc\u03af\u03b1) is a natural science that studies celestial objects and phenomena. It applies mathematics, physics, and chemistry, in an effort to explain the origin of those objects and phenomena and their evolution. Objects of interest include planets, moons, stars, galaxies, and comets; the phenomena include supernova explosions, gamma ray bursts, and cosmic microwave background radiation. More generally, all phenomena that originate outside Earth's atmosphere are within the purview of astronomy. A related but distinct subject, physical cosmology, is concerned with the study of the Universe as a whole.Astronomy is one of the oldest of the natural sciences. The early civilizations in recorded history, such as the Babylonians, Greeks, Indians, Egyptians, Nubians, Iranians, Chinese, Maya, and many ancient indigenous peoples of the Americas performed methodical observations of the night sky. Historically, astronomy has included disciplines as diverse as astrometry, celestial navigation, observational astronomy, and the making of calendars, but professional astronomy is now often considered to be synonymous with astrophysics.Professional astronomy is split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects, which is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. The two fields complement each other, with theoretical astronomy seeking to explain observational results and observations being used to confirm theoretical results.\nAstronomy is one of the few sciences in which amateurs still play an active role, especially in the discovery and observation of transient events. Amateur astronomers have made and contributed to many important astronomical discoveries, such as finding new comets.\n\n\n== Etymology ==\n\nAstronomy (from the Greek \u1f00\u03c3\u03c4\u03c1\u03bf\u03bd\u03bf\u03bc\u03af\u03b1 from \u1f04\u03c3\u03c4\u03c1\u03bf\u03bd astron, \"star\" and -\u03bd\u03bf\u03bc\u03af\u03b1 -nomia from \u03bd\u03cc\u03bc\u03bf\u03c2 nomos, \"law\" or \"culture\") means \"law of the stars\" (or \"culture of the stars\" depending on the translation). Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. Although the two fields share a common origin, they are now entirely distinct.\n\n\n=== Use of terms \"astronomy\" and \"astrophysics\" ===\nGenerally, either the term \"astronomy\" or \"astrophysics\" may be used to refer to this subject. Based on strict dictionary definitions, \"astronomy\" refers to \"the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties\" and \"astrophysics\" refers to the branch of astronomy dealing with \"the behavior, physical properties, and dynamic processes of celestial objects and phenomena.\" In some cases, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, \"astronomy\" may be used to describe the qualitative study of the subject, whereas \"astrophysics\" is used to describe the physics-oriented version of the subject. However, since most modern astronomical research deals with subjects related to physics, modern astronomy could actually be called astrophysics. Few fields, such as astrometry, are purely astronomy rather than also astrophysics. Various departments in which scientists carry out research on this subject may use \"astronomy\" and \"astrophysics,\" partly depending on whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. Some titles of the leading scientific journals in this field include The Astronomical Journal, The Astrophysical Journal, and Astronomy and Astrophysics.\n\n\n== History ==\n\n\n=== Ancient times ===\nIn early times, astronomy only comprised the observation and predictions of the motions of objects visible to the naked eye. In some locations, early cultures assembled massive artifacts that possibly had some astronomical purpose. In addition to their ceremonial uses, these observatories could be employed to determine the seasons, an important factor in knowing when to plant crops, as well as in understanding the length of the year.Before tools such as the telescope were invented, early study of the stars was conducted using the naked eye. As civilizations developed, most notably in Mesopotamia, Greece, Persia, India, China, Egypt, and Central America, astronomical observatories were assembled, and ideas on the nature of the Universe began to be explored. Most of early astronomy actually consisted of mapping the positions of the stars and planets, a science now referred to as astrometry. From these observations, early ideas about the motions of the planets were formed, and the nature of the Sun, Moon and the Earth in the Universe were explored philosophically. The Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. This is known as the geocentric model of the Universe, or the Ptolemaic system, named after Ptolemy.A particularly important early development was the beginning of mathematical and scientific astronomy, which began among the Babylonians, who laid the foundations for the later astronomical traditions that developed in many other civilizations. The Babylonians discovered that lunar eclipses recurred in a repeating cycle known as a saros.\n\nFollowing the Babylonians, significant advances in astronomy were made in ancient Greece and the Hellenistic world. Greek astronomy is characterized from the start by seeking a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and he proposed a heliocentric model of the solar system. In the 2nd century BC, Hipparchus discovered precession, calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. Hipparchus also created a comprehensive catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150\u201380 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe.\n\n\n=== Middle Ages ===\nDuring the Middle Ages, astronomy was mostly stagnant in medieval Europe, at least until the 13th century. However, astronomy flourished in the Islamic world and other parts of the world. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local Group,  was described by the Persian astronomer Azophi in his Book of Fixed Stars. The SN 1006 supernova, the brightest apparent magnitude stellar event in recorded history, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and the Chinese astronomers in 1006. Some of the prominent Islamic (mostly Persian and Arab) astronomers who made significant contributions to the science include Al-Battani, Thebit, Azophi, Albumasar, Biruni, Arzachel, Al-Birjandi, and the astronomers of the Maragheh and Samarkand observatories. Astronomers during that time introduced many Arabic names now used for individual stars. It is also believed that the ruins at Great Zimbabwe and Timbuktu may have housed an astronomical observatory. Europeans had previously believed that there had been no astronomical observation in pre-colonial Middle Ages sub-Saharan Africa but modern discoveries show otherwise.The Roman Catholic Church gave more financial and social support to the study of astronomy for over six centuries, from the recovery of ancient learning during the late Middle Ages into the Enlightenment, than any other, and, probably, all other, institutions. Among the Church's motives was finding the date for Easter.\n\n\n=== Scientific revolution ===\n\nDuring the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. His work was defended, expanded upon, and corrected by Galileo Galilei and Johannes Kepler. Galileo used telescopes to enhance his observations.Kepler was the first to devise a system that described correctly the details of the motion of the planets with the Sun at the center. However, Kepler did not succeed in formulating a theory behind the laws he wrote down. It was left to Newton's invention of celestial dynamics and his law of gravitation to finally explain the motions of the planets. Newton also developed the reflecting telescope.The English astronomer John Flamsteed catalogued over 3000 stars. Further discoveries paralleled the improvements in the size and quality of the telescope. More extensive star catalogues were produced by Lacaille. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found. The distance to a star was announced in 1838 when the parallax of 61 Cygni was measured by Friedrich Bessel.During the 18\u201319th centuries, the study of the three-body problem by Euler, Clairaut, and D'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Lagrange and Laplace, allowing the masses of the planets and moons to be estimated from their perturbations.Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and photography. Fraunhofer discovered about 600 bands in the spectrum of the Sun in 1814\u201315, which, in 1859, Kirchhoff ascribed to the presence of different elements. Stars were proven to be similar to the Earth's own Sun, but with a wide range of temperatures, masses, and sizes.The existence of the Earth's galaxy, the Milky Way, as a separate group of stars, was only proved in the 20th century, along with the existence of \"external\" galaxies. The observed recession of those galaxies led to the discovery of the expansion of the Universe. Theoretical astronomy led to speculations on the existence of objects such as black holes and neutron stars, which have been used to explain such observed phenomena as quasars, pulsars, blazars, and radio galaxies. Physical cosmology made huge advances during the 20th century, with the model of the Big Bang, which is heavily supported by evidence provided by cosmic microwave background radiation, Hubble's law, and the cosmological abundances of elements. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere. In February 2016, it was revealed that the LIGO project had detected evidence of gravitational waves in the previous September.\n\n\n== Observational astronomy ==\n\nOur main source of information about celestial bodies and other objects is visible light more generally electromagnetic radiation. Observational astronomy may be divided according to the observed region of the electromagnetic spectrum. Some parts of the spectrum can be observed from the Earth's surface, while other parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below.\n\n\n=== Radio astronomy ===\n\nRadio astronomy uses radiation outside the visible range with wavelengths greater than approximately one millimeter. Radio astronomy is different from most other forms of observational astronomy in that the observed radio waves can be treated as waves rather than as discrete photons. Hence, it is relatively easier to measure both the amplitude and phase of radio waves, whereas this is not as easily done at shorter wavelengths.Although some radio waves are emitted directly by astronomical objects, a product of thermal emission, most of the radio emission that is observed is the result of synchrotron radiation, which is produced when electrons orbit magnetic fields. Additionally, a number of spectral lines produced by interstellar gas, notably the hydrogen spectral line at 21 cm, are observable at radio wavelengths.A wide variety of objects are observable at radio wavelengths, including supernovae, interstellar gas, pulsars, and active galactic nuclei.\n\n\n=== Infrared astronomy ===\n\nInfrared astronomy is founded on the detection and analysis of infrared radiation, wavelengths longer than red light and outside the range of our vision. The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. The longer wavelengths of infrared can penetrate clouds of dust that block visible light, allowing the observation of young stars embedded in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous Galactic protostars and their host star clusters.\nWith the exception of infrared wavelengths close to visible light, such radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places on Earth or in space. Some molecules radiate strongly in the infrared. This allows the study of the chemistry of space; more specifically it can detect water in comets.\n\n\n=== Optical astronomy ===\n\nHistorically, optical astronomy, also called visible light astronomy, is the oldest form of astronomy. Images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 4000 \u00c5 to 7000 \u00c5 (400 nm to 700 nm), that same equipment can be used to observe some near-ultraviolet and near-infrared radiation.\n\n\n=== Ultraviolet astronomy ===\n\nUltraviolet astronomy employs ultraviolet wavelengths between approximately 100 and 3200 \u00c5 (10 to 320 nm). Light at those wavelengths are absorbed by the Earth's atmosphere, requiring observations at these wavelengths to be performed from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue stars (OB stars) that are very bright in this wave band. This includes the blue stars in other galaxies, which have been the targets of several ultraviolet surveys. Other objects commonly observed in ultraviolet light include planetary nebulae, supernova remnants, and active galactic nuclei. However, as ultraviolet light is easily absorbed by interstellar dust, an adjustment of ultraviolet measurements is necessary.\n\n\n=== X-ray astronomy ===\n\nX-ray astronomy uses X-ray wavelengths. Typically, X-ray radiation is produced by synchrotron emission (the result of electrons orbiting magnetic field lines), thermal emission from thin gases above 107 (10 million) kelvins, and thermal emission from thick gases above 107 Kelvin. Since X-rays are absorbed by the Earth's atmosphere, all X-ray observations must be performed from high-altitude balloons, rockets, or X-ray astronomy satellites. Notable X-ray sources include X-ray binaries, pulsars, supernova remnants, elliptical galaxies, clusters of galaxies, and active galactic nuclei.\n\n\n=== Gamma-ray astronomy ===\n\nGamma ray astronomy observes astronomical objects at the shortest wavelengths of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory or by specialized telescopes called atmospheric Cherenkov telescopes. The Cherenkov telescopes do not detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere.Most gamma-ray emitting sources are actually gamma-ray bursts, objects which only produce gamma radiation for a few milliseconds to thousands of seconds before fading away. Only 10% of gamma-ray sources are non-transient sources. These steady gamma-ray emitters include pulsars, neutron stars, and black hole candidates such as active galactic nuclei.\n\n\n=== Fields not based on the electromagnetic spectrum ===\nIn addition to electromagnetic radiation, a few other events originating from great distances may be observed from the Earth.\nIn neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles (atomic nuclei) that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of secondary particles which can be detected by current observatories. Some future neutrino detectors may also be sensitive to the particles produced when cosmic rays hit the Earth's atmosphere.Gravitational-wave astronomy is an emerging field of astronomy that employs gravitational-wave detectors to collect observational data about distant massive objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO. LIGO made its first detection on 14 September 2015, observing gravitational waves from a binary black hole. A second gravitational wave was detected on 26 December 2015 and additional observations should continue but gravitational waves require extremely sensitive instruments.The combination of observations made using electromagnetic radiation, neutrinos or gravitational waves and other complementary information, is known as multi-messenger astronomy.\n\n\n=== Astrometry and celestial mechanics ===\n\nOne of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars.\nCareful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. More recently the tracking of near-Earth objects will allow for predictions of close encounters or potential collisions of the Earth with those objects.The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the Universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of the radial velocity and proper motion of stars allows astronomers to plot the movement of these systems through the Milky Way galaxy. Astrometric results are the basis used to calculate the distribution of speculated dark matter in the galaxy.During the 1990s, the measurement of the stellar wobble of nearby stars was used to detect large extrasolar planets orbiting those stars.\n\n\n== Theoretical astronomy ==\n\nTheoretical astronomers use several tools including analytical models and computational numerical simulations; each has its particular advantages. Analytical models of a process are generally better for giving broader insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved.Theorists in astronomy endeavor to create theoretical models and from the results predict observational consequences of those models. The observation of a phenomenon predicted by a model allows astronomers to select between several alternate or conflicting models as the one best able to describe the phenomena.\nTheorists also try to generate or modify models to take into account new data. In the case of an inconsistency between the data and model's results, the general tendency is to try to make minimal modifications to the model so that it produces results that fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.\nPhenomena modeled by theoretical astronomers include: stellar dynamics and evolution; galaxy formation; large-scale distribution of matter in the Universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Astrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (astro)physics and the study of gravitational waves.\nSome widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, Cosmic inflation, dark matter, and fundamental theories of physics.\nA few examples of this process:\n\nDark matter and dark energy are the current leading topics in astronomy, as their discovery and controversy originated during the study of the galaxies.\n\n\n== Specific subfields ==\n\n\n=== Solar astronomy ===\n\nAt a distance of about eight light-minutes, the most frequently studied star is the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. The Sun is not considered a variable star, but it does undergo periodic changes in activity known as the sunspot cycle. This is an 11-year oscillation in sunspot number. Sunspots are regions of lower-than- average temperatures that are associated with intense magnetic activity.The Sun has steadily increased in luminosity by 40% since it first became a main-sequence star. The Sun has also undergone periodic changes in luminosity that can have a significant impact on the Earth. The Maunder minimum, for example, is believed to have caused the Little Ice Age phenomenon during the Middle Ages.The visible outer surface of the Sun is called the photosphere. Above this layer is a thin region known as the chromosphere. This is surrounded by a transition region of rapidly increasing temperatures, and finally by the super-heated corona.\nAt the center of the Sun is the core region, a volume of sufficient temperature and pressure for nuclear fusion to occur. Above the core is the radiation zone, where the plasma conveys the energy flux by means of radiation. Above that is the convection zone where the gas material transports energy primarily through physical displacement of the gas known as convection. It is believed that the movement of mass within the convection zone creates the magnetic activity that generates sunspots.A solar wind of plasma particles constantly streams outward from the Sun until, at the outermost limit of the Solar System, it reaches the heliopause. As the solar wind passes the Earth, it interacts with the Earth's magnetic field (magnetosphere) and deflects the solar wind, but traps some creating the Van Allen radiation belts that envelop the Earth. The aurora are created when solar wind particles are guided by the magnetic flux lines into the Earth's polar regions where the lines the descend into the atmosphere.\n\n\n=== Planetary science ===\n\nPlanetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of this planetary system, although many new discoveries are still being made.\n\nThe Solar System is subdivided into the inner planets, the asteroid belt, and the outer planets. The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer gas giant planets are Jupiter, Saturn, Uranus, and Neptune. Beyond Neptune lies the Kuiper Belt, and finally the Oort Cloud, which may extend as far as a light-year.\nThe planets were formed 4.6 billion years ago in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided and one such collision may have formed the Moon.Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer crust. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping.A planet or moon's interior heat is produced from the collisions that created the body, by the decay of radioactive materials (e.g. uranium, thorium, and 26Al), or tidal heating caused by interactions with other bodies. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.\n\n\n=== Stellar astronomy ===\n\nThe study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Star formation occurs in dense regions of dust and gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star.Almost all elements heavier than hydrogen and helium were created inside the cores of stars.The characteristics of the resulting star depend primarily upon its starting mass. The more massive the star, the greater its luminosity, and the more rapidly it fuses its hydrogen fuel into helium in its core. Over time, this hydrogen fuel is completely converted into helium, and the star begins to evolve. The fusion of helium requires a higher core temperature. A star with a high enough core temperature will push its outer layers outward while increasing its core density. The resulting red giant formed by the expanding outer layers enjoys a brief life span, before the helium fuel in the core is in turn consumed. Very massive stars can also undergo a series of evolutionary phases, as they fuse increasingly heavier elements.The final fate of the star depends on its mass, with stars of mass greater than about eight times the Sun becoming core collapse supernovae; while smaller stars blow off their outer layers and leave behind the inert core in the form of a white dwarf. The ejection of the outer layers forms a planetary nebula. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole. Closely orbiting binary stars can follow more complex evolutionary paths, such as mass transfer onto a white dwarf companion that can potentially cause a supernova. Planetary nebulae and supernovae distribute the  \"metals\" produced in the star by fusion to the interstellar medium; without them, all new stars (and their planetary systems) would be formed from hydrogen and helium alone.\n\n\n=== Galactic astronomy ===\n\nOur solar system orbits within the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is located within the dusty outer arms, there are large portions of the Milky Way that are obscured from view.\nIn the center of the Milky Way is the core, a bar-shaped bulge with what is believed to be a supermassive black hole at its center. This is surrounded by four primary arms that spiral from the core. This is a region of active star formation that contains many younger, population I stars. The disk is surrounded by a spheroid halo of older, population II stars, as well as relatively dense concentrations of stars known as globular clusters.Between the stars lies the interstellar medium, a region of sparse matter. In the densest regions, molecular clouds of molecular hydrogen and other elements create star-forming regions. These begin as a compact pre-stellar core or dark nebulae, which concentrate and collapse (in volumes determined by the Jeans length) to form compact protostars.As the more massive stars appear, they transform the cloud into an H II region (ionized atomic hydrogen) of glowing gas and plasma. The stellar wind and supernova explosions from these stars eventually cause the cloud to disperse, often leaving behind one or more young open clusters of stars. These clusters gradually disperse, and the stars join the population of the Milky Way.Kinematic studies of matter in the Milky Way and other galaxies have demonstrated that there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined.\n\n\n=== Extragalactic astronomy ===\n\nThe study of objects outside our galaxy is a branch of astronomy concerned with the formation and evolution of Galaxies, their morphology (description) and classification, the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. Finally, the latter is important for the understanding of the large-scale structure of the cosmos.\nMost galaxies are organized into distinct shapes that allow for classification schemes. They are commonly divided into spiral, elliptical and Irregular galaxies.As the name suggests, an elliptical galaxy has the cross-sectional shape of an ellipse. The stars move along random orbits with no preferred direction. These galaxies contain little or no interstellar dust, few star-forming regions, and generally older stars. Elliptical galaxies are more commonly found at the core of galactic clusters, and may have been formed through mergers of large galaxies.\nA spiral galaxy is organized into a flat, rotating disk, usually with a prominent bulge or bar at the center, and trailing bright arms that spiral outward. The arms are dusty regions of star formation within which massive young stars produce a blue tint. Spiral galaxies are typically surrounded by a halo of older stars. Both the Milky Way and one of our nearest galaxy neighbors, the Andromeda Galaxy, are spiral galaxies.\nIrregular galaxies are chaotic in appearance, and are neither spiral nor elliptical. About a quarter of all galaxies are irregular, and the peculiar shapes of such galaxies may be the result of gravitational interaction.\nAn active galaxy is a formation that emits a significant amount of its energy from a source other than its stars, dust and gas. It is powered by a compact region at the core, thought to be a super-massive black hole that is emitting radiation from in-falling material.\nA radio galaxy is an active galaxy that is very luminous in the radio portion of the spectrum, and is emitting immense plumes or lobes of gas. Active galaxies that emit shorter frequency, high-energy radiation include Seyfert galaxies, Quasars, and Blazars. Quasars are believed to be the most consistently luminous objects in the known universe.The large-scale structure of the cosmos is represented by groups and clusters of galaxies. This structure is organized into a hierarchy of groupings, with the largest being the superclusters. The collective matter is formed into filaments and walls, leaving large voids between.\n\n\n=== Physical cosmology ===\n\nCosmology (from the Greek \u03ba\u03cc\u03c3\u03bc\u03bf\u03c2 (kosmos) \"world, universe\" and \u03bb\u03cc\u03b3\u03bf\u03c2 (logos) \"word, study\" or literally \"logic\") could be considered the study of the Universe as a whole.\n\nObservations of the large-scale structure of the Universe, a branch known as physical cosmology, have provided a deep understanding of the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the big bang, wherein our Universe began at a single point in time, and thereafter expanded over the course of 13.8 billion years to its present condition. The concept of the big bang can be traced back to the discovery of the microwave background radiation in 1965.In the course of this expansion, the Universe underwent several evolutionary stages. In the very early moments, it is theorized that the Universe experienced a very rapid cosmic inflation, which homogenized the starting conditions. Thereafter, nucleosynthesis produced the elemental abundance of the early Universe. (See also nucleocosmochronology.)\nWhen the first neutral atoms formed from a sea of primordial ions, space became transparent to radiation, releasing the energy viewed today as the microwave background radiation. The expanding Universe then underwent a Dark Age due to the lack of stellar energy sources.A hierarchical structure of matter began to form from minute variations in the mass density of space. Matter accumulated in the densest regions, forming clouds of gas and the earliest stars, the Population III stars. These massive stars triggered the reionization process and are believed to have created many of the heavy elements in the early Universe, which, through nuclear decay, create lighter elements, allowing the cycle of nucleosynthesis to continue longer.Gravitational aggregations clustered into filaments, leaving voids in the gaps. Gradually, organizations of gas and dust merged to form the first primitive galaxies. Over time, these pulled in more matter, and were often organized into groups and clusters of galaxies, then into larger-scale superclusters.Fundamental to the structure of the Universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the Universe. For this reason, much effort is expended in trying to understand the physics of these components.\n\n\n== Interdisciplinary studies ==\nAstronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the Universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of vast amount of observational astrophysical data.\nThe study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent an overlap of the disciplines of astronomy and chemistry. As \"forensic astronomy\", finally, methods from astronomy have been used to solve problems of law and history.\n\n\n== Amateur astronomy ==\n\nAstronomy is one of the sciences to which amateurs can contribute the most.Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. Common targets of amateur astronomers include the Sun, the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events which interest them.Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky, who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs (e.g. the One-Mile Telescope).Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography.\n\n\n== Unsolved problems in astronomy ==\n\nAlthough the scientific discipline of astronomy has made tremendous strides in understanding the nature of the Universe and its contents, there remain some important unanswered questions. Answers to these may require the construction of new ground- and space-based instruments, and possibly new developments in theoretical and experimental physics.\n\nWhat is the origin of the stellar mass spectrum? That is, why do astronomers observe the same distribution of stellar masses \u2013 the initial mass function \u2013 apparently regardless of the initial conditions? A deeper understanding of the formation of stars and planets is needed.\nIs there other life in the Universe? Especially, is there other intelligent life? If so, what is the explanation for the Fermi paradox? The existence of life elsewhere has important scientific and philosophical implications. Is the Solar System normal or atypical?\nWhat is the nature of dark matter and dark energy? These dominate the evolution and fate of the cosmos, yet their true nature remains unknown. What will be the ultimate fate of the universe?\nHow did the first galaxies form? How did supermassive black holes form?\nWhat is creating the ultra-high-energy cosmic rays?\nWhy is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model?\nWhat really happens beyond the event horizon?\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nForbes, George (1909). History of Astronomy. London: Plain Label Books. ISBN 978-1-60303-159-2.  Available at Project Gutenberg, Google books\nHarpaz, Amos (1994). Stellar Evolution. A K Peters, Ltd. ISBN 978-1-56881-012-6. \nUns\u00f6ld, A.; Baschek, B. (2001). The New Cosmos: An Introduction to Astronomy and Astrophysics. Springer. ISBN 978-3-540-67877-9. \n\n\n== External links ==\n\nNASA/IPAC Extragalactic Database (NED) (NED-Distances)\nInternational Year of Astronomy 2009 IYA2009 Main website\nCosmic Journey: A History of Scientific Cosmology from the American Institute of Physics\nSouthern Hemisphere Astronomy\nCelestia Motherlode Educational site for Astronomical journeys through space\nKroto, Harry, Astrophysical Chemistry Lecture Series.\nCore books and Core journals in Astronomy, from the Smithsonian/NASA Astrophysics Data System\nA Journey with Fred Hoyle by Wickramasinghe, Chandra.\nAstronomy books from the History of Science Collection at Linda Hall Library", "combinatorics": "Combinatorics is an area of mathematics primarily concerned with counting, both as a means and an end in obtaining results, and certain properties of finite structures.  It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics, from evolutionary biology to computer science, etc.  \nTo fully understand the scope of combinatorics requires a great deal of further amplification, the details of which are not universally agreed upon. According to H. J. Ryser, a definition of the subject is difficult because it crosses so many mathematical subdivisions. In so far as an area can be described by the types of problems it addresses, combinatorics is involved with\n\nthe enumeration (counting) of specified structures, sometimes referred to as arrangements or configurations in a very general sense, associated with finite systems,\nthe existence of such structures that satisfy certain given criteria,\nthe construction of these structures, perhaps in many ways, and\noptimization, finding the \"best\" structure or solution among several possibilities, be it the \"largest\",  \"smallest\" or satisfying some other optimality criterion.Leon Mirsky has said: \"combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained.\" One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella. Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable) but discrete setting. \nCombinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics, notably in algebra, probability theory, topology, and geometry, as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an ad hoc solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right. One of the oldest and most accessible parts of combinatorics is graph theory, which by itself has numerous natural connections to other areas.  Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms.\nA mathematician who studies combinatorics is called a combinatorialist.\n\n\n== History ==\n\nBasic combinatorial concepts and enumerative results appeared throughout the ancient world. In the 6th century BCE, ancient Indian physician Sushruta asserts in Sushruta Samhita that 63 combinations can be made out of 6 different tastes, taken one at a time, two at a time, etc., thus computing all 26 \u2212 1 possibilities.  Greek historian Plutarch discusses an argument between Chrysippus (3rd century BCE) and Hipparchus (2nd century BCE) of a rather delicate enumerative problem, which was later shown to be related to Schr\u00f6der\u2013Hipparchus numbers. In the Ostomachion, Archimedes (3rd century BCE) considers a tiling puzzle.\nIn the Middle Ages, combinatorics continued to be studied, largely outside of the European civilization.  The Indian mathematician Mah\u0101v\u012bra (c. 850) provided formulae for the number of permutations and combinations, and these formulas may have been familiar to Indian mathematicians as early as the 6th century CE.  The philosopher and astronomer Rabbi Abraham ibn Ezra (c. 1140) established the symmetry of binomial coefficients, while a closed formula was obtained later by the talmudist and mathematician Levi ben Gerson (better known as Gersonides), in 1321.\nThe arithmetical triangle\u2014 a graphical diagram showing relationships among the binomial coefficients\u2014 was presented by mathematicians in treatises dating as far back as the 10th century, and would eventually become known as Pascal's triangle. Later, in Medieval England, campanology provided examples of what is now known as Hamiltonian cycles in certain Cayley graphs on permutations.During the Renaissance, together with the rest of mathematics and the sciences, combinatorics enjoyed a rebirth. Works of Pascal, Newton, Jacob Bernoulli and Euler became foundational in the emerging field. In modern times, the works of J. J. Sylvester (late 19th century) and Percy MacMahon (early 20th century) helped lay the foundation for enumerative and algebraic combinatorics. Graph theory also enjoyed an explosion of interest at the same time, especially in connection with the four color problem.\nIn the second half of the 20th century, combinatorics enjoyed a rapid growth, which led to establishment of dozens of new journals and conferences in the subject. In part, the growth was spurred by new connections and applications to other fields, ranging from algebra to probability, from functional analysis to number theory, etc.  These connections shed the boundaries between combinatorics and parts of mathematics and theoretical computer science, but at the same time led to a partial fragmentation of the field.\n\n\n== Approaches and subfields of combinatorics ==\n\n\n=== Enumerative combinatorics ===\n\nEnumerative combinatorics is the most classical area of combinatorics and concentrates on counting the number of certain combinatorial objects.  Although counting the number of elements in a set is a rather broad mathematical problem, many of the problems that arise in applications have a relatively simple combinatorial description. Fibonacci numbers is the basic example of a problem in enumerative combinatorics.  The twelvefold way provides a unified framework for counting permutations, combinations and partitions.\n\n\n=== Analytic combinatorics ===\n\nAnalytic combinatorics concerns the enumeration of combinatorial structures using tools from complex analysis and probability theory.  In contrast with enumerative combinatorics, which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae.\n\n\n=== Partition theory ===\n\nPartition theory studies various enumeration and asymptotic problems related to integer partitions, and is closely related to q-series, special functions and orthogonal polynomials.  Originally a part of number theory and analysis, it is now considered a part of combinatorics or an independent field.  It incorporates the bijective approach and various tools in analysis and analytic number theory, and has connections with statistical mechanics.\n\n\n=== Graph theory ===\n\nGraphs are basic objects in combinatorics.  The questions range from counting (e.g., the number of graphs on n vertices with k edges) to structural (e.g., which graphs contain Hamiltonian cycles) to algebraic questions (e.g., given a graph G and two numbers x and y, does the Tutte polynomial TG(x,y) have a combinatorial interpretation?).  Although there are very strong connections between graph theory and combinatorics, these two are sometimes thought of as separate subjects.  This is due to the fact that while combinatorial methods apply to many graph theory problems, the two are generally used to seek solutions to different problems.\n\n\n=== Design theory ===\n\nDesign theory is a study of combinatorial designs, which are collections of subsets with certain intersection properties. Block designs are combinatorial designs of a special type. This area is one of the oldest parts of combinatorics, such as in Kirkman's schoolgirl problem proposed in 1850. The solution of the problem is a special case of a Steiner system, which systems play an important role in the classification of finite simple groups. The area has further connections to coding theory and geometric combinatorics.\n\n\n=== Finite geometry ===\n\nFinite geometry is the study of geometric systems having only a finite number of points. Structures analogous to those found in continuous geometries (Euclidean plane, real projective space, etc.) but defined combinatorially are the main items studied. This area provides a rich source of examples for design theory. It should not be confused with discrete geometry (combinatorial geometry).\n\n\n=== Order theory ===\n\nOrder theory is the study of partially ordered sets, both finite and infinite.  Various examples of partial orders appear in algebra, geometry, number theory and throughout combinatorics and graph theory.  Notable classes and examples of partial orders include lattices and Boolean algebras.\n\n\n=== Matroid theory ===\n\nMatroid theory abstracts part of geometry.   It studies the properties of sets (usually, finite sets) of vectors in a vector space that do not depend on the particular coefficients in a linear dependence relation.  Not only the structure but also enumerative properties belong to matroid theory.  Matroid theory was introduced by Hassler Whitney and studied as a part of order theory.  It is now an independent field of study with a number of connections with other parts of combinatorics.\n\n\n=== Extremal combinatorics ===\n\nExtremal combinatorics studies extremal questions on set systems. The types of questions addressed in this case are about the largest possible graph which satisfies certain properties.  For example, the largest triangle-free graph on 2n vertices is a complete bipartite graph Kn,n. Often it is too hard even to find the extremal answer f(n) exactly and one can only give an asymptotic estimate.\nRamsey theory is another part of extremal combinatorics.  It states that any sufficiently large configuration will contain some sort of order. It is an advanced generalization of the pigeonhole principle.\n\n\n=== Probabilistic combinatorics ===\n\nIn probabilistic combinatorics, the questions are of the following type: what is the probability of a certain property for a random discrete object, such as a random graph? For instance, what is the average number of triangles in a random graph? Probabilistic methods are also used to determine the existence of combinatorial objects with certain prescribed properties (for which explicit examples might be difficult to find), simply by observing that the probability of randomly selecting an object with those properties is greater than 0. This approach (often referred to as the probabilistic method) proved highly effective in applications to extremal combinatorics and graph theory. A closely related area is the study of finite Markov chains, especially on combinatorial objects. Here again probabilistic tools are used to estimate the mixing time.\nOften associated with Paul Erd\u0151s, who did the pioneering work on the subject, probabilistic combinatorics was traditionally viewed as a set of tools to study problems in other parts of combinatorics. However, with the growth of applications to analysis of algorithms in computer science, as well as classical probability, additive and probabilistic number theory, the area recently grew to become an independent field of combinatorics.\n\n\n=== Algebraic combinatorics ===\n\nAlgebraic combinatorics is an area of mathematics that employs methods of abstract algebra, notably group theory and representation theory, in various combinatorial contexts and, conversely, applies combinatorial techniques to problems in algebra. Algebraic combinatorics is continuously expanding its scope, in both topics and techniques, and can be seen as the area of mathematics where the interaction of combinatorial and algebraic methods is particularly strong and significant.\n\n\n=== Combinatorics on words ===\n\nCombinatorics on words deals with formal languages.  It  arose independently within several branches of mathematics, including number theory, group theory and probability. It has applications to enumerative combinatorics, fractal analysis, theoretical computer science, automata theory and linguistics.  While many applications are new, the classical Chomsky\u2013Sch\u00fctzenberger hierarchy of classes of formal grammars is perhaps the best-known result in the field.\n\n\n=== Geometric combinatorics ===\n\nGeometric combinatorics is related to convex and discrete geometry, in particular polyhedral combinatorics.  It asks, for example, how many faces of each dimension a convex polytope can have.  Metric properties of polytopes play an important role as well, e.g. the   Cauchy theorem on the rigidity of convex polytopes.  Special polytopes are also considered, such as permutohedra, associahedra and Birkhoff polytopes.  We should note that combinatorial geometry is an old fashioned name for discrete geometry.\n\n\n=== Topological combinatorics ===\n\nCombinatorial analogs of concepts and methods in topology are used to study graph coloring, fair division, partitions, partially ordered sets, decision trees, necklace problems and discrete Morse theory.  It should not be confused with combinatorial topology which is an older name for algebraic topology.\n\n\n=== Arithmetic combinatorics ===\n\nArithmetic combinatorics arose out of the interplay between number theory, combinatorics, ergodic theory and harmonic analysis. It is about combinatorial estimates associated with arithmetic operations (addition, subtraction, multiplication, and division). Additive number theory (sometimes also called additive combinatorics) refers to the special case when only the operations of addition and subtraction are involved.  One important technique in arithmetic combinatorics is the ergodic theory of dynamical systems.\n\n\n=== Infinitary combinatorics ===\n\nInfinitary combinatorics, or combinatorial set theory, is an extension of ideas in combinatorics to infinite sets.  It is a part of set theory, an area of mathematical logic, but uses tools and ideas from both set theory and extremal combinatorics.\nGian-Carlo Rota used the name continuous combinatorics to describe geometric probability, since there are many analogies between counting and measure.\n\n\n== Related fields ==\n\n\n=== Combinatorial optimization ===\nCombinatorial optimization is the study of optimization on discrete and combinatorial objects. It started as a part of combinatorics and graph theory, but is now viewed as a branch of applied mathematics and computer science, related to operations research, algorithm theory and computational complexity theory.\n\n\n=== Coding theory ===\nCoding theory started as a part of design theory with early combinatorial constructions of error-correcting codes. The main idea of the subject is to design efficient and reliable methods of data transmission. It is now a large field of study, part of information theory.\n\n\n=== Discrete and computational geometry ===\nDiscrete geometry (also called combinatorial geometry) also began as a part of combinatorics, with early results on convex polytopes and kissing numbers.  With the emergence of applications of discrete geometry to computational geometry, these two fields partially merged and became a separate field of study.  There remain many connections with geometric and topological combinatorics, which themselves can be viewed as outgrowths of the early discrete geometry.\n\n\n=== Combinatorics and dynamical systems ===\nCombinatorial aspects of dynamical systems is another emerging field.  Here dynamical systems can be defined on combinatorial objects.  See for example \ngraph dynamical system.\n\n\n=== Combinatorics and physics ===\nThere are increasing interactions between combinatorics and physics, particularly statistical physics. Examples include an exact solution of the Ising model, and a connection between the Potts model on one hand, and the chromatic and Tutte polynomials on the other hand.\n\n\n== See also ==\n\nCombinatorial biology\nCombinatorial chemistry\nCombinatorial data analysis\nCombinatorial game theory\nCombinatorial group theory\nList of combinatorics topics\nPhylogenetics\n\n\n== Notes ==\n\n\n== References ==\nBj\u00f6rner, Anders; and Stanley, Richard P.; (2010); A Combinatorial Miscellany\nB\u00f3na, Mikl\u00f3s; (2011); A Walk Through Combinatorics (3rd Edition). ISBN 978-981-4335-23-2, ISBN 978-981-4460-00-2(pbk)\nGraham, Ronald L.; Groetschel, Martin; and Lov\u00e1sz, L\u00e1szl\u00f3; eds. (1996); Handbook of Combinatorics, Volumes 1 and 2.  Amsterdam, NL, and Cambridge, MA: Elsevier (North-Holland) and MIT Press. ISBN 0-262-07169-X\nLindner, Charles C.; and Rodger, Christopher A.; eds. (1997); Design Theory, CRC-Press; 1st. edition (October 31, 1997). ISBN 0-8493-3986-3.\nRiordan, John (2002) [1958], An Introduction to Combinatorial Analysis, Dover, ISBN 978-0-486-42536-8 \nRyser, Herbert John (1963), Combinatorial Mathematics, The Carus Mathematical Monographs(#14), The Mathematical Association of America \nStanley, Richard P. (1997, 1999); Enumerative Combinatorics, Volumes 1 and 2, Cambridge University Press. ISBN 0-521-55309-1, ISBN 0-521-56069-1\nvan Lint, Jacobus H.; and Wilson, Richard M.; (2001); A Course in Combinatorics, 2nd Edition, Cambridge University Press. ISBN 0-521-80340-3\n\n\n== External links ==\n\nHazewinkel, Michiel, ed. (2001) [1994], \"Combinatorial analysis\", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4 \nCombinatorial Analysis \u2013 an article in Encyclop\u00e6dia Britannica Eleventh Edition\nCombinatorics, a MathWorld article with many references.\nCombinatorics, from a MathPages.com portal.\nThe Hyperbook of Combinatorics, a collection of math articles links.\nThe Two Cultures of Mathematics by W. T. Gowers, article on problem solving vs theory building.\n\"Glossary of Terms in Combinatorics\"\nList of Combinatorics Software and Databases"}